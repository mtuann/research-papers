title,venue,year,author,comment,url,code
Backdoor-based Explainable AI Benchmark for High Fidelity Evaluation of Attribution Methods,arXiv,2405,"Peiyu Yang, Naveed Akhtar, Jiantong Jiang, Ajmal Mian",,https://arxiv.org/abs/2405.02344,
BadFusion: 2D-Oriented Backdoor Attacks against 3D Object Detection,arXiv,2405,"Saket S. Chaturvedi, Lan Zhang, Wenbin Zhang, Pan He, Xiaoyong Yuan",Accepted at IJCAI 2024 Conference,https://arxiv.org/abs/2405.03884,
DarkFed: A Data-Free Backdoor Attack in Federated Learning,arXiv,2405,"Minghui Li, Wei Wan, Yuxuan Ning, Shengshan Hu, Lulu Xue, Leo Yu Zhang, Yichen Wang",This paper has been accepted by IJCAI 2024,https://arxiv.org/abs/2405.03299,
Explanation as a Watermark: Towards Harmless and Multi-bit Model Ownership Verification via Watermarking Feature Attribution,arXiv,2405,"Shuo Shao, Yiming Li, Hongwei Yao, Yiling He, Zhan Qin, Kui Ren",,https://arxiv.org/abs/2405.04825,
Lazy Layers to Make Fine-Tuned Diffusion Models More Traceable,arXiv,2405,"Haozhe Liu, Wentian Zhang, Bing Li, Bernard Ghanem, Jürgen Schmidhuber",,https://arxiv.org/abs/2405.00466,
Poisoning-based Backdoor Attacks for Arbitrary Target Label with Positive Triggers,arXiv,2405,"Binxiao Huang, Jason Chun Lok, Chang Liu, Ngai Wong",,https://arxiv.org/abs/2405.05573,
Towards Robust Physical-world Backdoor Attacks on Lane Detection,arXiv,2405,"Xinwei Zhang, Aishan Liu, Tianyuan Zhang, Siyuan Liang, Xianglong Liu",,https://arxiv.org/abs/2405.05553,
Unlearning Backdoor Attacks through Gradient-Based Model Pruning,arXiv,2405,"Kealan Dunnett, Reza Arablouei, Dimity Miller, Volkan Dedeoglu, Raja Jurdak",,https://arxiv.org/abs/2405.03918,
Watermarking Neuromorphic Brains: Intellectual Property Protection in Spiking Neural Networks,arXiv,2405,"Hamed Poursiami, Ihsen Alouani, Maryam Parsa","7 pages, 7 figures",https://arxiv.org/abs/2405.04049,
A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks,arXiv,2404,Orson Mengara,"Accept by ""IEEE Access"" let's take a look at our global approach to the DNN(s) model(s) deployment chain in production: Danger NLP-Speech (Trigger universal approach)",https://arxiv.org/abs/2404.00076,
A Clean-graph Backdoor Attack against Graph Convolutional Networks with Poisoned Label Only,arXiv,2404,"Jiazhu Dai, Haoyu Sun",,https://arxiv.org/abs/2404.12704,
Assessing Cybersecurity Vulnerabilities in Code Large Language Models,arXiv,2404,"Md Imran Hossen, Jianyi Zhang, Yinzhi Cao, Xiali Hei",,https://arxiv.org/abs/2404.18567,
Backdoor Attack on Multilingual Machine Translation,arXiv,2404,"Jun Wang, Qiongkai Xu, Xuanli He, Benjamin I. P. Rubinstein, Trevor Cohn",NAACL main long paper,https://arxiv.org/abs/2404.02393,
Backdoor Attacks and Defenses on Semantic-Symbol Reconstruction in Semantic Communications,arXiv,2404,"Yuan Zhou, Rose Qingyang Hu, Yi Qian",This paper has been accepted by IEEE ICC 2024,https://arxiv.org/abs/2404.13279,
Backdoor Contrastive Learning via Bi-level Trigger Optimization,arXiv,2404,"Weiyu Sun, Xinyu Zhang, Hao Lu, Yingcong Chen, Ting Wang, Jinghui Chen, Lu Lin",Accepted by ICLR 2024,https://arxiv.org/abs/2404.07863,https://github.com/SWY666/SSL-backdoor-BLTO.
Beyond Traditional Threats: A Persistent Backdoor Attack on Federated Learning,arXiv,2404,"Tao Liu, Yuhang Zhang, Zhu Feng, Zhiqin Yang, Chen Xu, Dapeng Man, Wu Yang",,https://arxiv.org/abs/2404.17617,https://github.com/PhD-TaoLiu/FCBA.
Causal Deconfounding via Confounder Disentanglement for Dual-Target Cross-Domain Recommendation,arXiv,2404,"Jiajie Zhu, Yan Wang, Feng Zhu, Zhu Sun",,https://arxiv.org/abs/2404.11180,
CloudFort: Enhancing Robustness of 3D Point Cloud Classification Against Backdoor Attacks via Spatial Partitioning and Ensemble Prediction,arXiv,2404,"Wenhao Lan, Yijun Yang, Haihua Shen, Shan Li",,https://arxiv.org/abs/2404.14042,
Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs,arXiv,2404,"Javier Rando, Francesco Croce, Kryštof Mitka, Stepan Shabalin, Maksym Andriushchenko, Nicolas Flammarion, Florian Tramèr",Competition Report,https://arxiv.org/abs/2404.14461,
Concept Arithmetics for Circumventing Concept Inhibition in Diffusion Models,arXiv,2404,"Vitali Petsiuk, Kate Saenko",,https://arxiv.org/abs/2404.13706,
Decomposing and Editing Predictions by Modeling Model Computation,arXiv,2404,"Harshay Shah, Andrew Ilyas, Aleksander Madry",,https://arxiv.org/abs/2404.11534,https://github.com/MadryLab/modelcomponents
Detector Collapse: Backdooring Object Detection to Catastrophic Overload or Blindness,arXiv,2404,"Hangtao Zhang, Shengshan Hu, Yichen Wang, Leo Yu Zhang, Ziqi Zhou, Xianlong Wang, Yanjun Zhang, Chao Chen",Accepted by IJCAI-24,https://arxiv.org/abs/2404.11357,
Dual Model Replacement:invisible Multi-target Backdoor Attack based on Federal Learning,arXiv,2404,"Rong Wang, Guichen Zhou, Mingjun Gao, Yunpeng Xiao",,https://arxiv.org/abs/2404.13946,
Exploring Backdoor Vulnerabilities of Chat Models,arXiv,2404,"Yunzhuo Hao, Wenkai Yang, Yankai Lin",Code and data are available at https://github.com/hychaochao/Chat-Models-Backdoor-Attacking,https://arxiv.org/abs/2404.02406,https://github.com/hychaochao/Chat-Models-Backdoor-Attacking
Fragile Model Watermark for integrity protection: leveraging boundary volatility and sensitive sample-pairing,arXiv,2404,"ZhenZhe Gao, Zhenjun Tang, Zhaoxia Yin, Baoyuan Wu, Yue Lu",The article has been accepted by IEEE International Conference on Multimedia and Expo 2024,https://arxiv.org/abs/2404.07572,
How to Craft Backdoors with Unlabeled Data Alone?,arXiv,2404,"Yifei Wang, Wenhan Ma, Stefanie Jegelka, Yisen Wang",Accepted at ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models (DPFM),https://arxiv.org/abs/2404.06694,https://github.com/PKU-ML/nlb.
LSP Framework: A Compensatory Model for Defeating Trigger Reverse Engineering via Label Smoothing Poisoning,arXiv,2404,"Beichen Li, Yuanfang Guo, Heqi Peng, Yangxi Li, Yunhong Wang",,https://arxiv.org/abs/2404.12852,
Let's Focus: Focused Backdoor Attack against Federated Transfer Learning,arXiv,2404,"Marco Arazzi, Stefanos Koffas, Antonino Nocera, Stjepan Picek",,https://arxiv.org/abs/2404.19420,
On the critical path to implant backdoors and the effectiveness of potential mitigation techniques: Early learnings from XZ,arXiv,2404,"Mario Lins, René Mayrhofer, Michael Roland, Daniel Hofer, Martin Schwaighofer",,https://arxiv.org/abs/2404.08987,
Physical Backdoor Attack can Jeopardize Driving with Vision-Large-Language Models,arXiv,2404,"Zhenyang Ni, Rui Ye, Yuxi Wei, Zhen Xiang, Yanfeng Wang, Siheng Chen",,https://arxiv.org/abs/2404.12916,
Physical Backdoor: Towards Temperature-based Backdoor Attacks in the Physical World,arXiv,2404,"Wen Yin, Jian Lou, Pan Zhou, Yulai Xie, Dan Feng, Yuhua Sun, Tailai Zhang, Lichao Sun","To appear in CVPR 2024.11pages, 8 figures and 4 tables",https://arxiv.org/abs/2404.19417,
Poisoning Decentralized Collaborative Recommender System and Its Countermeasures,arXiv,2404,"Ruiqi Zheng, Liang Qu, Tong Chen, Kai Zheng, Yuhui Shi, Hongzhi Yin",,https://arxiv.org/abs/2404.01177,
Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models,arXiv,2404,"Yuxin Wen, Leo Marchyok, Sanghyun Hong, Jonas Geiping, Tom Goldstein, Nicholas Carlini",,https://arxiv.org/abs/2404.01231,
Privacy Backdoors: Stealing Data with Corrupted Pretrained Models,arXiv,2404,"Shanglun Feng, Florian Tramèr",Code at https://github.com/ShanglunFengatETHZ/PrivacyBackdoor,https://arxiv.org/abs/2404.00473,https://github.com/ShanglunFengatETHZ/PrivacyBackdoor
Severity Controlled Text-to-Image Generative Model Bias Manipulation,arXiv,2404,"Jordan Vice, Naveed Akhtar, Richard Hartley, Ajmal Mian","This research was supported by National Intelligence and Security Discovery Research Grants (project# NS220100007), funded by the Department of Defence Australia",https://arxiv.org/abs/2404.02530,
Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning,arXiv,2404,"Xiaopeng Xie, Ming Yan, Xiwen Zhou, Chenlong Zhao, Suli Wang, Yong Zhang, Joey Tianyi Zhou","10 pages, 6 figures, conference",https://arxiv.org/abs/2404.00461,
SpamDam: Towards Privacy-Preserving and Adversary-Resistant SMS Spam Detection,arXiv,2404,"Yekai Li, Rufan Zhang, Wenxin Rong, Xianghang Mi",,https://arxiv.org/abs/2404.09481,
The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data,arXiv,2404,"Zixuan Zhu, Rui Wang, Cong Zou, Lihua Jing","13 pages, 6 figures, published to ICCV",https://arxiv.org/abs/2404.11265,https://github.com/Zixuan-Zhu/VaB.
Towards Robust Trajectory Representations: Isolating Environmental Confounders with Causal Learning,arXiv,2404,"Kang Luo, Yuanshao Zhu, Wei Chen, Kun Wang, Zhengyang Zhou, Sijie Ruan, Yuxuan Liang",The paper has been accepted by IJCAI 2024,https://arxiv.org/abs/2404.14073,
Transferring Troubles: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning,arXiv,2404,"Xuanli He, Jun Wang, Qiongkai Xu, Pasquale Minervini, Pontus Stenetorp, Benjamin I. P. Rubinstein, Trevor Cohn",work in progress,https://arxiv.org/abs/2404.19597,
Trojan Detection in Large Language Models: Insights from The Trojan Detection Challenge,arXiv,2404,"Narek Maloyan, Ekansh Verma, Bulat Nutfullin, Bislan Ashinov",,https://arxiv.org/abs/2404.13660,
Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors,arXiv,2404,"Victoria Graf, Qin Liu, Muhao Chen",Accepted by NAACL 2024 Main Conference,https://arxiv.org/abs/2404.02356,
UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models,arXiv,2404,"Zihan Guan, Mengxuan Hu, Sheng Li, Anil Vullikanti","20 pages,18 figures",https://arxiv.org/abs/2404.01101,https://github.com/GuanZihan/official_UFID.
A general approach to enhance the survivability of backdoor attacks by decision path coupling,arXiv,2403,"Yufei Zhao, Dingji Wang, Bihuan Chen, Ziqian Chen, Xin Peng",,https://arxiv.org/abs/2403.02950,
AS-FIBA: Adaptive Selective Frequency-Injection for Backdoor Attack on Deep Face Restoration,arXiv,2403,"Zhenbo Song, Wenhao Gao, Kaihao Zhang, Wenhan Luo, Zhaoxin Fan, Jianfeng Lu",,https://arxiv.org/abs/2403.06430,
Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks,arXiv,2403,"Khondoker Murad Hossain, Tim Oates","6 pages, Accepted at the International Conference on Communications 2024. arXiv admin note: text overlap with arXiv:2212.08121",https://arxiv.org/abs/2403.08208,
An Embarrassingly Simple Defense Against Backdoor Attacks On SSL,arXiv,2403,"Aryan Satpathy, Nilaksh Nilaksh, Dhruva Rajwade","10 pages, 5 figures",https://arxiv.org/abs/2403.15918,https://github.com/Aryan-Satpathy/Backdoor.
Backdoor Attack with Mode Mixture Latent Modification,arXiv,2403,"Hongwei Zhang, Xiaoyin Xu, Dongsheng An, Xianfeng Gu, Min Zhang",,https://arxiv.org/abs/2403.07463,
Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency,arXiv,2403,"Soumyadeep Pal, Yuguang Yao, Ren Wang, Bingquan Shen, Sijia Liu",The Twelfth International Conference on Learning Representations (ICLR 2024),https://arxiv.org/abs/2403.10717,https://github.com/OPTML-Group/BackdoorMSPC.
BadEdit: Backdooring large language models by model editing,arXiv,2403,"Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing Liu, Wenhan Wang, Tianwei Zhang, Yang Liu",ICLR 2024,https://arxiv.org/abs/2403.13355,
CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation,arXiv,2403,"Baoyu Jing, Dawei Zhou, Kan Ren, Carl Yang",Preprint. Work in progress,https://arxiv.org/abs/2403.11960,
Causality-based Cross-Modal Representation Learning for Vision-and-Language Navigation,arXiv,2403,"Liuyi Wang, Zongtao He, Ronghao Dang, Huiyi Chen, Chengju Liu, Qijun Chen",16 pages,https://arxiv.org/abs/2403.03405,
Clean-image Backdoor Attacks,arXiv,2403,"Dazhong Rong, Guoyao Yu, Shuheng Shen, Xinyi Fu, Peng Qian, Jianhai Chen, Qinming He, Xing Fu, Weiqiang Wang",,https://arxiv.org/abs/2403.15010,
DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference,arXiv,2403,"Jialong Wu, Linhai Zhang, Deyu Zhou, Guoqiang Xu",Our code and results will be available at https://github.com/callanwu/DINER,https://arxiv.org/abs/2403.01166,https://github.com/callanwu/DINER
De-confounded Data-free Knowledge Distillation for Handling Distribution Shifts,arXiv,2403,"Yuzheng Wang, Dingkang Yang, Zhaoyu Chen, Yang Liu, Siao Liu, Wenqiang Zhang, Lihua Zhang, Lizhe Qi",Accepted by CVPR24,https://arxiv.org/abs/2403.19539,
Enhancing Adversarial Training with Prior Knowledge Distillation for Robust Image Compression,arXiv,2403,"Zhi Cao, Youneng Bao, Fanyang Meng, Chao Li, Wen Tan, Genhong Wang, Yongsheng Liang",,https://arxiv.org/abs/2403.06700,
Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications,arXiv,2403,"Fouad Trad, Ali Chehab",,https://arxiv.org/abs/2403.17787,
"Federated Learning: Attacks, Defenses, Opportunities, and Challenges",arXiv,2403,"Ghazaleh Shirvani, Saeid Ghasemshirazi, Behzad Beigzadeh",,https://arxiv.org/abs/2403.06067,
Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion,arXiv,2403,"Hossein Souri, Arpit Bansal, Hamid Kazemi, Liam Fowl, Aniruddha Saha, Jonas Geiping, Andrew Gordon Wilson, Rama Chellappa, Tom Goldstein, Micah Goldblum",,https://arxiv.org/abs/2403.16365,https://github.com/hsouri/GDP
Impart: An Imperceptible and Effective Label-Specific Backdoor Attack,arXiv,2403,"Jingke Zhao, Zan Wang, Yongwei Wang, Lanjun Wang",,https://arxiv.org/abs/2403.13017,
Invisible Backdoor Attack Through Singular Value Decomposition,arXiv,2403,"Wenmin Chen, Xiaowei Xu",,https://arxiv.org/abs/2403.13018,
LOTUS: Evasive and Resilient Backdoor Attacks through Sub-Partitioning,arXiv,2403,"Siyuan Cheng, Guanhong Tao, Yingqi Liu, Guangyu Shen, Shengwei An, Shiwei Feng, Xiangzhe Xu, Kaiyuan Zhang, Shiqing Ma, Xiangyu Zhang",IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024),https://arxiv.org/abs/2403.17188,https://github.com/Megum1/LOTUS.
LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario,arXiv,2403,"Hongyi Liu, Zirui Liu, Ruixiang Tang, Jiayi Yuan, Shaochen Zhong, Yu-Neng Chuang, Li Li, Rui Chen, Xia Hu",,https://arxiv.org/abs/2403.00108,
Manipulating Neural Path Planners via Slight Perturbations,arXiv,2403,"Zikang Xiong, Suresh Jagannathan",,https://arxiv.org/abs/2403.18256,
MirrorAttack: Backdoor Attack on 3D Point Cloud with a Distorting Mirror,arXiv,2403,"Yuhao Bian, Shengjing Tian, Xiuping Liu",15 pages,https://arxiv.org/abs/2403.05847,
Mitigating Label Flipping Attacks in Malicious URL Detectors Using Ensemble Trees,arXiv,2403,"Ehsan Nowroozi, Nada Jadalla, Samaneh Ghelichkhani, Alireza Jolfaei",,https://arxiv.org/abs/2403.02995,
On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder,arXiv,2403,"Tingxu Han, Shenghan Huang, Ziqi Ding, Weisong Sun, Yebo Feng, Chunrong Fang, Jun Li, Hanwei Qian, Cong Wu, Quanjun Zhang, Yang Liu, Zhenyu Chen",,https://arxiv.org/abs/2403.03846,
REPQC: Reverse Engineering and Backdooring Hardware Accelerators for Post-quantum Cryptography,arXiv,2403,"Samuel Pagliarini, Aikata Aikata, Malik Imran, Sujoy Sinha Roy",Accepted in AsiaCCS'24,https://arxiv.org/abs/2403.09352,
Real is not True: Backdoor Attacks Against Deepfake Detection,arXiv,2403,"Hong Sun, Ziqiang Li, Lei Liu, Bin Li",BigDIA 2023,https://arxiv.org/abs/2403.06610,
Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors,arXiv,2403,"Md Abdul Kadir, GowthamKrishna Addluri, Daniel Sonntag",,https://arxiv.org/abs/2403.16569,
Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs,arXiv,2403,"Jane Downer, Ren Wang, Binghui Wang",,https://arxiv.org/abs/2403.18136,
Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic Learning over Low-power Devices,arXiv,2403,"Hanqing Fu, Gaolei Li, Jun Wu, Jianhua Li, Xi Lin, Kai Zhou, Yuchen Liu",,https://arxiv.org/abs/2403.18607,
Task-Agnostic Detector for Insertion-Based Backdoor Attacks,arXiv,2403,"Weimin Lyu, Xiao Lin, Songzhu Zheng, Lu Pang, Haibin Ling, Susmit Jha, Chao Chen",Findings of NAACL 2024,https://arxiv.org/abs/2403.17155,
"Threats, Attacks, and Defenses in Machine Unlearning: A Survey",arXiv,2403,"Ziyao Liu, Huanyi Ye, Chen Chen, Kwok-Yan Lam",,https://arxiv.org/abs/2403.13682,
Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token Unlearning,arXiv,2403,"Siyuan Liang, Kuanrong Liu, Jiajun Gong, Jiawei Liang, Yuan Xun, Ee-Chien Chang, Xiaochun Cao","6 pages, 2 figures",https://arxiv.org/abs/2403.16257,
WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection,arXiv,2403,"Anudeex Shetty, Yue Teng, Ke He, Qiongkai Xu",Work in Progress,https://arxiv.org/abs/2403.01472,
A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents,arXiv,2402,"Lingbo Mo, Zeyi Liao, Boyuan Zheng, Yu Su, Chaowei Xiao, Huan Sun",,https://arxiv.org/abs/2402.10196,
Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space,arXiv,2402,"Zongru Wu, Zhuosheng Zhang, Pengzhou Cheng, Gongshen Liu",,https://arxiv.org/abs/2402.12026,https://github.com/ZrW00/MuScleLoRA.
Architectural Neural Backdoors from First Principles,arXiv,2402,"Harry Langford, Ilia Shumailov, Yiren Zhao, Robert Mullins, Nicolas Papernot",,https://arxiv.org/abs/2402.06957,
Backdoor Attack against One-Class Sequential Anomaly Detection Models,arXiv,2402,"He Cheng, Shuhan Yuan",This work is accepted by the PAKDD 2024. 12 pages,https://arxiv.org/abs/2402.10283,
Backdoor Attacks on Dense Passage Retrievers for Disseminating Misinformation,arXiv,2402,"Quanyu Long, Yue Deng, LeiLei Gan, Wenya Wang, Sinno Jialin Pan",,https://arxiv.org/abs/2402.13532,
Corrective Machine Unlearning,arXiv,2402,"Shashwat Goel, Ameya Prabhu, Philip Torr, Ponnurangam Kumaraguru, Amartya Sanyal","17 pages, 7 figures",https://arxiv.org/abs/2402.14015,
Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning,arXiv,2402,"Shuai Zhao, Leilei Gan, Luu Anh Tuan, Jie Fu, Lingjuan Lyu, Meihuizi Jia, Jinming Wen",NAACL Findings 2024,https://arxiv.org/abs/2402.12168,
DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models,arXiv,2402,"Yang Sui, Huy Phan, Jinqi Xiao, Tianfang Zhang, Zijie Tang, Cong Shi, Yan Wang, Yingying Chen, Bo Yuan",,https://arxiv.org/abs/2402.02739,
Domain Generalization via Causal Adjustment for Cross-Domain Sentiment Analysis,arXiv,2402,"Siyin Wang, Jie Zhou, Qin Chen, Qi Zhang, Tao Gui, Xuanjing Huang",,https://arxiv.org/abs/2402.14536,
Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,arXiv,2402,"Shen Li, Liuyi Yao, Jinyang Gao, Lan Zhang, Yaliang Li",,https://arxiv.org/abs/2402.14883,
Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge,arXiv,2402,"Ansh Arora, Xuanli He, Maximilian Mozes, Srinibas Swain, Mark Dras, Qiongkai Xu",work in progress,https://arxiv.org/abs/2402.19334,
Learning to Poison Large Language Models During Instruction Tuning,arXiv,2402,"Yao Qiang, Xiangyu Zhou, Saleh Zare Zade, Mohammad Amin Roshani, Douglas Zytko, Dongxiao Zhu",,https://arxiv.org/abs/2402.13459,GitHub
Low-Frequency Black-Box Backdoor Attack via Evolutionary Algorithm,arXiv,2402,"Yanqi Qiao, Dazhuang Liu, Rui Wang, Kaitai Liang",,https://arxiv.org/abs/2402.15653,
Measuring Impacts of Poisoning on Model Parameters and Neuron Activations: A Case Study of Poisoning CodeBERT,arXiv,2402,"Aftab Hussain, Md Rafiqul Islam Rabin, Navid Ayoobi, Mohammad Amin Alipour",,https://arxiv.org/abs/2402.12936,
Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment,arXiv,2402,"Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Junjie Hu, Yixuan Li, Patrick McDaniel, Muhao Chen, Bo Li, Chaowei Xiao",,https://arxiv.org/abs/2402.14968,
Model Pairing Using Embedding Translation for Backdoor Attack Detection on Open-Set Classification Tasks,arXiv,2402,"Alexander Unnervik, Hatef Otroshi Shahreza, Anjith George, Sébastien Marcel",Under review,https://arxiv.org/abs/2402.18718,
Model X-ray:Detect Backdoored Models via Decision Boundary,arXiv,2402,"Yanghao Su, Jie Zhang, Ting Xu, Tianwei Zhang, Weiming Zhang, Nenghai Yu",,https://arxiv.org/abs/2402.17465,
Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models,arXiv,2402,"Hongbin Liu, Michael K. Reiter, Neil Zhenqiang Gong","To appear in USENIX Security Symposium, 2024",https://arxiv.org/abs/2402.14977,
On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem,arXiv,2402,"Georg Pichler, Marco Romanelli, Divya Prakash Manivannan, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg",,https://arxiv.org/abs/2402.16926,
OrderBkd: Textual backdoor attack through repositioning,arXiv,2402,"Irina Alekseevskaia, Konstantin Arkhipenko",,https://arxiv.org/abs/2402.07689,https://github.com/alekseevskaia/OrderBkd.
Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection,arXiv,2402,"Jiawei Liang, Siyuan Liang, Aishan Liu, Xiaojun Jia, Junhao Kuang, Xiaochun Cao",ICLR 2024 Spotlight,https://arxiv.org/abs/2402.11473,\url{https://github.com/JWLiang007/PFF}
"Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization",arXiv,2402,"Rui Zhang, Hongwei Li, Rui Wen, Wenbo Jiang, Yuan Zhang, Michael Backes, Yun Shen, Yang Zhang",,https://arxiv.org/abs/2402.09179,
SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning,arXiv,2402,"Yu Zhang, Hui-Ling Zhen, Zehua Pei, Yingzhao Lian, Lihao Yin, Mingxuan Yuan, Bei Yu",,https://arxiv.org/abs/2402.11903,
Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models,arXiv,2402,"Pengzhou Cheng, Wei Du, Zongru Wu, Fengwei Zhang, Libo Chen, Gongshen Liu","16 pages, 16 figures, 13 tables",https://arxiv.org/abs/2402.18945,
Test-Time Backdoor Attacks on Multimodal Large Language Models,arXiv,2402,"Dong Lu, Tianyu Pang, Chao Du, Qian Liu, Xianjun Yang, Min Lin",,https://arxiv.org/abs/2402.08577,https://sail-sg.github.io/AnyDoor/.
The last Dance : Robust backdoor attack via diffusion models and bayesian approach,arXiv,2402,Orson Mengara,"Preprint (Last update): audio backdoor attack on Hugging Face's Transformer pre-trained models. This attack incorporates state-of-the-art Bayesian techniques, a modified Fokker-Planck equation (via Yang-Mills), and a diffusion model approach",https://arxiv.org/abs/2402.05967,
Time-Distributed Backdoor Attacks on Federated Spiking Learning,arXiv,2402,"Gorka Abad, Stjepan Picek, Aitor Urbieta",,https://arxiv.org/abs/2402.02886,
Universal Post-Training Reverse-Engineering Defense Against Backdoors in Deep Neural Networks,arXiv,2402,"Xi Li, Hang Wang, David J. Miller, George Kesidis",,https://arxiv.org/abs/2402.02034,
VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models,arXiv,2402,"Jiawei Liang, Siyuan Liang, Man Luo, Aishan Liu, Dongchen Han, Ee-Chien Chang, Xiaochun Cao",,https://arxiv.org/abs/2402.13851,
Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents,arXiv,2402,"Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, Xu Sun",The first two authors contribute equally. Code and data are available at https://github.com/lancopku/agent-backdoor-attacks,https://arxiv.org/abs/2402.11208,https://github.com/lancopku/agent-backdoor-attacks
A backdoor attack against link prediction tasks with graph neural networks,arXiv,2401,"Jiazhu Dai, Haoyu Sun",,https://arxiv.org/abs/2401.02663,
Adaptive Discounting of Training Time Attacks,arXiv,2401,"Ridhima Bector, Abhay Aradhya, Chai Quek, Zinovi Rabinovich","19 pages, 7 figures",https://arxiv.org/abs/2401.02652,"""bit.ly/github-rb-gDDPG""."
Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP,arXiv,2401,"Ruinan Jin, Chun-Yin Huang, Chenyu You, Xiaoxiao Li",Paper Accepted at the 2nd IEEE Conference on Secure and Trustworthy Machine Learning,https://arxiv.org/abs/2401.01911,
BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning,arXiv,2401,"Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Mingli Zhu, Ruotong Wang, Li Liu, Chao Shen",,https://arxiv.org/abs/2401.15002,
BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models,arXiv,2401,"Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, Bo Li",Accepted to ICLR2024,https://arxiv.org/abs/2401.12242,
Can We Trust the Unlabeled Target Data? Towards Backdoor Attack and Defense on Model Adaptation,arXiv,2401,"Lijun Sheng, Jian Liang, Ran He, Zilei Wang, Tieniu Tan","11 pages, 4 figures",https://arxiv.org/abs/2401.06030,
Detecting Face Synthesis Using a Concealed Fusion Model,arXiv,2401,"Roberto Leyva, Victor Sanchez, Gregory Epiphaniou, Carsten Maple",,https://arxiv.org/abs/2401.04257,
End-to-End Anti-Backdoor Learning on Images and Time Series,arXiv,2401,"Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, Yige Li, James Bailey",,https://arxiv.org/abs/2401.03215,
"Federated Unlearning: A Survey on Methods, Design Guidelines, and Evaluation Metrics",arXiv,2401,"Nicolò Romandini, Alessio Mora, Carlo Mazzocca, Rebecca Montanari, Paolo Bellavista","23 pages, 8 figures, and 6 tables",https://arxiv.org/abs/2401.05146,
Hijacking Attacks against Neural Networks by Analyzing Training Data,arXiv,2401,"Yunjie Ge, Qian Wang, Huayang Huang, Qi Li, Cong Wang, Chao Shen, Lingchen Zhao, Peipei Jiang, Zheng Fang, Shenyi Zhang","Full version with major polishing, compared to the Usenix Security 2024 edition",https://arxiv.org/abs/2401.09740,
Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control,arXiv,2401,"Ka-Ho Chow, Wenqi Wei, Lei Yu",,https://arxiv.org/abs/2401.01085,
Inferring Properties of Graph Neural Networks,arXiv,2401,"Dat Nguyen, Hieu M. Vu, Cong-Thanh Le, Bach Le, David Lo, ThanhVu Nguyen, Corina Pasareanu","20 pages main paper, 10 pages for appendix",https://arxiv.org/abs/2401.03790,
Instructional Fingerprinting of Large Language Models,arXiv,2401,"Jiashu Xu, Fei Wang, Mingyu Derek Ma, Pang Wei Koh, Chaowei Xiao, Muhao Chen",Accepted at NAACL 2024; 30 pages,https://arxiv.org/abs/2401.12255,https://cnut1648.github.io/Model-Fingerprint/.
Learning Backdoors for Mixed Integer Programs with Contrastive Learning,arXiv,2401,"Junyang Cai, Taoan Huang, Bistra Dilkina",,https://arxiv.org/abs/2401.10467,
MEA-Defender: A Robust Watermark against Model Extraction Attack,arXiv,2401,"Peizhuo Lv, Hualong Ma, Kai Chen, Jiachen Zhou, Shengzhi Zhang, Ruigang Liang, Shenchen Zhu, Pan Li, Yingjun Zhang","To Appear in IEEE Symposium on Security and Privacy 2024 (IEEE S&P 2024), MAY 20-23, 2024, SAN FRANCISCO, CA, USA",https://arxiv.org/abs/2401.15239,
MalModel: Hiding Malicious Payload in Mobile Deep Learning Models with Black-box Backdoor Attack,arXiv,2401,"Jiayi Hua, Kailong Wang, Meizhen Wang, Guangdong Bai, Xiapu Luo, Haoyu Wang","Due to the limitation ""The abstract field cannot be longer than 1,920 characters"", the abstract here is shorter than that in the PDF file",https://arxiv.org/abs/2401.02659,
"Multi-Trigger Backdoor Attacks: More Triggers, More Threats",arXiv,2401,"Yige Li, Xingjun Ma, Jiabo He, Hanxun Huang, Yu-Gang Jiang",,https://arxiv.org/abs/2401.15295,
Object-oriented backdoor attack against image captioning,arXiv,2401,"Meiling Li, Nan Zhong, Xinpeng Zhang, Zhenxing Qian, Sheng Li",,https://arxiv.org/abs/2401.02600,
Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training,arXiv,2401,"Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec",updated to add missing acknowledgements,https://arxiv.org/abs/2401.05566,
Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack,arXiv,2401,"Ruofei Wang, Renjie Wan, Zongyu Guo, Qing Guo, Rui Huang",Accepted by ICASSP2024,https://arxiv.org/abs/2401.02031,
TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks,arXiv,2401,"Khondoker Murad Hossain, Tim Oates",,https://arxiv.org/abs/2401.05432,
The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers,arXiv,2401,Orson Mengara,"Accepted by AAAI Workshop 2024, 8 pages",https://arxiv.org/abs/2401.01537,
"The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",arXiv,2401,"Haonan Wang, Qianli Shen, Yao Tong, Yang Zhang, Kenji Kawaguchi","This study reveals that by subtly inserting non-copyright-infringing poisoning data into a diffusion model's training dataset, it's possible to trigger the model to generate copyrighted content, highlighting vulnerabilities in current copyright protection strategies",https://arxiv.org/abs/2401.04136,
TransTroj: Transferable Backdoor Attacks to Pre-trained Models via Embedding Indistinguishability,arXiv,2401,"Hao Wang, Tao Xiang, Shangwei Guo, Jialing He, Hangcheng Liu, Tianwei Zhang","13 pages, 16 figures, 5 tables",https://arxiv.org/abs/2401.15883,https://github.com/haowang-cqu/TransTroj
Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning,arXiv,2401,"Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Fengjun Pan, Jinming Wen",,https://arxiv.org/abs/2401.05949,
WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition,arXiv,2401,"Zhengyao Song, Yongqiang Li, Danni Yuan, Li Liu, Shaokui Wei, Baoyuan Wu","13 pages, 21 figures",https://arxiv.org/abs/2401.13578,
AI Control: Improving Safety Despite Intentional Subversion,arXiv,2312,"Ryan Greenblatt, Buck Shlegeris, Kshitij Sachan, Fabien Roger",Edit: Fix minor typos and clarify abstract,https://arxiv.org/abs/2312.06942,
BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting,arXiv,2312,"Huming Qiu, Junjie Sun, Mi Zhang, Xudong Pan, Min Yang","To Appear in the 45th IEEE Symposium on Security and Privacy, May 20-23, 2024",https://arxiv.org/abs/2312.04902,
Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift,arXiv,2312,"Shengwei An, Sheng-Yen Chou, Kaiyuan Zhang, Qiuling Xu, Guanhong Tao, Guangyu Shen, Siyuan Cheng, Shiqing Ma, Pin-Yu Chen, Tsung-Yi Ho, Xiangyu Zhang",AAAI 2024,https://arxiv.org/abs/2312.00050,
FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning,arXiv,2312,"Hossein Fereidooni, Alessandro Pegoraro, Phillip Rieger, Alexandra Dmitrienko, Ahmad-Reza Sadeghi","To appear in the Network and Distributed System Security (NDSS) Symposium 2024. 16 pages, 8 figures, 12 tables, 1 algorithm, 3 equations",https://arxiv.org/abs/2312.04432,
Manipulating Trajectory Prediction with Backdoors,arXiv,2312,"Kaouther Messaoud, Kathrin Grosse, Mickael Chen, Matthieu Cord, Patrick Pérez, Alexandre Alahi","9 pages, 7 figures",https://arxiv.org/abs/2312.13863,
OCGEC: One-class Graph Embedding Classification for DNN Backdoor Detection,arXiv,2312,"Haoyu Jiang, Haiyang Yu, Nan Li, Ping Yi",v2,https://arxiv.org/abs/2312.01585,https://github.com/jhy549/OCGEC.
Performance-lossless Black-box Model Watermarking,arXiv,2312,"Na Zhao, Kejiang Chen, Weiming Zhang, Nenghai Yu",,https://arxiv.org/abs/2312.06488,
Synthesizing Physical Backdoor Datasets: An Automated Framework Leveraging Deep Generative Models,arXiv,2312,"Sze Jue Yang, Chinh D. La, Quang H. Nguyen, Kok-Seng Wong, Anh Tuan Tran, Chee Seng Chan, Khoa D. Doan",,https://arxiv.org/abs/2312.03419,
Universal Backdoor Attacks,arXiv,2312,"Benjamin Schneider, Nils Lukas, Florian Kerschbaum",Accepted for publication at ICLR 2024,https://arxiv.org/abs/2312.00157,https://github.com/Ben-Schneider-code/Universal-Backdoor-Attacks.
BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning,arXiv,2311,"Siyuan Liang, Mingli Zhu, Aishan Liu, Baoyuan Wu, Xiaochun Cao, Ee-Chien Chang",The paper lacks some work that needs to be cited,https://arxiv.org/abs/2311.12075,
BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP,arXiv,2311,"Jiawang Bai, Kuofeng Gao, Shaobo Min, Shu-Tao Xia, Zhifeng Li, Wei Liu","14 pages, 6 figures",https://arxiv.org/abs/2311.16194,
Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data,arXiv,2311,"Bart Pleiter, Behrad Tajalli, Stefanos Koffas, Gorka Abad, Jing Xu, Martha Larson, Stjepan Picek",,https://arxiv.org/abs/2311.07550,
Universal Jailbreak Backdoors from Poisoned Human Feedback,arXiv,2311,"Javier Rando, Florian Tramèr",Accepted as conference paper in ICLR 2024,https://arxiv.org/abs/2311.14455,
CBD: A Certified Backdoor Detector Based on Local Dominant Probability,arXiv,2310,"Zhen Xiang, Zidi Xiong, Bo Li",Accepted to NeurIPS 2023,https://arxiv.org/abs/2310.17498,
Composite Backdoor Attacks Against Large Language Models,arXiv,2310,"Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang","To Appear in Findings of the Association for Computational Linguistics: NAACL 2024, June 2024",https://arxiv.org/abs/2310.07676,
Defending Our Privacy With Backdoors,arXiv,2310,"Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting","18 pages, 11 figures",https://arxiv.org/abs/2310.08320,
Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning,arXiv,2310,"Shanshan Han, Wenxuan Wu, Baturalp Buyukates, Weizhao Jin, Qifan Zhang, Yuhang Yao, Salman Avestimehr, Chaoyang He",,https://arxiv.org/abs/2310.04055,
PETA: Parameter-Efficient Trojan Attacks,arXiv,2310,"Lauren Hong, Ting Wang",,https://arxiv.org/abs/2310.00648,
VFLAIR: A Research Library and Benchmark for Vertical Federated Learning,arXiv,2310,"Tianyuan Zou, Zixuan Gu, Yu He, Hideaki Takahashi, Yang Liu, Ya-Qin Zhang","39 pages, 22 figures, 19 tabels",https://arxiv.org/abs/2310.09827,"https://github.com/FLAIR-THU/VFLAIR),"
Watch Out! Simple Horizontal Class Backdoors Can Trivially Evade Defenses,arXiv,2310,"Hua Ma, Shang Wang, Yansong Gao, Zhi Zhang, Huming Qiu, Minhui Xue, Alsharif Abuadbba, Anmin Fu, Surya Nepal, Derek Abbott",To Appear in the 31st ACM Conference on Computer and Communications Security,https://arxiv.org/abs/2310.00542,
One-to-Multiple Clean-Label Image Camouflage (OmClic) based Backdoor Attack on Deep Learning,arXiv,2309,"Guohong Wang, Hua Ma, Yansong Gao, Alsharif Abuadbba, Zhi Zhang, Wei Kang, Said F. Al-Sarawib, Gongxuan Zhang, Derek Abbott",,https://arxiv.org/abs/2309.04036,
Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective,arXiv,2309,"Zhen Qin, Feiyi Chen, Chen Zhi, Xueqiang Yan, Shuiguang Deng",Accepted by AAAI 2024. Codes are publicly available at https://github.com/zhenqincn/Snowball,https://arxiv.org/abs/2309.16456,https://github.com/zhenqincn/Snowball
Safe and Robust Watermark Injection with a Single OoD Image,arXiv,2309,"Shuyang Yu, Junyuan Hong, Haobo Zhang, Haotao Wang, Zhangyang Wang, Jiayu Zhou",,https://arxiv.org/abs/2309.01786,
Seeing Is Not Always Believing: Invisible Collision Attack and Defence on Pre-Trained Models,arXiv,2309,"Minghang Deng, Zhong Zhang, Junming Shao","10 pages, 4 figures",https://arxiv.org/abs/2309.13579,
VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models,arXiv,2309,"Zihao Zhu, Mingda Zhang, Shaokui Wei, Bingzhe Wu, Baoyuan Wu",Accepted to ICLR 2024,https://arxiv.org/abs/2309.16211,\url{https://github.com/zihao-ai/vdc}.
Backdoor Federated Learning by Poisoning Backdoor-Critical Layers,arXiv,2308,"Haomin Zhuang, Mingxian Yu, Hao Wang, Yang Hua, Jian Li, Xu Yuan",Accepted to ICLR'24,https://arxiv.org/abs/2308.04466,
Breaking Speaker Recognition with PaddingBack,arXiv,2308,"Zhe Ye, Diqun Yan, Li Dong, Kailai Shen",,https://arxiv.org/abs/2308.04179,
"DFB: A Data-Free, Low-Budget, and High-Efficacy Clean-Label Backdoor Attack",arXiv,2308,"Binhao Ma, Jiahui Wang, Dejun Wang, Bo Meng",,https://arxiv.org/abs/2308.09487,
Adversarial Feature Map Pruning for Backdoor,arXiv,2307,"Dong Huang, Qingwen Bu",Accepted to ICLR 2024,https://arxiv.org/abs/2307.11565,https://github.com/retsuh-bqw/FMP.
Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection,arXiv,2307,"Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, Hongxia Jin",Accepted to NAACL 2024. Project page: https://poison-llm.github.io,https://arxiv.org/abs/2307.16888,https://poison-llm.github.io.
FedDefender: Backdoor Attack Defense in Federated Learning,arXiv,2307,"Waris Gill, Ali Anwar, Muhammad Ali Gulzar",Published in SE4SafeML 2023 (co-located with FSE 2023). See https://dl.acm.org/doi/abs/10.1145/3617574.3617858,https://arxiv.org/abs/2307.08672,
QDoor: Exploiting Approximate Synthesis for Backdoor Attacks in Quantum Neural Networks,arXiv,2307,"Cheng Chu, Fan Chen, Philip Richerme, Lei Jiang",,https://arxiv.org/abs/2307.09529,
Risk-optimized Outlier Removal for Robust 3D Point Cloud Classification,arXiv,2307,"Xinke Li, Junchi Lu, Henghui Ding, Changsheng Sun, Joey Tianyi Zhou, Chee Yeow Meng",,https://arxiv.org/abs/2307.10875,
A Proxy Attack-Free Strategy for Practically Improving the Poisoning Efficiency in Backdoor Attacks,arXiv,2306,"Ziqiang Li, Hong Sun, Pengfei Xia, Beihao Xia, Xue Rui, Wei Zhang, Qinglang Guo, Bin Li",Under review,https://arxiv.org/abs/2306.08313,
"Edge Learning for 6G-enabled Internet of Things: A Comprehensive Survey of Vulnerabilities, Datasets, and Defenses",arXiv,2306,"Mohamed Amine Ferrag, Othmane Friha, Burak Kantarci, Norbert Tihanyi, Lucas Cordeiro, Merouane Debbah, Djallel Hamouda, Muna Al-Hawawreh, Kim-Kwang Raymond Choo",This paper has been accepted for publication in IEEE Communications Surveys \& Tutorials,https://arxiv.org/abs/2306.10309,
Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios,arXiv,2306,"Ziqiang Li, Hong Sun, Pengfei Xia, Heng Li, Beihao Xia, Yi Wu, Bin Li",ICLR 2024,https://arxiv.org/abs/2306.08386,https://github.com/sunh1113/Efficient-backdoor-attacks-for-deep-neural-networks-in-real-world-scenarios
Machine Learning needs Better Randomness Standards: Randomised Smoothing and PRNG-based attacks,arXiv,2306,"Pranav Dahiya, Ilia Shumailov, Ross Anderson",USENIX Security 2024 (https://www.usenix.org/conference/usenixsecurity24/presentation/dahiya),https://arxiv.org/abs/2306.14043,
"Versatile Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers",arXiv,2306,"Ruotong Wang, Hongrui Chen, Zihao Zhu, Li Liu, Baoyuan Wu",,https://arxiv.org/abs/2306.00816,
From Shortcuts to Triggers: Backdoor Defense with Denoised PoE,arXiv,2305,"Qin Liu, Fei Wang, Chaowei Xiao, Muhao Chen",Accepted by NAACL 2024 Main Conference,https://arxiv.org/abs/2305.14910,
Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models,arXiv,2305,"Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, Muhao Chen",NAACL 2024,https://arxiv.org/abs/2305.14710,
Model Sparsity Can Simplify Machine Unlearning,arXiv,2304,"Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, Sijia Liu",NeurIPS'23 spotlight,https://arxiv.org/abs/2304.04934,https://github.com/OPTML-Group/Unlearn-Sparse.
RSBA: Robust Statistical Backdoor Attack under Privilege-Constrained Scenarios,arXiv,2304,"Xiaolei Liu, Ming Yi, Kangyi Ding, Bangzhou Xin, Yixiao Xu, Li Yan, Chao Shen","11 pages, 10 figures",https://arxiv.org/abs/2304.10985,
Influencer Backdoor Attack on Semantic Segmentation,arXiv,2303,"Haoheng Lan, Jindong Gu, Philip Torr, Hengshuang Zhao",,https://arxiv.org/abs/2303.12054,
Analyzing And Editing Inner Mechanisms Of Backdoored Language Models,arXiv,2302,"Max Lamparth, Anka Reuel",Final version accepted at FAccT 24,https://arxiv.org/abs/2302.12461,
Attacks in Adversarial Machine Learning: A Systematic Survey from the Life-cycle Perspective,arXiv,2302,"Baoyuan Wu, Zihao Zhu, Li Liu, Qingshan Liu, Zhaofeng He, Siwei Lyu","35 pages, 4 figures, 10 tables, 313 reference papers",https://arxiv.org/abs/2302.09457,
Provable Robustness Against a Union of $\ell_0$ Adversarial Attacks,arXiv,2302,"Zayd Hammoudeh, Daniel Lowd",Accepted at AAAI 2024 -- Extended version including the supplementary material,https://arxiv.org/abs/2302.11628,
SATBA: An Invisible Backdoor Attack Based On Spatial Attention,arXiv,2302,"Huasong Zhou, Xiaowei Xu, Xiaodong Wang, Leon Bevan Bullock","9 pages, 9 figures",https://arxiv.org/abs/2302.13056,
Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data,arXiv,2302,"Gorka Abad, Oguzhan Ersoy, Stjepan Picek, Aitor Urbieta",To appear in Network and Distributed System Security (NDSS) Symposium 2024,https://arxiv.org/abs/2302.06279,
Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering,arXiv,2301,"Rui Zhu, Di Tang, Siyuan Tang, Guanhong Tao, Shiqing Ma, Xiaofeng Wang, Haixu Tang",,https://arxiv.org/abs/2301.12318,
PECAN: A Deterministic Certified Defense Against Backdoor Attacks,arXiv,2301,"Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni",,https://arxiv.org/abs/2301.11824,
CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning,arXiv,2211,"Jinghuai Zhang, Hongbin Liu, Jinyuan Jia, Neil Zhenqiang Gong",CVPR 2024,https://arxiv.org/abs/2211.08229,
Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning,arXiv,2211,"Yunchao Zhang, Zonglin Di, Kaiwen Zhou, Cihang Xie, Xin Eric Wang",,https://arxiv.org/abs/2211.14769,
BAFFLE: Hiding Backdoors in Offline Reinforcement Learning Datasets,arXiv,2210,"Chen Gong, Zhou Yang, Yunpeng Bai, Junda He, Jieke Shi, Kecen Li, Arunesh Sinha, Bowen Xu, Xinwen Hou, David Lo, Tianhao Wang",Accepted at IEEE S&P (Oakland) 2024,https://arxiv.org/abs/2210.04688,
ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks,arXiv,2210,"Eleanor Clifford, Ilia Shumailov, Yiren Zhao, Ross Anderson, Robert Mullins","10 pages, 7 figures, to be published in IEEE Secure and Trustworthy Machine Learning 2024. For website see https://ml.backdoors.uk . For source code, see https://git.sr.ht/~tim-clifford/impnet_source",https://arxiv.org/abs/2210.00108,
Invariant Aggregator for Defending against Federated Backdoor Attacks,arXiv,2210,"Xiaoyang Wang, Dimitrios Dimitriadis, Sanmi Koyejo, Shruti Tople",AISTATS 2024 camera-ready,https://arxiv.org/abs/2210.01834,
Causal Intervention for Fairness in Multi-behavior Recommendation,arXiv,2209,"Xi Wang, Wenjie Wang, Fuli Feng, Wenge Rong, Chuantao Yin, Zhang Xiong",This paper is accepted by IEEE Transactions on Computational Social Systems,https://arxiv.org/abs/2209.04589,
SSL-WM: A Black-Box Watermarking Approach for Encoders Pre-trained by Self-supervised Learning,arXiv,2209,"Peizhuo Lv, Pan Li, Shenchen Zhu, Shengzhi Zhang, Kai Chen, Ruigang Liang, Chang Yue, Fan Xiang, Yuling Cai, Hualong Ma, Yingjun Zhang, Guozhu Meng","To Appear in the Network and Distributed System Security (NDSS) Symposium 2024, 26 February - 1 March 2024, San Diego, CA, USA",https://arxiv.org/abs/2209.03563,
Machine Learning Security against Data Poisoning: Are We There Yet?,arXiv,2204,"Antonio Emanuele Cinà, Kathrin Grosse, Ambra Demontis, Battista Biggio, Fabio Roli, Marcello Pelillo","preprint, 10 pages, 3 figures. Paper accepted to the IEEE Computer - Special Issue on Trustworthy AI",https://arxiv.org/abs/2204.05986,
