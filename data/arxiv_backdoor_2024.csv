title,venue,year,author,comment,url,code
Backdoor-based Explainable AI Benchmark for High Fidelity Evaluation of Attribution Methods,arXiv,2405,"Peiyu Yang, Naveed Akhtar, Jiantong Jiang, Ajmal Mian",,https://arxiv.org/abs/2405.02344,
BadFusion: 2D-Oriented Backdoor Attacks against 3D Object Detection,arXiv,2405,"Saket S. Chaturvedi, Lan Zhang, Wenbin Zhang, Pan He, Xiaoyong Yuan",Accepted at IJCAI 2024 Conference,https://arxiv.org/abs/2405.03884,
DarkFed: A Data-Free Backdoor Attack in Federated Learning,arXiv,2405,"Minghui Li, Wei Wan, Yuxuan Ning, Shengshan Hu, Lulu Xue, Leo Yu Zhang, Yichen Wang",This paper has been accepted by IJCAI 2024,https://arxiv.org/abs/2405.03299,
Explanation as a Watermark: Towards Harmless and Multi-bit Model Ownership Verification via Watermarking Feature Attribution,arXiv,2405,"Shuo Shao, Yiming Li, Hongwei Yao, Yiling He, Zhan Qin, Kui Ren",,https://arxiv.org/abs/2405.04825,
Lazy Layers to Make Fine-Tuned Diffusion Models More Traceable,arXiv,2405,"Haozhe Liu, Wentian Zhang, Bing Li, Bernard Ghanem, Jürgen Schmidhuber",,https://arxiv.org/abs/2405.00466,
Poisoning-based Backdoor Attacks for Arbitrary Target Label with Positive Triggers,arXiv,2405,"Binxiao Huang, Jason Chun Lok, Chang Liu, Ngai Wong",,https://arxiv.org/abs/2405.05573,
Towards Robust Physical-world Backdoor Attacks on Lane Detection,arXiv,2405,"Xinwei Zhang, Aishan Liu, Tianyuan Zhang, Siyuan Liang, Xianglong Liu",,https://arxiv.org/abs/2405.05553,
Unlearning Backdoor Attacks through Gradient-Based Model Pruning,arXiv,2405,"Kealan Dunnett, Reza Arablouei, Dimity Miller, Volkan Dedeoglu, Raja Jurdak",,https://arxiv.org/abs/2405.03918,
Watermarking Neuromorphic Brains: Intellectual Property Protection in Spiking Neural Networks,arXiv,2405,"Hamed Poursiami, Ihsen Alouani, Maryam Parsa","7 pages, 7 figures",https://arxiv.org/abs/2405.04049,
A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks,arXiv,2404,Orson Mengara,"Accept by ""IEEE Access"" let's take a look at our global approach to the DNN(s) model(s) deployment chain in production: Danger NLP-Speech (Trigger universal approach)",https://arxiv.org/abs/2404.00076,
A Clean-graph Backdoor Attack against Graph Convolutional Networks with Poisoned Label Only,arXiv,2404,"Jiazhu Dai, Haoyu Sun",,https://arxiv.org/abs/2404.12704,
Assessing Cybersecurity Vulnerabilities in Code Large Language Models,arXiv,2404,"Md Imran Hossen, Jianyi Zhang, Yinzhi Cao, Xiali Hei",,https://arxiv.org/abs/2404.18567,
Backdoor Attack on Multilingual Machine Translation,arXiv,2404,"Jun Wang, Qiongkai Xu, Xuanli He, Benjamin I. P. Rubinstein, Trevor Cohn",NAACL main long paper,https://arxiv.org/abs/2404.02393,
Backdoor Attacks and Defenses on Semantic-Symbol Reconstruction in Semantic Communications,arXiv,2404,"Yuan Zhou, Rose Qingyang Hu, Yi Qian",This paper has been accepted by IEEE ICC 2024,https://arxiv.org/abs/2404.13279,
Backdoor Contrastive Learning via Bi-level Trigger Optimization,arXiv,2404,"Weiyu Sun, Xinyu Zhang, Hao Lu, Yingcong Chen, Ting Wang, Jinghui Chen, Lu Lin",Accepted by ICLR 2024,https://arxiv.org/abs/2404.07863,https://github.com/SWY666/SSL-backdoor-BLTO.
Beyond Traditional Threats: A Persistent Backdoor Attack on Federated Learning,arXiv,2404,"Tao Liu, Yuhang Zhang, Zhu Feng, Zhiqin Yang, Chen Xu, Dapeng Man, Wu Yang",,https://arxiv.org/abs/2404.17617,https://github.com/PhD-TaoLiu/FCBA.
Causal Deconfounding via Confounder Disentanglement for Dual-Target Cross-Domain Recommendation,arXiv,2404,"Jiajie Zhu, Yan Wang, Feng Zhu, Zhu Sun",,https://arxiv.org/abs/2404.11180,
CloudFort: Enhancing Robustness of 3D Point Cloud Classification Against Backdoor Attacks via Spatial Partitioning and Ensemble Prediction,arXiv,2404,"Wenhao Lan, Yijun Yang, Haihua Shen, Shan Li",,https://arxiv.org/abs/2404.14042,
Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs,arXiv,2404,"Javier Rando, Francesco Croce, Kryštof Mitka, Stepan Shabalin, Maksym Andriushchenko, Nicolas Flammarion, Florian Tramèr",Competition Report,https://arxiv.org/abs/2404.14461,
Concept Arithmetics for Circumventing Concept Inhibition in Diffusion Models,arXiv,2404,"Vitali Petsiuk, Kate Saenko",,https://arxiv.org/abs/2404.13706,
Decomposing and Editing Predictions by Modeling Model Computation,arXiv,2404,"Harshay Shah, Andrew Ilyas, Aleksander Madry",,https://arxiv.org/abs/2404.11534,https://github.com/MadryLab/modelcomponents
Detector Collapse: Backdooring Object Detection to Catastrophic Overload or Blindness,arXiv,2404,"Hangtao Zhang, Shengshan Hu, Yichen Wang, Leo Yu Zhang, Ziqi Zhou, Xianlong Wang, Yanjun Zhang, Chao Chen",Accepted by IJCAI-24,https://arxiv.org/abs/2404.11357,
Dual Model Replacement:invisible Multi-target Backdoor Attack based on Federal Learning,arXiv,2404,"Rong Wang, Guichen Zhou, Mingjun Gao, Yunpeng Xiao",,https://arxiv.org/abs/2404.13946,
Exploring Backdoor Vulnerabilities of Chat Models,arXiv,2404,"Yunzhuo Hao, Wenkai Yang, Yankai Lin",Code and data are available at https://github.com/hychaochao/Chat-Models-Backdoor-Attacking,https://arxiv.org/abs/2404.02406,https://github.com/hychaochao/Chat-Models-Backdoor-Attacking
Fragile Model Watermark for integrity protection: leveraging boundary volatility and sensitive sample-pairing,arXiv,2404,"ZhenZhe Gao, Zhenjun Tang, Zhaoxia Yin, Baoyuan Wu, Yue Lu",The article has been accepted by IEEE International Conference on Multimedia and Expo 2024,https://arxiv.org/abs/2404.07572,
How to Craft Backdoors with Unlabeled Data Alone?,arXiv,2404,"Yifei Wang, Wenhan Ma, Stefanie Jegelka, Yisen Wang",Accepted at ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models (DPFM),https://arxiv.org/abs/2404.06694,https://github.com/PKU-ML/nlb.
LSP Framework: A Compensatory Model for Defeating Trigger Reverse Engineering via Label Smoothing Poisoning,arXiv,2404,"Beichen Li, Yuanfang Guo, Heqi Peng, Yangxi Li, Yunhong Wang",,https://arxiv.org/abs/2404.12852,
Let's Focus: Focused Backdoor Attack against Federated Transfer Learning,arXiv,2404,"Marco Arazzi, Stefanos Koffas, Antonino Nocera, Stjepan Picek",,https://arxiv.org/abs/2404.19420,
On the critical path to implant backdoors and the effectiveness of potential mitigation techniques: Early learnings from XZ,arXiv,2404,"Mario Lins, René Mayrhofer, Michael Roland, Daniel Hofer, Martin Schwaighofer",,https://arxiv.org/abs/2404.08987,
Physical Backdoor Attack can Jeopardize Driving with Vision-Large-Language Models,arXiv,2404,"Zhenyang Ni, Rui Ye, Yuxi Wei, Zhen Xiang, Yanfeng Wang, Siheng Chen",,https://arxiv.org/abs/2404.12916,
Physical Backdoor: Towards Temperature-based Backdoor Attacks in the Physical World,arXiv,2404,"Wen Yin, Jian Lou, Pan Zhou, Yulai Xie, Dan Feng, Yuhua Sun, Tailai Zhang, Lichao Sun","To appear in CVPR 2024.11pages, 8 figures and 4 tables",https://arxiv.org/abs/2404.19417,
Poisoning Decentralized Collaborative Recommender System and Its Countermeasures,arXiv,2404,"Ruiqi Zheng, Liang Qu, Tong Chen, Kai Zheng, Yuhui Shi, Hongzhi Yin",,https://arxiv.org/abs/2404.01177,
Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models,arXiv,2404,"Yuxin Wen, Leo Marchyok, Sanghyun Hong, Jonas Geiping, Tom Goldstein, Nicholas Carlini",,https://arxiv.org/abs/2404.01231,
Privacy Backdoors: Stealing Data with Corrupted Pretrained Models,arXiv,2404,"Shanglun Feng, Florian Tramèr",Code at https://github.com/ShanglunFengatETHZ/PrivacyBackdoor,https://arxiv.org/abs/2404.00473,https://github.com/ShanglunFengatETHZ/PrivacyBackdoor
Severity Controlled Text-to-Image Generative Model Bias Manipulation,arXiv,2404,"Jordan Vice, Naveed Akhtar, Richard Hartley, Ajmal Mian","This research was supported by National Intelligence and Security Discovery Research Grants (project# NS220100007), funded by the Department of Defence Australia",https://arxiv.org/abs/2404.02530,
Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning,arXiv,2404,"Xiaopeng Xie, Ming Yan, Xiwen Zhou, Chenlong Zhao, Suli Wang, Yong Zhang, Joey Tianyi Zhou","10 pages, 6 figures, conference",https://arxiv.org/abs/2404.00461,
SpamDam: Towards Privacy-Preserving and Adversary-Resistant SMS Spam Detection,arXiv,2404,"Yekai Li, Rufan Zhang, Wenxin Rong, Xianghang Mi",,https://arxiv.org/abs/2404.09481,
The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data,arXiv,2404,"Zixuan Zhu, Rui Wang, Cong Zou, Lihua Jing","13 pages, 6 figures, published to ICCV",https://arxiv.org/abs/2404.11265,https://github.com/Zixuan-Zhu/VaB.
Towards Robust Trajectory Representations: Isolating Environmental Confounders with Causal Learning,arXiv,2404,"Kang Luo, Yuanshao Zhu, Wei Chen, Kun Wang, Zhengyang Zhou, Sijie Ruan, Yuxuan Liang",The paper has been accepted by IJCAI 2024,https://arxiv.org/abs/2404.14073,
Transferring Troubles: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning,arXiv,2404,"Xuanli He, Jun Wang, Qiongkai Xu, Pasquale Minervini, Pontus Stenetorp, Benjamin I. P. Rubinstein, Trevor Cohn",work in progress,https://arxiv.org/abs/2404.19597,
Trojan Detection in Large Language Models: Insights from The Trojan Detection Challenge,arXiv,2404,"Narek Maloyan, Ekansh Verma, Bulat Nutfullin, Bislan Ashinov",,https://arxiv.org/abs/2404.13660,
Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors,arXiv,2404,"Victoria Graf, Qin Liu, Muhao Chen",Accepted by NAACL 2024 Main Conference,https://arxiv.org/abs/2404.02356,
UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models,arXiv,2404,"Zihan Guan, Mengxuan Hu, Sheng Li, Anil Vullikanti","20 pages,18 figures",https://arxiv.org/abs/2404.01101,https://github.com/GuanZihan/official_UFID.
A general approach to enhance the survivability of backdoor attacks by decision path coupling,arXiv,2403,"Yufei Zhao, Dingji Wang, Bihuan Chen, Ziqian Chen, Xin Peng",,https://arxiv.org/abs/2403.02950,
AS-FIBA: Adaptive Selective Frequency-Injection for Backdoor Attack on Deep Face Restoration,arXiv,2403,"Zhenbo Song, Wenhao Gao, Kaihao Zhang, Wenhan Luo, Zhaoxin Fan, Jianfeng Lu",,https://arxiv.org/abs/2403.06430,
Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks,arXiv,2403,"Khondoker Murad Hossain, Tim Oates","6 pages, Accepted at the International Conference on Communications 2024. arXiv admin note: text overlap with arXiv:2212.08121",https://arxiv.org/abs/2403.08208,
An Embarrassingly Simple Defense Against Backdoor Attacks On SSL,arXiv,2403,"Aryan Satpathy, Nilaksh Nilaksh, Dhruva Rajwade","10 pages, 5 figures",https://arxiv.org/abs/2403.15918,https://github.com/Aryan-Satpathy/Backdoor.
Backdoor Attack with Mode Mixture Latent Modification,arXiv,2403,"Hongwei Zhang, Xiaoyin Xu, Dongsheng An, Xianfeng Gu, Min Zhang",,https://arxiv.org/abs/2403.07463,
Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency,arXiv,2403,"Soumyadeep Pal, Yuguang Yao, Ren Wang, Bingquan Shen, Sijia Liu",The Twelfth International Conference on Learning Representations (ICLR 2024),https://arxiv.org/abs/2403.10717,https://github.com/OPTML-Group/BackdoorMSPC.
BadEdit: Backdooring large language models by model editing,arXiv,2403,"Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing Liu, Wenhan Wang, Tianwei Zhang, Yang Liu",ICLR 2024,https://arxiv.org/abs/2403.13355,
CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation,arXiv,2403,"Baoyu Jing, Dawei Zhou, Kan Ren, Carl Yang",Preprint. Work in progress,https://arxiv.org/abs/2403.11960,
Causality-based Cross-Modal Representation Learning for Vision-and-Language Navigation,arXiv,2403,"Liuyi Wang, Zongtao He, Ronghao Dang, Huiyi Chen, Chengju Liu, Qijun Chen",16 pages,https://arxiv.org/abs/2403.03405,
Clean-image Backdoor Attacks,arXiv,2403,"Dazhong Rong, Guoyao Yu, Shuheng Shen, Xinyi Fu, Peng Qian, Jianhai Chen, Qinming He, Xing Fu, Weiqiang Wang",,https://arxiv.org/abs/2403.15010,
DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference,arXiv,2403,"Jialong Wu, Linhai Zhang, Deyu Zhou, Guoqiang Xu",Our code and results will be available at https://github.com/callanwu/DINER,https://arxiv.org/abs/2403.01166,https://github.com/callanwu/DINER
De-confounded Data-free Knowledge Distillation for Handling Distribution Shifts,arXiv,2403,"Yuzheng Wang, Dingkang Yang, Zhaoyu Chen, Yang Liu, Siao Liu, Wenqiang Zhang, Lihua Zhang, Lizhe Qi",Accepted by CVPR24,https://arxiv.org/abs/2403.19539,
Enhancing Adversarial Training with Prior Knowledge Distillation for Robust Image Compression,arXiv,2403,"Zhi Cao, Youneng Bao, Fanyang Meng, Chao Li, Wen Tan, Genhong Wang, Yongsheng Liang",,https://arxiv.org/abs/2403.06700,
Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications,arXiv,2403,"Fouad Trad, Ali Chehab",,https://arxiv.org/abs/2403.17787,
"Federated Learning: Attacks, Defenses, Opportunities, and Challenges",arXiv,2403,"Ghazaleh Shirvani, Saeid Ghasemshirazi, Behzad Beigzadeh",,https://arxiv.org/abs/2403.06067,
Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion,arXiv,2403,"Hossein Souri, Arpit Bansal, Hamid Kazemi, Liam Fowl, Aniruddha Saha, Jonas Geiping, Andrew Gordon Wilson, Rama Chellappa, Tom Goldstein, Micah Goldblum",,https://arxiv.org/abs/2403.16365,https://github.com/hsouri/GDP
Impart: An Imperceptible and Effective Label-Specific Backdoor Attack,arXiv,2403,"Jingke Zhao, Zan Wang, Yongwei Wang, Lanjun Wang",,https://arxiv.org/abs/2403.13017,
Invisible Backdoor Attack Through Singular Value Decomposition,arXiv,2403,"Wenmin Chen, Xiaowei Xu",,https://arxiv.org/abs/2403.13018,
LOTUS: Evasive and Resilient Backdoor Attacks through Sub-Partitioning,arXiv,2403,"Siyuan Cheng, Guanhong Tao, Yingqi Liu, Guangyu Shen, Shengwei An, Shiwei Feng, Xiangzhe Xu, Kaiyuan Zhang, Shiqing Ma, Xiangyu Zhang",IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024),https://arxiv.org/abs/2403.17188,https://github.com/Megum1/LOTUS.
LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario,arXiv,2403,"Hongyi Liu, Zirui Liu, Ruixiang Tang, Jiayi Yuan, Shaochen Zhong, Yu-Neng Chuang, Li Li, Rui Chen, Xia Hu",,https://arxiv.org/abs/2403.00108,
Manipulating Neural Path Planners via Slight Perturbations,arXiv,2403,"Zikang Xiong, Suresh Jagannathan",,https://arxiv.org/abs/2403.18256,
MirrorAttack: Backdoor Attack on 3D Point Cloud with a Distorting Mirror,arXiv,2403,"Yuhao Bian, Shengjing Tian, Xiuping Liu",15 pages,https://arxiv.org/abs/2403.05847,
Mitigating Label Flipping Attacks in Malicious URL Detectors Using Ensemble Trees,arXiv,2403,"Ehsan Nowroozi, Nada Jadalla, Samaneh Ghelichkhani, Alireza Jolfaei",,https://arxiv.org/abs/2403.02995,
On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder,arXiv,2403,"Tingxu Han, Shenghan Huang, Ziqi Ding, Weisong Sun, Yebo Feng, Chunrong Fang, Jun Li, Hanwei Qian, Cong Wu, Quanjun Zhang, Yang Liu, Zhenyu Chen",,https://arxiv.org/abs/2403.03846,
REPQC: Reverse Engineering and Backdooring Hardware Accelerators for Post-quantum Cryptography,arXiv,2403,"Samuel Pagliarini, Aikata Aikata, Malik Imran, Sujoy Sinha Roy",Accepted in AsiaCCS'24,https://arxiv.org/abs/2403.09352,
Real is not True: Backdoor Attacks Against Deepfake Detection,arXiv,2403,"Hong Sun, Ziqiang Li, Lei Liu, Bin Li",BigDIA 2023,https://arxiv.org/abs/2403.06610,
Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors,arXiv,2403,"Md Abdul Kadir, GowthamKrishna Addluri, Daniel Sonntag",,https://arxiv.org/abs/2403.16569,
Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs,arXiv,2403,"Jane Downer, Ren Wang, Binghui Wang",,https://arxiv.org/abs/2403.18136,
Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic Learning over Low-power Devices,arXiv,2403,"Hanqing Fu, Gaolei Li, Jun Wu, Jianhua Li, Xi Lin, Kai Zhou, Yuchen Liu",,https://arxiv.org/abs/2403.18607,
Task-Agnostic Detector for Insertion-Based Backdoor Attacks,arXiv,2403,"Weimin Lyu, Xiao Lin, Songzhu Zheng, Lu Pang, Haibin Ling, Susmit Jha, Chao Chen",Findings of NAACL 2024,https://arxiv.org/abs/2403.17155,
"Threats, Attacks, and Defenses in Machine Unlearning: A Survey",arXiv,2403,"Ziyao Liu, Huanyi Ye, Chen Chen, Kwok-Yan Lam",,https://arxiv.org/abs/2403.13682,
Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token Unlearning,arXiv,2403,"Siyuan Liang, Kuanrong Liu, Jiajun Gong, Jiawei Liang, Yuan Xun, Ee-Chien Chang, Xiaochun Cao","6 pages, 2 figures",https://arxiv.org/abs/2403.16257,
WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection,arXiv,2403,"Anudeex Shetty, Yue Teng, Ke He, Qiongkai Xu",Work in Progress,https://arxiv.org/abs/2403.01472,
A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents,arXiv,2402,"Lingbo Mo, Zeyi Liao, Boyuan Zheng, Yu Su, Chaowei Xiao, Huan Sun",,https://arxiv.org/abs/2402.10196,
Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space,arXiv,2402,"Zongru Wu, Zhuosheng Zhang, Pengzhou Cheng, Gongshen Liu",,https://arxiv.org/abs/2402.12026,https://github.com/ZrW00/MuScleLoRA.
Architectural Neural Backdoors from First Principles,arXiv,2402,"Harry Langford, Ilia Shumailov, Yiren Zhao, Robert Mullins, Nicolas Papernot",,https://arxiv.org/abs/2402.06957,
Backdoor Attack against One-Class Sequential Anomaly Detection Models,arXiv,2402,"He Cheng, Shuhan Yuan",This work is accepted by the PAKDD 2024. 12 pages,https://arxiv.org/abs/2402.10283,
Backdoor Attacks on Dense Passage Retrievers for Disseminating Misinformation,arXiv,2402,"Quanyu Long, Yue Deng, LeiLei Gan, Wenya Wang, Sinno Jialin Pan",,https://arxiv.org/abs/2402.13532,
Corrective Machine Unlearning,arXiv,2402,"Shashwat Goel, Ameya Prabhu, Philip Torr, Ponnurangam Kumaraguru, Amartya Sanyal","17 pages, 7 figures",https://arxiv.org/abs/2402.14015,
Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning,arXiv,2402,"Shuai Zhao, Leilei Gan, Luu Anh Tuan, Jie Fu, Lingjuan Lyu, Meihuizi Jia, Jinming Wen",NAACL Findings 2024,https://arxiv.org/abs/2402.12168,
DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models,arXiv,2402,"Yang Sui, Huy Phan, Jinqi Xiao, Tianfang Zhang, Zijie Tang, Cong Shi, Yan Wang, Yingying Chen, Bo Yuan",,https://arxiv.org/abs/2402.02739,
Domain Generalization via Causal Adjustment for Cross-Domain Sentiment Analysis,arXiv,2402,"Siyin Wang, Jie Zhou, Qin Chen, Qi Zhang, Tao Gui, Xuanjing Huang",,https://arxiv.org/abs/2402.14536,
Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,arXiv,2402,"Shen Li, Liuyi Yao, Jinyang Gao, Lan Zhang, Yaliang Li",,https://arxiv.org/abs/2402.14883,
Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge,arXiv,2402,"Ansh Arora, Xuanli He, Maximilian Mozes, Srinibas Swain, Mark Dras, Qiongkai Xu",work in progress,https://arxiv.org/abs/2402.19334,
Learning to Poison Large Language Models During Instruction Tuning,arXiv,2402,"Yao Qiang, Xiangyu Zhou, Saleh Zare Zade, Mohammad Amin Roshani, Douglas Zytko, Dongxiao Zhu",,https://arxiv.org/abs/2402.13459,GitHub
Low-Frequency Black-Box Backdoor Attack via Evolutionary Algorithm,arXiv,2402,"Yanqi Qiao, Dazhuang Liu, Rui Wang, Kaitai Liang",,https://arxiv.org/abs/2402.15653,
Measuring Impacts of Poisoning on Model Parameters and Neuron Activations: A Case Study of Poisoning CodeBERT,arXiv,2402,"Aftab Hussain, Md Rafiqul Islam Rabin, Navid Ayoobi, Mohammad Amin Alipour",,https://arxiv.org/abs/2402.12936,
Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment,arXiv,2402,"Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Junjie Hu, Yixuan Li, Patrick McDaniel, Muhao Chen, Bo Li, Chaowei Xiao",,https://arxiv.org/abs/2402.14968,
Model Pairing Using Embedding Translation for Backdoor Attack Detection on Open-Set Classification Tasks,arXiv,2402,"Alexander Unnervik, Hatef Otroshi Shahreza, Anjith George, Sébastien Marcel",Under review,https://arxiv.org/abs/2402.18718,
Model X-ray:Detect Backdoored Models via Decision Boundary,arXiv,2402,"Yanghao Su, Jie Zhang, Ting Xu, Tianwei Zhang, Weiming Zhang, Nenghai Yu",,https://arxiv.org/abs/2402.17465,
Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models,arXiv,2402,"Hongbin Liu, Michael K. Reiter, Neil Zhenqiang Gong","To appear in USENIX Security Symposium, 2024",https://arxiv.org/abs/2402.14977,
On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem,arXiv,2402,"Georg Pichler, Marco Romanelli, Divya Prakash Manivannan, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg",,https://arxiv.org/abs/2402.16926,
OrderBkd: Textual backdoor attack through repositioning,arXiv,2402,"Irina Alekseevskaia, Konstantin Arkhipenko",,https://arxiv.org/abs/2402.07689,https://github.com/alekseevskaia/OrderBkd.
Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection,arXiv,2402,"Jiawei Liang, Siyuan Liang, Aishan Liu, Xiaojun Jia, Junhao Kuang, Xiaochun Cao",ICLR 2024 Spotlight,https://arxiv.org/abs/2402.11473,\url{https://github.com/JWLiang007/PFF}
"Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization",arXiv,2402,"Rui Zhang, Hongwei Li, Rui Wen, Wenbo Jiang, Yuan Zhang, Michael Backes, Yun Shen, Yang Zhang",,https://arxiv.org/abs/2402.09179,
SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning,arXiv,2402,"Yu Zhang, Hui-Ling Zhen, Zehua Pei, Yingzhao Lian, Lihao Yin, Mingxuan Yuan, Bei Yu",,https://arxiv.org/abs/2402.11903,
Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models,arXiv,2402,"Pengzhou Cheng, Wei Du, Zongru Wu, Fengwei Zhang, Libo Chen, Gongshen Liu","16 pages, 16 figures, 13 tables",https://arxiv.org/abs/2402.18945,
Test-Time Backdoor Attacks on Multimodal Large Language Models,arXiv,2402,"Dong Lu, Tianyu Pang, Chao Du, Qian Liu, Xianjun Yang, Min Lin",,https://arxiv.org/abs/2402.08577,https://sail-sg.github.io/AnyDoor/.
The last Dance : Robust backdoor attack via diffusion models and bayesian approach,arXiv,2402,Orson Mengara,"Preprint (Last update): audio backdoor attack on Hugging Face's Transformer pre-trained models. This attack incorporates state-of-the-art Bayesian techniques, a modified Fokker-Planck equation (via Yang-Mills), and a diffusion model approach",https://arxiv.org/abs/2402.05967,
Time-Distributed Backdoor Attacks on Federated Spiking Learning,arXiv,2402,"Gorka Abad, Stjepan Picek, Aitor Urbieta",,https://arxiv.org/abs/2402.02886,
Universal Post-Training Reverse-Engineering Defense Against Backdoors in Deep Neural Networks,arXiv,2402,"Xi Li, Hang Wang, David J. Miller, George Kesidis",,https://arxiv.org/abs/2402.02034,
VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models,arXiv,2402,"Jiawei Liang, Siyuan Liang, Man Luo, Aishan Liu, Dongchen Han, Ee-Chien Chang, Xiaochun Cao",,https://arxiv.org/abs/2402.13851,
Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents,arXiv,2402,"Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, Xu Sun",The first two authors contribute equally. Code and data are available at https://github.com/lancopku/agent-backdoor-attacks,https://arxiv.org/abs/2402.11208,https://github.com/lancopku/agent-backdoor-attacks
A backdoor attack against link prediction tasks with graph neural networks,arXiv,2401,"Jiazhu Dai, Haoyu Sun",,https://arxiv.org/abs/2401.02663,
A clean-label graph backdoor attack method in node classification task,arXiv,2401,"Xiaogang Xing, Ming Xu, Yujing Bai, Dongdong Yang",14pages,https://arxiv.org/abs/2401.00163,
Adaptive Discounting of Training Time Attacks,arXiv,2401,"Ridhima Bector, Abhay Aradhya, Chai Quek, Zinovi Rabinovich","19 pages, 7 figures",https://arxiv.org/abs/2401.02652,"""bit.ly/github-rb-gDDPG""."
Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP,arXiv,2401,"Ruinan Jin, Chun-Yin Huang, Chenyu You, Xiaoxiao Li",Paper Accepted at the 2nd IEEE Conference on Secure and Trustworthy Machine Learning,https://arxiv.org/abs/2401.01911,
BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning,arXiv,2401,"Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Mingli Zhu, Ruotong Wang, Li Liu, Chao Shen",,https://arxiv.org/abs/2401.15002,
BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models,arXiv,2401,"Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, Bo Li",Accepted to ICLR2024,https://arxiv.org/abs/2401.12242,
Can We Trust the Unlabeled Target Data? Towards Backdoor Attack and Defense on Model Adaptation,arXiv,2401,"Lijun Sheng, Jian Liang, Ran He, Zilei Wang, Tieniu Tan","11 pages, 4 figures",https://arxiv.org/abs/2401.06030,
Detecting Face Synthesis Using a Concealed Fusion Model,arXiv,2401,"Roberto Leyva, Victor Sanchez, Gregory Epiphaniou, Carsten Maple",,https://arxiv.org/abs/2401.04257,
Does Few-shot Learning Suffer from Backdoor Attacks?,arXiv,2401,"Xinwei Liu, Xiaojun Jia, Jindong Gu, Yuan Xun, Siyuan Liang, Xiaochun Cao",AAAI2024,https://arxiv.org/abs/2401.01377,
End-to-End Anti-Backdoor Learning on Images and Time Series,arXiv,2401,"Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, Yige Li, James Bailey",,https://arxiv.org/abs/2401.03215,
"Federated Unlearning: A Survey on Methods, Design Guidelines, and Evaluation Metrics",arXiv,2401,"Nicolò Romandini, Alessio Mora, Carlo Mazzocca, Rebecca Montanari, Paolo Bellavista","23 pages, 8 figures, and 6 tables",https://arxiv.org/abs/2401.05146,
Hijacking Attacks against Neural Networks by Analyzing Training Data,arXiv,2401,"Yunjie Ge, Qian Wang, Huayang Huang, Qi Li, Cong Wang, Chao Shen, Lingchen Zhao, Peipei Jiang, Zheng Fang, Shenyi Zhang","Full version with major polishing, compared to the Usenix Security 2024 edition",https://arxiv.org/abs/2401.09740,
Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control,arXiv,2401,"Ka-Ho Chow, Wenqi Wei, Lei Yu",,https://arxiv.org/abs/2401.01085,
Inferring Properties of Graph Neural Networks,arXiv,2401,"Dat Nguyen, Hieu M. Vu, Cong-Thanh Le, Bach Le, David Lo, ThanhVu Nguyen, Corina Pasareanu","20 pages main paper, 10 pages for appendix",https://arxiv.org/abs/2401.03790,
Instructional Fingerprinting of Large Language Models,arXiv,2401,"Jiashu Xu, Fei Wang, Mingyu Derek Ma, Pang Wei Koh, Chaowei Xiao, Muhao Chen",Accepted at NAACL 2024; 30 pages,https://arxiv.org/abs/2401.12255,https://cnut1648.github.io/Model-Fingerprint/.
Is It Possible to Backdoor Face Forgery Detection with Natural Triggers?,arXiv,2401,"Xiaoxuan Han, Songlin Yang, Wei Wang, Ziwen He, Jing Dong",,https://arxiv.org/abs/2401.00414,
Learning Backdoors for Mixed Integer Programs with Contrastive Learning,arXiv,2401,"Junyang Cai, Taoan Huang, Bistra Dilkina",,https://arxiv.org/abs/2401.10467,
MEA-Defender: A Robust Watermark against Model Extraction Attack,arXiv,2401,"Peizhuo Lv, Hualong Ma, Kai Chen, Jiachen Zhou, Shengzhi Zhang, Ruigang Liang, Shenchen Zhu, Pan Li, Yingjun Zhang","To Appear in IEEE Symposium on Security and Privacy 2024 (IEEE S&P 2024), MAY 20-23, 2024, SAN FRANCISCO, CA, USA",https://arxiv.org/abs/2401.15239,
MalModel: Hiding Malicious Payload in Mobile Deep Learning Models with Black-box Backdoor Attack,arXiv,2401,"Jiayi Hua, Kailong Wang, Meizhen Wang, Guangdong Bai, Xiapu Luo, Haoyu Wang","Due to the limitation ""The abstract field cannot be longer than 1,920 characters"", the abstract here is shorter than that in the PDF file",https://arxiv.org/abs/2401.02659,
"Multi-Trigger Backdoor Attacks: More Triggers, More Threats",arXiv,2401,"Yige Li, Xingjun Ma, Jiabo He, Hanxun Huang, Yu-Gang Jiang",,https://arxiv.org/abs/2401.15295,
Object-oriented backdoor attack against image captioning,arXiv,2401,"Meiling Li, Nan Zhong, Xinpeng Zhang, Zhenxing Qian, Sheng Li",,https://arxiv.org/abs/2401.02600,
SSL-OTA: Unveiling Backdoor Threats in Self-Supervised Learning for Object Detection,arXiv,2401,"Qiannan Wang, Changchun Yin, Liming Fang, Lu Zhou, Zhe Liu, Run Wang, Chenhao Lin",,https://arxiv.org/abs/2401.00137,
Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training,arXiv,2401,"Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec",updated to add missing acknowledgements,https://arxiv.org/abs/2401.05566,
Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack,arXiv,2401,"Ruofei Wang, Renjie Wan, Zongyu Guo, Qing Guo, Rui Huang",Accepted by ICASSP2024,https://arxiv.org/abs/2401.02031,
TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks,arXiv,2401,"Khondoker Murad Hossain, Tim Oates",,https://arxiv.org/abs/2401.05432,
The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers,arXiv,2401,Orson Mengara,"Accepted by AAAI Workshop 2024, 8 pages",https://arxiv.org/abs/2401.01537,
"The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",arXiv,2401,"Haonan Wang, Qianli Shen, Yao Tong, Yang Zhang, Kenji Kawaguchi","This study reveals that by subtly inserting non-copyright-infringing poisoning data into a diffusion model's training dataset, it's possible to trigger the model to generate copyrighted content, highlighting vulnerabilities in current copyright protection strategies",https://arxiv.org/abs/2401.04136,
TransTroj: Transferable Backdoor Attacks to Pre-trained Models via Embedding Indistinguishability,arXiv,2401,"Hao Wang, Tao Xiang, Shangwei Guo, Jialing He, Hangcheng Liu, Tianwei Zhang","13 pages, 16 figures, 5 tables",https://arxiv.org/abs/2401.15883,https://github.com/haowang-cqu/TransTroj
Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning,arXiv,2401,"Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Fengjun Pan, Jinming Wen",,https://arxiv.org/abs/2401.05949,
WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition,arXiv,2401,"Zhengyao Song, Yongqiang Li, Danni Yuan, Li Liu, Shaokui Wei, Baoyuan Wu","13 pages, 21 figures",https://arxiv.org/abs/2401.13578,
AI Control: Improving Safety Despite Intentional Subversion,arXiv,2312,"Ryan Greenblatt, Buck Shlegeris, Kshitij Sachan, Fabien Roger",Edit: Fix minor typos and clarify abstract,https://arxiv.org/abs/2312.06942,
Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks,arXiv,2312,"Danni Yuan, Shaokui Wei, Mingda Zhang, Li Liu, Baoyuan Wu",,https://arxiv.org/abs/2312.06230,https://github.com/SCLBD/bdzoo2/blob/dev/detection_pretrain/agpd.py.
BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting,arXiv,2312,"Huming Qiu, Junjie Sun, Mi Zhang, Xudong Pan, Min Yang","To Appear in the 45th IEEE Symposium on Security and Privacy, May 20-23, 2024",https://arxiv.org/abs/2312.04902,
BadRL: Sparse Targeted Backdoor Attack Against Reinforcement Learning,arXiv,2312,"Jing Cui, Yufei Han, Yuzhe Ma, Jianbin Jiao, Junge Zhang",Extended version of the submission accepted by AAAI 2024. It is revised by integrating review comments,https://arxiv.org/abs/2312.12585,
"Data and Model Poisoning Backdoor Attacks on Wireless Federated Learning, and the Defense Mechanisms: A Comprehensive Survey",arXiv,2312,"Yichen Wan, Youyang Qu, Wei Ni, Yong Xiang, Longxiang Gao, Ekram Hossain",,https://arxiv.org/abs/2312.08667,
DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models,arXiv,2312,"Jiachen Zhou, Peizhuo Lv, Yibing Lan, Guozhu Meng, Kai Chen, Hualong Ma",Accepted by AAAI2024,https://arxiv.org/abs/2312.11057,
Decomposing Hard SAT Instances with Metaheuristic Optimization,arXiv,2312,"Daniil Chivilikhin, Artem Pavlenko, Alexander Semenov",This is a preprint of the paper published in Intern. J. Artificial Intelligence. 2023. V. 21. No. 2. P. 61-92,https://arxiv.org/abs/2312.10436,
Defenses in Adversarial Machine Learning: A Survey,arXiv,2312,"Baoyuan Wu, Shaokui Wei, Mingli Zhu, Meixi Zheng, Zihao Zhu, Mingda Zhang, Hongrui Chen, Danni Yuan, Li Liu, Qingshan Liu","21 pages, 5 figures, 2 tables, 237 reference papers",https://arxiv.org/abs/2312.08890,
Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift,arXiv,2312,"Shengwei An, Sheng-Yen Chou, Kaiyuan Zhang, Qiuling Xu, Guanhong Tao, Guangyu Shen, Siyuan Cheng, Shiqing Ma, Pin-Yu Chen, Tsung-Yi Ho, Xiangyu Zhang",AAAI 2024,https://arxiv.org/abs/2312.00050,
Enhancing Robustness of Foundation Model Representations under Provenance-related Distribution Shifts,arXiv,2312,"Xiruo Ding, Zhecheng Sheng, Brian Hur, Feng Chen, Serguei V. S. Pakhomov, Trevor Cohen","Accepted in Workshop on Distribution Shifts, 37th Conference on Neural Information Processing Systems (NeurIPS 2023)",https://arxiv.org/abs/2312.05435,
Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking,arXiv,2312,"Shengsheng Qian, Yifei Wang, Dizhan Xue, Shengjie Zhang, Huaiwen Zhang, Changsheng Xu",,https://arxiv.org/abs/2312.07955,https://github.com/LivXue/PoisonCAM.
FedBayes: A Zero-Trust Federated Learning Aggregation to Defend Against Adversarial Attacks,arXiv,2312,"Marc Vucovich, Devin Quinn, Kevin Choi, Christopher Redino, Abdul Rahman, Edward Bowen",Accepted to IEEE CCWC 2024,https://arxiv.org/abs/2312.04587,
FlowMur: A Stealthy and Practical Audio Backdoor Attack with Limited Knowledge,arXiv,2312,"Jiahe Lan, Jie Wang, Baochen Yan, Zheng Yan, Elisa Bertino",To appear at lEEE Symposium on Security & Privacy (Oakland) 2024,https://arxiv.org/abs/2312.09665,
FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning Attacks in Federated Learning,arXiv,2312,"Hossein Fereidooni, Alessandro Pegoraro, Phillip Rieger, Alexandra Dmitrienko, Ahmad-Reza Sadeghi","To appear in the Network and Distributed System Security (NDSS) Symposium 2024. 16 pages, 8 figures, 12 tables, 1 algorithm, 3 equations",https://arxiv.org/abs/2312.04432,
Manipulating Trajectory Prediction with Backdoors,arXiv,2312,"Kaouther Messaoud, Kathrin Grosse, Mickael Chen, Matthieu Cord, Patrick Pérez, Alexandre Alahi","9 pages, 7 figures",https://arxiv.org/abs/2312.13863,
OCGEC: One-class Graph Embedding Classification for DNN Backdoor Detection,arXiv,2312,"Haoyu Jiang, Haiyang Yu, Nan Li, Ping Yi",v2,https://arxiv.org/abs/2312.01585,https://github.com/jhy549/OCGEC.
On the Difficulty of Defending Contrastive Learning against Backdoor Attacks,arXiv,2312,"Changjiang Li, Ren Pang, Bochuan Cao, Zhaohan Xi, Jinghui Chen, Shouling Ji, Ting Wang",USENIX Security 24,https://arxiv.org/abs/2312.09057,
Performance-lossless Black-box Model Watermarking,arXiv,2312,"Na Zhao, Kejiang Chen, Weiming Zhang, Nenghai Yu",,https://arxiv.org/abs/2312.06488,
Pre-trained Trojan Attacks for Visual Recognition,arXiv,2312,"Aishan Liu, Xinwei Zhang, Yisong Xiao, Yuguang Zhou, Siyuan Liang, Jiakai Wang, Xianglong Liu, Xiaochun Cao, Dacheng Tao",19 pages,https://arxiv.org/abs/2312.15172,
Progressive Poisoned Data Isolation for Training-time Backdoor Defense,arXiv,2312,"Yiming Chen, Haiwei Wu, Jiantao Zhou",Accepted to AAAI2024,https://arxiv.org/abs/2312.12724,
Punctuation Matters! Stealthy Backdoor Attack for Language Models,arXiv,2312,"Xuan Sheng, Zhicheng Li, Zhaoyang Han, Xiangmao Chang, Piji Li",NLPCC 2023,https://arxiv.org/abs/2312.15867,
Robust Backdoor Detection for Deep Learning via Topological Evolution Dynamics,arXiv,2312,"Xiaoxing Mo, Yechao Zhang, Leo Yu Zhang, Wei Luo, Nan Sun, Shengshan Hu, Shang Gao, Yang Xiang",18 pages. To appear in IEEE Symposium on Security and Privacy 2024,https://arxiv.org/abs/2312.02673,GitHub.
Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections,arXiv,2312,"Yuanpu Cao, Bochuan Cao, Jinghui Chen",,https://arxiv.org/abs/2312.00027,
Synthesizing Physical Backdoor Datasets: An Automated Framework Leveraging Deep Generative Models,arXiv,2312,"Sze Jue Yang, Chinh D. La, Quang H. Nguyen, Kok-Seng Wong, Anh Tuan Tran, Chee Seng Chan, Khoa D. Doan",,https://arxiv.org/abs/2312.03419,
Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger,arXiv,2312,"Yiming Li, Mingyan Zhu, Junfeng Guo, Tao Wei, Shu-Tao Xia, Zhan Qin",14 pages,https://arxiv.org/abs/2312.04584,
UCCA: A Verified Architecture for Compartmentalization of Untrusted Code Sections in Resource-Constrained Devices,arXiv,2312,"Liam Tyler, Ivan De Oliveira Nunes",,https://arxiv.org/abs/2312.02348,
UltraClean: A Simple Framework to Train Robust Neural Networks against Backdoor Attacks,arXiv,2312,"Bingyin Zhao, Yingjie Lao",,https://arxiv.org/abs/2312.10657,https://github.com/bxz9200/UltraClean.
Universal Backdoor Attacks,arXiv,2312,"Benjamin Schneider, Nils Lukas, Florian Kerschbaum",Accepted for publication at ICLR 2024,https://arxiv.org/abs/2312.00157,https://github.com/Ben-Schneider-code/Universal-Backdoor-Attacks.
Who Leaked the Model? Tracking IP Infringers in Accountable Federated Learning,arXiv,2312,"Shuyang Yu, Junyuan Hong, Yi Zeng, Fei Wang, Ruoxi Jia, Jiayu Zhou",,https://arxiv.org/abs/2312.03205,
A Survey on Vulnerability of Federated Learning: A Learning Algorithm Perspective,arXiv,2311,"Xianghua Xie, Chen Hu, Hanchi Ren, Jingjing Deng",https://github.com/Rand2AI/Awesome-Vulnerability-of-Federated-Learning,https://arxiv.org/abs/2311.16065,https://github.com/Rand2AI/Awesome-Vulnerability-of-Federated-Learning
AGNES: Abstraction-guided Framework for Deep Neural Networks Security,arXiv,2311,"Akshay Dhonthi, Marcello Eiermann, Ernst Moritz Hahn, Vahid Hashemi","14 pages, 6 Figures, 4 Tables, Accepted at 25th International Conference on Verification, Model Checking, and Abstract Interpretation (VMCAI 2024)",https://arxiv.org/abs/2311.04009,
Attacks of fairness in Federated Learning,arXiv,2311,"Joseph Rance, Filip Svoboda",,https://arxiv.org/abs/2311.12715,
BAGEL: Backdoor Attacks against Federated Contrastive Learning,arXiv,2311,"Yao Huang, Kongyang Chen, Jiannong Cao, Jiaxing Shen, Shaowei Wang, Yun Peng, Weilong Peng, Kechao Cai",,https://arxiv.org/abs/2311.16113,
Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment,arXiv,2311,"Haoran Wang, Kai Shu",,https://arxiv.org/abs/2311.09433,https://github.com/wang2226/Backdoor-Activation-Attack
Backdoor Threats from Compromised Foundation Models to Federated Learning,arXiv,2311,"Xi Li, Songhe Wang, Chen Wu, Hao Zhou, Jiaqi Wang",This paper has been accepted by FL@FM-NeurIPS 23 (International Workshop on Federated Learning in the Age of Foundation Models in Conjunction with NeurIPS 2023). The corresponding author is Jiaqi Wang (jqwang@psu.edu),https://arxiv.org/abs/2311.00144,
BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning,arXiv,2311,"Siyuan Liang, Mingli Zhu, Aishan Liu, Baoyuan Wu, Xiaochun Cao, Ee-Chien Chang",The paper lacks some work that needs to be cited,https://arxiv.org/abs/2311.12075,
BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP,arXiv,2311,"Jiawang Bai, Kuofeng Gao, Shaobo Min, Shu-Tao Xia, Zhifeng Li, Wei Liu","14 pages, 6 figures",https://arxiv.org/abs/2311.16194,
Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems,arXiv,2311,"Guangjing Wang, Ce Zhou, Yuanda Wang, Bocheng Chen, Hanqing Guo, Qiben Yan",,https://arxiv.org/abs/2311.11796,
Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models,arXiv,2311,"Yueqing Liang, Lu Cheng, Ali Payani, Kai Shu",Under review,https://arxiv.org/abs/2311.09428,
Can We Trust the Similarity Measurement in Federated Learning?,arXiv,2311,"Zhilin Wang, Qin Hu, Xukai Zou",,https://arxiv.org/abs/2311.03369,
Distributed Attacks over Federated Reinforcement Learning-enabled Cell Sleep Control,arXiv,2311,"Han Zhang, Hao Zhou, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Yigit Ozcan, Melike Erol-Kantarci",,https://arxiv.org/abs/2311.15894,
Does Differential Privacy Prevent Backdoor Attacks in Practice?,arXiv,2311,"Fereshteh Razmi, Jian Lou, Li Xiong",,https://arxiv.org/abs/2311.06227,
Effective Backdoor Mitigation Depends on the Pre-training Objective,arXiv,2311,"Sahil Verma, Gantavya Bhatt, Avi Schwarzschild, Soumye Singhal, Arnav Mohanty Das, Chirag Shah, John P Dickerson, Jeff Bilmes",Accepted for oral presentation at BUGS workshop @ NeurIPS 2023 (https://neurips2023-bugs.github.io/),https://arxiv.org/abs/2311.14948,(https://neurips2023-bugs.github.io/)
Efficient Trigger Word Insertion,arXiv,2311,"Yueqi Zeng, Ziqiang Li, Pengfei Xia, Lei Liu, Bin Li",,https://arxiv.org/abs/2311.13957,
FedTruth: Byzantine-Robust and Backdoor-Resilient Federated Learning Framework,arXiv,2311,"Sheldon C. Ebron Jr., Kan Yang",,https://arxiv.org/abs/2311.10248,
From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects in Diffusion Models,arXiv,2311,"Zhuoshi Pan, Yuguang Yao, Gaowen Liu, Bingquan Shen, H. Vicky Zhao, Ramana Rao Kompella, Sijia Liu","10 pages, 6 figures, 7 tables",https://arxiv.org/abs/2311.02373,https://github.com/OPTML-Group/BiBadDiff.
FunctionMarker: Watermarking Language Datasets via Knowledge Injection,arXiv,2311,"Shuai Li, Kejiang Chen, Kunsheng Tang, Wen Huang, Jie Zhang, Weiming Zhang, Nenghai Yu",,https://arxiv.org/abs/2311.09535,
Mitigating Backdoors within Deep Neural Networks in Data-limited Configuration,arXiv,2311,"Soroush Hashemifar, Saeed Parsa, Morteza Zakeri-Nasrabadi",,https://arxiv.org/abs/2311.07417,
On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models,arXiv,2311,"Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, Chaowei Xiao",,https://arxiv.org/abs/2311.09641,
Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective,arXiv,2311,"Ming-Yu Chung, Sheng-Yen Chou, Chia-Mu Yu, Pin-Yu Chen, Sy-Yen Kuo, Tsung-Yi Ho","19 pages, 4 figures",https://arxiv.org/abs/2311.16646,
Rethinking Radiology Report Generation via Causal Reasoning and Counterfactual Augmentation,arXiv,2311,"Xiao Song, Jiafan Liu, Yun Li, Wenbin Lei, Ruxin Wang","10 pages,5 figures",https://arxiv.org/abs/2311.13307,
SaFL: Sybil-aware Federated Learning with Application to Face Recognition,arXiv,2311,"Mahdi Ghafourian, Julian Fierrez, Ruben Vera-Rodriguez, Ruben Tolosana, Aythami Morales",,https://arxiv.org/abs/2311.04346,
TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4,arXiv,2311,"Zihao Tan, Qingliang Chen, Yongjian Huang, Chen Liang",,https://arxiv.org/abs/2311.17429,
Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data,arXiv,2311,"Bart Pleiter, Behrad Tajalli, Stefanos Koffas, Gorka Abad, Jing Xu, Martha Larson, Stjepan Picek",,https://arxiv.org/abs/2311.07550,
Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations,arXiv,2311,"Wenjie Mo, Jiashu Xu, Qin Liu, Jiongxiao Wang, Jun Yan, Chaowei Xiao, Muhao Chen",,https://arxiv.org/abs/2311.09763,
TextGuard: Provable Defense against Backdoor Attacks on Text Classification,arXiv,2311,"Hengzhi Pei, Jinyuan Jia, Wenbo Guo, Bo Li, Dawn Song",Accepted by NDSS Symposium 2024,https://arxiv.org/abs/2311.11225,https://github.com/AI-secure/TextGuard.
Trust your BMS: Designing a Lightweight Authentication Architecture for Industrial Networks,arXiv,2311,"Fikret Basic, Christian Steger, Christian Seifert, Robert Kofler","Accepted copy for Publication at the 23rd International Conference on Industrial Technology (ICIT), IEEE, 2022",https://arxiv.org/abs/2311.05498,
Universal Jailbreak Backdoors from Poisoned Human Feedback,arXiv,2311,"Javier Rando, Florian Tramèr",Accepted as conference paper in ICLR 2024,https://arxiv.org/abs/2311.14455,
Unveiling Backdoor Risks Brought by Foundation Models in Heterogeneous Federated Learning,arXiv,2311,"Xi Li, Chen Wu, Jiaqi Wang",Jiaqi Wang is the corresponding author. arXiv admin note: text overlap with arXiv:2311.00144,https://arxiv.org/abs/2311.18350,
Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service,arXiv,2311,"Yuanmin Tang, Jing Yu, Keke Gai, Xiangyan Qu, Yue Hu, Gang Xiong, Qi Wu",,https://arxiv.org/abs/2311.05863,https://github.com/Pter61/vlpmarker.
Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning,arXiv,2310,"Taejin Kim, Jiarui Li, Shubhranshu Singh, Nikhil Madaan, Carlee Joe-Wong","8 pages, 6 main pages of text, 4 figures, 2 tables. Made for a Neurips workshop on backdoor attacks",https://arxiv.org/abs/2310.11594,
Attention-Enhancing Backdoor Attacks Against BERT-based Models,arXiv,2310,"Weimin Lyu, Songzhu Zheng, Lu Pang, Haibin Ling, Chao Chen",Findings of EMNLP 2023,https://arxiv.org/abs/2310.14480,
Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes,arXiv,2310,"Xiruo Ding, Zhecheng Sheng, Meliha Yetişgen, Serguei Pakhomov, Trevor Cohen",Accepted in AMIA 2023 Annual Symposium,https://arxiv.org/abs/2310.02451,
Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks,arXiv,2310,"Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman",,https://arxiv.org/abs/2310.05862,
CBD: A Certified Backdoor Detector Based on Local Dominant Probability,arXiv,2310,"Zhen Xiang, Zidi Xiong, Bo Li",Accepted to NeurIPS 2023,https://arxiv.org/abs/2310.17498,
Causal Inference Using LLM-Guided Discovery,arXiv,2310,"Aniket Vashishtha, Abbavaram Gowtham Reddy, Abhinav Kumar, Saketh Bachu, Vineeth N Balasubramanian, Amit Sharma",,https://arxiv.org/abs/2310.15117,
Causality and Independence Enhancement for Biased Node Classification,arXiv,2310,"Guoxin Chen, Yongqing Wang, Fangda Guo, Qinglang Guo, Jiangli Shao, Huawei Shen, Xueqi Cheng","10 pages, 5 figures, accepted by CIKM2023",https://arxiv.org/abs/2310.09586,
Composite Backdoor Attacks Against Large Language Models,arXiv,2310,"Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang","To Appear in Findings of the Association for Computational Linguistics: NAACL 2024, June 2024",https://arxiv.org/abs/2310.07676,
Confidence-driven Sampling for Backdoor Attacks,arXiv,2310,"Pengfei He, Han Xu, Yue Xing, Jie Ren, Yingqian Cui, Shenglai Zeng, Jiliang Tang, Makoto Yamada, Mohammad Sabokrou",,https://arxiv.org/abs/2310.05263,
Defending Our Privacy With Backdoors,arXiv,2310,"Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting","18 pages, 11 figures",https://arxiv.org/abs/2310.08320,
Demystifying Poisoning Backdoor Attacks from a Statistical Perspective,arXiv,2310,"Ganghua Wang, Xun Xian, Jayanth Srinivasa, Ashish Kundu, Xuan Bi, Mingyi Hong, Jie Ding",,https://arxiv.org/abs/2310.10780,
Domain Watermark: Effective and Harmless Dataset Copyright Protection is Closed at Hand,arXiv,2310,"Junfeng Guo, Yiming Li, Lixu Wang, Shu-Tao Xia, Heng Huang, Cong Liu, Bo Li",This paper is accepted by NeurIPS 2023. The first two authors contributed equally to this work. 30 pages,https://arxiv.org/abs/2310.14942,\url{https://github.com/JunfengGo/Domain-Watermark}.
Exploiting Machine Unlearning for Backdoor Attacks in Deep Learning System,arXiv,2310,"Peixin Zhang, Jun Sun, Mingtian Tan, Xinyu Wang",,https://arxiv.org/abs/2310.10659,
Explore the Effect of Data Selection on Poison Efficiency in Backdoor Attacks,arXiv,2310,"Ziqiang Li, Pengfei Xia, Hong Sun, Yueqi Zeng, Wei Zhang, Bin Li",Under Review,https://arxiv.org/abs/2310.09744,
FLEDGE: Ledger-based Federated Learning Resilient to Inference and Backdoor Attacks,arXiv,2310,"Jorge Castillo, Phillip Rieger, Hossein Fereidooni, Qian Chen, Ahmad Sadeghi",To appear in Annual Computer Security Applications Conference (ACSAC) 2023,https://arxiv.org/abs/2310.02113,
FLTracer: Accurate Poisoning Attack Provenance in Federated Learning,arXiv,2310,"Xinyu Zhang, Qingyu Liu, Zhongjie Ba, Yuan Hong, Tianhang Zheng, Feng Lin, Li Lu, Kui Ren","18 pages, 27 figures",https://arxiv.org/abs/2310.13424,\url{https://github.com/Eyr3/FLTracer}.}
Genetic Algorithm-Based Dynamic Backdoor Attack on Federated Learning-Based Network Traffic Classification,arXiv,2310,"Mahmoud Nazzal, Nura Aljaafari, Ahmed Sawalmeh, Abdallah Khreishah, Muhammad Anan, Abdulelah Algosaibi, Mohammed Alnaeem, Adel Aldalbahi, Abdulaziz Alhumam, Conrado P. Vizcarra, Shadan Alhamed",,https://arxiv.org/abs/2310.06855,
GhostEncoder: Stealthy Backdoor Attacks with Dynamic Triggers to Pre-trained Encoders in Self-supervised Learning,arXiv,2310,"Qiannan Wang, Changchun Yin, Zhe Liu, Liming Fang, Run Wang, Chenhao Lin","24 pages,8 figures",https://arxiv.org/abs/2310.00626,
High Dimensional Causal Inference with Variational Backdoor Adjustment,arXiv,2310,"Daniel Israel, Aditya Grover, Guy Van den Broeck",,https://arxiv.org/abs/2310.06100,
Invisible Threats: Backdoor Attack in OCR Systems,arXiv,2310,"Mauro Conti, Nicola Farronato, Stefanos Koffas, Luca Pajola, Stjepan Picek",,https://arxiv.org/abs/2310.08259,
Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning,arXiv,2310,"Shanshan Han, Wenxuan Wu, Baturalp Buyukates, Weizhao Jin, Qifan Zhang, Yuhang Yao, Salman Avestimehr, Chaoyang He",,https://arxiv.org/abs/2310.04055,
Label Poisoning is All You Need,arXiv,2310,"Rishi D. Jha, Jonathan Hayase, Sewoong Oh",,https://arxiv.org/abs/2310.18933,
Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers,arXiv,2310,"Wencong You, Zayd Hammoudeh, Daniel Lowd",Accepted at EMNLP 2023 Findings,https://arxiv.org/abs/2310.18603,
"Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning",arXiv,2310,"Rui Wen, Tianhao Wang, Michael Backes, Yang Zhang, Ahmed Salem",,https://arxiv.org/abs/2310.11397,
Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data,arXiv,2310,"Lukas Struppek, Martin B. Hentschel, Clifton Poth, Dominik Hintersdorf, Kristian Kersting","Published at NeurIPS 2023 Workshop on Backdoors in Deep Learning: The Good, the Bad, and the Ugly",https://arxiv.org/abs/2310.06372,
On the Detection of Image-Scaling Attacks in Machine Learning,arXiv,2310,"Erwin Quiring, Andreas Müller, Konrad Rieck",Accepted at ACSAC'23,https://arxiv.org/abs/2310.15085,
PETA: Parameter-Efficient Trojan Attacks,arXiv,2310,"Lauren Hong, Ting Wang",,https://arxiv.org/abs/2310.00648,
PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models,arXiv,2310,"Hongwei Yao, Jian Lou, Zhan Qin","To Appear in IEEE ICASSP 2024, code is available at: https://github.com/grasses/PoisonPrompt",https://arxiv.org/abs/2310.12439,https://github.com/grasses/PoisonPrompt
Prompt Backdoors in Visual Prompt Learning,arXiv,2310,"Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang",,https://arxiv.org/abs/2310.07632,
SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models,arXiv,2310,"Boyang Zhang, Zheng Li, Ziqing Yang, Xinlei He, Michael Backes, Mario Fritz, Yang Zhang","To appear in the 33rd USENIX Security Symposium, August 2024, Philadelphia, PA, USA",https://arxiv.org/abs/2310.12665,
Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypots,arXiv,2310,"Ruixiang Tang, Jiayi Yuan, Yiming Li, Zirui Liu, Rui Chen, Xia Hu",,https://arxiv.org/abs/2310.18633,
Towards Stable Backdoor Purification through Feature Shift Tuning,arXiv,2310,"Rui Min, Zeyu Qin, Li Shen, Minhao Cheng",NeurIPS 2023 paper. The first two authors contributed equally,https://arxiv.org/abs/2310.01875,https://github.com/AISafety-HKUST/stable_backdoor_purification.
VFLAIR: A Research Library and Benchmark for Vertical Federated Learning,arXiv,2310,"Tianyuan Zou, Zixuan Gu, Yu He, Hideaki Takahashi, Yang Liu, Ya-Qin Zhang","39 pages, 22 figures, 19 tabels",https://arxiv.org/abs/2310.09827,"https://github.com/FLAIR-THU/VFLAIR),"
Watch Out! Simple Horizontal Class Backdoors Can Trivially Evade Defenses,arXiv,2310,"Hua Ma, Shang Wang, Yansong Gao, Zhi Zhang, Huming Qiu, Minhui Xue, Alsharif Abuadbba, Anmin Fu, Surya Nepal, Derek Abbott",To Appear in the 31st ACM Conference on Computer and Communications Security,https://arxiv.org/abs/2310.00542,
WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks,arXiv,2310,"Jun Xia, Zhihao Yue, Yingbo Zhou, Zhiwei Ling, Xian Wei, Mingsong Chen",,https://arxiv.org/abs/2310.11595,
Backdoor Attacks and Countermeasures in Natural Language Processing Models: A Comprehensive Security Review,arXiv,2309,"Pengzhou Cheng, Zongru Wu, Wei Du, Haodong Zhao, Wei Lu, Gongshen Liu","21 pages, 4 figures",https://arxiv.org/abs/2309.06055,
BadSQA: Stealthy Backdoor Attacks Using Presence Events as Triggers in Non-Intrusive Speech Quality Assessment,arXiv,2309,"Ying Ren, Kailai Shen, Zhe Ye, Diqun Yan","5 pages, 6 figures,conference",https://arxiv.org/abs/2309.01480,
Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach,arXiv,2309,"Mishfad Shaikh Veedu, James Melbourne, Murti V. Salapaka",34 pages,https://arxiv.org/abs/2309.02571,
Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks,arXiv,2309,"Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, Ting Wang",Accepted by NeurIPS'23,https://arxiv.org/abs/2309.13256,
FTA: Stealthy and Adaptive Backdoor Attack with Flexible Triggers on Federated Learning,arXiv,2309,"Yanqi Qiao, Dazhuang Liu, Congwen Chen, Rui Wang, Kaitai Liang",,https://arxiv.org/abs/2309.00127,
MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems,arXiv,2309,"Hanqing Guo, Xun Chen, Junfeng Guo, Li Xiao, Qiben Yan",Accepted by Mobicom 2023,https://arxiv.org/abs/2309.06981,
MarkNerf:Watermarking for Neural Radiance Field,arXiv,2309,"Lifeng Chen, Jia Liu, Yan Ke, Wenquan Sun, Weina Dong, Xiaozhong Pan",,https://arxiv.org/abs/2309.11747,
One-to-Multiple Clean-Label Image Camouflage (OmClic) based Backdoor Attack on Deep Learning,arXiv,2309,"Guohong Wang, Hua Ma, Yansong Gao, Alsharif Abuadbba, Zhi Zhang, Wei Kang, Said F. Al-Sarawib, Gongxuan Zhang, Derek Abbott",,https://arxiv.org/abs/2309.04036,
Physical Invisible Backdoor Based on Camera Imaging,arXiv,2309,"Yusheng Guo, Nan Zhong, Zhenxing Qian, Xinpeng Zhang",,https://arxiv.org/abs/2309.07428,
Post-Training Overfitting Mitigation in DNN Classifiers,arXiv,2309,"Hang Wang, David J. Miller, George Kesidis",,https://arxiv.org/abs/2309.16827,
Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective,arXiv,2309,"Zhen Qin, Feiyi Chen, Chen Zhi, Xueqiang Yan, Shuiguang Deng",Accepted by AAAI 2024. Codes are publicly available at https://github.com/zhenqincn/Snowball,https://arxiv.org/abs/2309.16456,https://github.com/zhenqincn/Snowball
Robust Backdoor Attacks on Object Detection in Real World,arXiv,2309,"Yaguan Qian, Boyuan Ji, Shuke He, Shenhui Huang, Xiang Ling, Bin Wang, Wei Wang","22 pages, 13figures",https://arxiv.org/abs/2309.08953,
Safe and Robust Watermark Injection with a Single OoD Image,arXiv,2309,"Shuyang Yu, Junyuan Hong, Haobo Zhang, Haotao Wang, Zhangyang Wang, Jiayu Zhou",,https://arxiv.org/abs/2309.01786,
Seeing Is Not Always Believing: Invisible Collision Attack and Defence on Pre-Trained Models,arXiv,2309,"Minghang Deng, Zhong Zhang, Junming Shao","10 pages, 4 figures",https://arxiv.org/abs/2309.13579,
Steganography for Neural Radiance Fields by Backdooring,arXiv,2309,"Weina Dong, Jia Liu, Yan Ke, Lifeng Chen, Wenquan Sun, Xiaozhong Pan","6 pages, 7 figures",https://arxiv.org/abs/2309.10503,
Towards Robust Model Watermark via Reducing Parametric Vulnerability,arXiv,2309,"Guanhao Gan, Yiming Li, Dongxian Wu, Shu-Tao Xia",This paper is accepted by ICCV 2023,https://arxiv.org/abs/2309.04777,\url{https://github.com/GuanhaoGan/robust-model-watermarking}.
Trojan Taxonomy in Quantum Computing,arXiv,2309,"Subrata Das, Swaroop Ghosh","6 pages, 2 figures",https://arxiv.org/abs/2309.10981,
VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models,arXiv,2309,"Zihao Zhu, Mingda Zhang, Shaokui Wei, Bingzhe Wu, Baoyuan Wu",Accepted to ICLR 2024,https://arxiv.org/abs/2309.16211,\url{https://github.com/zihao-ai/vdc}.
A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks,arXiv,2308,"Haomiao Yang, Kunlan Xiang, Mengyu Ge, Hongwei Li, Rongxing Lu, Shui Yu",,https://arxiv.org/abs/2308.14367,
BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection,arXiv,2308,"Tinghao Xie, Xiangyu Qi, Ping He, Yiming Li, Jiachen T. Wang, Prateek Mittal",,https://arxiv.org/abs/2308.12439,
Backdoor Federated Learning by Poisoning Backdoor-Critical Layers,arXiv,2308,"Haomin Zhuang, Mingxian Yu, Hao Wang, Yang Hua, Jian Li, Xu Yuan",Accepted to ICLR'24,https://arxiv.org/abs/2308.04466,
Backdoor Mitigation by Correcting the Distribution of Neural Activations,arXiv,2308,"Xi Li, Zhen Xiang, David J. Miller, George Kesidis",,https://arxiv.org/abs/2308.09850,
Backdooring Textual Inversion for Concept Censorship,arXiv,2308,"Yutong Wu, Jie Zhang, Florian Kerschbaum, Tianwei Zhang",,https://arxiv.org/abs/2308.10718,https://concept-censorship.github.io.
Breaking Speaker Recognition with PaddingBack,arXiv,2308,"Zhe Ye, Diqun Yan, Li Dong, Kailai Shen",,https://arxiv.org/abs/2308.04179,
Causal SAR ATR with Limited Data via Dual Invariance,arXiv,2308,"Chenwei Wang, You Qin, Li Li, Siyi Luo, Yulin Huang, Jifang Pei, Yin Zhang, Jianyu Yang",,https://arxiv.org/abs/2308.09412,
"DFB: A Data-Free, Low-Budget, and High-Efficacy Clean-Label Backdoor Attack",arXiv,2308,"Binhao Ma, Jiahui Wang, Dejun Wang, Bo Meng",,https://arxiv.org/abs/2308.09487,
Diffusion Model in Causal Inference with Unmeasured Confounders,arXiv,2308,Tatsuhiro Shimizu,"14 pages, 18 figures",https://arxiv.org/abs/2308.03669,
Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack,arXiv,2308,"Sze Jue Yang, Quang Nguyen, Chee Seng Chan, Khoa D. Doan",14 pages. This paper shows everyone can mount a powerful and stealthy backdoor attack with the widely-used lossy image compression,https://arxiv.org/abs/2308.16684,
FLAIRS: FPGA-Accelerated Inference-Resistant & Secure Federated Learning,arXiv,2308,"Huimin Li, Phillip Rieger, Shaza Zeitouni, Stjepan Picek, Ahmad-Reza Sadeghi",,https://arxiv.org/abs/2308.00553,
FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks,arXiv,2308,"Ehsanul Kabir, Zeyu Song, Md Rafi Ur Rashid, Shagufta Mehnaz",,https://arxiv.org/abs/2308.05832,
Generating Hard Ising Instances With Planted Solutions Using Post-Quantum Cryptographic Protocols,arXiv,2308,"Salvatore Mandrà, Gianni Mossi, Eleanor G. Rieffel",,https://arxiv.org/abs/2308.09704,
Hiding Backdoors within Event Sequence Data via Poisoning Attacks,arXiv,2308,"Elizaveta Kovtun, Alina Ermilova, Dmitry Berestnev, Alexey Zaytsev",,https://arxiv.org/abs/2308.10201,
Improved Activation Clipping for Universal Backdoor Mitigation and Test-Time Detection,arXiv,2308,"Hang Wang, Zhen Xiang, David J. Miller, George Kesidis",,https://arxiv.org/abs/2308.04617,
LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors,arXiv,2308,"Chengkun Wei, Wenlong Meng, Zhikun Zhang, Min Chen, Minghu Zhao, Wenjing Fang, Lei Wang, Zihui Zhang, Wenzhi Chen","To Appear in the Network and Distributed System Security (NDSS) Symposium 2024, 26 February - 1 March 2024, San Diego, CA, USA; typos corrected",https://arxiv.org/abs/2308.13904,
MDTD: A Multi Domain Trojan Detector for Deep Neural Networks,arXiv,2308,"Arezoo Rajabi, Surudhi Asokraj, Fengqing Jiang, Luyao Niu, Bhaskar Ramasubramanian, Jim Ritcey, Radha Poovendran",Accepted to ACM Conference on Computer and Communications Security (ACM CCS) 2023,https://arxiv.org/abs/2308.15673,
ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP,arXiv,2308,"Lu Yan, Zhuo Zhang, Guanhong Tao, Kaiyuan Zhang, Xuan Chen, Guangyu Shen, Xiangyu Zhang",,https://arxiv.org/abs/2308.02122,
PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification,arXiv,2308,"Yizhen Yuan, Rui Kong, Shenghao Xie, Yuanchun Li, Yunxin Liu",accepted by ACM MM 2023,https://arxiv.org/abs/2308.11822,
Protect Federated Learning Against Backdoor Attacks via Data-Free Trigger Generation,arXiv,2308,"Yanxin Yang, Ming Hu, Yue Cao, Jun Xia, Yihao Huang, Yang Liu, Mingsong Chen",,https://arxiv.org/abs/2308.11333,
SSL-Auth: An Authentication Framework by Fragile Watermarking for Pre-trained Encoders in Self-supervised Learning,arXiv,2308,"Xiaobei Li, Changchun Yin, Liyue Zhu, Xiaogang Xu, Liming Fang, Run Wang, Chenhao Lin",,https://arxiv.org/abs/2308.04673,
TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models,arXiv,2308,"Indranil Sur, Karan Sikka, Matthew Walmer, Kaushik Koneripalli, Anirban Roy, Xiao Lin, Ajay Divakaran, Susmit Jha","Published as conference paper at ICCV 2023. 13 pages, 6 figures, 7 tables",https://arxiv.org/abs/2308.03906,https://github.com/SRI-CSL/TIJO.
Temporal-Distributed Backdoor Attack Against Video Based Action Recognition,arXiv,2308,"Xi Li, Songhe Wang, Ruiquan Huang, Mahanth Gowda, George Kesidis",accepted by AAAI 2024,https://arxiv.org/abs/2308.11070,
Test-Time Backdoor Defense via Detecting and Repairing,arXiv,2308,"Jiyang Guan, Jian Liang, Ran He",,https://arxiv.org/abs/2308.06107,
Unveiling Causalities in SAR ATR: A Causal Interventional Approach for Limited Data,arXiv,2308,"Chenwei Wang, Xin Chen, You Qin, Siyi Luo, Yulin Huang, Jifang Pei, Jianyu Yang",,https://arxiv.org/abs/2308.09396,
XGBD: Explanation-Guided Graph Backdoor Detection,arXiv,2308,"Zihan Guan, Mengnan Du, Ninghao Liu","8 pages, 9 figures",https://arxiv.org/abs/2308.04406,https://github.com/GuanZihan/GNN_backdoor_detection.
3D-IDS: Doubly Disentangled Dynamic Intrusion Detection,arXiv,2307,"Chenyang Qiu, Yingsheng Geng, Junrui Lu, Kaida Chen, Shitong Zhu, Ya Su, Guoshun Nan, Can Zhang, Junsong Fu, Qimei Cui, Xiaofeng Tao",Accepted and appeared in the proceedings of the KDD 2023 Research Track,https://arxiv.org/abs/2307.11079,
A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives,arXiv,2307,"Yudong Gao, Honglong Chen, Peng Sun, Junjian Li, Anqing Zhang, Zhibo Wang","10 pages, 7 figures. Submit to ACM MM 2023",https://arxiv.org/abs/2307.10184,
Adversarial Feature Map Pruning for Backdoor,arXiv,2307,"Dong Huang, Qingwen Bu",Accepted to ICLR 2024,https://arxiv.org/abs/2307.11565,https://github.com/retsuh-bqw/FMP.
Application of BadNets in Spam Filters,arXiv,2307,"Swagnik Roychoudhury, Akshaj Kumar Veldanda","5 pages, 4 figures, submitted to ICDE23 ASTRIDE, https://astride-2023.github.io/assets/papers/CameraReady14.pdf",https://arxiv.org/abs/2307.09649,https://astride-2023.github.io/assets/papers/CameraReady14.pdf
Attacking by Aligning: Clean-Label Backdoor Attacks on Object Detection,arXiv,2307,"Yize Cheng, Wenbin Hu, Minhao Cheng",,https://arxiv.org/abs/2307.10487,
BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models,arXiv,2307,"Jordan Vice, Naveed Akhtar, Richard Hartley, Ajmal Mian","This research was supported by National Intelligence and Security Discovery Research Grants (project# NS220100007), funded by the Department of Defence Australia",https://arxiv.org/abs/2307.16489,"https://github.com/JJ-Vice/BAGM,"
Backdoor Attacks against Voice Recognition Systems: A Survey,arXiv,2307,"Baochen Yan, Jiahe Lan, Zheng Yan","33 pages, 7 figures",https://arxiv.org/abs/2307.13643,
Backdoor Attacks for In-Context Learning with Language Models,arXiv,2307,"Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, Nicholas Carlini",AdvML Frontiers Workshop 2023,https://arxiv.org/abs/2307.14692,
Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection,arXiv,2307,"Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, Hongxia Jin",Accepted to NAACL 2024. Project page: https://poison-llm.github.io,https://arxiv.org/abs/2307.16888,https://poison-llm.github.io.
Beating Backdoor Attack at Its Own Game,arXiv,2307,"Min Liu, Alberto Sangiovanni-Vincentelli, Xiangyu Yue",Accepted to ICCV 2023,https://arxiv.org/abs/2307.15539,https://github.com/damianliumin/non-adversarial_backdoor.
Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy,arXiv,2307,"Zihao Zhu, Mingda Zhang, Shaokui Wei, Li Shen, Yanbo Fan, Baoyuan Wu",,https://arxiv.org/abs/2307.07328,
Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection,arXiv,2307,"Hao Fu, Prashanth Krishnamurthy, Siddharth Garg, Farshad Khorrami",Published in the IEEE Transactions on Information Forensics and Security,https://arxiv.org/abs/2307.05422,
Exploring Encrypted Keyboards to Defeat Client-Side Scanning in End-to-End Encryption Systems,arXiv,2307,"Mashari Alatawi, Nitesh Saxena","24 pages, 20 figures. Published in the 25th Annual International Conference on Information Security and Cryptology (ICISC), November/December 2022",https://arxiv.org/abs/2307.03426,
FedDefender: Backdoor Attack Defense in Federated Learning,arXiv,2307,"Waris Gill, Ali Anwar, Muhammad Ali Gulzar",Published in SE4SafeML 2023 (co-located with FSE 2023). See https://dl.acm.org/doi/abs/10.1145/3617574.3617858,https://arxiv.org/abs/2307.08672,
Federated Unlearning via Active Forgetting,arXiv,2307,"Yuyuan Li, Chaochao Chen, Xiaolin Zheng, Jiaming Zhang",,https://arxiv.org/abs/2307.03363,
Fedward: Flexible Federated Backdoor Defense Framework with Non-IID Data,arXiv,2307,"Zekai Chen, Fuyi Wang, Zhiwei Zheng, Ximeng Liu, Yujie Lin",Accepted by IEEE ICME 2023,https://arxiv.org/abs/2307.00356,
QDoor: Exploiting Approximate Synthesis for Backdoor Attacks in Quantum Neural Networks,arXiv,2307,"Cheng Chu, Fan Chen, Philip Richerme, Lei Jiang",,https://arxiv.org/abs/2307.09529,
Rethinking Backdoor Attacks,arXiv,2307,"Alaa Khaddaj, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Hadi Salman, Andrew Ilyas, Aleksander Madry",ICML 2023,https://arxiv.org/abs/2307.10163,
Risk-optimized Outlier Removal for Robust 3D Point Cloud Classification,arXiv,2307,"Xinke Li, Junchi Lu, Henghui Ding, Changsheng Sun, Joey Tianyi Zhou, Chee Yeow Meng",,https://arxiv.org/abs/2307.10875,
Security and Privacy Issues of Federated Learning,arXiv,2307,Jahid Hasan,"6 pages, 2 figures",https://arxiv.org/abs/2307.12181,
Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples,arXiv,2307,"Shaokui Wei, Mingda Zhang, Hongyuan Zha, Baoyuan Wu",,https://arxiv.org/abs/2307.10562,
Towards Stealthy Backdoor Attacks against Speech Recognition via Elements of Sound,arXiv,2307,"Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Stefanos Koffas, Yiming Li",13 pages,https://arxiv.org/abs/2307.08208,\url{https://github.com/HanboCai/BadSpeech_SoE}.
You Can Backdoor Personalized Federated Learning,arXiv,2307,"Tiandi Ye, Cen Chen, Yinggui Wang, Xiang Li, Ming Gao",Submitted to TKDD,https://arxiv.org/abs/2307.15971,https://github.com/BapFL/code.
A First Order Meta Stackelberg Method for Robust Federated Learning,arXiv,2306,"Yunian Pan, Tao Li, Henger Li, Tianyi Xu, Zizhan Zheng, Quanyan Zhu",Accepted to ICML 2023 Workshop on The 2nd New Frontiers In Adversarial Machine Learning. Associated technical report arXiv:2306.13273,https://arxiv.org/abs/2306.13800,
A First Order Meta Stackelberg Method for Robust Federated Learning (Technical Report),arXiv,2306,"Henger Li, Tianyi Xu, Tao Li, Yunian Pan, Quanyan Zhu, Zizhan Zheng",Accepted to ICML 2023 Workshop on The 2nd New Frontiers In Adversarial Machine Learning. Workshop Proceedings version: arXiv:2306.13800,https://arxiv.org/abs/2306.13273,
A Proxy Attack-Free Strategy for Practically Improving the Poisoning Efficiency in Backdoor Attacks,arXiv,2306,"Ziqiang Li, Hong Sun, Pengfei Xia, Beihao Xia, Xue Rui, Wei Zhang, Qinglang Guo, Bin Li",Under review,https://arxiv.org/abs/2306.08313,
Adversarial Backdoor Attack by Naturalistic Data Poisoning on Trajectory Prediction in Autonomous Driving,arXiv,2306,"Mozhgan Pourkeshavarz, Mohammad Sabokrou, Amir Rasouli",,https://arxiv.org/abs/2306.15755,
Avoid Adversarial Adaption in Federated Learning by Multi-Metric Investigations,arXiv,2306,"Torsten Krauß, Alexandra Dmitrienko","25 pages, 14 figures, 23 tables, 11 equations",https://arxiv.org/abs/2306.03600,
Backdoor Attack with Sparse and Invisible Trigger,arXiv,2306,"Yinghua Gao, Yiming Li, Xueluan Gong, Zhifeng Li, Shu-Tao Xia, Qian Wang",The first two authors contributed equally to this work. 13 pages,https://arxiv.org/abs/2306.06209,\url{https://github.com/YinghuaGao/SIBA}.
Bkd-FedGNN: A Benchmark for Classification Backdoor Attacks on Federated Graph Neural Network,arXiv,2306,"Fan Liu, Siqi Lai, Yansong Ning, Hao Liu",,https://arxiv.org/abs/2306.10351,https://github.com/usail-hkust/BkdFedGCN.
Can predictive models be used for causal inference?,arXiv,2306,"Maximilian Pichler, Florian Hartig","47 pages, 4 figures",https://arxiv.org/abs/2306.10551,
Causal Inference With Outcome-Dependent Missingness And Self-Censoring,arXiv,2306,"Jacob M Chen, Daniel Malinsky, Rohit Bhattacharya",15 pages. In proceedings of the 39th Conference on Uncertainty in Artificial Intelligence,https://arxiv.org/abs/2306.05511,
DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation,arXiv,2306,"Zhicong Yan, Shenghong Li, Ruijie Zhao, Yuan Tian, Yuanyuan Zhao",It has been accepted by asiaccs,https://arxiv.org/abs/2306.08009,\url{https://github.com/yanzhicong/DHBE}
"Edge Learning for 6G-enabled Internet of Things: A Comprehensive Survey of Vulnerabilities, Datasets, and Defenses",arXiv,2306,"Mohamed Amine Ferrag, Othmane Friha, Burak Kantarci, Norbert Tihanyi, Lucas Cordeiro, Merouane Debbah, Djallel Hamouda, Muna Al-Hawawreh, Kim-Kwang Raymond Choo",This paper has been accepted for publication in IEEE Communications Surveys \& Tutorials,https://arxiv.org/abs/2306.10309,
Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios,arXiv,2306,"Ziqiang Li, Hong Sun, Pengfei Xia, Heng Li, Beihao Xia, Yi Wu, Bin Li",ICLR 2024,https://arxiv.org/abs/2306.08386,https://github.com/sunh1113/Efficient-backdoor-attacks-for-deep-neural-networks-in-real-world-scenarios
Efficient Backdoor Removal Through Natural Gradient Fine-tuning,arXiv,2306,"Nazmul Karim, Abdullah Al Arafat, Umar Khalid, Zhishan Guo, Naznin Rahnavard",,https://arxiv.org/abs/2306.17441,
Enrollment-stage Backdoor Attacks on Speaker Recognition Systems via Adversarial Ultrasound,arXiv,2306,"Xinfeng Li, Junning Ze, Chen Yan, Yushi Cheng, Xiaoyu Ji, Wenyuan Xu",Published in Internet of Things Journal (IoT-J),https://arxiv.org/abs/2306.16022,
Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion,arXiv,2306,"Zhe Ye, Terui Mao, Li Dong, Diqun Yan",Accepted by INTERSPEECH 2023,https://arxiv.org/abs/2306.15875,
G$^2$uardFL: Safeguarding Federated Learning Against Backdoor Attacks through Attributed Client Graph Clustering,arXiv,2306,"Hao Yu, Chuan Ma, Meng Liu, Tianyu Du, Ming Ding, Tao Xiang, Shouling Ji, Xinwang Liu","19 pages, 7 figures",https://arxiv.org/abs/2306.04984,
Hidden Backdoor Attack against Deep Learning-Based Wireless Signal Modulation Classifiers,arXiv,2306,"Yunsong Huang, Weicheng Liu, Hui-Ming Wang",,https://arxiv.org/abs/2306.10753,
Machine Learning needs Better Randomness Standards: Randomised Smoothing and PRNG-based attacks,arXiv,2306,"Pranav Dahiya, Ilia Shumailov, Ross Anderson",USENIX Security 2024 (https://www.usenix.org/conference/usenixsecurity24/presentation/dahiya),https://arxiv.org/abs/2306.14043,
Mitigating Backdoor Attack Via Prerequisite Transformation,arXiv,2306,Han Gao,"7 pages,7 figures,2 tables",https://arxiv.org/abs/2306.01983,
Multi-target Backdoor Attacks for Code Pre-trained Models,arXiv,2306,"Yanzhou Li, Shangqing Liu, Kangjie Chen, Xiaofei Xie, Tianwei Zhang, Yang Liu",ACL 2023 main conference,https://arxiv.org/abs/2306.08350,
Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features,arXiv,2306,"Mingli Zhu, Shaokui Wei, Hongyuan Zha, Baoyuan Wu",,https://arxiv.org/abs/2306.16697,
OVLA: Neural Network Ownership Verification using Latent Watermarks,arXiv,2306,"Feisi Fu, Wenchao Li",,https://arxiv.org/abs/2306.13215,
Poisoning Network Flow Classifiers,arXiv,2306,"Giorgio Severi, Simona Boboila, Alina Oprea, John Holodnak, Kendra Kratkiewicz, Jason Matterer","14 pages, 8 figures",https://arxiv.org/abs/2306.01655,
Practical and General Backdoor Attacks against Vertical Federated Learning,arXiv,2306,"Yuexin Xuan, Xiaojun Chen, Zhendong Zhao, Bisheng Tang, Ye Dong","17 pages, 7 figures, To appear in ECML PKDD 2023",https://arxiv.org/abs/2306.10746,
Privacy Inference-Empowered Stealthy Backdoor Attack on Federated Learning under Non-IID Scenarios,arXiv,2306,"Haochen Mei, Gaolei Li, Jun Wu, Longfei Zheng",It can be accepted IJCNN,https://arxiv.org/abs/2306.08011,
Revisiting Data-Free Knowledge Distillation with Poisoned Teachers,arXiv,2306,"Junyuan Hong, Yi Zeng, Shuyang Yu, Lingjuan Lyu, Ruoxi Jia, Jiayu Zhou",Accepted to ICML 2023,https://arxiv.org/abs/2306.02368,https://github.com/illidanlab/ABD.
Tree based Progressive Regression Model for Watch-Time Prediction in Short-video Recommendation,arXiv,2306,"Xiao Lin, Xiaokai Chen, Linfeng Song, Jingwei Liu, Biao Li, Peng Jiang",,https://arxiv.org/abs/2306.03392,
"Versatile Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers",arXiv,2306,"Ruotong Wang, Hongrui Chen, Zihao Zhu, Li Liu, Baoyuan Wu",,https://arxiv.org/abs/2306.00816,
VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models,arXiv,2306,"Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho","Accepted by NeurIPS 2023, NeurIPS 2023 BUGS Workshop Oral",https://arxiv.org/abs/2306.06874,GitHub:
A Deep Dive into NFT Rug Pulls,arXiv,2305,"Jintao Huang, Ningyu He, Kai Ma, Jiang Xiao, Haoyu Wang",,https://arxiv.org/abs/2305.06108,
Adversarial Clean Label Backdoor Attacks and Defenses on Text Classification Systems,arXiv,2305,"Ashim Gupta, Amrith Krishna",RepL4NLP 2023 at ACL 2023,https://arxiv.org/abs/2305.19607,
Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark,arXiv,2305,"Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie",Accepted by ACL 2023,https://arxiv.org/abs/2305.10036,
Attacking Pre-trained Recommendation,arXiv,2305,"Yiqing Wu, Ruobing Xie, Zhao Zhang, Yongchun Zhu, FuZhen Zhuang, Jie Zhou, Yongjun Xu, Qing He",Accepted by SIGIR 2023,https://arxiv.org/abs/2305.03995,
Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study,arXiv,2305,"Yiqi Zhong, Xianming Liu, Deming Zhai, Junjun Jiang, Xiangyang Ji",,https://arxiv.org/abs/2305.18384,
Backdoor Learning on Sequence to Sequence Models,arXiv,2305,"Lichang Chen, Minhao Cheng, Heng Huang",14 pages,https://arxiv.org/abs/2305.02424,
Backdoor to the Hidden Ground State: Planted Vertex Cover Example,arXiv,2305,"Xin-Yi Fan, Hai-Jun Zhou","Preprint format, double-spaced, single column, 12 pages",https://arxiv.org/abs/2305.06610,
Backdooring Neural Code Search,arXiv,2305,"Weisong Sun, Yuchen Chen, Guanhong Tao, Chunrong Fang, Xiangyu Zhang, Quanjun Zhang, Bin Luo",Accepted to the 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023),https://arxiv.org/abs/2305.17506,
BadCS: A Backdoor Attack Framework for Code search,arXiv,2305,"Shiyi Qi, Yuanhang Yang, Shuzhzeng Gao, Cuiyun Gao, Zenglin Xu",,https://arxiv.org/abs/2305.05503,
BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks,arXiv,2305,"Zihan Guan, Mengxuan Hu, Zhongliang Zhou, Jielu Zhang, Sheng Li, Ninghao Liu","2 pages, 3 figures",https://arxiv.org/abs/2305.03289,
Causal Interventions-based Few-Shot Named Entity Recognition,arXiv,2305,"Zhen Yang, Yongbin Liu, Chunping Ouyang",,https://arxiv.org/abs/2305.01914,
Chest X-ray Image Classification: A Causal Perspective,arXiv,2305,"Weizhi Nie, Chen Zhang, Dan Song, Lina Zhao, Yunpeng Bai, Keliang Xie, Anan Liu",,https://arxiv.org/abs/2305.12072,
DABS: Data-Agnostic Backdoor attack at the Server in Federated Learning,arXiv,2305,"Wenqiang Sun, Sen Li, Yuchang Sun, Jun Zhang",Accepted by Backdoor Attacks and Defenses in Machine Learning (BANDS) Workshop at ICLR 2023,https://arxiv.org/abs/2305.01267,
Defending against Insertion-based Textual Backdoor Attacks via Attribution,arXiv,2305,"Jiazhao Li, Zhuofeng Wu, Wei Ping, Chaowei Xiao, V. G. Vinod Vydiswaran",Findings of ACL 2023. Camera-ready version,https://arxiv.org/abs/2305.02394,
Differentially-Private Decision Trees and Provable Robustness to Data Poisoning,arXiv,2305,"Daniël Vos, Jelle Vos, Tianyu Li, Zekeriya Erkin, Sicco Verwer",,https://arxiv.org/abs/2305.15394,
Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias,arXiv,2305,"Zhiyuan Zhang, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun",Accepted by Findings of ACL 2023,https://arxiv.org/abs/2305.04547,
FedGrad: Mitigating Backdoor Attacks in Federated Learning Through Local Ultimate Gradients Inspection,arXiv,2305,"Thuy Dung Nguyen, Anh Duy Nguyen, Kok-Seng Wong, Huy Hieu Pham, Thanh Hung Nguyen, Phi Le Nguyen, Truong Thao Nguyen",Accepted for presentation at the International Joint Conference on Neural Networks (IJCNN 2023),https://arxiv.org/abs/2305.00328,
From Shortcuts to Triggers: Backdoor Defense with Denoised PoE,arXiv,2305,"Qin Liu, Fei Wang, Chaowei Xiao, Muhao Chen",Accepted by NAACL 2024 Main Conference,https://arxiv.org/abs/2305.14910,
IMBERT: Making BERT Immune to Insertion-based Backdoor Attacks,arXiv,2305,"Xuanli He, Jun Wang, Benjamin Rubinstein, Trevor Cohn",accepted to Third Workshop on Trustworthy Natural Language Processing,https://arxiv.org/abs/2305.16503,
Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models,arXiv,2305,"Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, Muhao Chen",NAACL 2024,https://arxiv.org/abs/2305.14710,
Manipulating Visually-aware Federated Recommender Systems and Its Countermeasures,arXiv,2305,"Wei Yuan, Shilong Yuan, Chaoqun Yang, Quoc Viet Hung Nguyen, Hongzhi Yin",,https://arxiv.org/abs/2305.08183,
Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation,arXiv,2305,"Xuanli He, Qiongkai Xu, Jun Wang, Benjamin Rubinstein, Trevor Cohn",accepted to EMNLP2023 (main conference),https://arxiv.org/abs/2305.11596,
NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models,arXiv,2305,"Kai Mei, Zheng Li, Zhenting Wang, Yang Zhang, Shiqing Ma",,https://arxiv.org/abs/2305.17826,https://github.com/RU-System-Software-and-Security/Notable.
Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models,arXiv,2305,"Yihao Huang, Felix Juefei-Xu, Qing Guo, Jie Zhang, Yutong Wu, Ming Hu, Tianlin Li, Geguang Pu, Yang Liu","16 pages, accepted by AAAI 2024",https://arxiv.org/abs/2305.10701,
Pick your Poison: Undetectability versus Robustness in Data Poisoning Attacks,arXiv,2305,"Nils Lukas, Florian Kerschbaum",Preprint,https://arxiv.org/abs/2305.09671,
Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models,arXiv,2305,"Shuai Zhao, Jinming Wen, Luu Anh Tuan, Junbo Zhao, Jie Fu",Accepted to appear at the main conference of EMNLP 2023,https://arxiv.org/abs/2305.01219,
Reconstructive Neuron Pruning for Backdoor Defense,arXiv,2305,"Yige Li, Xixiang Lyu, Xingjun Ma, Nodens Koren, Lingjuan Lyu, Bo Li, Yu-Gang Jiang",Accepted by ICML23,https://arxiv.org/abs/2305.14876,\url{https://github.com/bboylyg/RNP}.
Stealthy Low-frequency Backdoor Attack against Deep Neural Networks,arXiv,2305,"Xinrui Liu, Yu-an Tan, Yajie Wang, Kefan Qiu, Yuanzhang Li",,https://arxiv.org/abs/2305.09677,
Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning,arXiv,2305,"Shengfang Zhai, Yinpeng Dong, Qingni Shen, Shi Pu, Yuejian Fang, Hang Su",Carmera-ready version. To appear in ACM MM 2023. Code will be released at: https://github.com/sf-zhai/BadT2I,https://arxiv.org/abs/2305.04175,https://github.com/sf-zhai/BadT2I
Towards Invisible Backdoor Attacks in the Frequency Domain against Deep Neural Networks,arXiv,2305,"Xinrui Liu, Yajie Wang, Yu-an Tan, Kefan Qiu, Yuanzhang Li",arXiv admin note: text overlap with arXiv:2305.09677,https://arxiv.org/abs/2305.10596,
Turning Privacy-preserving Mechanisms against Federated Learning,arXiv,2305,"Marco Arazzi, Mauro Conti, Antonino Nocera, Stjepan Picek",,https://arxiv.org/abs/2305.05355,
UMD: Unsupervised Model Detection for X2X Backdoor Attacks,arXiv,2305,"Zhen Xiang, Zidi Xiong, Bo Li",Proceedings of the 40th International Conference on Machine Learning,https://arxiv.org/abs/2305.18651,
UOR: Universal Backdoor Attacks on Pre-trained Language Models,arXiv,2305,"Wei Du, Peixuan Li, Boqun Li, Haodong Zhao, Gongshen Liu",,https://arxiv.org/abs/2305.09574,
Unconfounded Propensity Estimation for Unbiased Ranking,arXiv,2305,"Dan Luo, Lixin Zou, Qingyao Ai, Zhiyu Chen, Chenliang Li, Dawei Yin, Brian D. Davison","11 pages, 5 figures",https://arxiv.org/abs/2305.09918,
Watermarking Classification Dataset for Copyright Protection,arXiv,2305,"Yixin Liu, Hongsheng Hu, Xun Chen, Xuyun Zhang, Lichao Sun",,https://arxiv.org/abs/2305.13257,
Adversary Aware Continual Learning,arXiv,2304,"Muhammad Umer, Robi Polikar",,https://arxiv.org/abs/2304.14483,
"Anomalies, $η$ , $η$' as keys to glueballs",arXiv,2304,Jean-Marie Frère,Corfu workshop proceedings 2022,https://arxiv.org/abs/2304.09083,
BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT,arXiv,2304,"Jiawen Shi, Yixin Liu, Pan Zhou, Lichao Sun",This paper is accepted as a poster in NDSS2023,https://arxiv.org/abs/2304.12298,
BadVFL: Backdoor Attacks in Vertical Federated Learning,arXiv,2304,"Mohammad Naseri, Yufei Han, Emiliano De Cristofaro",Accepted for publication at the 45th IEEE Symposium on Security & Privacy (S&P 2024). Please cite accordingly,https://arxiv.org/abs/2304.08847,
Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning,arXiv,2304,"Yanbo Dai, Songze Li",This paper was accepted to ICML 2023,https://arxiv.org/abs/2304.12961,
ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger,arXiv,2304,"Jiazhao Li, Yijin Yang, Zhuofeng Wu, V. G. Vinod Vydiswaran, Chaowei Xiao",,https://arxiv.org/abs/2304.14475,
Compositional Probabilistic and Causal Inference using Tractable Circuit Models,arXiv,2304,"Benjie Wang, Marta Kwiatkowska","30 pages, AISTATS 2023",https://arxiv.org/abs/2304.08278,
Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning,arXiv,2304,"Ajinkya Tejankar, Maziar Sanjabi, Qifan Wang, Sinong Wang, Hamed Firooz, Hamed Pirsiavash, Liang Tan",Accepted to CVPR 2023,https://arxiv.org/abs/2304.01482,https://github.com/UCDvision/PatchSearch
Detecting Domain-Generation Algorithm (DGA) Based Fully-Qualified Domain Names (FQDNs) with Shannon Entropy,arXiv,2304,Adam Dorian Wong,,https://arxiv.org/abs/2304.07943,GitHub
Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization,arXiv,2304,"Mingli Zhu, Shaokui Wei, Li Shen, Yanbo Fan, Baoyuan Wu",,https://arxiv.org/abs/2304.11823,
Evil from Within: Machine Learning Backdoors through Hardware Trojans,arXiv,2304,"Alexander Warnecke, Julian Speith, Jan-Niklas Möller, Konrad Rieck, Christof Paar",,https://arxiv.org/abs/2304.08411,
Exploiting Logic Locking for a Neural Trojan Attack on Machine Learning Accelerators,arXiv,2304,"Hongye Xu, Dongfang Liu, Cory Merkel, Michael Zuzak",Accepted in GLSVLSI 2023,https://arxiv.org/abs/2304.06017,
Get Rid Of Your Trail: Remotely Erasing Backdoors in Federated Learning,arXiv,2304,"Manaar Alam, Hithem Lamri, Michail Maniatakos",,https://arxiv.org/abs/2304.10638,
Model Sparsity Can Simplify Machine Unlearning,arXiv,2304,"Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, Sijia Liu",NeurIPS'23 spotlight,https://arxiv.org/abs/2304.04934,https://github.com/OPTML-Group/Unlearn-Sparse.
RSBA: Robust Statistical Backdoor Attack under Privilege-Constrained Scenarios,arXiv,2304,"Xiaolei Liu, Ming Yi, Kangyi Ding, Bangzhou Xin, Yixiao Xu, Li Yan, Chao Shen","11 pages, 10 figures",https://arxiv.org/abs/2304.10985,
Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning,arXiv,2304,"Hao Chen, Chen Gong, Yizhe Wang, Xinwen Hou",,https://arxiv.org/abs/2304.00252,
Rethinking the Trigger-injecting Position in Graph Backdoor Attack,arXiv,2304,"Jing Xu, Gorka Abad, Stjepan Picek",,https://arxiv.org/abs/2304.02277,
Secure Federated Learning against Model Poisoning Attacks via Client Filtering,arXiv,2304,"Duygu Nur Yaldiz, Tuo Zhang, Salman Avestimehr",,https://arxiv.org/abs/2304.00160,
UNICORN: A Unified Backdoor Trigger Inversion Framework,arXiv,2304,"Zhenting Wang, Kai Mei, Juan Zhai, Shiqing Ma",,https://arxiv.org/abs/2304.02786,https://github.com/RU-System-Software-and-Security/UNICORN.
Universal Adversarial Backdoor Attacks to Fool Vertical Federated Learning in Cloud-Edge Collaboration,arXiv,2304,"Peng Chen, Xin Du, Zhihui Lu, Hongfeng Chai","14 pages, 7 figures",https://arxiv.org/abs/2304.11432,
A Universal Identity Backdoor Attack against Speaker Verification based on Siamese Network,arXiv,2303,"Haodong Zhao, Wei Du, Junjie Guo, Gongshen Liu",Accepted by the Interspeech 2022. The first two authors contributed equally to this work,https://arxiv.org/abs/2303.16031,
AdaptGuard: Defending Against Universal Attacks for Model Adaptation,arXiv,2303,"Lijun Sheng, Jian Liang, Ran He, Zilei Wang, Tieniu Tan",ICCV2023,https://arxiv.org/abs/2303.10594,https://github.com/TomSheng21/AdaptGuard.
"Backdoor Attacks and Defenses in Federated Learning: Survey, Challenges and Future Research Directions",arXiv,2303,"Thuy Dung Nguyen, Tuan Nguyen, Phi Le Nguyen, Hieu H. Pham, Khoa Doan, Kok-Seng Wong",,https://arxiv.org/abs/2303.02213,
Backdoor Attacks with Input-unique Triggers in NLP,arXiv,2303,"Xukun Zhou, Jiwei Li, Tianwei Zhang, Lingjuan Lyu, Muqiao Yang, Jun He",,https://arxiv.org/abs/2303.14325,
Backdoor Defense via Adaptively Splitting Poisoned Dataset,arXiv,2303,"Kuofeng Gao, Yang Bai, Jindong Gu, Yong Yang, Shu-Tao Xia",Accepted by CVPR 2023,https://arxiv.org/abs/2303.12993,https://github.com/KuofengGao/ASD.
Backdoor Defense via Deconfounded Representation Learning,arXiv,2303,"Zaixi Zhang, Qi Liu, Zhicai Wang, Zepu Lu, Qingyong Hu",Accepted by CVPR 2023,https://arxiv.org/abs/2303.06818,\url{https://github.com/zaixizhang/CBD}.
Backdoor for Debias: Mitigating Model Bias with Backdoor Attack-based Artificial Bias,arXiv,2303,"Shangxi Wu, Qiuyang He, Fangzhao Wu, Jitao Sang, Yaowei Wang, Changsheng Xu",,https://arxiv.org/abs/2303.01504,
Black-box Backdoor Defense via Zero-shot Image Purification,arXiv,2303,"Yucheng Shi, Mengnan Du, Xuansheng Wu, Zihan Guan, Jin Sun, Ninghao Liu",Accepted by NeurIPS 2023,https://arxiv.org/abs/2303.12175,https://github.com/sycny/ZIP.
CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning,arXiv,2303,"Hritik Bansal, Nishad Singhi, Yu Yang, Fan Yin, Aditya Grover, Kai-Wei Chang",22 pages. Accepted at ICCV 2023,https://arxiv.org/abs/2303.03323,https://github.com/nishadsinghi/CleanCLIP.
Context De-confounded Emotion Recognition,arXiv,2303,"Dingkang Yang, Zhaoyu Chen, Yuzheng Wang, Shunli Wang, Mingcheng Li, Siao Liu, Xiao Zhao, Shuai Huang, Zhiyan Dong, Peng Zhai, Lihua Zhang",Accepted by CVPR 2023. CCIM is available at https://github.com/ydk122024/CCIM,https://arxiv.org/abs/2303.11921,https://github.com/ydk122024/CCIM
Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency,arXiv,2303,"Xiaogeng Liu, Minghui Li, Haoyu Wang, Shengshan Hu, Dengpan Ye, Hai Jin, Libing Wu, Chaowei Xiao",Accepted by CVPR2023. Code is available at https://github.com/CGCL-codes/TeCo,https://arxiv.org/abs/2303.18191,https://github.com/CGCL-codes/TeCo
Detecting Backdoors in Pre-trained Encoders,arXiv,2303,"Shiwei Feng, Guanhong Tao, Siyuan Cheng, Guangyu Shen, Xiangzhe Xu, Yingqi Liu, Kaiyuan Zhang, Shiqing Ma, Xiangyu Zhang",Accepted at CVPR 2023. Code is available at https://github.com/GiantSeaweed/DECREE,https://arxiv.org/abs/2303.15180,https://github.com/GiantSeaweed/DECREE
Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking,arXiv,2303,"Ruixiang Tang, Qizhang Feng, Ninghao Liu, Fan Yang, Xia Hu",,https://arxiv.org/abs/2303.11470,
Do Backdoors Assist Membership Inference Attacks?,arXiv,2303,"Yumeki Goto, Nami Ashizawa, Toshiki Shibahara, Naoto Yanai",,https://arxiv.org/abs/2303.12589,
Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs,arXiv,2303,"Hasan Abed Al Kader Hammoud, Adel Bibi, Philip H. S. Torr, Bernard Ghanem",Accepted at CVPRW (The Art of Robustness),https://arxiv.org/abs/2303.13211,
Graph Neural Networks for Hardware Vulnerability Analysis -- Can you Trust your GNN?,arXiv,2303,"Lilas Alrahis, Ozgur Sinanoglu",Will be presented at 2023 IEEE VLSI Test Symposium (VTS),https://arxiv.org/abs/2303.16690,
Influencer Backdoor Attack on Semantic Segmentation,arXiv,2303,"Haoheng Lan, Jindong Gu, Philip Torr, Hengshuang Zhao",,https://arxiv.org/abs/2303.12054,
Interventional Bag Multi-Instance Learning On Whole-Slide Pathological Images,arXiv,2303,"Tiancheng Lin, Zhimiao Yu, Hongyu Hu, Yi Xu, Chang Wen Chen",Accepted by CVPR 2023; Code at https://github.com/HHHedo/IBMIL,https://arxiv.org/abs/2303.06873,https://github.com/HHHedo/IBMIL.
Learning the Wrong Lessons: Inserting Trojans During Knowledge Distillation,arXiv,2303,"Leonard Tang, Tom Shlomi, Alexander Cai",ICLR 2023 Workshop on Backdoor Attacks and Defenses in Machine Learning,https://arxiv.org/abs/2303.05593,
Learning to Backdoor Federated Learning,arXiv,2303,"Henger Li, Chen Wu, Sencun Zhu, Zizhan Zheng",,https://arxiv.org/abs/2303.03320,
Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder,arXiv,2303,"Tao Sun, Lu Pang, Chao Chen, Haibin Ling",,https://arxiv.org/abs/2303.15564,https://github.com/tsun/BDMAE.
Mitigating Backdoors in Federated Learning with FLD,arXiv,2303,"Yihang Lin, Pengyuan Zhou, Zhiqian Wu, Yong Liao",,https://arxiv.org/abs/2303.00302,
Multi-metrics adaptively identifies backdoors in Federated learning,arXiv,2303,"Siquan Huang, Yijiang Li, Chong Chen, Leyu Shi, Ying Gao","14 pages, 8 figures and 7 tables; 2023 IEEE/CVF International Conference on Computer Vision (ICCV)",https://arxiv.org/abs/2303.06601,
NCL: Textual Backdoor Defense Using Noise-augmented Contrastive Learning,arXiv,2303,"Shengfang Zhai, Qingni Shen, Xiaoyi Chen, Weilong Wang, Cong Li, Yuejian Fang, Zhonghai Wu","6 pages, 5 figures. To appear in ICASSP 2023",https://arxiv.org/abs/2303.01742,
Optimal Smoothing Distribution Exploration for Backdoor Neutralization in Deep Learning-based Traffic Systems,arXiv,2303,"Yue Wang, Wending Li, Michail Maniatakos, Saif Eddin Jabari",,https://arxiv.org/abs/2303.14197,
Physical Backdoor Trigger Activation of Autonomous Vehicle using Reachability Analysis,arXiv,2303,"Wenqing Li, Yue Wang, Muhammad Shafique, Saif Eddin Jabari",,https://arxiv.org/abs/2303.13992,
PoisonedGNN: Backdoor Attack on Graph Neural Networks-based Hardware Security Systems,arXiv,2303,"Lilas Alrahis, Satwik Patnaik, Muhammad Abdullah Hanif, Muhammad Shafique, Ozgur Sinanoglu",This manuscript is currently under review at IEEE Transactions on Computers,https://arxiv.org/abs/2303.14009,
Recursive Euclidean Distance Based Robust Aggregation Technique For Federated Learning,arXiv,2303,"Charuka Herath, Yogachandran Rahulamathavan, Xiaolan Liu",,https://arxiv.org/abs/2303.11337,
Rethinking interpretation: Input-agnostic saliency mapping of deep visual classifiers,arXiv,2303,"Naveed Akhtar, Mohammad A. A. K. Jalwana",Accepted for publication in AAAI 2023,https://arxiv.org/abs/2303.17836,
Robust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor Attacks,arXiv,2303,"Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman",,https://arxiv.org/abs/2303.06854,
SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning,arXiv,2303,"Mengxin Zheng, Jiaqi Xue, Zihao Wang, Xun Chen, Qian Lou, Lei Jiang, Xiaofeng Wang","9 pages, 6 figures, 4 tables",https://arxiv.org/abs/2303.09079,
Single Image Backdoor Inversion via Robust Smoothed Classifiers,arXiv,2303,"Mingjie Sun, J. Zico Kolter",CVPR 2023. v2: improved writing,https://arxiv.org/abs/2303.00215,https://github.com/locuslab/smoothinv.
Unnoticeable Backdoor Attacks on Graph Neural Networks,arXiv,2303,"Enyan Dai, Minhua Lin, Xiang Zhang, Suhang Wang",,https://arxiv.org/abs/2303.01263,
A semantic backdoor attack against Graph Convolutional Networks,arXiv,2302,"Jiazhu Dai, Zhipeng Xiong",,https://arxiv.org/abs/2302.14353,
ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms,arXiv,2302,"Minzhou Pan, Yi Zeng, Lingjuan Lyu, Xue Lin, Ruoxi Jia","18 pages, with 13 pages of main text",https://arxiv.org/abs/2302.11408,https://github.com/ruoxi-jia-group/ASSET.
Analyzing And Editing Inner Mechanisms Of Backdoored Language Models,arXiv,2302,"Max Lamparth, Anka Reuel",Final version accepted at FAccT 24,https://arxiv.org/abs/2302.12461,
Attacks in Adversarial Machine Learning: A Systematic Survey from the Life-cycle Perspective,arXiv,2302,"Baoyuan Wu, Zihao Zhu, Li Liu, Qingshan Liu, Zhaofeng He, Siwei Lyu","35 pages, 4 figures, 10 tables, 313 reference papers",https://arxiv.org/abs/2302.09457,
Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger,arXiv,2302,"Yi Yu, Yufei Wang, Wenhan Yang, Shijian Lu, Yap-peng Tan, Alex C. Kot",Accepted by CVPR 2023,https://arxiv.org/abs/2302.14677,
Backdoor Attacks to Pre-trained Unified Foundation Models,arXiv,2302,"Zenghui Yuan, Yixin Liu, Kai Zhang, Pan Zhou, Lichao Sun",This paper is accepted as a poster for NDSS 2023,https://arxiv.org/abs/2302.09360,
"Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions",arXiv,2302,Marwan Omar,,https://arxiv.org/abs/2302.06801,GitHub
BackdoorBox: A Python Toolbox for Backdoor Learning,arXiv,2302,"Yiming Li, Mengxi Ya, Yang Bai, Yong Jiang, Shu-Tao Xia",BackdoorBox V0.1. The first two authors contributed equally to this toolbox. 13 pages,https://arxiv.org/abs/2302.01762,\url{https://github.com/THUYimingLi/BackdoorBox}.
Defending Against Backdoor Attacks by Layer-wise Feature Analysis,arXiv,2302,"Najeeb Moharram Jebreel, Josep Domingo-Ferrer, Yiming Li",This paper is accepted by PAKDD 2023,https://arxiv.org/abs/2302.12758,
FreeEagle: Detecting Complex Neural Trojans in Data-Free Cases,arXiv,2302,"Chong Fu, Xuhong Zhang, Shouling Ji, Ting Wang, Peng Lin, Yanghe Feng, Jianwei Yin",Accepted by USENIX Security 2023,https://arxiv.org/abs/2302.14500,
Imperceptible Sample-Specific Backdoor to DNN with Denoising Autoencoder,arXiv,2302,"Jiliang Zhang, Jing Xu, Zhi Zhang, Yansong Gao","8 pages, 8 figures",https://arxiv.org/abs/2302.04457,
Mithridates: Auditing and Boosting Backdoor Resistance of Machine Learning Pipelines,arXiv,2302,"Eugene Bagdasaryan, Vitaly Shmatikov",,https://arxiv.org/abs/2302.04977,
On Feasibility of Server-side Backdoor Attacks on Split Learning,arXiv,2302,"Behrad Tajalli, Oguzhan Ersoy, Stjepan Picek",,https://arxiv.org/abs/2302.09578,
Provable Robustness Against a Union of $\ell_0$ Adversarial Attacks,arXiv,2302,"Zayd Hammoudeh, Daniel Lowd",Accepted at AAAI 2024 -- Extended version including the supplementary material,https://arxiv.org/abs/2302.11628,
QTrojan: A Circuit Backdoor Against Quantum Neural Networks,arXiv,2302,"Cheng Chu, Lei Jiang, Martin Swany, Fan Chen",,https://arxiv.org/abs/2302.08090,
Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks,arXiv,2302,"Zeyu Qin, Liuyi Yao, Daoyuan Chen, Yaliang Li, Bolin Ding, Minhao Cheng",KDD 2023,https://arxiv.org/abs/2302.01677,https://github.com/alibaba/FederatedScope/tree/backdoor-bench.
RobustNLP: A Technique to Defend NLP Models Against Backdoor Attacks,arXiv,2302,Marwan Omar,,https://arxiv.org/abs/2302.09420,https://github.com/marwanomar1/Backdoor-Learning-for-NLP
SATBA: An Invisible Backdoor Attack Based On Spatial Attention,arXiv,2302,"Huasong Zhou, Xiaowei Xu, Xiaodong Wang, Leon Bevan Bullock","9 pages, 9 figures",https://arxiv.org/abs/2302.13056,
SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency,arXiv,2302,"Junfeng Guo, Yiming Li, Xun Chen, Hanqing Guo, Lichao Sun, Cong Liu",,https://arxiv.org/abs/2302.03251,https://github.com/JunfengGo/SCALE-UP.
Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data,arXiv,2302,"Gorka Abad, Oguzhan Ersoy, Stjepan Picek, Aitor Urbieta",To appear in Network and Distributed System Security (NDSS) Symposium 2024,https://arxiv.org/abs/2302.06279,
SoK: A Systematic Evaluation of Backdoor Trigger Characteristics in Image Classification,arXiv,2302,"Gorka Abad, Jing Xu, Stefanos Koffas, Behrad Tajalli, Stjepan Picek, Mauro Conti",,https://arxiv.org/abs/2302.01740,
SplitOut: Out-of-the-Box Training-Hijacking Detection in Split Learning via Outlier Detection,arXiv,2302,"Ege Erdogan, Unat Teksen, Mehmet Salih Celiktenyildiz, Alptekin Kupcu, A. Ercument Cicek",,https://arxiv.org/abs/2302.08618,
Training-free Lexical Backdoor Attacks on Language Models,arXiv,2302,"Yujin Huang, Terry Yue Zhuo, Qiongkai Xu, Han Hu, Xingliang Yuan, Chunyang Chen","Accepted to International World Wide Web Conference 2023, Security, Privacy & Trust Track",https://arxiv.org/abs/2302.04116,https://github.com/Jinxhy/TFLexAttack.
Universal Soldier: Using Universal Adversarial Perturbations for Detecting Backdoor Attacks,arXiv,2302,"Xiaoyun Xu, Oguzhan Ersoy, Stjepan Picek",,https://arxiv.org/abs/2302.00747,
BDMMT: Backdoor Sample Detection for Language Models through Model Mutation Testing,arXiv,2301,"Jiali Wei, Ming Fan, Wenjing Jiao, Wuxia Jin, Ting Liu",,https://arxiv.org/abs/2301.10412,
BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense,arXiv,2301,"Siyuan Cheng, Guanhong Tao, Yingqi Liu, Shengwei An, Xiangzhe Xu, Shiwei Feng, Guangyu Shen, Kaiyuan Zhang, Qiuling Xu, Shiqing Ma, Xiangyu Zhang",,https://arxiv.org/abs/2301.06241,
Backdoor Attacks Against Dataset Distillation,arXiv,2301,"Yugeng Liu, Zheng Li, Michael Backes, Yun Shen, Yang Zhang",,https://arxiv.org/abs/2301.01197,
Backdoor Attacks in Peer-to-Peer Federated Learning,arXiv,2301,"Gokberk Yar, Simona Boboila, Cristina Nita-Rotaru, Alina Oprea",,https://arxiv.org/abs/2301.09732,
BayBFed: Bayesian Backdoor Defense for Federated Learning,arXiv,2301,"Kavita Kumari, Phillip Rieger, Hossein Fereidooni, Murtuza Jadliwala, Ahmad-Reza Sadeghi",,https://arxiv.org/abs/2301.09508,
Distilling Cognitive Backdoor Patterns within an Image,arXiv,2301,"Hanxun Huang, Xingjun Ma, Sarah Erfani, James Bailey",ICLR2023,https://arxiv.org/abs/2301.10908,\url{https://github.com/HanxunH/CognitiveDistillation}.
Facial Misrecognition Systems: Simple Weight Manipulations Force DNNs to Err Only on Specific Persons,arXiv,2301,"Irad Zehavi, Adi Shamir",,https://arxiv.org/abs/2301.03118,
Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering,arXiv,2301,"Rui Zhu, Di Tang, Siyuan Tang, Guanhong Tao, Shiqing Ma, Xiaofeng Wang, Haixu Tang",,https://arxiv.org/abs/2301.12318,
"Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition",arXiv,2301,"Hasan Abed Al Kader Hammoud, Shuming Liu, Mohammed Alkhrashi, Fahad AlBalawi, Bernard Ghanem",,https://arxiv.org/abs/2301.00986,
Mutual Information Regularization for Vertical Federated Learning,arXiv,2301,"Tianyuan Zou, Yang Liu, Ya-Qin Zhang","12 pages, 9 figures",https://arxiv.org/abs/2301.01142,
On the Vulnerability of Backdoor Defenses for Federated Learning,arXiv,2301,"Pei Fang, Jinghui Chen","Accepted by AAAI 2023 (15 pages, 12 figures, 7 tables)",https://arxiv.org/abs/2301.08170,
PECAN: A Deterministic Certified Defense Against Backdoor Attacks,arXiv,2301,"Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni",,https://arxiv.org/abs/2301.11824,
Salient Conditional Diffusion for Defending Against Backdoor Attacks,arXiv,2301,"Brandon B. May, N. Joseph Tatro, Dylan Walker, Piyush Kumar, Nathan Shnidman","14 pages, 5 figures. Edit: Added new baselines",https://arxiv.org/abs/2301.13862,
"Silent Killer: A Stealthy, Clean-Label, Black-Box Backdoor Attack",arXiv,2301,"Tzvi Lederer, Gallil Maimon, Lior Rokach",,https://arxiv.org/abs/2301.02615,
Stealthy Backdoor Attack for Code Models,arXiv,2301,"Zhou Yang, Bowen Xu, Jie M. Zhang, Hong Jin Kang, Jieke Shi, Junda He, David Lo","18 pages, Under review of IEEE Transactions on Software Engineering",https://arxiv.org/abs/2301.02496,
Towards Understanding How Self-training Tolerates Data Backdoor Poisoning,arXiv,2301,"Soumyadeep Pal, Ren Wang, Yuguang Yao, Sijia Liu",Accepted at SafeAI 2023: AAAI's Workshop on Artificial Intelligence Safety,https://arxiv.org/abs/2301.08751,
Trojaning semi-supervised learning model via poisoning wild images on the web,arXiv,2301,"Le Feng, Zhenxing Qian, Sheng Li, Xinpeng Zhang",,https://arxiv.org/abs/2301.00435,
Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis,arXiv,2301,"Wei Guo, Benedetta Tondi, Mauro Barni",,https://arxiv.org/abs/2301.04554,
AI Security for Geoscience and Remote Sensing: Challenges and Future Trends,arXiv,2212,"Yonghao Xu, Tao Bai, Weikang Yu, Shizhen Chang, Peter M. Atkinson, Pedram Ghamisi",,https://arxiv.org/abs/2212.09360,
Backdoor Attack Detection in Computer Vision by Applying Matrix Factorization on the Weights of Deep Networks,arXiv,2212,"Khondoker Murad Hossain, Tim Oates","7 pages, 4 figures, 5 tables, AAAI Workshop on Safe AI 2023",https://arxiv.org/abs/2212.08121,
Backdoor Mitigation in Deep Neural Networks via Strategic Retraining,arXiv,2212,"Akshay Dhonthi, Ernst Moritz Hahn, Vahid Hashemi","13 Pages, 7 Tables, 4 Figures. Accepted at the International Symposium of Formal Methods 2023 (FM 2023)",https://arxiv.org/abs/2212.07278,
FedCC: Robust Federated Learning against Model Poisoning Attacks,arXiv,2212,"Hyejun Jeong, Hamin Son, Seohu Lee, Jayun Hyun, Tai-Myoung Chung",,https://arxiv.org/abs/2212.01976,
Fine-Tuning Is All You Need to Mitigate Backdoor Attacks,arXiv,2212,"Zeyang Sha, Xinlei He, Pascal Berrang, Mathias Humbert, Yang Zhang","17 pages, 17 figures",https://arxiv.org/abs/2212.09067,
Flareon: Stealthy any2any Backdoor Injection via Poisoned Augmentation,arXiv,2212,"Tianrui Qin, Xianghuan He, Xitong Gao, Yiren Zhao, Kejiang Ye, Cheng-Zhong Xu",,https://arxiv.org/abs/2212.09979,https://github.com/lafeat/flareon.
How to Backdoor Diffusion Models?,arXiv,2212,"Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho",Accepted by CVPR 2023,https://arxiv.org/abs/2212.05400,https://github.com/IBM/BadDiffusion.
Mind Your Heart: Stealthy Backdoor Attack on Dynamic Deep Neural Network in Edge Computing,arXiv,2212,"Tian Dong, Ziyuan Zhang, Han Qiu, Tianwei Zhang, Hewu Li, Terry Wang",Accepted to IEEE INFOCOM 2023,https://arxiv.org/abs/2212.11751,
Pre-trained Encoders in Self-Supervised Learning Improve Secure and Privacy-preserving Supervised Learning,arXiv,2212,"Hongbin Liu, Wenjie Qu, Jinyuan Jia, Neil Zhenqiang Gong",,https://arxiv.org/abs/2212.03334,
Rethinking Backdoor Data Poisoning Attacks in the Context of Semi-Supervised Learning,arXiv,2212,"Marissa Connor, Vincent Emanuele","18 pages, 14 figures",https://arxiv.org/abs/2212.02582,
"Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models",arXiv,2212,"Rui Zhu, Di Tang, Siyuan Tang, XiaoFeng Wang, Haixu Tang",,https://arxiv.org/abs/2212.04687,
Task-Oriented Communications for NextG: End-to-End Deep Learning and AI Security Aspects,arXiv,2212,"Yalin E. Sagduyu, Sennur Ulukus, Aylin Yener",,https://arxiv.org/abs/2212.09668,
VSVC: Backdoor attack against Keyword Spotting based on Voiceprint Selection and Voice Conversion,arXiv,2212,"Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Shunhui Ji","7 pages,5 figures",https://arxiv.org/abs/2212.10103,
Vulnerabilities of Deep Learning-Driven Semantic Communications to Backdoor (Trojan) Attacks,arXiv,2212,"Yalin E. Sagduyu, Tugba Erpek, Sennur Ulukus, Aylin Yener",,https://arxiv.org/abs/2212.11205,
XMAM:X-raying Models with A Matrix to Reveal Backdoor Attacks for Federated Learning,arXiv,2212,"Jianyi Zhang, Fangjiao Zhang, Qichao Jin, Zhiqiang Wang, Xiaodong Lin, Xiali Hei",23 pages,https://arxiv.org/abs/2212.13675,
XRand: Differentially Private Defense against Explanation-Guided Attacks,arXiv,2212,"Truc Nguyen, Phung Lai, NhatHai Phan, My T. Thai",To be published at AAAI 2023,https://arxiv.org/abs/2212.04454,
A Survey on Backdoor Attack and Defense in Natural Language Processing,arXiv,2211,"Xuan Sheng, Zhaoyang Han, Piji Li, Xiangmao Chang","12 pages, QRS2022",https://arxiv.org/abs/2211.11958,
BATT: Backdoor Attack with Transformation-based Triggers,arXiv,2211,"Tong Xu, Yiming Li, Yong Jiang, Shu-Tao Xia",This paper is accepted by ICASSP 2023. 5 pages,https://arxiv.org/abs/2211.01806,
Backdoor Attacks for Remote Sensing Data with Wavelet Transform,arXiv,2211,"Nikolaus Dräger, Yonghao Xu, Pedram Ghamisi",,https://arxiv.org/abs/2211.08044,\url{https://github.com/ndraeger/waba}.
Backdoor Attacks on Multiagent Collaborative Systems,arXiv,2211,"Shuo Chen, Yue Qiu, Jie Zhang",11 pages,https://arxiv.org/abs/2211.11455,
Backdoor Attacks on Time Series: A Generative Approach,arXiv,2211,"Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, James Bailey",,https://arxiv.org/abs/2211.07915,
Backdoor Cleansing with Unlabeled Data,arXiv,2211,"Lu Pang, Tao Sun, Haibin Ling, Chao Chen",Accepted to CVPR 2023,https://arxiv.org/abs/2211.12044,https://github.com/luluppang/BCU.
Backdoor Defense via Suppressing Model Shortcuts,arXiv,2211,"Sheng Yang, Yiming Li, Yong Jiang, Shu-Tao Xia",This paper is accepted by ICASSP 2023. 5 pages,https://arxiv.org/abs/2211.05631,
Backdoor Vulnerabilities in Normally Trained Deep Learning Models,arXiv,2211,"Guanhong Tao, Zhenting Wang, Siyuan Cheng, Shiqing Ma, Shengwei An, Yingqi Liu, Guangyu Shen, Zhuo Zhang, Yunshu Mao, Xiangyu Zhang",,https://arxiv.org/abs/2211.15929,
BadPrompt: Backdoor Attacks on Continuous Prompts,arXiv,2211,"Xiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, Xiaojie Yuan",Accepted at NeurIPS 2022,https://arxiv.org/abs/2211.14719,https://github.com/papersPapers/BadPrompt.
Be Careful with Rotation: A Uniform Backdoor Pattern for 3D Shape,arXiv,2211,"Linkun Fan, Fazhi He, Qing Guo, Wei Tang, Xiaolin Hong, Bing Li",,https://arxiv.org/abs/2211.16192,
Computational Short Cuts in Infinite Domain Constraint Satisfaction,arXiv,2211,"Peter Jonsson, Victor Lagerkvist, Sebastian Ordyniak",,https://arxiv.org/abs/2211.10144,
CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning,arXiv,2211,"Jinghuai Zhang, Hongbin Liu, Jinyuan Jia, Neil Zhenqiang Gong",CVPR 2024,https://arxiv.org/abs/2211.08229,
Dormant Neural Trojans,arXiv,2211,"Feisi Fu, Panagiota Kiourti, Wenchao Li",,https://arxiv.org/abs/2211.01808,
Going In Style: Audio Backdoors Through Stylistic Transformations,arXiv,2211,"Stefanos Koffas, Luca Pajola, Stjepan Picek, Mauro Conti",Accepted to ICASSP '23 and the first two authors contributed equally,https://arxiv.org/abs/2211.03117,https://github.com/skoffas/going-in-style.
Invisible Backdoor Attack with Dynamic Triggers against Person Re-identification,arXiv,2211,"Wenli Sun, Xinyang Jiang, Shuguang Dou, Dongsheng Li, Duoqian Miao, Cheng Deng, Cairong Zhao",,https://arxiv.org/abs/2211.10933,
M-to-N Backdoor Paradigm: A Stealthy and Fuzzy Attack to Deep Learning Models,arXiv,2211,"Linshan Hou, Zhongyun Hua, Yuhong Li, Leo Yu Zhang",,https://arxiv.org/abs/2211.01875,
MSDT: Masked Language Model Scoring Defense in Text Domain,arXiv,2211,"Jaechul Roh, Minhao Cheng, Yajun Fang","5 pages, 1 figure, 4 tables, accepted as a conference paper at IEEE UV 2022, Boston, USA",https://arxiv.org/abs/2211.05371,https://github.com/jcroh0508/MSDT.
Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning,arXiv,2211,"Yunchao Zhang, Zonglin Di, Kaiwen Zhou, Cihang Xie, Xin Eric Wang",,https://arxiv.org/abs/2211.14769,
On the Security Vulnerabilities of Text-to-SQL Models,arXiv,2211,"Xutan Peng, Yipeng Zhang, Jingfeng Yang, Mark Stevenson",ISSRE 2023: Best Paper Candidate,https://arxiv.org/abs/2211.15363,
PBSM: Backdoor attack against Keyword spotting based on pitch boosting and sound masking,arXiv,2211,"Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Shunhui Ji","5 pages, 4 figures",https://arxiv.org/abs/2211.08697,
Physics-Constrained Backdoor Attacks on Power System Fault Localization,arXiv,2211,"Jianing Bai, Ren Wang, Zuyi Li",,https://arxiv.org/abs/2211.04445,
Provable Defense against Backdoor Policies in Reinforcement Learning,arXiv,2211,"Shubham Kumar Bharti, Xuezhou Zhang, Adish Singla, Xiaojin Zhu",Accepted at Neurips 2022,https://arxiv.org/abs/2211.10530,
Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis,arXiv,2211,"Lukas Struppek, Dominik Hintersdorf, Kristian Kersting",Published as a conference paper at ICCV 2023,https://arxiv.org/abs/2211.02408,
The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning,arXiv,2211,"Virat Shejwalkar, Lingjuan Lyu, Amir Houmansadr",,https://arxiv.org/abs/2211.00453,
UPTON: Preventing Authorship Leakage from Public Text Release via Data Poisoning,arXiv,2211,"Ziyao Wang, Thai Le, Dongwon Lee",,https://arxiv.org/abs/2211.09717,
Untargeted Backdoor Attack against Object Detection,arXiv,2211,"Chengxiao Luo, Yiming Li, Yong Jiang, Shu-Tao Xia",This paper is accepted by ICASSP 2023. 5 pages,https://arxiv.org/abs/2211.05638,
Watermarking in Secure Federated Learning: A Verification Framework Based on Client-Side Backdooring,arXiv,2211,"Wenyuan Yang, Shuo Shao, Yue Yang, Xiyao Liu, Ximeng Liu, Zhihua Xia, Gerald Schaefer, Hui Fang",,https://arxiv.org/abs/2211.07138,
An Embarrassingly Simple Backdoor Attack on Self-supervised Learning,arXiv,2210,"Changjiang Li, Ren Pang, Zhaohan Xi, Tianyu Du, Shouling Ji, Yuan Yao, Ting Wang",The 2023 International Conference on Computer Vision (ICCV '23),https://arxiv.org/abs/2210.07346,
Apple of Sodom: Hidden Backdoors in Superior Sentence Embeddings via Contrastive Learning,arXiv,2210,"Xiaoyi Chen, Baisong Xin, Shengfang Zhai, Shiqing Ma, Qingni Shen, Zhonghai Wu",,https://arxiv.org/abs/2210.11082,
BAFFLE: Hiding Backdoors in Offline Reinforcement Learning Datasets,arXiv,2210,"Chen Gong, Zhou Yang, Yunpeng Bai, Junda He, Jieke Shi, Kecen Li, Arunesh Sinha, Bowen Xu, Xinwen Hou, David Lo, Tianhao Wang",Accepted at IEEE S&P (Oakland) 2024,https://arxiv.org/abs/2210.04688,
Backdoor Attack and Defense in Federated Generative Adversarial Network-based Medical Image Synthesis,arXiv,2210,"Ruinan Jin, Xiaoxiao Li","25 pages, 7 figures. arXiv admin note: text overlap with arXiv:2207.00762",https://arxiv.org/abs/2210.10886,
Backdoor Attacks in the Supply Chain of Masked Image Modeling,arXiv,2210,"Xinyue Shen, Xinlei He, Zheng Li, Yun Shen, Michael Backes, Yang Zhang",,https://arxiv.org/abs/2210.01632,
COLLIDER: A Robust Training Framework for Backdoor Data,arXiv,2210,"Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie",Accepted to the 16th Asian Conference on Computer Vision (ACCV 2022),https://arxiv.org/abs/2210.06704,
CrowdGuard: Federated Backdoor Detection in Federated Learning,arXiv,2210,"Phillip Rieger, Torsten Krauß, Markus Miettinen, Alexandra Dmitrienko, Ahmad-Reza Sadeghi","To appear in the Network and Distributed System Security (NDSS) Symposium 2024. Phillip Rieger and Torsten Krauß contributed equally to this contribution. 19 pages, 8 figures, 5 tables, 4 algorithms, 5 equations",https://arxiv.org/abs/2210.07714,
Detecting Backdoors in Deep Text Classifiers,arXiv,2210,"You Guo, Jun Wang, Trevor Cohn","8 pages, 10 figures",https://arxiv.org/abs/2210.11264,
Dim-Krum: Backdoor-Resistant Federated Learning for NLP with Dimension-wise Krum-Based Aggregation,arXiv,2210,"Zhiyuan Zhang, Qi Su, Xu Sun",Accepted by Findings of EMNLP 2022,https://arxiv.org/abs/2210.06894,
Emerging Threats in Deep Learning-Based Autonomous Driving: A Comprehensive Survey,arXiv,2210,"Hui Cao, Wenlong Zou, Yinkun Wang, Ting Song, Mengjun Liu","28 pages,10 figures",https://arxiv.org/abs/2210.11237,
Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks,arXiv,2210,"Sishuo Chen, Wenkai Yang, Zhiyuan Zhang, Xiaohan Bi, Xu Sun",Findings of EMNLP 2022,https://arxiv.org/abs/2210.07907,https://github.com/lancopku/DAN.
FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning,arXiv,2210,"Kaiyuan Zhang, Guanhong Tao, Qiuling Xu, Siyuan Cheng, Shengwei An, Yingqi Liu, Shiwei Feng, Guangyu Shen, Pin-Yu Chen, Shiqing Ma, Xiangyu Zhang",Accepted by ICLR 2023. Code is available at https://github.com/KaiyuanZh/FLIP,https://arxiv.org/abs/2210.12873,https://github.com/KaiyuanZh/FLIP.
FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information,arXiv,2210,"Xiaoyu Cao, Jinyuan Jia, Zaixi Zhang, Neil Zhenqiang Gong",To appear in IEEE S&P 2023,https://arxiv.org/abs/2210.10936,
Few-shot Backdoor Attacks via Neural Tangent Kernels,arXiv,2210,"Jonathan Hayase, Sewoong Oh","20 pages, 13 figures",https://arxiv.org/abs/2210.05929,
Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models,arXiv,2210,"Zhiyuan Zhang, Lingjuan Lyu, Xingjun Ma, Chenguang Wang, Xu Sun",Accepted by Findings of EMNLP 2022,https://arxiv.org/abs/2210.09545,
ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks,arXiv,2210,"Eleanor Clifford, Ilia Shumailov, Yiren Zhao, Ross Anderson, Robert Mullins","10 pages, 7 figures, to be published in IEEE Secure and Trustworthy Machine Learning 2024. For website see https://ml.backdoors.uk . For source code, see https://git.sr.ht/~tim-clifford/impnet_source",https://arxiv.org/abs/2210.00108,
Invariant Aggregator for Defending against Federated Backdoor Attacks,arXiv,2210,"Xiaoyang Wang, Dimitrios Dimitriadis, Sanmi Koyejo, Shruti Tople",AISTATS 2024 camera-ready,https://arxiv.org/abs/2210.01834,
Marksman Backdoor: Backdoor Attacks with Arbitrary Target Class,arXiv,2210,"Khoa D. Doan, Yingjie Lao, Ping Li",Accepted to NeurIPS 2022,https://arxiv.org/abs/2210.09194,
Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks via Motifs,arXiv,2210,"Haibin Zheng, Haiyang Xiong, Jinyin Chen, Haonan Ma, Guohan Huang",,https://arxiv.org/abs/2210.13710,
Multi-feature Dataset for Windows PE Malware Classification,arXiv,2210,"Muhammad Irfan Yousuf, Izza Anwer, Tanzeela Shakir, Minahil Siddiqui, Maysoon Shahid","9 Pages, 1 Figure, 5 Tables",https://arxiv.org/abs/2210.16285,
Neural Architectural Backdoors,arXiv,2210,"Ren Pang, Changjiang Li, Zhaohan Xi, Shouling Ji, Ting Wang",,https://arxiv.org/abs/2210.12179,
Poison Attack and Defense on Deep Source Code Processing Models,arXiv,2210,"Jia Li, Zhuo Li, Huangzhao Zhang, Ge Li, Zhi Jin, Xing Hu, Xin Xia","23 pages, 9 figures",https://arxiv.org/abs/2210.17029,
Rethinking the Reverse-engineering of Trojan Triggers,arXiv,2210,"Zhenting Wang, Kai Mei, Hailun Ding, Juan Zhai, Shiqing Ma",,https://arxiv.org/abs/2210.15127,https://github.com/RU-System-Software-and-Security/FeatureRE.
Shielding Federated Learning: Mitigating Byzantine Attacks with Less Constraints,arXiv,2210,"Minghui Li, Wei Wan, Jianrong Lu, Shengshan Hu, Junyu Shi, Leo Yu Zhang, Man Zhou, Yifeng Zheng","This paper has been accepted by the 18th International Conference on Mobility, Sensing and Networking (MSN 2022)",https://arxiv.org/abs/2210.01437,
Thinking Two Moves Ahead: Anticipating Other Users Improves Backdoor Attacks in Federated Learning,arXiv,2210,"Yuxin Wen, Jonas Geiping, Liam Fowl, Hossein Souri, Rama Chellappa, Micah Goldblum, Tom Goldstein",Code is available at \url{https://github.com/YuxinWenRick/thinking-two-moves-ahead},https://arxiv.org/abs/2210.09305,\url{https://github.com/YuxinWenRick/thinking-two-moves-ahead}
Towards Out-of-Distribution Sequential Event Prediction: A Causal Treatment,arXiv,2210,"Chenxiao Yang, Qitian Wu, Qingsong Wen, Zhiqiang Zhou, Liang Sun, Junchi Yan",in NeurIPS 2022,https://arxiv.org/abs/2210.13005,
Training set cleansing of backdoor poisoning by self-supervised representation learning,arXiv,2210,"H. Wang, S. Karami, O. Dia, H. Ritter, E. Emamjomeh-Zadeh, J. Chen, Z. Xiang, D. J. Miller, G. Kesidis",,https://arxiv.org/abs/2210.10272,
Trap and Replace: Defending Backdoor Attacks by Trapping Them into an Easy-to-Replace Subnetwork,arXiv,2210,"Haotao Wang, Junyuan Hong, Aston Zhang, Jiayu Zhou, Zhangyang Wang",Accepted by NeurIPS 2022. Code is available at https://github.com/VITA-Group/Trap-and-Replace-Backdoor-Defense,https://arxiv.org/abs/2210.06428,https://github.com/VITA-Group/Trap-and-Replace-Backdoor-Defense
Understanding Impacts of Task Similarity on Backdoor Attack and Detection,arXiv,2210,"Di Tang, Rui Zhu, XiaoFeng Wang, Haixu Tang, Yi Chen",,https://arxiv.org/abs/2210.06509,
Universal Evasion Attacks on Summarization Scoring,arXiv,2210,"Wenchuan Mu, Kwan Hui Lim",,https://arxiv.org/abs/2210.14260,
Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection,arXiv,2210,"Yiming Li, Yang Bai, Yong Jiang, Yong Yang, Shu-Tao Xia, Bo Li","This work is accepted by the NeurIPS 2022 (Oral, TOP 2%). The first two authors contributed equally to this work. 25 pages. We have fixed some typos in the previous version",https://arxiv.org/abs/2210.00875,\url{https://github.com/THUYimingLi/Untargeted_Backdoor_Watermark}.
Watermarking Pre-trained Language Models with Backdooring,arXiv,2210,"Chenxi Gu, Chengsong Huang, Xiaoqing Zheng, Kai-Wei Chang, Cho-Jui Hsieh",,https://arxiv.org/abs/2210.07543,
$ρ$-GNF : A Novel Sensitivity Analysis Approach Under Unobserved Confounders,arXiv,2209,"Sourabh Balgi, Jose M. Peña, Adel Daoud","10 main pages (+4 reference pages, +6 appendix), 8 Figures, Under review",https://arxiv.org/abs/2209.07111,
A Benchmark Comparison of Python Malware Detection Approaches,arXiv,2209,"Duc-Ly Vu, Zachary Newman, John Speed Meyers","12 pages, 3 figures, 3 tables",https://arxiv.org/abs/2209.13288,
An Adaptive Black-box Defense against Trojan Attacks (TrojDef),arXiv,2209,"Guanxiong Liu, Abdallah Khreishah, Fatima Sharadgah, Issa Khalil",,https://arxiv.org/abs/2209.01721,
Augmentation Backdoors,arXiv,2209,"Joseph Rance, Yiren Zhao, Ilia Shumailov, Robert Mullins","12 pages, 8 figures",https://arxiv.org/abs/2209.15139,
BadRes: Reveal the Backdoors through Residual Connection,arXiv,2209,"Mingrui He, Tianyu Chen, Haoyi Zhou, Shanghang Zhang, Jianxin Li","16pages, 9 figures",https://arxiv.org/abs/2209.07125,
Black-box Dataset Ownership Verification via Backdoor Watermarking,arXiv,2209,"Yiming Li, Mingyan Zhu, Xue Yang, Yong Jiang, Tao Wei, Shu-Tao Xia",This paper is accepted by IEEE TIFS. 15 pages. The preliminary short version of this paper was posted on arXiv (arXiv:2010.05821) and presented in a non-archival NeurIPS Workshop (2020),https://arxiv.org/abs/2209.06015,\url{https://github.com/THUYimingLi/DVBW}.
Causal Intervention for Fairness in Multi-behavior Recommendation,arXiv,2209,"Xi Wang, Wenjie Wang, Fuli Feng, Wenge Rong, Chuantao Yin, Zhang Xiong",This paper is accepted by IEEE Transactions on Computational Social Systems,https://arxiv.org/abs/2209.04589,
Defending Against Backdoor Attack on Graph Nerual Network by Explainability,arXiv,2209,"Bingchen Jiang, Zhao Li","10 pages, 10 figures",https://arxiv.org/abs/2209.02902,
Federated Zero-Shot Learning for Visual Recognition,arXiv,2209,"Zhi Chen, Yadan Luo, Sen Wang, Jingjing Li, Zi Huang",,https://arxiv.org/abs/2209.01994,
Multi-Tenant Cloud FPGA: A Survey on Security,arXiv,2209,"Muhammed Kawser Ahmed, Joel Mandebi, Sujan Kumar Saha, Christophe Bobda",,https://arxiv.org/abs/2209.11158,
SSL-WM: A Black-Box Watermarking Approach for Encoders Pre-trained by Self-supervised Learning,arXiv,2209,"Peizhuo Lv, Pan Li, Shenchen Zhu, Shengzhi Zhang, Kai Chen, Ruigang Liang, Chang Yue, Fan Xiang, Yuling Cai, Hualong Ma, Yingjun Zhang, Guozhu Meng","To Appear in the Network and Distributed System Security (NDSS) Symposium 2024, 26 February - 1 March 2024, San Diego, CA, USA",https://arxiv.org/abs/2209.03563,
"The ""Beatrix'' Resurrections: Robust Backdoor Detection via Gram Matrices",arXiv,2209,"Wanlun Ma, Derui Wang, Ruoxi Sun, Minhui Xue, Sheng Wen, Yang Xiang","18 pages, 23 figures. Accepted to NDSS 2023. Camera-ready version. Code availability: https://github.com/wanlunsec/Beatrix",https://arxiv.org/abs/2209.11715,https://github.com/wanlunsec/Beatrix
TransCAB: Transferable Clean-Annotation Backdoor to Object Detection with Natural Trigger in Real-World,arXiv,2209,"Hua Ma, Yinshan Li, Yansong Gao, Zhi Zhang, Alsharif Abuadbba, Anmin Fu, Said F. Al-Sarawi, Nepal Surya, Derek Abbott",,https://arxiv.org/abs/2209.02339,
Universal Backdoor Attacks Detection via Adaptive Adversarial Probe,arXiv,2209,"Yuhang Wang, Huafeng Shi, Rui Min, Ruijia Wu, Siyuan Liang, Yichao Wu, Ding Liang, Aishan Liu","8 pages, 8 figures",https://arxiv.org/abs/2209.05244,
A Knowledge Distillation-Based Backdoor Attack in Federated Learning,arXiv,2208,"Yifan Wang, Wei Fan, Keke Yang, Naji Alhusaini, Jing Li",,https://arxiv.org/abs/2208.06176,
An anomaly detection approach for backdoored neural networks: face recognition as a case study,arXiv,2208,"Alexander Unnervik, Sébastien Marcel","Accepted at Biosig 2022, 8 pages, 4 figures",https://arxiv.org/abs/2208.10231,
Bidirectional Contrastive Split Learning for Visual Question Answering,arXiv,2208,"Yuwei Sun, Hideya Ochiai",Accepted for AAAI 2024,https://arxiv.org/abs/2208.11435,
Confidence Matters: Inspecting Backdoors in Deep Neural Networks via Distribution Transfer,arXiv,2208,"Tong Wang, Yuan Yao, Feng Xu, Miao Xu, Shengwei An, Ting Wang",,https://arxiv.org/abs/2208.06592,
Data-free Backdoor Removal based on Channel Lipschitzness,arXiv,2208,"Runkai Zheng, Rongjun Tang, Jianze Li, Li Liu",Accepted to ECCV 2022,https://arxiv.org/abs/2208.03111,https://github.com/rkteddy/channel-Lipschitzness-based-pruning.
Debiased Cross-modal Matching for Content-based Micro-video Background Music Recommendation,arXiv,2208,"Jinng Yi, Zhenzhong Chen",,https://arxiv.org/abs/2208.03633,\url{https://github.com/jing-1/DebCM}.
Deep Fidelity in DNN Watermarking: A Study of Backdoor Watermarking for Classification Models,arXiv,2208,"Guang Hua, Andrew Beng Jin Teoh",Published in Pattern Recognition,https://arxiv.org/abs/2208.00563,https://github.com/ghua-ac/dnn_watermark.
Defense against Backdoor Attacks via Identifying and Purifying Bad Neurons,arXiv,2208,"Mingyuan Fan, Yang Liu, Cen Chen, Ximeng Liu, Wenzhong Guo",,https://arxiv.org/abs/2208.06537,
Disentangle and Remerge: Interventional Knowledge Distillation for Few-Shot Object Detection from A Conditional Causal Perspective,arXiv,2208,"Jiangmeng Li, Yanan Zhang, Wenwen Qiang, Lingyu Si, Chengbo Jiao, Xiaohui Hu, Changwen Zheng, Fuchun Sun",Accepted by AAAI 2023,https://arxiv.org/abs/2208.12681,https://github.com/ZYN-1101/DandR.git.
Dispersed Pixel Perturbation-based Imperceptible Backdoor Trigger for Image Classifier Models,arXiv,2208,"Yulong Wang, Minghui Zhao, Shenghong Li, Xin Yuan, Wei Ni",,https://arxiv.org/abs/2208.09336,
FedPrompt: Communication-Efficient and Privacy Preserving Prompt Tuning in Federated Learning,arXiv,2208,"Haodong Zhao, Wei Du, Fangqi Li, Peixuan Li, Gongshen Liu",,https://arxiv.org/abs/2208.12268,
Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attacks,arXiv,2208,"Tian Yu Liu, Yu Yang, Baharan Mirzasoleiman",Code available at: https://github.com/tianyu139/friendly-noise,https://arxiv.org/abs/2208.10224,https://github.com/tianyu139/friendly-noise
Imperceptible and Robust Backdoor Attack in 3D Point Cloud,arXiv,2208,"Kuofeng Gao, Jiawang Bai, Baoyuan Wu, Mengxi Ya, Shu-Tao Xia",,https://arxiv.org/abs/2208.08052,
Link-Backdoor: Backdoor Attack on Link Prediction via Node Injection,arXiv,2208,"Haibin Zheng, Haiyang Xiong, Haonan Ma, Guohan Huang, Jinyin Chen",,https://arxiv.org/abs/2208.06776,https://github.com/Seaocn/Link-Backdoor.
Neural network fragile watermarking with no model performance degradation,arXiv,2208,"Zhaoxia Yin, Heng Yin, Xinpeng Zhang",Published in 2022 IEEE International Conference on Image Processing (ICIP),https://arxiv.org/abs/2208.07585,
PerD: Perturbation Sensitivity-based Neural Trojan Detection Framework on NLP Applications,arXiv,2208,"Diego Garcia-soto, Huili Chen, Farinaz Koushanfar",,https://arxiv.org/abs/2208.04943,
RIBAC: Towards Robust and Imperceptible Backdoor Attack against Compact DNN,arXiv,2208,"Huy Phan, Cong Shi, Yi Xie, Tianfang Zhang, Zhuohang Li, Tianming Zhao, Jian Liu, Yan Wang, Yingying Chen, Bo Yuan",Code is available at https://github.com/huyvnphan/ECCV2022-RIBAC,https://arxiv.org/abs/2208.10608,https://github.com/huyvnphan/ECCV2022-RIBAC
Solving the Capsulation Attack against Backdoor-based Deep Neural Network Watermarks by Reversing Triggers,arXiv,2208,"Fangqi Li, Shilin Wang, Yun Zhu",,https://arxiv.org/abs/2208.14127,
TrojViT: Trojan Insertion in Vision Transformers,arXiv,2208,"Mengxin Zheng, Qian Lou, Lei Jiang","10 pages, 4 figures, 11 tables",https://arxiv.org/abs/2208.13049,https://github.com/mxzheng/TrojViT
Backdoor Attack is a Devil in Federated GAN-based Medical Image Synthesis,arXiv,2207,"Ruinan Jin, Xiaoxiao Li","13 pages, 4 figures, Accepted by MICCAI 2022 SASHIMI Workshop",https://arxiv.org/abs/2207.00762,
Backdoor Attacks on Crowd Counting,arXiv,2207,"Yuhua Sun, Tailai Zhang, Xingjun Ma, Pan Zhou, Jian Lou, Zichuan Xu, Xing Di, Yu Cheng,  Lichao","To appear in ACMMM 2022. 10pages, 6 figures and 2 tables",https://arxiv.org/abs/2207.05641,
BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label,arXiv,2207,"Shengshan Hu, Ziqi Zhou, Yechao Zhang, Leo Yu Zhang, Yifeng Zheng, Yuanyuan HE, Hai Jin","This paper has been accepted by the 30th ACM International Conference on Multimedia (MM '22, October 10--14, 2022, Lisboa, Portugal)",https://arxiv.org/abs/2207.00278,
Defense Against Multi-target Trojan Attacks,arXiv,2207,"Haripriya Harikumar, Santu Rana, Kien Do, Sunil Gupta, Wei Zong, Willy Susilo, Svetha Venkastesh",,https://arxiv.org/abs/2207.03895,
FL-Defender: Combating Targeted Attacks in Federated Learning,arXiv,2207,"Najeeb Jebreel, Josep Domingo-Ferrer",,https://arxiv.org/abs/2207.00872,
FRIB: Low-poisoning Rate Invisible Backdoor Attack based on Feature Repair,arXiv,2207,"Hui Xia, Xiugui Yang, Xiangyun Qian, Rui Zhang",,https://arxiv.org/abs/2207.12863,
High-Level Approaches to Hardware Security: A Tutorial,arXiv,2207,"Hammond Pearce, Ramesh Karri, Benjamin Tan","Accepted in IEEE TECS. 41 pages, 13 figures",https://arxiv.org/abs/2207.10466,
Invisible Backdoor Attacks Using Data Poisoning in the Frequency Domain,arXiv,2207,"Chang Yue, Peizhuo Lv, Ruigang Liang, Kai Chen",,https://arxiv.org/abs/2207.04209,
Just Rotate it: Deploying Backdoor Attacks via Rotation Transformation,arXiv,2207,"Tong Wu, Tianhao Wang, Vikash Sehwag, Saeed Mahloujifar, Prateek Mittal",25 pages,https://arxiv.org/abs/2207.10825,
One-shot Neural Backdoor Erasing via Adversarial Weight Masking,arXiv,2207,"Shuwen Chai, Jinghui Chen","Accepted by NeurIPS 2022 (19 pages, 6 figures, 10 tables)",https://arxiv.org/abs/2207.04497,
Technical Report: Assisting Backdoor Federated Learning with Whole Population Knowledge Alignment,arXiv,2207,"Tian Liu, Xueyang Hu, Tao Shu",,https://arxiv.org/abs/2207.12327,
Transferable Graph Backdoor Attack,arXiv,2207,"Shuiqiao Yang, Bao Gia Doan, Paul Montague, Olivier De Vel, Tamas Abraham, Seyit Camtepe, Damith C. Ranasinghe, Salil S. Kanhere","Accepted by the 25th International Symposium on Research in Attacks, Intrusions, and Defenses",https://arxiv.org/abs/2207.00425,
Versatile Weight Attack via Flipping Limited Bits,arXiv,2207,"Jiawang Bai, Baoyuan Wu, Zhifeng Li, Shu-tao Xia",Extension of our ICLR 2021 work: arXiv:2102.10496,https://arxiv.org/abs/2207.12405,
A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks,arXiv,2206,"Ganqu Cui, Lifan Yuan, Bingxiang He, Yangyi Chen, Zhiyuan Liu, Maosong Sun",NeurIPS 2022 Datasets & Benchmarks; Toolkits avaliable at https://github.com/thunlp/OpenBackdoor,https://arxiv.org/abs/2206.08514,https://github.com/thunlp/OpenBackdoor
A temporal chrominance trigger for clean-label backdoor attack against anti-spoof rebroadcast detection,arXiv,2206,"Wei Guo, Benedetta Tondi, Mauro Barni",,https://arxiv.org/abs/2206.01102,
Architectural Backdoors in Neural Networks,arXiv,2206,"Mikel Bober-Irizar, Ilia Shumailov, Yiren Zhao, Robert Mullins, Nicolas Papernot",,https://arxiv.org/abs/2206.07840,
Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior,arXiv,2206,"Jean-Stanislas Denain, Jacob Steinhardt","Fixed backdoor localization results, made changes to abstract and introduction",https://arxiv.org/abs/2206.13498,
Backdoor Attacks on Vision Transformers,arXiv,2206,"Akshayvarun Subramanya, Aniruddha Saha, Soroush Abbasi Koohpayegani, Ajinkya Tejankar, Hamed Pirsiavash",,https://arxiv.org/abs/2206.08477,https://github.com/UCDvision/backdoor_transformer.git
BackdoorBench: A Comprehensive Benchmark of Backdoor Learning,arXiv,2206,"Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Chao Shen","Accepted at NeurIPS 2022 Datasets and Benchmarks Track; 44 pages; 8 backdoor attacks; 9 backdoor defenses; 8,000 pairs of attack-defense evaluations; several analysis and 5 analysis tools",https://arxiv.org/abs/2206.12654,\url{https://backdoorbench.github.io}.
CASSOCK: Viable Backdoor Attacks against DNN in The Wall of Source-Specific Backdoor Defences,arXiv,2206,"Shang Wang, Yansong Gao, Anmin Fu, Zhi Zhang, Yuqing Zhang, Willy Susilo, Dongxi Liu","13 pages,14 figures",https://arxiv.org/abs/2206.00145,
Contributor-Aware Defenses Against Adversarial Backdoor Attacks,arXiv,2206,"Glenn Dawson, Muhammad Umer, Robi Polikar",,https://arxiv.org/abs/2206.03583,
DECK: Model Hardening for Defending Pervasive Backdoors,arXiv,2206,"Guanhong Tao, Yingqi Liu, Siyuan Cheng, Shengwei An, Zhuo Zhang, Qiuling Xu, Guangyu Shen, Xiangyu Zhang",,https://arxiv.org/abs/2206.09272,
Defending Backdoor Attacks on Vision Transformer via Patch Processing,arXiv,2206,"Khoa D. Doan, Yingjie Lao, Peng Yang, Ping Li",,https://arxiv.org/abs/2206.12381,
Enhancing Clean Label Backdoor Attack with Two-phase Specific Triggers,arXiv,2206,"Nan Luo, Yuanzhang Li, Yajie Wang, Shangbo Wu, Yu-an Tan, Quanxin Zhang",,https://arxiv.org/abs/2206.04881,
Interventional Contrastive Learning with Meta Semantic Regularizer,arXiv,2206,"Wenwen Qiang, Jiangmeng Li, Changwen Zheng, Bing Su, Hui Xiong",Accepted by ICML 2022,https://arxiv.org/abs/2206.14702,
Is Multi-Modal Necessarily Better? Robustness Evaluation of Multi-modal Fake News Detection,arXiv,2206,"Jinyin Chen, Chengyu Jia, Haibin Zheng, Ruoxi Chen, Chenbo Fu",,https://arxiv.org/abs/2206.08788,
Kallima: A Clean-label Framework for Textual Backdoor Attacks,arXiv,2206,"Xiaoyi Chen, Yinpeng Dong, Zeyu Sun, Shengfang Zhai, Qingni Shen, Zhonghai Wu",,https://arxiv.org/abs/2206.01832,
Membership Inference via Backdooring,arXiv,2206,"Hongsheng Hu, Zoran Salcic, Gillian Dobbie, Jinjun Chen, Lichao Sun, Xuyun Zhang",This paper has been accepted by IJCAI-22,https://arxiv.org/abs/2206.04823,
Natural Backdoor Datasets,arXiv,2206,"Emily Wenger, Roma Bhattacharjee, Arjun Nitin Bhagoji, Josephine Passananti, Emilio Andere, Haitao Zheng, Ben Y. Zhao",18 pages,https://arxiv.org/abs/2206.10673,
Neurotoxin: Durable Backdoors in Federated Learning,arXiv,2206,"Zhengming Zhang, Ashwinee Panda, Linyue Song, Yaoqing Yang, Michael W. Mahoney, Joseph E. Gonzalez, Kannan Ramchandran, Prateek Mittal",Appears in ICML 2022,https://arxiv.org/abs/2206.10341,
On the Permanence of Backdoors in Evolving Models,arXiv,2206,"Huiying Li, Arjun Nitin Bhagoji, Yuxin Chen, Haitao Zheng, Ben Y. Zhao",,https://arxiv.org/abs/2206.04677,
Open Vocabulary Object Detection with Proposal Mining and Prediction Equalization,arXiv,2206,"Peixian Chen, Kekai Sheng, Mengdan Zhang, Mingbao Lin, Yunhang Shen, Shaohui Lin, Bo Ren, Ke Li",,https://arxiv.org/abs/2206.11134,
Turning a Curse into a Blessing: Enabling In-Distribution-Data-Free Backdoor Removal via Stabilized Model Inversion,arXiv,2206,"Si Chen, Yi Zeng, Jiachen T. Wang, Won Park, Xun Chen, Lingjuan Lyu, Zhuoqing Mao, Ruoxi Jia","Because of an equation and author informational error, this paper has been withdrawn by the submitter",https://arxiv.org/abs/2206.07018,
A Temporal-Pattern Backdoor Attack to Deep Reinforcement Learning,arXiv,2205,"Yinbo Yu, Jiajia Liu, Shouqing Li, Kepu Huang, Xudong Feng",,https://arxiv.org/abs/2205.02589,
Addressing Confounding Feature Issue for Causal Recommendation,arXiv,2205,"Xiangnan He, Yang Zhang, Fuli Feng, Chonggang Song, Lingling Yi, Guohui Ling, Yongdong Zhang",Accepted by TOIS 2022. Codes are available on Github: https://github.com/zyang1580/DCR,https://arxiv.org/abs/2205.06532,Github:
BITE: Textual Backdoor Attacks with Iterative Trigger Injection,arXiv,2205,"Jun Yan, Vansh Gupta, Xiang Ren",Accepted to ACL 2023,https://arxiv.org/abs/2205.12700,
Backdoor Attacks on Bayesian Neural Networks using Reverse Distribution,arXiv,2205,"Zhixin Pan, Prabhat Mishra","9 pages, 7 figures",https://arxiv.org/abs/2205.09167,
BadDet: Backdoor Attacks on Object Detection,arXiv,2205,"Shih-Han Chan, Yinpeng Dong, Jun Zhu, Xiaolu Zhang, Jun Zhou",,https://arxiv.org/abs/2205.14497,
BagFlip: A Certified Defense against Data Poisoning,arXiv,2205,"Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni",Neurips 2022,https://arxiv.org/abs/2205.13634,
Circumventing Backdoor Defenses That Are Based on Latent Separability,arXiv,2205,"Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, Prateek Mittal",,https://arxiv.org/abs/2205.13613,
Comprehensive Privacy Analysis on Federated Recommender System against Attribute Inference Attacks,arXiv,2205,"Shijie Zhang, Wei Yuan, Hongzhi Yin",,https://arxiv.org/abs/2205.11857,
Deep Deconfounded Content-based Tag Recommendation for UGC with Causal Intervention,arXiv,2205,"Yaochen Zhu, Xubin Ren, Jing Yi, Zhenzhong Chen",,https://arxiv.org/abs/2205.14380,
Defending Against Stealthy Backdoor Attacks,arXiv,2205,"Sangeet Sagar, Abhinav Bhatt, Abhijith Srinivas Bidaralli",,https://arxiv.org/abs/2205.14246,
Imperceptible Backdoor Attack: From Input Space to Feature Representation,arXiv,2205,"Nan Zhong, Zhenxing Qian, Xinpeng Zhang",IJCAI 2022,https://arxiv.org/abs/2205.03190,https://github.com/Ekko-zn/IJCAI2022-Backdoor.
Interpolating Compressed Parameter Subspaces,arXiv,2205,"Siddhartha Datta, Nigel Shadbolt",,https://arxiv.org/abs/2205.09891,
MM-BD: Post-Training Detection of Backdoor Attacks with Arbitrary Backdoor Pattern Types Using a Maximum Margin Statistic,arXiv,2205,"Hang Wang, Zhen Xiang, David J. Miller, George Kesidis",,https://arxiv.org/abs/2205.06900,
Model-Contrastive Learning for Backdoor Defense,arXiv,2205,"Zhihao Yue, Jun Xia, Zhiwei Ling, Ming Hu, Ting Wang, Xian Wei, Mingsong Chen",,https://arxiv.org/abs/2205.04411,https://github.com/WeCanShow/MCL.
PerDoor: Persistent Non-Uniform Backdoors in Federated Learning using Adversarial Perturbations,arXiv,2205,"Manaar Alam, Esha Sarkar, Michail Maniatakos",,https://arxiv.org/abs/2205.13523,
Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free,arXiv,2205,"Tianlong Chen, Zhenyu Zhang, Yihua Zhang, Shiyu Chang, Sijia Liu, Zhangyang Wang",,https://arxiv.org/abs/2205.11819,https://github.com/VITA-Group/Backdoor-LTH.
SafeNet: The Unreasonable Effectiveness of Ensembles in Private Collaborative Learning,arXiv,2205,"Harsh Chaudhari, Matthew Jagielski, Alina Oprea",,https://arxiv.org/abs/2205.09986,
Towards A Proactive ML Approach for Detecting Backdoor Poison Samples,arXiv,2205,"Xiangyu Qi, Tinghao Xie, Jiachen T. Wang, Tong Wu, Saeed Mahloujifar, Prateek Mittal",USENIX Security 2023,https://arxiv.org/abs/2205.13616,
Towards a Defense Against Federated Backdoor Attacks Under Continuous Training,arXiv,2205,"Shuaiqi Wang, Jonathan Hayase, Giulia Fanti, Sewoong Oh",,https://arxiv.org/abs/2205.11736,
Using Constraint Programming and Graph Representation Learning for Generating Interpretable Cloud Security Policies,arXiv,2205,"Mikhail Kazdagli, Mohit Tiwari, Akshat Kumar",to be published in IJCAI/ECAI'22,https://arxiv.org/abs/2205.01240,
Verifying Neural Networks Against Backdoor Attacks,arXiv,2205,"Long H. Pham, Jun Sun",,https://arxiv.org/abs/2205.06992,
WeDef: Weakly Supervised Backdoor Defense for Text Classification,arXiv,2205,"Lesheng Jin, Zihan Wang, Jingbo Shang",,https://arxiv.org/abs/2205.11803,
AdaTest:Reinforcement Learning and Adaptive Sampling for On-chip Hardware Trojan Detection,arXiv,2204,"Huili Chen, Xinqiao Zhang, Ke Huang, Farinaz Koushanfar",,https://arxiv.org/abs/2204.06117,
An Adaptive Black-box Backdoor Detection Method for Deep Neural Networks,arXiv,2204,"Xinqiao Zhang, Huili Chen, Ke Huang, Farinaz Koushanfar",arXiv admin note: substantial text overlap with arXiv:2102.01815,https://arxiv.org/abs/2204.04329,https://github.com/xinqiaozhang/adatrojan
Backdoor Attack against NLP models with Robustness-Aware Perturbation defense,arXiv,2204,"Shaik Mohammed Maqsood, Viveros Manuela Ceron, Addluri GowthamKrishna",,https://arxiv.org/abs/2204.05758,
Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient Ensembling,arXiv,2204,"KiYoon Yoo, Nojun Kwak","Accepted to EMNLP 2022, 9 pages and Appendix",https://arxiv.org/abs/2204.14017,
Backdooring Explainable Machine Learning,arXiv,2204,"Maximilian Noppel, Lukas Peter, Christian Wressnegger",,https://arxiv.org/abs/2204.09498,
Causality-based Neural Network Repair,arXiv,2204,"Bing Sun, Jun Sun, Hong Long Pham, Jie Shi",,https://arxiv.org/abs/2204.09274,
Data-Efficient Backdoor Attacks,arXiv,2204,"Pengfei Xia, Ziqiang Li, Wei Zhang, Bin Li",Accepted to IJCAI 2022 Long Oral,https://arxiv.org/abs/2204.12281,https://github.com/xpf/Data-Efficient-Backdoor-Attacks.
Detecting Backdoor Poisoning Attacks on Deep Neural Networks by Heatmap Clustering,arXiv,2204,"Lukas Schulth, Christian Berghoff, Matthias Neu",,https://arxiv.org/abs/2204.12848,
Eliminating Backdoor Triggers for Deep Neural Networks Using Attention Relation Graph Distillation,arXiv,2204,"Jun Xia, Ting Wang, Jiepin Ding, Xian Wei, Mingsong Chen",,https://arxiv.org/abs/2204.09975,
Exploring the Universal Vulnerability of Prompt-based Learning Paradigm,arXiv,2204,"Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Zhiyuan Liu",Accepted to Findings of NAACL 2022,https://arxiv.org/abs/2204.05239,https://github.com/leix28/prompt-universal-vulnerability
Knowledge-Free Black-Box Watermark and Ownership Proof for Image Classification Neural Networks,arXiv,2204,"Fangqi Li, Shilin Wang",11 pages,https://arxiv.org/abs/2204.04522,
Machine Learning Security against Data Poisoning: Are We There Yet?,arXiv,2204,"Antonio Emanuele Cinà, Kathrin Grosse, Ambra Demontis, Battista Biggio, Fabio Roli, Marcello Pelillo","preprint, 10 pages, 3 figures. Paper accepted to the IEEE Computer - Special Issue on Trustworthy AI",https://arxiv.org/abs/2204.05986,
Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information,arXiv,2204,"Yi Zeng, Minzhou Pan, Hoang Anh Just, Lingjuan Lyu, Meikang Qiu, Ruoxi Jia",13 pages of the main text,https://arxiv.org/abs/2204.05255,
Planting Undetectable Backdoors in Machine Learning Models,arXiv,2204,"Shafi Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, Or Zamir",,https://arxiv.org/abs/2204.06974,
Towards A Critical Evaluation of Robustness for Deep Learning Backdoor Countermeasures,arXiv,2204,"Huming Qiu, Hua Ma, Zhi Zhang, Alsharif Abuadbba, Wei Kang, Anmin Fu, Yansong Gao",,https://arxiv.org/abs/2204.06273,
Dynamic Backdoors with Global Average Pooling,arXiv,2203,"Stefanos Koffas, Stjepan Picek, Mauro Conti",,https://arxiv.org/abs/2203.02079,
Low-Loss Subspace Compression for Clean Gains against Multi-Agent Backdoor Attacks,arXiv,2203,"Siddhartha Datta, Nigel Shadbolt",,https://arxiv.org/abs/2203.03692,
Model Inversion Attack against Transfer Learning: Inverting a Model without Accessing It,arXiv,2203,"Dayong Ye, Huiqiang Chen, Shuai Zhou, Tianqing Zhu, Wanlei Zhou, Shouling Ji",,https://arxiv.org/abs/2203.06570,
OrphicX: A Causality-Inspired Latent Variable Model for Interpreting Graph Neural Networks,arXiv,2203,"Wanyu Lin, Hao Lan, Hao Wang, Baochun Li","Accepted by CVPR 2022, an oral presentation, source code: https://github.com/WanyuGroup/CVPR2022-OrphicX",https://arxiv.org/abs/2203.15209,https://github.com/WanyuGroup/CVPR2022-OrphicX
Physical Backdoor Attacks to Lane Detection Systems in Autonomous Driving,arXiv,2203,"Xingshuo Han, Guowen Xu, Yuan Zhou, Xuehuan Yang, Jiwei Li, Tianwei Zhang",Accepted by ACM MultiMedia 2022,https://arxiv.org/abs/2203.00858,
PiDAn: A Coherence Optimization Approach for Backdoor Attack Detection and Mitigation in Deep Neural Networks,arXiv,2203,"Yue Wang, Wenqing Li, Esha Sarkar, Muhammad Shafique, Michail Maniatakos, Saif Eddin Jabari",,https://arxiv.org/abs/2203.09289,
Sniper Backdoor: Single Client Targeted Backdoor Attack in Federated Learning,arXiv,2203,"Gorka Abad, Servio Paguada, Oguzhan Ersoy, Stjepan Picek, Víctor Julio Ramírez-Durán, Aitor Urbieta",,https://arxiv.org/abs/2203.08689,
Trojan Horse Training for Breaking Defenses against Backdoor Attacks in Deep Learning,arXiv,2203,"Arezoo Rajabi, Bhaskar Ramasubramanian, Radha Poovendran",Submitted to conference,https://arxiv.org/abs/2203.15506,
AntidoteRT: Run-time Detection and Correction of Poison Attacks on Neural Networks,arXiv,2202,"Muhammad Usman, Youcheng Sun, Divya Gopinath, Corina S. Pasareanu",,https://arxiv.org/abs/2202.01179,
BEAS: Blockchain Enabled Asynchronous & Secure Federated Machine Learning,arXiv,2202,"Arup Mondal, Harpreet Virk, Debayan Gupta",The Third AAAI Workshop on Privacy-Preserving Artificial Intelligence (PPAI-22) at the Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22),https://arxiv.org/abs/2202.02817,
Backdoor Defense in Federated Learning Using Differential Testing and Outlier Detection,arXiv,2202,"Yein Kim, Huili Chen, Farinaz Koushanfar",,https://arxiv.org/abs/2202.11196,
Backdoor Defense via Decoupling the Training Process,arXiv,2202,"Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, Kui Ren",This work is accepted by the ICLR 2022. The first two authors contributed equally to this work. 25 pages,https://arxiv.org/abs/2202.03423,\url{https://github.com/SCLBD/DBD}.
Constrained Optimization with Dynamic Bound-scaling for Effective NLPBackdoor Defense,arXiv,2202,"Guangyu Shen, Yingqi Liu, Guanhong Tao, Qiuling Xu, Zhuo Zhang, Shengwei An, Shiqing Ma, Xiangyu Zhang",,https://arxiv.org/abs/2202.05749,
Debiasing Backdoor Attack: A Benign Application of Backdoor Attack in Eliminating Data Bias,arXiv,2202,"Shangxi Wu, Qiuyang He, Yi Zhang, Jitao Sang",,https://arxiv.org/abs/2202.10582,
False Memory Formation in Continual Learners Through Imperceptible Backdoor Trigger,arXiv,2202,"Muhammad Umer, Robi Polikar",,https://arxiv.org/abs/2202.04479,
Identifying Backdoor Attacks in Federated Learning via Anomaly Detection,arXiv,2202,"Yuxi Mi, Yiheng Sun, Jihong Guan, Shuigeng Zhou",APWeb-WAIM 2023,https://arxiv.org/abs/2202.04311,
Jigsaw Puzzle: Selective Backdoor Attack to Subvert Malware Classifiers,arXiv,2202,"Limin Yang, Zhi Chen, Jacopo Cortellazzi, Feargus Pendlebury, Kevin Tu, Fabio Pierazzi, Lorenzo Cavallaro, Gang Wang","18 pages, 3 figures",https://arxiv.org/abs/2202.05470,
Label-Smoothed Backdoor Attack,arXiv,2202,"Minlong Peng, Zidi Xiong, Mingming Sun, Ping Li",Backdoor Attack,https://arxiv.org/abs/2202.11203,\url{https://github.com/v-mipeng/LabelSmoothedAttack.git}}.
More is Better (Mostly): On the Backdoor Attacks in Federated Graph Neural Networks,arXiv,2202,"Jing Xu, Rui Wang, Stefanos Koffas, Kaitai Liang, Stjepan Picek","15 pages, 13 figures",https://arxiv.org/abs/2202.03195,
On the Effectiveness of Adversarial Training against Backdoor Attacks,arXiv,2202,"Yinghua Gao, Dongxian Wu, Jingfeng Zhang, Guanhao Gan, Shu-Tao Xia, Gang Niu, Masashi Sugiyama",,https://arxiv.org/abs/2202.10627,
Partial Identification with Noisy Covariates: A Robust Optimization Approach,arXiv,2202,"Wenshuo Guo, Mingzhang Yin, Yixin Wang, Michael I. Jordan",Proceedings of Conference on Causal Learning and Reasoning (CLeaR) 2022,https://arxiv.org/abs/2202.10665,
PolicyCleanse: Backdoor Detection and Mitigation in Reinforcement Learning,arXiv,2202,"Junfeng Guo, Ang Li, Cong Liu",Accepted by ICCV 2023,https://arxiv.org/abs/2202.03609,
Progressive Backdoor Erasing via connecting Backdoor and Adversarial Attacks,arXiv,2202,"Bingxu Mu, Zhenxing Niu, Le Wang, Xue Wang, Rong Jin, Gang Hua",,https://arxiv.org/abs/2202.06312,
Resurrecting Trust in Facial Recognition: Mitigating Backdoor Attacks in Face Recognition to Prevent Potential Privacy Breaches,arXiv,2202,"Reena Zelenkova, Jack Swallow, M. A. P. Chamikara, Dongxi Liu, Mohan Baruwal Chhetri, Seyit Camtepe, Marthie Grobler, Mahathir Almashor",15 pages,https://arxiv.org/abs/2202.10320,
SAT Backdoors: Depth Beats Size,arXiv,2202,"Jan Dreier, Sebastian Ordyniak, Stefan Szeider",,https://arxiv.org/abs/2202.08326,
Threats to Pre-trained Language Models: Survey and Taxonomy,arXiv,2202,"Shangwei Guo, Chunlong Xie, Jiwei Li, Lingjuan Lyu, Tianwei Zhang",,https://arxiv.org/abs/2202.06862,
Training with More Confidence: Mitigating Injected and Natural Backdoors During Training,arXiv,2202,"Zhenting Wang, Hailun Ding, Juan Zhai, Shiqing Ma",,https://arxiv.org/abs/2202.06382,https://github.com/RU-System-Software-and-Security/NONE.
A new idea for RSA backdoors,arXiv,2201,Marco Cesati,"19 pages, 5 figures",https://arxiv.org/abs/2201.13153,
Backdoor Defense with Machine Unlearning,arXiv,2201,"Yang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li Wang, Jianfeng Ma",,https://arxiv.org/abs/2201.09538,
Backdoors Stuck At The Frontdoor: Multi-Agent Backdoor Attacks That Backfire,arXiv,2201,"Siddhartha Datta, Nigel Shadbolt",,https://arxiv.org/abs/2201.12211,
"Blockchain-based Collaborated Federated Learning for Improved Security, Privacy and Reliability",arXiv,2201,"Amir Afaq, Zeeshan Ahmed, Noman Haider, Muhammad Imran",Preliminary work,https://arxiv.org/abs/2201.08551,
Compression-Resistant Backdoor Attack against Deep Neural Networks,arXiv,2201,"Mingfu Xue, Xin Wang, Shichang Sun, Yushu Zhang, Jian Wang, Weiqiang Liu",,https://arxiv.org/abs/2201.00672,
Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World,arXiv,2201,"Hua Ma, Yinshan Li, Yansong Gao, Alsharif Abuadbba, Zhi Zhang, Anmin Fu, Hyoungshick Kim, Said F. Al-Sarawi, Nepal Surya, Derek Abbott",,https://arxiv.org/abs/2201.08619,
DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection,arXiv,2201,"Phillip Rieger, Thien Duc Nguyen, Markus Miettinen, Ahmad-Reza Sadeghi","18 pages, 8 figures; to appear in the Network and Distributed System Security Symposium (NDSS)",https://arxiv.org/abs/2201.00763,
Federated Unlearning with Knowledge Distillation,arXiv,2201,"Chen Wu, Sencun Zhu, Prasenjit Mitra",,https://arxiv.org/abs/2201.09441,
Few-Shot Backdoor Attacks on Visual Object Tracking,arXiv,2201,"Yiming Li, Haoxiang Zhong, Xingjun Ma, Yong Jiang, Shu-Tao Xia","This work is accepted by the ICLR 2022. The first two authors contributed equally to this work. In this version, we fix some typos and errors contained in the last one. 21 pages",https://arxiv.org/abs/2201.13178,
Hiding Behind Backdoors: Self-Obfuscation Against Generative Models,arXiv,2201,"Siddhartha Datta, Nigel Shadbolt",,https://arxiv.org/abs/2201.09774,
How to Backdoor HyperNetwork in Personalized Federated Learning?,arXiv,2201,"Phung Lai, NhatHai Phan, Issa Khalil, Abdallah Khreishah, Xintao Wu",,https://arxiv.org/abs/2201.07063,
Identifying a Training-Set Attack's Target Using Renormalized Influence Estimation,arXiv,2201,"Zayd Hammoudeh, Daniel Lowd",Accepted at CCS'2022 -- Extended version including the supplementary material,https://arxiv.org/abs/2201.10055,https://github.com/ZaydH/target_identification.
Imperceptible and Multi-channel Backdoor Attack against Deep Neural Networks,arXiv,2201,"Mingfu Xue, Shifeng Ni, Yinghao Wu, Yushu Zhang, Jian Wang, Weiqiang Liu",,https://arxiv.org/abs/2201.13164,
Neighboring Backdoor Attacks on Graph Convolutional Network,arXiv,2201,"Liang Chen, Qibiao Peng, Jintang Li, Yang Liu, Jiawei Chen, Yong Li, Zibin Zheng",12 pages,https://arxiv.org/abs/2201.06202,
On the Satisfaction Probability of $k$-CNF Formulas,arXiv,2201,Till Tantau,"43 pages, version updated after the presentation of the results at the CCC 2022 conference",https://arxiv.org/abs/2201.08895,
Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios,arXiv,2201,"Zhen Xiang, David J. Miller, George Kesidis",Accepted to ICLR2022,https://arxiv.org/abs/2201.08474,https://github.com/zhenxianglance/2ClassBADetection.
RFLBAT: A Robust Federated Learning Algorithm against Backdoor Attack,arXiv,2201,"Yongkang Wang, Dihua Zhai, Yufeng Zhan, Yuanqing Xia",,https://arxiv.org/abs/2201.03772,
Rethink the Evaluation for Attack Strength of Backdoor Attacks in Natural Language Processing,arXiv,2201,"Lingfeng Shen, Haiyun Jiang, Lemao Liu, Shuming Shi",,https://arxiv.org/abs/2201.02993,
Towards Adversarial Evaluations for Inexact Machine Unlearning,arXiv,2201,"Shashwat Goel, Ameya Prabhu, Amartya Sanyal, Ser-Nam Lim, Philip Torr, Ponnurangam Kumaraguru",Tech Report,https://arxiv.org/abs/2201.06640,
Watermarking Pre-trained Encoders in Contrastive Learning,arXiv,2201,"Yutong Wu, Han Qiu, Tianwei Zhang, Jiwei L, Meikang Qiu",,https://arxiv.org/abs/2201.08217,
Batch Label Inference and Replacement Attacks in Black-Boxed Vertical Federated Learning,arXiv,2112,"Yang Liu, Tianyuan Zou, Yan Kang, Wenhan Liu, Yuanqin He, Zhihao Yi, Qiang Yang","13 pages, 9 figures, 3 tables, related previous work see arXiv:2007.03608",https://arxiv.org/abs/2112.05409,
CatchBackdoor: Backdoor Testing by Critical Trojan Neural Path Identification via Differential Fuzzing,arXiv,2112,"Haibo Jin, Ruoxi Chen, Jinyin Chen, Yao Cheng, Chong Fu, Ting Wang, Yue Yu, Zhaoyan Ming",There are some problems in the experiment so we need to withdraw this paper. We will upload the new version after revision,https://arxiv.org/abs/2112.13064,
Causal Attention for Interpretable and Generalizable Graph Classification,arXiv,2112,"Yongduo Sui, Xiang Wang, Jiancan Wu, Min Lin, Xiangnan He, Tat-Seng Chua",Accepted to KDD 2022,https://arxiv.org/abs/2112.15089,
CoviChain: A Blockchain Based COVID-19 Vaccination Passport,arXiv,2112,"Philip Bradish, Sarang Chaudhari, Michael Clear, Hitesh Tewari",,https://arxiv.org/abs/2112.01097,
Dual-Key Multimodal Backdoors for Visual Question Answering,arXiv,2112,"Matthew Walmer, Karan Sikka, Indranil Sur, Abhinav Shrivastava, Susmit Jha","Published as conference paper at CVPR 2022. 22 pages, 11 figures, 12 tables",https://arxiv.org/abs/2112.07668,
FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis,arXiv,2112,"Yu Feng, Benteng Ma, Jing Zhang, Shanshan Zhao, Yong Xia, Dacheng Tao",Accepted by CVPR 2022,https://arxiv.org/abs/2112.01148,https://github.com/HazardFY/FIBA.
Few-shot Backdoor Defense Using Shapley Estimation,arXiv,2112,"Jiyang Guan, Zhuozhuo Tu, Ran He, Dacheng Tao",,https://arxiv.org/abs/2112.14889,
Protecting Your NLG Models with Semantic and Robust Watermarks,arXiv,2112,"Tao Xiang, Chunlong Xie, Shangwei Guo, Jiwei Li, Tianwei Zhang",,https://arxiv.org/abs/2112.05428,
Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures,arXiv,2112,"Eugene Bagdasaryan, Vitaly Shmatikov",IEEE S&P 2022. arXiv admin note: text overlap with arXiv:2107.10443,https://arxiv.org/abs/2112.05224,
Test-Time Detection of Backdoor Triggers for Poisoned Deep Neural Networks,arXiv,2112,"Xi Li, Zhen Xiang, David J. Miller, George Kesidis",,https://arxiv.org/abs/2112.03350,
Weakly-Supervised Video Object Grounding via Causal Intervention,arXiv,2112,"Wei Wang, Junyu Gao, Changsheng Xu",,https://arxiv.org/abs/2112.00475,
A General Framework for Defending Against Backdoor Attacks via Influence Graph,arXiv,2111,"Xiaofei Sun, Jiwei Li, Xiaoya Li, Ziyao Wang, Tianwei Zhang, Han Qiu, Fei Wu, Chun Fan",,https://arxiv.org/abs/2111.14309,
A Kernel Test for Causal Association via Noise Contrastive Backdoor Adjustment,arXiv,2111,"Robert Hu, Dino Sejdinovic, Robin J. Evans",,https://arxiv.org/abs/2111.13226,\hyperlink{https://github.com/MrHuff/kgformula}{\texttt{https://github.com/MrHuff/kgformula}}.
An Overview of Backdoor Attacks Against Deep Neural Networks and Possible Defences,arXiv,2111,"Wei Guo, Benedetta Tondi, Mauro Barni",,https://arxiv.org/abs/2111.08429,
Anomaly Localization in Model Gradients Under Backdoor Attacks Against Federated Learning,arXiv,2111,Zeki Bilgin,13 pages and the code is available,https://arxiv.org/abs/2111.14683,https://github.com/ArcelikAcikKaynak/Federated_Learning.git}}.
Backdoor Attack through Frequency Domain,arXiv,2111,"Tong Wang, Yuan Yao, Feng Xu, Shengwei An, Hanghang Tong, Ting Wang",,https://arxiv.org/abs/2111.10991,
Backdoor Pre-trained Models Can Transfer to All,arXiv,2111,"Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang Fang, Jianwei Yin, Ting Wang",,https://arxiv.org/abs/2111.00197,
DBIA: Data-free Backdoor Injection Attack against Transformer Networks,arXiv,2111,"Peizhuo Lv, Hualong Ma, Jiachen Zhou, Ruigang Liang, Kai Chen, Shengzhi Zhang, Yunfei Yang",,https://arxiv.org/abs/2111.11870,
Enhancing Backdoor Attacks with Multi-Level MMD Regularization,arXiv,2111,"Pengfei Xia, Hongjing Niu, Ziqiang Li, Bin Li",,https://arxiv.org/abs/2111.05077,https://github.com/xpf/Multi-Level-MMD-Regularization.
"Federated Learning Attacks Revisited: A Critical Discussion of Gaps, Assumptions, and Evaluation Setups",arXiv,2111,"Aidmar Wainakh, Ephraim Zimmer, Sandeep Subedi, Jens Keim, Tim Grube, Shankar Karuppayah, Alejandro Sanchez Guinea, Max Mühlhäuser","In Section 5.2, incomplete information are mentioned on reference [9] (""How To Backdoor Federated Learning""). This part of text will be revised and enriched",https://arxiv.org/abs/2111.03363,
NTD: Non-Transferability Enabled Backdoor Detection,arXiv,2111,"Yinshan Li, Hua Ma, Zhi Zhang, Yansong Gao, Alsharif Abuadbba, Anmin Fu, Yifeng Zheng, Said F. Al-Sarawi, Derek Abbott",,https://arxiv.org/abs/2111.11157,
TnT Attacks! Universal Naturalistic Adversarial Patches Against Deep Neural Network Systems,arXiv,2111,"Bao Gia Doan, Minhui Xue, Shiqing Ma, Ehsan Abbasnejad, Damith C. Ranasinghe",Accepted for publication in the IEEE Transactions on Information Forensics & Security (TIFS),https://arxiv.org/abs/2111.09999,
Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks,arXiv,2111,"Xiangyu Qi, Tinghao Xie, Ruizhe Pan, Jifeng Zhu, Yong Yang, Kai Bu",,https://arxiv.org/abs/2111.12965,
Triggerless Backdoor Attack for NLP Tasks with Clean Labels,arXiv,2111,"Leilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Yi Yang, Shangwei Guo, Chun Fan",Accepted to appear at the main conference of NAACL 2022,https://arxiv.org/abs/2111.07970,
AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis,arXiv,2110,"Junfeng Guo, Ang Li, Cong Liu",,https://arxiv.org/abs/2110.14880,
Adversarial Neuron Pruning Purifies Backdoored Deep Models,arXiv,2110,"Dongxian Wu, Yisen Wang",To appear in NeurIPS 2021,https://arxiv.org/abs/2110.14430,
Adversarial Unlearning of Backdoors via Implicit Hypergradient,arXiv,2110,"Yi Zeng, Si Chen, Won Park, Z. Morley Mao, Ming Jin, Ruoxi Jia",In proceeding of the Tenth International Conference on Learning Representations (ICLR 2022),https://arxiv.org/abs/2110.03735,
Anti-Backdoor Learning: Training Clean Models on Poisoned Data,arXiv,2110,"Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma",Accepted to NeurIPS 2021,https://arxiv.org/abs/2110.11571,\url{https://github.com/bboylyg/ABL}.
BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models,arXiv,2110,"Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei Li, Chun Fan",,https://arxiv.org/abs/2110.02467,
Bugs in our Pockets: The Risks of Client-Side Scanning,arXiv,2110,"Hal Abelson, Ross Anderson, Steven M. Bellovin, Josh Benaloh, Matt Blaze, Jon Callas, Whitfield Diffie, Susan Landau, Peter G. Neumann, Ronald L. Rivest, Jeffrey I. Schiller, Bruce Schneier, Vanessa Teague, Carmela Troncoso","46 pages, 3 figures",https://arxiv.org/abs/2110.07450,
CoProtector: Protect Open-Source Code against Unauthorized Training Usage with Data Poisoning,arXiv,2110,"Zhensu Sun, Xiaoning Du, Fu Song, Mingze Ni, Li Li","8 pages, accepted to WWW 2022",https://arxiv.org/abs/2110.12925,Github
Detecting Backdoor Attacks Against Point Cloud Classifiers,arXiv,2110,"Zhen Xiang, David J. Miller, Siheng Chen, Xi Li, George Kesidis",,https://arxiv.org/abs/2110.10354,
Don't Knock! Rowhammer at the Backdoor of DNN Models,arXiv,2110,"M. Caner Tol, Saad Islam, Andrew J. Adiletta, Berk Sunar, Ziming Zhang",2023 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN),https://arxiv.org/abs/2110.07683,
Dyn-Backdoor: Backdoor Attack on Dynamic Link Prediction,arXiv,2110,"Jinyin Chen, Haiyang Xiong, Haibin Zheng, Jian Zhang, Guodong Jiang, Yi Liu","11 pages,6 figures",https://arxiv.org/abs/2110.03875,
Finding Backdoors to Integer Programs: A Monte Carlo Tree Search Framework,arXiv,2110,"Elias B. Khalil, Pashootan Vaezipoor, Bistra Dilkina",Published in the Proceedings of AAAI 2022,https://arxiv.org/abs/2110.08423,
Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer,arXiv,2110,"Fanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, Maosong Sun",Accepted by the main conference of EMNLP 2021 as a long paper. The camera-ready version,https://arxiv.org/abs/2110.07139,https://github.com/thunlp/StyleAttack.
PipAttack: Poisoning Federated Recommender Systems forManipulating Item Promotion,arXiv,2110,"Shijie Zhang, Hongzhi Yin, Tong Chen, Zi Huang, Quoc Viet Hung Nguyen, Lizhen Cui",Proceedings of the 15th ACM International Conference on Web Search and Data Mining (WSDM '22),https://arxiv.org/abs/2110.10926,
Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks,arXiv,2110,"Shawn Shan, Arjun Nitin Bhagoji, Haitao Zheng, Ben Y. Zhao",18 pages,https://arxiv.org/abs/2110.06904,
Qu-ANTI-zation: Exploiting Quantization Artifacts for Achieving Adversarial Outcomes,arXiv,2110,"Sanghyun Hong, Michael-Andrei Panaitescu-Liess, Yiğitcan Kaya, Tudor Dumitraş",Accepted to NeurIPS 2021 [Poster],https://arxiv.org/abs/2110.13541,https://github.com/Secure-AI-Systems-Group/Qu-ANTI-zation
RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models,arXiv,2110,"Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, Xu Sun","EMNLP 2021 (main conference), long paper, camera-ready version",https://arxiv.org/abs/2110.07831,https://github.com/lancopku/RAP.
Securing Federated Learning: A Covert Communication-based Approach,arXiv,2110,"Yuan-Ai Xie, Jiawen Kang, Dusit Niyato, Nguyen Thi Thanh Van, Nguyen Cong Luong, Zhixin Liu, Han Yu",,https://arxiv.org/abs/2110.02221,
Semantic Host-free Trojan Attack,arXiv,2110,"Haripriya Harikumar, Kien Do, Santu Rana, Sunil Gupta, Svetha Venkatesh",,https://arxiv.org/abs/2110.13414,
Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks,arXiv,2110,"Yangyi Chen, Fanchao Qi, Hongcheng Gao, Zhiyuan Liu, Maosong Sun","Accepted to EMNLP 2022, main conference",https://arxiv.org/abs/2110.08247,\url{https://github.com/thunlp/StyleAttack}.
Trigger Hunting with a Topological Prior for Trojan Detection,arXiv,2110,"Xiaoling Hu, Xiao Lin, Michael Cogswell, Yi Yao, Susmit Jha, Chao Chen","17 pages, 10 figures",https://arxiv.org/abs/2110.08335,
Watermarking Graph Neural Networks based on Backdoor Attacks,arXiv,2110,"Jing Xu, Stefanos Koffas, Oguzhan Ersoy, Stjepan Picek","18 pages, 9 figures",https://arxiv.org/abs/2110.11024,
When can relative risks provide causal estimates?,arXiv,2110,A. J. Webster,2 figures,https://arxiv.org/abs/2110.01688,
Widen The Backdoor To Let More Attackers In,arXiv,2110,"Siddhartha Datta, Giulio Lovisotto, Ivan Martinovic, Nigel Shadbolt",,https://arxiv.org/abs/2110.04571,
A Synergetic Attack against Neural Network Classifiers combining Backdoor and Adversarial Examples,arXiv,2109,"Guanxiong Liu, Issa Khalil, Abdallah Khreishah, NhatHai Phan",,https://arxiv.org/abs/2109.01275,
BFClass: A Backdoor-free Text Classification Framework,arXiv,2109,"Zichao Li, Dheeraj Mekala, Chengyu Dong, Jingbo Shang",Accepted to appear in Findings of EMNLP 2021,https://arxiv.org/abs/2109.10855,
Backdoor Attack and Defense for Deep Regression,arXiv,2109,"Xi Li, George Kesidis, David J. Miller, Vladimir Lucic",,https://arxiv.org/abs/2109.02381,
Backdoor Attack on Hash-based Image Retrieval via Clean-label Data Poisoning,arXiv,2109,"Kuofeng Gao, Jiawang Bai, Bin Chen, Dongxian Wu, Shu-Tao Xia",Accepted by BMVC 2023,https://arxiv.org/abs/2109.08868,https://github.com/KuofengGao/CIBA.
Backdoor Attacks on Federated Learning with Lottery Ticket Hypothesis,arXiv,2109,"Zeyuan Yin, Ye Yuan, Panfeng Guo, Pan Zhou",,https://arxiv.org/abs/2109.10512,
Check Your Other Door! Creating Backdoor Attacks in the Frequency Domain,arXiv,2109,"Hasan Abed Al Kader Hammoud, Bernard Ghanem",Accepted to BMVC 2022,https://arxiv.org/abs/2109.05507,
Excess Capacity and Backdoor Poisoning,arXiv,2109,"Naren Sarayu Manoj, Avrim Blum",Accepted to NeurIPS 2021,https://arxiv.org/abs/2109.00685,
FooBaR: Fault Fooling Backdoor Attack on Neural Network Training,arXiv,2109,"Jakub Breier, Xiaolu Hou, Martín Ochoa, Jesus Solano",Published in IEEE TDSC,https://arxiv.org/abs/2109.11249,
Honey or Poison? Solving the Trigger Curse in Few-shot Event Detection via Causal Intervention,arXiv,2109,"Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun",Accepted to the main conference of EMNLP2021,https://arxiv.org/abs/2109.05747,
How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data,arXiv,2109,"Zhiyuan Zhang, Lingjuan Lyu, Weiqiang Wang, Lichao Sun, Xu Sun",Accepted by ICLR 2022,https://arxiv.org/abs/2109.01300,
SanitAIs: Unsupervised Data Augmentation to Sanitize Trojaned Neural Networks,arXiv,2109,"Kiran Karra, Chace Ashcraft, Cash Costello","7 pages, 5 figures",https://arxiv.org/abs/2109.04566,
Security Review of Ethereum Beacon Clients,arXiv,2109,"Jean-Philippe Aumasson, Denis Kolegov, Evangelia Stathopoulou","43 pages, 3 figures, 3 tables",https://arxiv.org/abs/2109.11677,
Trade or Trick? Detecting and Characterizing Scam Tokens on Uniswap Decentralized Exchange,arXiv,2109,"Pengcheng Xia, Haoyu wang, Bingyu Gao, Weihang Su, Zhou Yu, Xiapu Luo, Chao Zhang, Xusheng Xiao, Guoai Xu",,https://arxiv.org/abs/2109.00229,
Trojan Signatures in DNN Weights,arXiv,2109,"Greg Fields, Mohammad Samragh, Mojan Javaheripi, Farinaz Koushanfar, Tara Javidi","8 pages, 13 figures",https://arxiv.org/abs/2109.02836,
Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning,arXiv,2108,"Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, Xipeng Qiu",Accepted by EMNLP2021 main conference,https://arxiv.org/abs/2108.13888,
BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning,arXiv,2108,"Jinyuan Jia, Yupei Liu, Neil Zhenqiang Gong","To appear in IEEE Symposium on Security and Privacy, 2022",https://arxiv.org/abs/2108.00352,https://github.com/jjy1994/BadEncoder.
Causal Inference in Educational Systems: A Graphical Modeling Approach,arXiv,2108,"Manie Tadayon, Greg Pottie",,https://arxiv.org/abs/2108.00654,
Poison Ink: Robust and Invisible Backdoor Attack,arXiv,2108,"Jie Zhang, Dongdong Chen, Qidong Huang, Jing Liao, Weiming Zhang, Huamin Feng, Gang Hua, Nenghai Yu",IEEE Transactions on Image Processing (TIP),https://arxiv.org/abs/2108.02488,
Quantization Backdoors to Deep Learning Commercial Frameworks,arXiv,2108,"Hua Ma, Huming Qiu, Yansong Gao, Zhi Zhang, Alsharif Abuadbba, Minhui Xue, Anmin Fu, Zhang Jiliang, Said Al-Sarawi, Derek Abbott",,https://arxiv.org/abs/2108.09187,
TRAPDOOR: Repurposing backdoors to detect dataset bias in machine learning-based genomic analysis,arXiv,2108,"Esha Sarkar, Michail Maniatakos",,https://arxiv.org/abs/2108.10132,
The Devil is in the GAN: Backdoor Attacks and Defenses in Deep Generative Models,arXiv,2108,"Ambrish Rawat, Killian Levacher, Mathieu Sinn","17 pages, 11 figures, 3 tables",https://arxiv.org/abs/2108.01644,
Can You Hear It? Backdoor Attacks via Ultrasonic Triggers,arXiv,2107,"Stefanos Koffas, Jing Xu, Mauro Conti, Stjepan Picek",,https://arxiv.org/abs/2107.14569,
Spinning Sequence-to-Sequence Models with Meta-Backdoors,arXiv,2107,"Eugene Bagdasaryan, Vitaly Shmatikov","Outdated. Superseded by arXiv:2112.05224 and published at IEEE S&P'22 with title: ""Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures""",https://arxiv.org/abs/2107.10443,
Structural Watermarking to Deep Neural Networks via Network Channel Pruning,arXiv,2107,"Xiangyu Zhao, Yinzhe Yao, Hanzhou Wu, Xinpeng Zhang",Accepted by IEEE International Workshop on Information Forensics and Security 2021,https://arxiv.org/abs/2107.08688,
Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting,arXiv,2107,"Xiangyu Qi, Jifeng Zhu, Chulin Xie, Yong Yang","6 pages, 3 figures, ICLR 2021 Workshop on Security and Safety in Machine Learning System",https://arxiv.org/abs/2107.07240,
Towards Unbiased Visual Emotion Recognition via Causal Intervention,arXiv,2107,"Yuedong Chen, Xu Yang, Tat-Jen Cham, Jianfei Cai","Accepted to ACM Multimedia 2022, code is available at https://github.com/donydchen/causal_emotion",https://arxiv.org/abs/2107.12096,https://github.com/donydchen/causal_emotion
Understanding the Security of Deepfake Detection,arXiv,2107,"Xiaoyu Cao, Neil Zhenqiang Gong",To appear in ICDF2C 2021,https://arxiv.org/abs/2107.02045,
A Unified Framework for Task-Driven Data Quality Management,arXiv,2106,"Tianhao Wang, Yi Zeng, Ming Jin, Ruoxi Jia",,https://arxiv.org/abs/2106.05484,
Backdoor Learning Curves: Explaining Backdoor Poisoning Beyond Influence Functions,arXiv,2106,"Antonio Emanuele Cinà, Kathrin Grosse, Sebastiano Vascon, Ambra Demontis, Battista Biggio, Fabio Roli, Marcello Pelillo",preprint; 28 pages,https://arxiv.org/abs/2106.07214,
CRFL: Certifiably Robust Federated Learning against Backdoor Attacks,arXiv,2106,"Chulin Xie, Minghao Chen, Pin-Yu Chen, Bo Li",ICML 2021,https://arxiv.org/abs/2106.08283,https://github.com/AI-secure/CRFL.
De-biasing Distantly Supervised Named Entity Recognition via Causal Intervention,arXiv,2106,"Wenkai Zhang, Hongyu Lin, Xianpei Han, Le Sun",Accepted to ACL2021(main conference),https://arxiv.org/abs/2106.09233,
Deconfounded Video Moment Retrieval with Causal Intervention,arXiv,2106,"Xun Yang, Fuli Feng, Wei Ji, Meng Wang, Tat-Seng Chua",This work has been accepted by SIGIR 2021,https://arxiv.org/abs/2106.01534,\color{blue}{\url{https://github.com/Xun-Yang/Causal_Video_Moment_Retrieval}}
Defending Against Backdoor Attacks in Natural Language Generation,arXiv,2106,"Xiaofei Sun, Xiaoya Li, Yuxian Meng, Xiang Ao, Lingjuan Lyu, Jiwei Li, Tianwei Zhang",To appear at AAAI 2023,https://arxiv.org/abs/2106.01810,
Detect and remove watermark in deep neural networks via generative adversarial networks,arXiv,2106,"Haoqi Wang, Mingfu Xue, Shichang Sun, Yushu Zhang, Jian Wang, Weiqiang Liu",,https://arxiv.org/abs/2106.08104,
Handcrafted Backdoors in Deep Neural Networks,arXiv,2106,"Sanghyun Hong, Nicholas Carlini, Alexey Kurakin",Accepted to NeurIPS 2022 [Oral],https://arxiv.org/abs/2106.04690,
Interventional Video Grounding with Dual Contrastive Learning,arXiv,2106,"Guoshun Nan, Rui Qiao, Yao Xiao, Jun Liu, Sicong Leng, Hao Zhang, Wei Lu",Accepted in CVPR 2021,https://arxiv.org/abs/2106.11013,GitHub:
Learning Pseudo-Backdoors for Mixed Integer Programs,arXiv,2106,"Aaron Ferber, Jialin Song, Bistra Dilkina, Yisong Yue","2 pages, 1 page reference, SOCS extended abstract",https://arxiv.org/abs/2106.05080,
Machine Learning with Electronic Health Records is vulnerable to Backdoor Trigger Attacks,arXiv,2106,"Byunggill Joe, Akshay Mehra, Insik Shin, Jihun Hamm",,https://arxiv.org/abs/2106.07925,
Poisoning and Backdooring Contrastive Learning,arXiv,2106,"Nicholas Carlini, Andreas Terzis",,https://arxiv.org/abs/2106.09667,
Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch,arXiv,2106,"Hossein Souri, Liam Fowl, Rama Chellappa, Micah Goldblum, Tom Goldstein",NeurIPS 2022,https://arxiv.org/abs/2106.08970,https://github.com/hsouri/Sleeper-Agent.
Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution,arXiv,2106,"Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, Maosong Sun",Accepted by the main conference of ACL-IJCNLP as a long paper. Camera-ready version,https://arxiv.org/abs/2106.06361,https://github.com/thunlp/BkdAtk-LWS.
A Master Key Backdoor for Universal Impersonation Attack against DNN-based Face Verification,arXiv,2105,"Wei Guo, Benedetta Tondi, Mauro Barni",,https://arxiv.org/abs/2105.00249,
BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning,arXiv,2105,"Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, Dawn Song",,https://arxiv.org/abs/2105.00579,
Backdoor Attacks on Self-Supervised Learning,arXiv,2105,"Aniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Koohpayegani, Hamed Pirsiavash",CVPR 2022 (Oral),https://arxiv.org/abs/2105.10123,https://github.com/UMBCvision/SSL-Backdoor
Deconfounded Recommendation for Alleviating Bias Amplification,arXiv,2105,"Wenjie Wang, Fuli Feng, Xiangnan He, Xiang Wang, Tat-Seng Chua",,https://arxiv.org/abs/2105.10648,
DeepObliviate: A Powerful Charm for Erasing Data Residual Memory in Deep Neural Networks,arXiv,2105,"Yingzhe He, Guozhu Meng, Kai Chen, Jinwen He, Xingbo Hu","16 pages, 10 figures, conference",https://arxiv.org/abs/2105.06209,
Detecting Backdoor in Deep Neural Networks via Intentional Adversarial Perturbations,arXiv,2105,"Mingfu Xue, Yinghao Wu, Zhiyu Wu, Yushu Zhang, Jian Wang, Weiqiang Liu",,https://arxiv.org/abs/2105.14259,
Hidden Backdoors in Human-Centric Language Models,arXiv,2105,"Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, Jialiang Lu",,https://arxiv.org/abs/2105.00164,
Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger,arXiv,2105,"Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, Maosong Sun",Accepted by ACL-IJCNLP 2021 as a long paper. Camera-ready version,https://arxiv.org/abs/2105.12400,https://github.com/thunlp/HiddenKiller.
Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks,arXiv,2105,"Charles Jin, Melinda Sun, Martin Rinard",ICLR 2023. Code is available at https://github.com/charlesjin/compatibility_clustering/,https://arxiv.org/abs/2105.03692,https://github.com/charlesjin/compatibility_clustering/
Intrusion Detection System in Smart Home Network Using Bidirectional LSTM and Convolutional Neural Networks Hybrid Model,arXiv,2105,"Nelly Elsayed, Zaghloul Saad Zaghloul, Sylvia Worlali Azumah, Chengcheng Li","4 pages, 6 figures, Accepted in the MWSCAS 2021. This material is based upon work supported by the National Science Foundation under Grant No. (CNS-1801593). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation",https://arxiv.org/abs/2105.12096,
Poisoning MorphNet for Clean-Label Backdoor Attack to Point Clouds,arXiv,2105,"Guiyu Tian, Wenhao Jiang, Wei Liu, Yadong Mu",,https://arxiv.org/abs/2105.04839,
A Backdoor Attack against 3D Point Cloud Classifiers,arXiv,2104,"Zhen Xiang, David J. Miller, Siheng Chen, Xi Li, George Kesidis",,https://arxiv.org/abs/2104.05808,
Backdoor Attack in the Physical World,arXiv,2104,"Yiming Li, Tongqing Zhai, Yong Jiang, Zhifeng Li, Shu-Tao Xia","This work was done when Yiming Li was an intern at Tencent AI Lab, supported by the Tencent Rhino-Bird Elite Training Program (2020). This is a 6-pages short version of our ongoing work, `Rethinking the Trigger of Backdoor Attack' (arXiv:2004.04692). It is accepted by the non-archival ICLR 2021 workshop on Robust and Reliable Machine Learning in the Real World. arXiv admin note: substantial text overlap with arXiv:2004.04692",https://arxiv.org/abs/2104.02361,
Explainability-based Backdoor Attacks Against Graph Neural Networks,arXiv,2104,"Jing Xu,  Minhui,  Xue, Stjepan Picek","6 pages, 4 figures",https://arxiv.org/abs/2104.03674,
Interventional Aspect-Based Sentiment Analysis,arXiv,2104,"Zhen Bi, Ningyu Zhang, Ganqiang Ye, Haiyang Yu, Xi Chen, Huajun Chen",Work in progress,https://arxiv.org/abs/2104.11681,
Manipulating SGD with Data Ordering Attacks,arXiv,2104,"Ilia Shumailov, Zakhar Shumaylov, Dmitry Kazhdan, Yiren Zhao, Nicolas Papernot, Murat A. Erdogdu, Ross Anderson",,https://arxiv.org/abs/2104.09667,
Protecting the Intellectual Properties of Deep Neural Networks with an Additional Class and Steganographic Images,arXiv,2104,"Shichang Sun, Mingfu Xue, Jian Wang, Weiqiang Liu",,https://arxiv.org/abs/2104.09203,
Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective,arXiv,2104,"Yi Zeng, Won Park, Z. Morley Mao, Ruoxi Jia",,https://arxiv.org/abs/2104.03413,
Reversible Watermarking in Deep Convolutional Neural Networks for Integrity Authentication,arXiv,2104,"Xiquan Guan, Huamin Feng, Weiming Zhang, Hang Zhou, Jie Zhang, Nenghai Yu",Accepted to ACM MM 2020,https://arxiv.org/abs/2104.04268,
Robust Backdoor Attacks against Deep Neural Networks in Real Physical World,arXiv,2104,"Mingfu Xue, Can He, Shichang Sun, Jian Wang, Weiqiang Liu",,https://arxiv.org/abs/2104.07395,
SGBA: A Stealthy Scapegoat Backdoor Attack against Deep Neural Networks,arXiv,2104,"Ying He, Zhili Shen, Chang Xia, Jingyu Hua, Wei Tong, Sheng Zhong",,https://arxiv.org/abs/2104.01026,
SPECTRE: Defending Against Backdoor Attacks Using Robust Statistics,arXiv,2104,"Jonathan Hayase, Weihao Kong, Raghav Somani, Sewoong Oh",29 pages 19 figures,https://arxiv.org/abs/2104.11315,https://github.com/SewoongLab/spectre-defense
Stealthy Backdoors as Compression Artifacts,arXiv,2104,"Yulong Tian, Fnu Suya, Fengyuan Xu, David Evans","20 pages, 9 figures, 14 tables",https://arxiv.org/abs/2104.15129,
Towards Causal Federated Learning For Enhanced Robustness and Privacy,arXiv,2104,"Sreya Francis, Irene Tenison, Irina Rish",,https://arxiv.org/abs/2104.06557,
Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models,arXiv,2103,"Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, Bin He","NAACL-HLT 2021, Long Paper",https://arxiv.org/abs/2103.15543,https://github.com/lancopku/Embedding-Poisoning.
Black-box Detection of Backdoor Attacks with Limited Information and Data,arXiv,2103,"Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, Jun Zhu",,https://arxiv.org/abs/2103.13127,
DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations,arXiv,2103,"Eitan Borgnia, Jonas Geiping, Valeriia Cherepanova, Liam Fowl, Arjun Gupta, Amin Ghiasi, Furong Huang, Micah Goldblum, Tom Goldstein","11 pages, 5 figures",https://arxiv.org/abs/2103.02079,
EX-RAY: Distinguishing Injected Backdoor from Natural Features in Neural Networks by Examining Differential Feature Symmetry,arXiv,2103,"Yingqi Liu, Guangyu Shen, Guanhong Tao, Zhenting Wang, Shiqing Ma, Xiangyu Zhang",21 pages,https://arxiv.org/abs/2103.08820,
Fooling LiDAR Perception via Adversarial Trajectory Perturbation,arXiv,2103,"Yiming Li, Congcong Wen, Felix Juefei-Xu, Chen Feng",2021 IEEE International Conference on Computer Vision (ICCV) [Oral Presentation],https://arxiv.org/abs/2103.15326,https://ai4ce.github.io/FLAT/.
Hidden Backdoor Attack against Semantic Segmentation Models,arXiv,2103,"Yiming Li, Yanjie Li, Yalei Lv, Yong Jiang, Shu-Tao Xia","This is a 6-pages short version of our ongoing work. It is accepted by the non-archival ICLR workshop on Security and Safety in Machine Learning Systems, 2021. The first two authors contributed equally to this work",https://arxiv.org/abs/2103.04038,
HufuNet: Embedding the Left Piece as Watermark and Keeping the Right Piece for Ownership Verification in Deep Neural Networks,arXiv,2103,"Peizhuo Lv, Pan Li, Shengzhi Zhang, Kai Chen, Ruigang Liang, Yue Zhao, Yingjiu Li",,https://arxiv.org/abs/2103.13628,
Learning Domain Invariant Representations for Generalizable Person Re-Identification,arXiv,2103,"Yi-Fan Zhang, Zhang Zhang, Da Li, Zhen Jia, Liang Wang, Tieniu Tan",,https://arxiv.org/abs/2103.15890,
On the Hierarchical Community Structure of Practical Boolean Formulas,arXiv,2103,"Chunxiao Li, Jonathan Chung, Soham Mukherjee, Marc Vinyals, Noah Fleming, Antonina Kolokolova, Alice Mu, Vijay Ganesh",,https://arxiv.org/abs/2103.14992,
PointBA: Towards Backdoor Attacks in 3D Point Cloud,arXiv,2103,"Xinke Li, Zhirui Chen, Yue Zhao, Zekun Tong, Yabang Zhao, Andrew Lim, Joey Tianyi Zhou",Accepted by ICCV 2021,https://arxiv.org/abs/2103.16074,
T-Miner: A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification,arXiv,2103,"Ahmadreza Azizi, Ibrahim Asadullah Tahmid, Asim Waheed, Neal Mangaokar, Jiameng Pu, Mobin Javed, Chandan K. Reddy, Bimal Viswanath","Accepted to Usenix Security 2021; First two authors contributed equally to this work; 18 pages, 11 tables",https://arxiv.org/abs/2103.04264,
TOP: Backdoor Detection in Neural Networks via Transferability of Perturbation,arXiv,2103,"Todd Huster, Emmanuel Ekwedike",,https://arxiv.org/abs/2103.10274,
Adversarial Targeted Forgetting in Regularization and Generative Based Continual Learning Models,arXiv,2102,"Muhammad Umer, Robi Polikar",arXiv admin note: text overlap with arXiv:2002.07111,https://arxiv.org/abs/2102.08355,
Backdoor Scanning for Deep Neural Networks through K-Arm Optimization,arXiv,2102,"Guangyu Shen, Yingqi Liu, Guanhong Tao, Shengwei An, Qiuling Xu, Siyuan Cheng, Shiqing Ma, Xiangyu Zhang",,https://arxiv.org/abs/2102.05123,
Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning,arXiv,2102,"Syed Zawad, Ahsan Ali, Pin-Yu Chen, Ali Anwar, Yi Zhou, Nathalie Baracaldo, Yuan Tian, Feng Yan",Accepted in AAAI 2021,https://arxiv.org/abs/2102.00655,
Meta Federated Learning,arXiv,2102,"Omid Aramoon, Pin-Yu Chen, Gang Qu, Yuan Tian","11 pages, 5 figures",https://arxiv.org/abs/2102.05561,
Necessary and sufficient graphical conditions for optimal adjustment sets in causal graphical models with hidden variables,arXiv,2102,Jakob Runge,"41 pages, published as spotlight paper in 35th Conference on Neural Information Processing Systems (NeurIPS 2021); this version has an updated Supplementary Material with corrected proofs (also updated in NeurIPS proceedings)",https://arxiv.org/abs/2102.10324,\url{https://github.com/jakobrunge/tigramite}.
Recursive Backdoors for SAT,arXiv,2102,"Nikolas Mählmann, Sebastian Siebertz, Alexandre Vigny",,https://arxiv.org/abs/2102.04707,
Robust Federated Learning with Attack-Adaptive Aggregation,arXiv,2102,"Ching Pui Wan, Qifeng Chen","14 pages, submitted to FTL-IJCAI'21",https://arxiv.org/abs/2102.05257,
SAFELearning: Enable Backdoor Detectability In Federated Learning With Secure Aggregation,arXiv,2102,"Zhuosheng Zhang, Jiarui Li, Shucheng Yu, Christian Makaya",,https://arxiv.org/abs/2102.02402,
Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits,arXiv,2102,"Jiawang Bai, Baoyuan Wu, Yong Zhang, Yiming Li, Zhifeng Li, Shu-Tao Xia",Accepted by ICLR 2021,https://arxiv.org/abs/2102.10496,
WaNet -- Imperceptible Warping-based Backdoor Attack,arXiv,2102,"Anh Nguyen, Anh Tran",Accepted to ICLR 2021,https://arxiv.org/abs/2102.10369,
What Doesn't Kill You Makes You Robust(er): How to Adversarially Train against Data Poisoning,arXiv,2102,"Jonas Geiping, Liam Fowl, Gowthami Somepalli, Micah Goldblum, Michael Moeller, Tom Goldstein","25 pages, 15 figures",https://arxiv.org/abs/2102.13624,
DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection,arXiv,2101,"Yuanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, Yunxin Liu",ICSE 2021,https://arxiv.org/abs/2101.06896,
Explainability Matters: Backdoor Attacks on Medical Imaging,arXiv,2101,"Munachiso Nwadike, Takumi Miyawaki, Esha Sarkar, Michail Maniatakos, Farah Shamout",,https://arxiv.org/abs/2101.00008,
FLAME: Taming Backdoors in Federated Learning (Extended Version 1),arXiv,2101,"Thien Duc Nguyen, Phillip Rieger, Huili Chen, Hossein Yalame, Helen Möllering, Hossein Fereidooni, Samuel Marchal, Markus Miettinen, Azalia Mirhoseini, Shaza Zeitouni, Farinaz Koushanfar, Ahmad-Reza Sadeghi, Thomas Schneider","This extended version incorporates a novel section (Section 10) that provides a comprehensive analysis of recent proposed attacks, notably ""3DFed: Adaptive and extensible framework for covert backdoor attack in federated learning"" by Li et al. This new section addresses flawed assertions made in the papers that aim to bypass FLAME or misinterpreted its fundamental design principles",https://arxiv.org/abs/2101.02281,
Graph Embedding for Recommendation against Attribute Inference Attacks,arXiv,2101,"Shijie Zhang, Hongzhi Yin, Tong Chen, Zi Huang, Lizhen Cui, Xiangliang Zhang",,https://arxiv.org/abs/2101.12549,
Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks,arXiv,2101,"Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma","19 pages, 14 figures, ICLR 2021",https://arxiv.org/abs/2101.05930,https://github.com/bboylyg/NAD.
On Provable Backdoor Defense in Collaborative Learning,arXiv,2101,"Ximing Qiao, Yuhua Bai, Siping Hu, Ang Li, Yiran Chen, Hai Li",,https://arxiv.org/abs/2101.08177,
Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-Level Backdoor Attacks,arXiv,2101,"Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Xin Jiang, Maosong Sun",Published in Machine Intelligence Research (https://link.springer.com/article/10.1007/s11633-022-1377-5),https://arxiv.org/abs/2101.06969,\url{https://github.com/thunlp/NeuBA}.
What Do Deep Nets Learn? Class-wise Patterns Revealed in the Input Space,arXiv,2101,"Shihao Zhao, Xingjun Ma, Yisen Wang, James Bailey, Bo Li, Yu-Gang Jiang",,https://arxiv.org/abs/2101.06898,
Budgeted and Non-budgeted Causal Bandits,arXiv,2012,"Vineet Nair, Vishakha Patil, Gaurav Sinha",,https://arxiv.org/abs/2012.07058,
Certified Robustness of Nearest Neighbors against Data Poisoning and Backdoor Attacks,arXiv,2012,"Jinyuan Jia, Yupei Liu, Xiaoyu Cao, Neil Zhenqiang Gong","To appear in AAAI Conference on Artificial Intelligence, 2022",https://arxiv.org/abs/2012.03765,
"Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses",arXiv,2012,"Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander Madry, Bo Li, Tom Goldstein",,https://arxiv.org/abs/2012.10544,
Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification,arXiv,2012,"Siyuan Cheng, Yingqi Liu, Shiqing Ma, Xiangyu Zhang",,https://arxiv.org/abs/2012.11212,
DeepSweep: An Evaluation Framework for Mitigating DNN Backdoor Attacks using Data Augmentation,arXiv,2012,"Han Qiu, Yi Zeng, Shangwei Guo, Tianwei Zhang, Meikang Qiu, Bhavani Thuraisingham",,https://arxiv.org/abs/2012.07006,
Detecting Trojaned DNNs Using Counterfactual Attributions,arXiv,2012,"Karan Sikka, Indranil Sur, Susmit Jha, Anirban Roy, Ajay Divakaran",,https://arxiv.org/abs/2012.02275,
Effect of backdoor attacks over the complexity of the latent space distribution,arXiv,2012,"Henry D. Chacon, Paul Rad",,https://arxiv.org/abs/2012.01931,
Federated Mimic Learning for Privacy Preserving Intrusion Detection,arXiv,2012,"Noor Ali Al-Athba Al-Marri, Bekir Sait Ciftler, Mohamed Abdallah","6 pages, 6 figures, accepted to Blackseacom 2020",https://arxiv.org/abs/2012.06974,
HaS-Nets: A Heal and Select Mechanism to Defend DNNs Against Backdoor Attacks for Data Collection Scenarios,arXiv,2012,"Hassan Ali, Surya Nepal, Salil S. Kanhere, Sanjay Jha","21 pages, 36 figures, conference paper",https://arxiv.org/abs/2012.07474,
Invisible Backdoor Attack with Sample-Specific Triggers,arXiv,2012,"Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, Siwei Lyu",It is accepted by ICCV 2021,https://arxiv.org/abs/2012.03816,
"TrojanZoo: Towards Unified, Holistic, and Practical Evaluation of Neural Backdoors",arXiv,2012,"Ren Pang, Zheng Zhang, Xiangshan Gao, Zhaohan Xi, Shouling Ji, Peng Cheng, Xiapu Luo, Ting Wang",Accepted as a full paper at EuroS&P 2022,https://arxiv.org/abs/2012.09302,
BaFFLe: Backdoor detection via Feedback-based Federated Learning,arXiv,2011,"Sebastien Andreina, Giorgia Azzurra Marson, Helen Möllering, Ghassan Karame","11 pages, 5 figures; to appear in the 41st IEEE International Conference on Distributed Computing Systems (ICDCS'21)",https://arxiv.org/abs/2011.02167,
Backdoor Attacks on the DNN Interpretation System,arXiv,2011,"Shihong Fang, Anna Choromanska","Published at the 2022 AAAI Conference on Artificial Intelligence (AAAI), 2022",https://arxiv.org/abs/2011.10698,
Detecting Backdoors in Neural Networks Using Novel Feature-Based Anomaly Detection,arXiv,2011,"Hao Fu, Akshaj Kumar Veldanda, Prashanth Krishnamurthy, Siddharth Garg, Farshad Khorrami",,https://arxiv.org/abs/2011.02526,
Dynamic backdoor attacks against federated learning,arXiv,2011,Anbu Huang,8 pages,https://arxiv.org/abs/2011.07429,
EEG-Based Brain-Computer Interfaces Are Vulnerable to Backdoor Attacks,arXiv,2011,"Lubin Meng, Jian Huang, Zhigang Zeng, Xue Jiang, Shan Yu, Tzyy-Ping Jung, Chin-Teng Lin, Ricardo Chavarriaga, Dongrui Wu",,https://arxiv.org/abs/2011.00101,
Mitigating Backdoor Attacks in Federated Learning,arXiv,2011,"Chen Wu, Xian Yang, Sencun Zhu, Prasenjit Mitra",,https://arxiv.org/abs/2011.01767,
ONION: A Simple and Effective Defense Against Textual Backdoor Attacks,arXiv,2011,"Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, Maosong Sun",Accepted by the main conference of EMNLP 2021 as a short paper. The camera-ready version,https://arxiv.org/abs/2011.10369,https://github.com/thunlp/ONION.
Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff,arXiv,2011,"Eitan Borgnia, Valeriia Cherepanova, Liam Fowl, Amin Ghiasi, Jonas Geiping, Micah Goldblum, Tom Goldstein, Arjun Gupta",Authors ordered alphabetically,https://arxiv.org/abs/2011.09527,
BAAAN: Backdoor Attacks Against Autoencoder and GAN-Based Machine Learning Models,arXiv,2010,"Ahmed Salem, Yannick Sautter, Michael Backes, Mathias Humbert, Yang Zhang",,https://arxiv.org/abs/2010.03007,
Backdoor Attack against Speaker Verification,arXiv,2010,"Tongqing Zhai, Yiming Li, Ziqi Zhang, Baoyuan Wu, Yong Jiang, Shu-Tao Xia",Accepted by the ICASSP 2021. The first two authors contributed equally to this work,https://arxiv.org/abs/2010.11607,\url{https://github.com/zhaitongqing233/Backdoor-attack-against-speaker-verification}.
BlockFLA: Accountable Federated Learning via Hybrid Blockchain Architecture,arXiv,2010,"Harsh Bimal Desai, Mustafa Safa Ozdayi, Murat Kantarcioglu",,https://arxiv.org/abs/2010.07427,
Don't Trigger Me! A Triggerless Backdoor Attack Against Deep Neural Networks,arXiv,2010,"Ahmed Salem, Michael Backes, Yang Zhang",,https://arxiv.org/abs/2010.03282,
Embedding and Extraction of Knowledge in Tree Ensemble Classifiers,arXiv,2010,"Wei Huang, Xingyu Zhao, Xiaowei Huang",,https://arxiv.org/abs/2010.08281,
Federated Learning in Adversarial Settings,arXiv,2010,"Raouf Kerkouche, Gergely Ács, Claude Castelluccia",,https://arxiv.org/abs/2010.07808,
GOAT: GPU Outsourcing of Deep Learning Training With Asynchronous Probabilistic Integrity Verification Inside Trusted Execution Environment,arXiv,2010,"Aref Asvadishirehjini, Murat Kantarcioglu, Bradley Malin",,https://arxiv.org/abs/2010.08855,
Input-Aware Dynamic Backdoor Attack,arXiv,2010,"Anh Nguyen, Anh Tran",Accepted to NeurIPS 2020,https://arxiv.org/abs/2010.08138,https://github.com/VinAIResearch/input-aware-backdoor-attack-release.
L-RED: Efficient Post-Training Detection of Imperceptible Backdoor Attacks without Access to the Training Set,arXiv,2010,"Zhen Xiang, David J. Miller, George Kesidis",,https://arxiv.org/abs/2010.09987,
On Evaluating Neural Network Backdoor Defenses,arXiv,2010,"Akshaj Veldanda, Siddharth Garg",,https://arxiv.org/abs/2010.12186,
Open-sourced Dataset Protection via Backdoor Watermarking,arXiv,2010,"Yiming Li, Ziqi Zhang, Jiawang Bai, Baoyuan Wu, Yong Jiang, Shu-Tao Xia","Accepted by the NeurIPS Workshop on Dataset Curation and Security, 2020. 6 pages",https://arxiv.org/abs/2010.05821,
Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder,arXiv,2010,"Alvin Chan, Yi Tay, Yew-Soon Ong, Aston Zhang","Accepted in EMNLP-Findings 2020, Camera Ready Version",https://arxiv.org/abs/2010.02684,
"Poisoned classifiers are not only backdoored, they are fundamentally broken",arXiv,2010,"Mingjie Sun, Siddhant Agarwal, J. Zico Kolter",,https://arxiv.org/abs/2010.09080,
Reverse Engineering Imperceptible Backdoor Attacks on Deep Neural Networks for Detection and Training Set Cleansing,arXiv,2010,"Zhen Xiang, David J. Miller, George Kesidis",,https://arxiv.org/abs/2010.07489,
CLEANN: Accelerated Trojan Shield for Embedded Neural Networks,arXiv,2009,"Mojan Javaheripi, Mohammad Samragh, Gregory Fields, Tara Javidi, Farinaz Koushanfar",,https://arxiv.org/abs/2009.02326,
Interventional Few-Shot Learning,arXiv,2009,"Zhongqi Yue, Hanwang Zhang, Qianru Sun, Xian-Sheng Hua",Accepted by NeurIPS 2020,https://arxiv.org/abs/2009.13000,https://github.com/yue-zhongqi/ifsl.
Light Can Hack Your Face! Black-box Backdoor Attack on Face Recognition Systems,arXiv,2009,"Haoliang Li, Yufei Wang, Xiaofei Xie, Yang Liu, Shiqi Wang, Renjie Wan, Lap-Pui Chau, Alex C. Kot",First two authors contributed equally,https://arxiv.org/abs/2009.06996,
Local and Central Differential Privacy for Robustness and Privacy in Federated Learning,arXiv,2009,"Mohammad Naseri, Jamie Hayes, Emiliano De Cristofaro",,https://arxiv.org/abs/2009.03561,
Semantic-preserving Reinforcement Learning Attack Against Graph Neural Networks for Malware Detection,arXiv,2009,"Lan Zhang, Peng Liu, Yoon-Ho Choi, Ping Chen",,https://arxiv.org/abs/2009.05602,
What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors,arXiv,2009,"Yi-Shan Lin, Wen-Chuan Lee, Z. Berkay Celik",,https://arxiv.org/abs/2009.10639,
Can Adversarial Weight Perturbations Inject Neural Backdoors?,arXiv,2008,"Siddhant Garg, Adarsh Kumar, Vibhor Goel, Yingyu Liang",Accepted as a conference paper at CIKM 2020,https://arxiv.org/abs/2008.01761,
Control instabilities and incite slow-slip in generalized Burridge-Knopoff models,arXiv,2008,Ioannis Stefanou,,https://arxiv.org/abs/2008.03755,
DeVLBert: Learning Deconfounded Visio-Linguistic Representations,arXiv,2008,"Shengyu Zhang, Tan Jiang, Tan Wang, Kun Kuang, Zhou Zhao, Jianke Zhu, Jin Yu, Hongxia Yang, Fei Wu","10 pages, 4 figures, to appear in ACM MM 2020 proceedings",https://arxiv.org/abs/2008.06884,
Noise-Response Analysis of Deep Neural Networks Quantifies Robustness and Fingerprints Structural Malware,arXiv,2008,"N. Benjamin Erichson, Dane Taylor, Qixuan Wu, Michael W. Mahoney","9 pages, 7 figures, accepted to the SIAM International Conference on Data Mining (SDM 21)",https://arxiv.org/abs/2008.00123,
One-pixel Signature: Characterizing CNN Models for Backdoor Detection,arXiv,2008,"Shanjiaoyang Huang, Weiqi Peng, Zhiwei Jia, Zhuowen Tu",Accepted at ECCV 2020,https://arxiv.org/abs/2008.07711,
Removing Backdoor-Based Watermarks in Neural Networks with Limited Data,arXiv,2008,"Xuankai Liu, Fengting Li, Bihan Wen, Qi Li",,https://arxiv.org/abs/2008.00407,
Towards Class-Oriented Poisoning Attacks Against Neural Networks,arXiv,2008,"Bingyin Zhao, Yingjie Lao","14 pages, 9 figures, accepted by Winter Conference on Applications of Computer Vision (WACV) 2022",https://arxiv.org/abs/2008.00047,
WAFFLE: Watermarking in Federated Learning,arXiv,2008,"Buse Gul Atli, Yuxi Xia, Samuel Marchal, N. Asokan","Will appear in the proceedings of SRDS 2021; 14 pages, 11 figures, 10 tables",https://arxiv.org/abs/2008.07298,
"Attack of the Tails: Yes, You Really Can Backdoor Federated Learning",arXiv,2007,"Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, Dimitris Papailiopoulos",,https://arxiv.org/abs/2007.05084,
Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review,arXiv,2007,"Yansong Gao, Bao Gia Doan, Zhi Zhang, Siqi Ma, Jiliang Zhang, Anmin Fu, Surya Nepal, Hyoungshick Kim","29 pages, 9 figures, 2 tables",https://arxiv.org/abs/2007.10760,
Backdoor Learning: A Survey,arXiv,2007,"Yiming Li, Yong Jiang, Zhifeng Li, Shu-Tao Xia",17 pages. A curated list of backdoor learning resources in this paper is presented in the Github Repo (https://github.com/THUYimingLi/backdoor-learning-resources). We will try our best to continuously maintain this Github Repo,https://arxiv.org/abs/2007.08745,\url{https://github.com/THUYimingLi/backdoor-learning-resources}.
Backdoor attacks and defenses in feature-partitioned collaborative learning,arXiv,2007,"Yang Liu, Zhihao Yi, Tianjian Chen",to be published in FL-ICML 2020 workshop,https://arxiv.org/abs/2007.03608,
Bounding The Number of Linear Regions in Local Area for Neural Networks with ReLU Activations,arXiv,2007,"Rui Zhu, Bo Lin, Haixu Tang","11 pages, 3 figures",https://arxiv.org/abs/2007.06803,
Cassandra: Detecting Trojaned Networks from Adversarial Perturbations,arXiv,2007,"Xiaoyu Zhang, Ajmal Mian, Rohit Gupta, Nazanin Rahnavard, Mubarak Shah",,https://arxiv.org/abs/2007.14433,
Deep Learning Backdoors,arXiv,2007,"Shaofeng Li, Shiqing Ma, Minhui Xue, Benjamin Zi Hao Zhao",,https://arxiv.org/abs/2007.08273,
Defending against Backdoors in Federated Learning with Robust Learning Rate,arXiv,2007,"Mustafa Safa Ozdayi, Murat Kantarcioglu, Yulia R. Gel",Published at AAAI 2021,https://arxiv.org/abs/2007.03767,
Mitigating backdoor attacks in LSTM-based Text Classification Systems by Backdoor Keyword Identification,arXiv,2007,"Chuanshuai Chen, Jiazhu Dai",,https://arxiv.org/abs/2007.12070,
Practical Detection of Trojan Neural Networks: Data-Limited and Data-Free Cases,arXiv,2007,"Ren Wang, Gaoyuan Zhang, Sijia Liu, Pin-Yu Chen, Jinjun Xiong, Meng Wang",,https://arxiv.org/abs/2007.15802,
Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks,arXiv,2007,"Yunfei Liu, Xingjun Ma, James Bailey, Feng Lu",Accepted by ECCV-2020,https://arxiv.org/abs/2007.02343,
Towards a Backdoorless Network Architecture Based on Remote Attestation and Backdoor Inspection,arXiv,2007,"Takayuki Sasaki, Yusuke Shimada",,https://arxiv.org/abs/2007.14748,
A Causally Formulated Hazard Ratio Estimation through Backdoor Adjustment on Structural Causal Model,arXiv,2006,"Riddhiman Adib, Paul Griffin, Sheikh Iqbal Ahamed, Mohammad Adibuzzaman","19 pages, Accepted at Machine Learning for Healthcare 2020",https://arxiv.org/abs/2006.12573,
Backdoor Attacks Against Deep Learning Systems in the Physical World,arXiv,2006,"Emily Wenger, Josephine Passananti, Arjun Bhagoji, Yuanshun Yao, Haitao Zheng, Ben Y. Zhao",Accepted to the 2021 Conference on Computer Vision and Pattern Recognition (CVPR 2021); 14 pages,https://arxiv.org/abs/2006.14580,
Backdoor Attacks on Federated Meta-Learning,arXiv,2006,"Chien-Lun Chen, Leana Golubchik, Marco Paolieri","13 pages, 19 figures, NeurIPS Workshop on Scalability, Privacy, and Security in Federated Learning (NeurIPS-SpicyFL), 2020",https://arxiv.org/abs/2006.07026,
Backdoor Attacks to Graph Neural Networks,arXiv,2006,"Zaixi Zhang, Jinyuan Jia, Binghui Wang, Neil Zhenqiang Gong","In ACM SACMAT, 2021",https://arxiv.org/abs/2006.11165,
Backdoor Smoothing: Demystifying Backdoor Attacks on Deep Neural Networks,arXiv,2006,"Kathrin Grosse, Taesung Lee, Battista Biggio, Youngja Park, Michael Backes, Ian Molloy","9 pages, 7 figures, under submission",https://arxiv.org/abs/2006.06721,
Backdoors in Neural Models of Source Code,arXiv,2006,"Goutham Ramakrishnan, Aws Albarghouthi",,https://arxiv.org/abs/2006.06841,
BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements,arXiv,2006,"Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, Yang Zhang",To appear in Annual Computer Security Applications Conference (ACSAC) 2021,https://arxiv.org/abs/2006.01043,
Can We Mitigate Backdoor Attack Using Adversarial Detection Methods?,arXiv,2006,"Kaidi Jin, Tianwei Zhang, Chao Shen, Yufei Chen, Ming Fan, Chenhao Lin, Ting Liu",Accepted by IEEE TDSC,https://arxiv.org/abs/2006.14871,
FaceHack: Triggering backdoored facial recognition systems using facial characteristics,arXiv,2006,"Esha Sarkar, Hadjer Benkraouda, Michail Maniatakos",,https://arxiv.org/abs/2006.11623,
Graph Backdoor,arXiv,2006,"Zhaohan Xi, Ren Pang, Shouling Ji, Ting Wang","USENIX Security Symposium 2021, implementation: https://github.com/HarrialX/GraphBackdoor",https://arxiv.org/abs/2006.11890,https://github.com/HarrialX/GraphBackdoor
Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks,arXiv,2006,"Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, Tom Goldstein","19 pages, 4 figures",https://arxiv.org/abs/2006.12557,
Natural Backdoor Attack on Text Data,arXiv,2006,Lichao Sun,under submission,https://arxiv.org/abs/2006.16176,
Scalable Backdoor Detection in Neural Networks,arXiv,2006,"Haripriya Harikumar, Vuong Le, Santu Rana, Sourangshu Bhattacharya, Sunil Gupta, Svetha Venkatesh",,https://arxiv.org/abs/2006.05646,
Subpopulation Data Poisoning Attacks,arXiv,2006,"Matthew Jagielski, Giorgio Severi, Niklas Pousette Harger, Alina Oprea","May12 update: add sever + backdoor defenses, comparison to witches' brew attack, better comparison to related work, transferability of representations for cmatch",https://arxiv.org/abs/2006.14026,
Adversarial examples are useful too!,arXiv,2005,Ali Borji,,https://arxiv.org/abs/2005.06107,
Blind Backdoors in Deep Learning Models,arXiv,2005,"Eugene Bagdasaryan, Vitaly Shmatikov",,https://arxiv.org/abs/2005.03823,
Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness,arXiv,2005,"Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, Xue Lin",accepted by ICLR 2020,https://arxiv.org/abs/2005.00060,
Enabling Deletion in Append-Only Blockchains (Short Summary / Work in Progress),arXiv,2005,Michael Kuperberg,"4 pages, 2 figures",https://arxiv.org/abs/2005.06026,
Improved torsion point attacks on SIDH variants,arXiv,2005,"Victoria de Quehen, Péter Kutas, Chris Leonardi, Chloe Martindale, Lorenz Panny, Christophe Petit, Katherine E. Stange",37 pages including 3 appendices,https://arxiv.org/abs/2005.14681,
NeuroAttack: Undermining Spiking Neural Networks Security through Externally Triggered Bit-Flips,arXiv,2005,"Valerio Venceslai, Alberto Marchisio, Ihsen Alouani, Maurizio Martina, Muhammad Shafique",Accepted for publication at the 2020 International Joint Conference on Neural Networks (IJCNN),https://arxiv.org/abs/2005.08041,
Bias Busters: Robustifying DL-based Lithographic Hotspot Detectors Against Backdooring Attacks,arXiv,2004,"Kang Liu, Benjamin Tan, Gaurav Rajavendra Reddy, Siddharth Garg, Yiorgos Makris, Ramesh Karri",,https://arxiv.org/abs/2004.12492,
How to transform the Apple's application 'Find My' into a toolbox for whistleblowers,arXiv,2004,Amadou Moctar Kane,18 pages,https://arxiv.org/abs/2004.00108,
Neural Network Laundering: Removing Black-Box Backdoor Watermarks from Deep Neural Networks,arXiv,2004,"William Aiken, Hyoungshick Kim, Simon Woo","15 pages, 12 figures, 8 tables, formatted for ASIACCS 2020",https://arxiv.org/abs/2004.11368,
Rethinking the Trigger of Backdoor Attack,arXiv,2004,"Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, Shutao Xia",18 pages,https://arxiv.org/abs/2004.04692,
Systematic Evaluation of Backdoor Data Poisoning Attacks on Image Classifiers,arXiv,2004,"Loc Truong, Chace Jones, Brian Hutchinson, Andrew August, Brenda Praggastis, Robert Jasper, Nicole Nichols, Aaron Tuor",,https://arxiv.org/abs/2004.11514,
Weight Poisoning Attacks on Pre-trained Models,arXiv,2004,"Keita Kurita, Paul Michel, Graham Neubig",Published as a long paper at ACL 2020,https://arxiv.org/abs/2004.06660,https://github.com/neulab/RIPPLe.
Analyzing Accuracy Loss in Randomized Smoothing Defenses,arXiv,2003,"Yue Gao, Harrison Rosenberg, Kassem Fawaz, Somesh Jha, Justin Hsu","19 pages, 6 figures, 2 tables",https://arxiv.org/abs/2003.01595,
Backdooring and Poisoning Neural Networks with Image-Scaling Attacks,arXiv,2003,"Erwin Quiring, Konrad Rieck",IEEE Deep Learning and Security Workshop (DLS) 2020,https://arxiv.org/abs/2003.08633,
Clean-Label Backdoor Attacks on Video Recognition Models,arXiv,2003,"Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, Yu-Gang Jiang",CVPR2020,https://arxiv.org/abs/2003.03030,
Deconfounded Image Captioning: A Causal Retrospect,arXiv,2003,"Xu Yang, Hanwang Zhang, Jianfei Cai",,https://arxiv.org/abs/2003.03923,
Differentiable Causal Backdoor Discovery,arXiv,2003,"Limor Gultchin, Matt J. Kusner, Varun Kanade, Ricardo Silva","Published in the Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics (AISTATS) 2020, Palermo, Italy",https://arxiv.org/abs/2003.01461,
Dynamic Backdoor Attacks Against Machine Learning Models,arXiv,2003,"Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, Yang Zhang",,https://arxiv.org/abs/2003.03675,
Estimating Treatment Effects with Observed Confounders and Mediators,arXiv,2003,"Shantanu Gupta, Zachary C. Lipton, David Childers",,https://arxiv.org/abs/2003.11991,
Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers,arXiv,2003,"Giorgio Severi, Jim Meyer, Scott Coull, Alina Oprea","18 pages, 5 figures. To appear in USENIX Security 2021",https://arxiv.org/abs/2003.01031,
PoisHygiene: Detecting and Mitigating Poisoning Attacks in Neural Networks,arXiv,2003,"Junfeng Guo, Ting Wang, Cong Liu",,https://arxiv.org/abs/2003.11110,
RAB: Provable Robustness Against Backdoor Attacks,arXiv,2003,"Maurice Weber, Xiaojun Xu, Bojan Karlaš, Ce Zhang, Bo Li",IEEE Symposium on Security and Privacy 2023,https://arxiv.org/abs/2003.08904,
Stop-and-Go: Exploring Backdoor Attacks on Deep Reinforcement Learning-based Traffic Congestion Control Systems,arXiv,2003,"Yue Wang, Esha Sarkar, Wenqing Li, Michail Maniatakos, Saif Eddin Jabari",,https://arxiv.org/abs/2003.07859,
Towards Backdoor Attacks and Defense in Robust Machine Learning Models,arXiv,2003,"Ezekiel Soremekun, Sakshi Udeshi, Sudipta Chattopadhyay","Accepted in Computers & Security, 2023",https://arxiv.org/abs/2003.00865,
Towards Probabilistic Verification of Machine Unlearning,arXiv,2003,"David Marco Sommer, Liwei Song, Sameer Wagh, Prateek Mittal",code is available at https://github.com/inspire-group/unlearning-verification,https://arxiv.org/abs/2003.04247,https://github.com/inspire-group/unlearning-verification
Defending against Backdoor Attack on Deep Neural Networks,arXiv,2002,"Kaidi Xu, Sijia Liu, Pin-Yu Chen, Pu Zhao, Xue Lin",This workshop manuscript is not a publication and will not be published anywhere,https://arxiv.org/abs/2002.12162,
Learning to Detect Malicious Clients for Robust Federated Learning,arXiv,2002,"Suyi Li, Yong Cheng, Wei Wang, Yang Liu, Tianjian Chen","7 pages, 5 figures",https://arxiv.org/abs/2002.00211,
NNoculation: Catching BadNets in the Wild,arXiv,2002,"Akshaj Kumar Veldanda, Kang Liu, Benjamin Tan, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Brendan Dolan-Gavitt, Siddharth Garg",,https://arxiv.org/abs/2002.08313,
On Certifying Robustness against Backdoor Attacks via Randomized Smoothing,arXiv,2002,"Binghui Wang, Xiaoyu Cao, Jinyuan jia, Neil Zhenqiang Gong","CVPR 2020 Workshop on Adversarial Machine Learning in Computer Vision, 2020. DeepMind Best Extended Abstract",https://arxiv.org/abs/2002.11750,
Radioactive data: tracing through training,arXiv,2002,"Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Hervé Jégou",,https://arxiv.org/abs/2002.00937,
Targeted Forgetting and False Memory Formation in Continual Learners through Adversarial Backdoor Attacks,arXiv,2002,"Muhammad Umer, Glenn Dawson, Robi Polikar",,https://arxiv.org/abs/2002.07111,
Backdoor Attacks against Transfer Learning with Pre-trained Deep Learning Models,arXiv,2001,"Shuo Wang, Surya Nepal, Carsten Rudolph, Marthie Grobler, Shangyu Chen, Tianle Chen",,https://arxiv.org/abs/2001.03274,
Key-dependent Security of Stream Ciphers,arXiv,2001,Eric Filiol,"11 pages + 1 figure. This work has been presented in two parts at the Kaz'Hack'Stan 2019 conference in Nur-Sultan, Kazakhstan and at BSides Lisbon 2019, Lisbon, Portugal",https://arxiv.org/abs/2001.00515,
Attack-Resistant Federated Learning with Residual-based Reweighting,arXiv,1912,"Shuhao Fu, Chulin Xie, Bo Li, Qifeng Chen","8 pages, 6 figures and 4 tables",https://arxiv.org/abs/1912.11464,
A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models,arXiv,1911,"Ren Pang, Hua Shen, Xinyang Zhang, Shouling Ji, Yevgeniy Vorobeychik, Xiapu Luo, Alex Liu, Ting Wang",Accepted as a full paper at ACM CCS 2020,https://arxiv.org/abs/1911.01559,
"Revealing Perceptible Backdoors, without the Training Set, via the Maximum Achievable Misclassification Fraction Statistic",arXiv,1911,"Zhen Xiang, David J. Miller, Hang Wang, George Kesidis",,https://arxiv.org/abs/1911.07970,
Piracy Resistant Watermarks for Deep Neural Networks,arXiv,1910,"Huiying Li, Emily Wenger, Shawn Shan, Ben Y. Zhao, Haitao Zheng",18 pages,https://arxiv.org/abs/1910.01226,
Shielding Collaborative Learning: Mitigating Poisoning Attacks through Client-Side Detection,arXiv,1910,"Lingchen Zhao, Shengshan Hu, Qian Wang, Jianlin Jiang, Chao Shen, Xiangyang Luo, Pengfei Hu",,https://arxiv.org/abs/1910.13111,
Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization,arXiv,1909,"Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, Xinpeng Zhang",,https://arxiv.org/abs/1909.02742,
Walling up Backdoors in Intrusion Detection Systems,arXiv,1909,"Maximilian Bachl, Alexander Hartl, Joachim Fabini, Tanja Zseby",,https://arxiv.org/abs/1909.07866,
Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection,arXiv,1908,"Di Tang, XiaoFeng Wang, Haixu Tang, Kehuan Zhang",,https://arxiv.org/abs/1908.00686,
Detection of Backdoors in Trained Classifiers Without Access to the Training Set,arXiv,1908,"Zhen Xiang, David J. Miller, George Kesidis",,https://arxiv.org/abs/1908.10498,
Februus: Input Purification Defense Against Trojan Attacks on Deep Neural Network Systems,arXiv,1908,"Bao Gia Doan, Ehsan Abbasnejad, Damith C. Ranasinghe","16 pages, to appear in the 36th Annual Computer Security Applications Conference (ACSAC 2020)",https://arxiv.org/abs/1908.03369,
Model Agnostic Defence against Backdoor Attacks in Machine Learning,arXiv,1908,"Sakshi Udeshi, Shanshan Peng, Gerald Woo, Lionell Loh, Louth Rawshan, Sudipta Chattopadhyay","IEEE Transactions on Reliability, 2022",https://arxiv.org/abs/1908.02203,
Data Structures Meet Cryptography: 3SUM with Preprocessing,arXiv,1907,"Alexander Golovnev, Siyao Guo, Thibaut Horel, Sunoo Park, Vinod Vaikuntanathan",,https://arxiv.org/abs/1907.08355,
Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs,arXiv,1906,"Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, Heiko Hoffmann",CVPR 2020 Oral,https://arxiv.org/abs/1906.10842,https://umbcvision.github.io/Universal-Litmus-Patterns/.
A Benchmark API Call Dataset for Windows PE Malware Classification,arXiv,1905,"Ferhat Ozgur Catak, Ahmet Faruk Yazı",Updated version,https://arxiv.org/abs/1905.01999,
Biometric Backdoors: A Poisoning Attack Against Unsupervised Template Updating,arXiv,1905,"Giulio Lovisotto, Simon Eberz, Ivan Martinovic",12 pages,https://arxiv.org/abs/1905.09162,
Bypassing Backdoor Detection Algorithms in Deep Learning,arXiv,1905,"Te Juin Lester Tan, Reza Shokri",IEEE European Symposium on Security and Privacy 2020,https://arxiv.org/abs/1905.13409,
STRIP: A Defence Against Trojan Attacks on Deep Neural Networks,arXiv,1902,"Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith C. Ranasinghe, Surya Nepal",13 pages,https://arxiv.org/abs/1902.06531,
Backdoor Decomposable Monotone Circuits and their Propagation Complete Encodings,arXiv,1811,"Petr Kučera, Petr Savický","The paper was significantly rewritten to improve readability, it is now an extended version of the paper accepted to AAAI 2021",https://arxiv.org/abs/1811.09435,
Mitigating Sybils in Federated Learning Poisoning,arXiv,1808,"Clement Fung, Chris J. M. Yoon, Ivan Beschastnikh","16 pages, Extended technical version of conference paper ""The Limitations of Federated Learning in Sybil Settings"" accepted at RAID 2020",https://arxiv.org/abs/1808.04866,https://github.com/DistributedML/FoolsGold
Solving Integer Linear Programs with a Small Number of Global Variables and Constraints,arXiv,1706,"Pavel Dvořák, Eduard Eiben, Robert Ganian, Dušan Knop, Sebastian Ordyniak",24 pages; an extended abstract appeared in proceedings of IJCAI 2017,https://arxiv.org/abs/1706.06084,
