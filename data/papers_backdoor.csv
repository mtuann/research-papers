title,author,venue_name,publish_date,paper_url,doi,abstract,code,source
A frequency-injection backdoor attack against DNN-Based finger vein verification,Zhang H.,Computers and Security,2024-09-01,"<a href=""ScienceDirect (2024-09-01) : A frequency-injection backdoor attack against DNN-Based finger vein verification"" target=""_blank"">[https://doi.org/10.1016/j.cose.2024.103956]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.cose.2024.103956]</a>",,,ScienceDirect
A topological data analysis approach for detecting data poisoning attacks against machine learning based network intrusion detection systems,Monkam G.F.,Computers and Security,2024-09-01,"<a href=""ScienceDirect (2024-09-01) : A topological data analysis approach for detecting data poisoning attacks against machine learning based network intrusion detection systems"" target=""_blank"">[https://doi.org/10.1016/j.cose.2024.103929]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.cose.2024.103929]</a>",,,ScienceDirect
Multi-target label backdoor attacks on graph neural networks,Wang K.,Pattern Recognition,2024-08-01,"<a href=""ScienceDirect (2024-08-01) : Multi-target label backdoor attacks on graph neural networks"" target=""_blank"">[https://doi.org/10.1016/j.patcog.2024.110449]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.patcog.2024.110449]</a>",,,ScienceDirect
Adversarial filtering based evasion and backdoor attacks to EEG-based brain-computer interfaces,Meng L.,Information Fusion,2024-07-01,"<a href=""ScienceDirect (2024-07-01) : Adversarial filtering based evasion and backdoor attacks to EEG-based brain-computer interfaces"" target=""_blank"">[https://doi.org/10.1016/j.inffus.2024.102316]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.inffus.2024.102316]</a>",,,ScienceDirect
Blockchain-based immunization against kleptographic attacks,Jiang C.,Science China Information Sciences,2024-07-01,"<a href=""ScienceDirect (2024-07-01) : Blockchain-based immunization against kleptographic attacks"" target=""_blank"">[https://doi.org/10.1007/s11432-023-3883-4]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/s11432-023-3883-4]</a>",,,ScienceDirect
GhostEncoder: Stealthy backdoor attacks with dynamic triggers to pre-trained encoders in self-supervised learning,Wang Q.,Computers and Security,2024-07-01,"<a href=""ScienceDirect (2024-07-01) : GhostEncoder: Stealthy backdoor attacks with dynamic triggers to pre-trained encoders in self-supervised learning"" target=""_blank"">[https://doi.org/10.1016/j.cose.2024.103855]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.cose.2024.103855]</a>",,,ScienceDirect
"A multifaceted survey on privacy preservation of federated learning: progress, challenges, and opportunities","Sanchita Saha, Ashlesha Hota, ... Sukumar Nandi",Artificial Intelligence Review,2024-06-21,"<a href=""Springer (2024-06-21) : A multifaceted survey on privacy preservation of federated learning: progress, challenges, and opportunities"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-024-10766-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-024-10766-7]</a>",Federated learning (FL) refers to a system of training and stabilizing local machine learning models at the global level by aggregating the learning...,,Springer
Trustworthy cyber-physical power systems using AI: dueling algorithms for PMU anomaly detection and cybersecurity,"Umit Cali, Ferhat Ozgur Catak, Ugur Halden",Artificial Intelligence Review,2024-06-21,"<a href=""Springer (2024-06-21) : Trustworthy cyber-physical power systems using AI: dueling algorithms for PMU anomaly detection and cybersecurity"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-024-10827-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-024-10827-x]</a>",Energy systems require radical changes due to the conflicting needs of combating climate change and meeting rising energy demands. These...,,Springer
Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment,"Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Junjie Hu, Yixuan Li, Patrick McDaniel, Muhao Chen, Bo Li, Chaowei Xiao","arXiv
arXiv","2024-06-20
2024-02","<a href=""arXiv (2024-06-20) : Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment"" target=""_blank"">[http://arxiv.org/abs/2402.14968v3]</a>
<a href=""DBLP (2024-02) : Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.14968]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.14968]</a>","Despite the general capabilities of Large Language Models (LLM), these models still request fine-tuning or adaptation with customized data when meeting specific business demands. However, this process inevitably introduces new threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack) under the setting of Language-Model-as-a-Service (LMaaS), where the model's safety has been significantly compromised by fine-tuning users' uploaded examples contain just a few harmful examples. Though potential defenses have been proposed that the service providers can integrate safety examples into the fine-tuning dataset to reduce safety issues, such approaches require incorporating a substantial amount of data, making it inefficient. To effectively defend against the FJAttack with limited safety examples under LMaaS, we propose the Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In particular, service providers will construct prefixed safety examples with a secret prompt, acting as a ""backdoor trigger"". By integrating prefixed safety examples into the fine-tuning dataset, the subsequent fine-tuning process effectively acts as the ""backdoor attack"", establishing a strong correlation between the secret prompt and safety generations. Consequently, safe responses are ensured once service providers prepend this secret prompt ahead of any user input during inference. Our comprehensive experiments demonstrate that through the Backdoor Enhanced Safety Alignment with adding as few as 11 prefixed safety examples, the maliciously fine-tuned LLMs will achieve similar safety performance as the original aligned models without harming the benign performance. Furthermore, we also present the effectiveness of our method in a more practical setting where the fine-tuning data consists of both FJAttack examples and the fine-tuning task data.
","
","arXiv
DBLP"
An efficient security testing for android application based on behavior and activities using RFE-MLP and ensemble classifier,"Pawan Kumar, Sukhdip Singh",Multimedia Tools and Applications,2024-06-20,"<a href=""Springer (2024-06-20) : An efficient security testing for android application based on behavior and activities using RFE-MLP and ensemble classifier"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-024-19517-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-024-19517-w]</a>","An enormous amount of applications that are available for download permits users to enhance the functionality of the devices with brand-new features,...",,Springer
Robust and privacy-preserving collaborative training: a comprehensive survey,"Fei Yang, Xu Zhang, ... Yang Liu",Artificial Intelligence Review,2024-06-20,"<a href=""Springer (2024-06-20) : Robust and privacy-preserving collaborative training: a comprehensive survey"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-024-10797-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-024-10797-0]</a>","Increasing numbers of artificial intelligence systems are employing collaborative machine learning techniques, such as federated learning, to build a...",,Springer
Trading Devil: Robust backdoor attack via Stochastic investment models and Bayesian approach,Orson Mengara,arXiv,2024-06-20,"<a href=""arXiv (2024-06-20) : Trading Devil: Robust backdoor attack via Stochastic investment models and Bayesian approach"" target=""_blank"">[http://arxiv.org/abs/2406.10719v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","With the growing use of voice-activated systems and speech recognition technologies, the danger of backdoor attacks on audio data has grown significantly. This research looks at a specific type of attack, known as a Stochastic investment-based backdoor attack (MarketBack), in which adversaries strategically manipulate the stylistic properties of audio to fool speech recognition systems. The security and integrity of machine learning models are seriously threatened by backdoor attacks, in order to maintain the reliability of audio applications and systems, the identification of such attacks becomes crucial in the context of audio data. Experimental results demonstrated that MarketBack is feasible to achieve an average attack success rate close to 100% in seven victim models when poisoning less than 1% of the training data.",,arXiv
Attack and Defense of Deep Learning Models in the Field of Web Attack Detection,"L Shi, S Dong","arXiv preprint arXiv:2406.12605, 2024",2024-06-19,"<a href=""Google Scholar (2024-06-19) : Attack and Defense of Deep Learning Models in the Field of Web Attack Detection"" target=""_blank"">[https://arxiv.org/abs/2406.12605]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2406.12605]</a>","vulnerable to backdoor attacks, where … backdoor attacks are well studied in image recognition, they are largely unexplored in WAD. This paper introduces backdoor attacks …",,Google Scholar
Composite Concept Extraction through Backdooring,"Banibrata Ghosh, Haripriya Harikumar, Khoa D Doan, Svetha Venkatesh, Santu Rana",arXiv,2024-06-19,"<a href=""arXiv (2024-06-19) : Composite Concept Extraction through Backdooring"" target=""_blank"">[http://arxiv.org/abs/2406.13411v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Learning composite concepts, such as \textquotedbl red car\textquotedbl , from individual examples -- like a white car representing the concept of \textquotedbl car\textquotedbl{} and a red strawberry representing the concept of \textquotedbl red\textquotedbl -- is inherently challenging. This paper introduces a novel method called Composite Concept Extractor (CoCE), which leverages techniques from traditional backdoor attacks to learn these composite concepts in a zero-shot setting, requiring only examples of individual concepts. By repurposing the trigger-based model backdooring mechanism, we create a strategic distortion in the manifold of the target object (e.g., \textquotedbl car\textquotedbl ) induced by example objects with the target property (e.g., \textquotedbl red\textquotedbl ) from objects \textquotedbl red strawberry\textquotedbl , ensuring the distortion selectively affects the target objects with the target property. Contrastive learning is then employed to further refine this distortion, and a method is formulated for detecting objects that are influenced by the distortion. Extensive experiments with in-depth analysis across different datasets demonstrate the utility and applicability of our proposed approach.",,arXiv
OptFBFN: IOT threat mitigation in software-defined networks based on fuzzy approach,"B. Dhanalaxmi, Yeligeti Raju, ... Kandula Damodhar Rao",Cluster Computing,2024-06-19,"<a href=""Springer (2024-06-19) : OptFBFN: IOT threat mitigation in software-defined networks based on fuzzy approach"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04616-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04616-y]</a>","Software-Defined Networking (SDN) has emerged as a new architectural paradigm in computer networks, aiming to enhance network capabilities and...",,Springer
X-Detect: explainable adversarial patch detection for object detectors in retail,"Omer Hofman, Amit Giloni, ... Asaf Shabtai",Machine Learning,2024-06-19,"<a href=""Springer (2024-06-19) : X-Detect: explainable adversarial patch detection for object detectors in retail"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-024-06548-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-024-06548-5]</a>","Object detection models, which are widely used in various domains (such as retail), have been shown to be vulnerable to adversarial attacks. Existing...",,Springer
CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models,"Yuetai Li, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Dinuka Sahabandu, Bhaskar Ramasubramanian, Radha Poovendran",arXiv,2024-06-18,"<a href=""arXiv (2024-06-18) : CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models"" target=""_blank"">[http://arxiv.org/abs/2406.12257v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","The remarkable performance of large language models (LLMs) in generation tasks has enabled practitioners to leverage publicly available models to power custom applications, such as chatbots and virtual assistants. However, the data used to train or fine-tune these LLMs is often undisclosed, allowing an attacker to compromise the data and inject backdoors into the models. In this paper, we develop a novel inference time defense, named CleanGen, to mitigate backdoor attacks for generation tasks in LLMs. CleanGenis a lightweight and effective decoding strategy that is compatible with the state-of-the-art (SOTA) LLMs. Our insight behind CleanGen is that compared to other LLMs, backdoored LLMs assign significantly higher probabilities to tokens representing the attacker-desired contents. These discrepancies in token probabilities enable CleanGen to identify suspicious tokens favored by the attacker and replace them with tokens generated by another LLM that is not compromised by the same attacker, thereby avoiding generation of attacker-desired content. We evaluate CleanGen against five SOTA backdoor attacks. Our results show that CleanGen achieves lower attack success rates (ASR) compared to five SOTA baseline defenses for all five backdoor attacks. Moreover, LLMs deploying CleanGen maintain helpfulness in their responses when serving benign user queries with minimal added computational overhead.",,arXiv
NoiSec: Harnessing Noise for Security against Adversarial and Backdoor Attacks,"Md Hasan Shahriar, Ning Wang, Y. Thomas Hou, Wenjing Lou",arXiv,2024-06-18,"<a href=""arXiv (2024-06-18) : NoiSec: Harnessing Noise for Security against Adversarial and Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2406.13073v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","The exponential adoption of machine learning (ML) is propelling the world into a future of intelligent automation and data-driven solutions. However, the proliferation of malicious data manipulation attacks against ML, namely adversarial and backdoor attacks, jeopardizes its reliability in safety-critical applications. The existing detection methods against such attacks are built upon assumptions, limiting them in diverse practical scenarios. Thus, motivated by the need for a more robust and unified defense mechanism, we investigate the shared traits of adversarial and backdoor attacks and propose NoiSec that leverages solely the noise, the foundational root cause of such attacks, to detect any malicious data alterations. NoiSec is a reconstruction-based detector that disentangles the noise from the test input, extracts the underlying features from the noise, and leverages them to recognize systematic malicious manipulation. Experimental evaluations conducted on the CIFAR10 dataset demonstrate the efficacy of NoiSec, achieving AUROC scores exceeding 0.954 and 0.852 under white-box and black-box adversarial attacks, respectively, and 0.992 against backdoor attacks. Notably, NoiSec maintains a high detection performance, keeping the false positive rate within only 1\%. Comparative analyses against MagNet-based baselines reveal NoiSec's superior performance across various attack scenarios.",,arXiv
Watch Out! Simple Horizontal Class Backdoor Can Trivially Evade Defense,"Hua Ma, Shang Wang, Yansong Gao, Zhi Zhang, Huming Qiu, Minhui Xue, Alsharif Abuadbba, Anmin Fu, Surya Nepal, Derek Abbott",arXiv,2024-06-18,"<a href=""arXiv (2024-06-18) : Watch Out! Simple Horizontal Class Backdoor Can Trivially Evade Defense"" target=""_blank"">[http://arxiv.org/abs/2310.00542v3]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","All current backdoor attacks on deep learning (DL) models fall under the category of a vertical class backdoor (VCB) -- class-dependent. In VCB attacks, any sample from a class activates the implanted backdoor when the secret trigger is present. Existing defense strategies overwhelmingly focus on countering VCB attacks, especially those that are source-class-agnostic. This narrow focus neglects the potential threat of other simpler yet general backdoor types, leading to false security implications. This study introduces a new, simple, and general type of backdoor attack coined as the horizontal class backdoor (HCB) that trivially breaches the class dependence characteristic of the VCB, bringing a fresh perspective to the community. HCB is now activated when the trigger is presented together with an innocuous feature, regardless of class. For example, the facial recognition model misclassifies a person who wears sunglasses with a smiling innocuous feature into the targeted person, such as an administrator, regardless of which person. The key is that these innocuous features are horizontally shared among classes but are only exhibited by partial samples per class. Extensive experiments on attacking performance across various tasks, including MNIST, facial recognition, traffic sign recognition, object detection, and medical diagnosis, confirm the high efficiency and effectiveness of the HCB. We rigorously evaluated the evasiveness of the HCB against a series of eleven representative countermeasures, including Fine-Pruning (RAID 18'), STRIP (ACSAC 19'), Neural Cleanse (Oakland 19'), ABS (CCS 19'), Februus (ACSAC 20'), NAD (ICLR 21'), MNTD (Oakland 21'), SCAn (USENIX SEC 21'), MOTH (Oakland 22'), Beatrix (NDSS 23'), and MM-BD (Oakland 24'). None of these countermeasures prove robustness, even when employing a simplistic trigger, such as a small and static white-square patch.",,arXiv
A survey of safety and trustworthiness of large language models through the lens of verification and validation,"Xiaowei Huang, Wenjie Ruan, ... Mustafa A. Mustafa",Artificial Intelligence Review,2024-06-17,"<a href=""Springer (2024-06-17) : A survey of safety and trustworthiness of large language models through the lens of verification and validation"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-024-10824-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-024-10824-0]</a>",Large language models (LLMs) have exploded a new heatwave of AI for their ability to engage end-users in human-level conversations with detailed and...,,Springer
Is poisoning a real threat to LLM alignment? Maybe more so than you think,"P Pathmanathan, S Chakraborty, X Liu, Y Liang…","arXiv preprint arXiv …, 2024",2024-06-17,"<a href=""Google Scholar (2024-06-17) : Is poisoning a real threat to LLM alignment? Maybe more so than you think"" target=""_blank"">[https://arxiv.org/abs/2406.12091]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2406.12091]</a>","types of attacks, ie, backdoor and non-backdoor attacks, … which, when it comes to backdoor attacks, require at least 4\… translates into backdoor vs non-backdoor attacks. …",,Google Scholar
Res2Net-ERNN: deep learning based cyberattack classification in software defined network,"Mamatha Maddu, Yamarthi Narasimha Rao",Cluster Computing,2024-06-17,"<a href=""Springer (2024-06-17) : Res2Net-ERNN: deep learning based cyberattack classification in software defined network"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04581-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04581-6]</a>","Software-defined networking (SDN) is known for its enhanced network programmability and adaptability, but maintaining strong safety precautions to...",,Springer
Imperceptible Rhythm Backdoor Attacks: Exploring Rhythm Transformation for Embedding Undetectable Vulnerabilities on Speech Recognition,"Wenhan Yao, Jiangkun Yang, Yongqiang He, Jia Liu, Weiping Wen",arXiv,2024-06-16,"<a href=""arXiv (2024-06-16) : Imperceptible Rhythm Backdoor Attacks: Exploring Rhythm Transformation for Embedding Undetectable Vulnerabilities on Speech Recognition"" target=""_blank"">[http://arxiv.org/abs/2406.10932v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Speech recognition is an essential start ring of human-computer interaction, and recently, deep learning models have achieved excellent success in this task. However, when the model training and private data provider are always separated, some security threats that make deep neural networks (DNNs) abnormal deserve to be researched. In recent years, the typical backdoor attacks have been researched in speech recognition systems. The existing backdoor methods are based on data poisoning. The attacker adds some incorporated changes to benign speech spectrograms or changes the speech components, such as pitch and timbre. As a result, the poisoned data can be detected by human hearing or automatic deep algorithms. To improve the stealthiness of data poisoning, we propose a non-neural and fast algorithm called Random Spectrogram Rhythm Transformation (RSRT) in this paper. The algorithm combines four steps to generate stealthy poisoned utterances. From the perspective of rhythm component transformation, our proposed trigger stretches or squeezes the mel spectrograms and recovers them back to signals. The operation keeps timbre and content unchanged for good stealthiness. Our experiments are conducted on two kinds of speech recognition tasks, including testing the stealthiness of poisoned samples by speaker verification and automatic speech recognition. The results show that our method has excellent effectiveness and stealthiness. The rhythm trigger needs a low poisoning rate and gets a very high attack success rate.",,arXiv
Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning,"Y Zhang, Z Di, K Zhou, C Xie…","Proceedings of the 2024 …, 2024",2024-06-16,"<a href=""Google Scholar (2024-06-16) : Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning"" target=""_blank"">[https://aclanthology.org/2024.naacl-long.57/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://aclanthology.org/2024.naacl-long.57/]</a>","backdoor attack in FedVLN need to be redesigned. To this end, we introduce a simple yet effective backdoor attack … We then propose a targeted backdoor attack Nav…",,Google Scholar
Really Unlearned? Verifying Machine Unlearning via Influential Sample Pairs,"H Xu, T Zhu, L Zhang, W Zhou","arXiv preprint arXiv:2406.10953, 2024",2024-06-16,"<a href=""Google Scholar (2024-06-16) : Really Unlearned? Verifying Machine Unlearning via Influential Sample Pairs"" target=""_blank"">[https://arxiv.org/abs/2406.10953]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2406.10953]</a>","backdoor attacks, in order to assess if the model provider successfully performs unlearning by evaluating the success rate of the backdoor … membership inference attacks (…",,Google Scholar
E-SAGE: Explainability-based Defense Against Backdoor Attacks on Graph Neural Networks,"Dingqiang Yuan, Xiaohua Xu, Lei Yu, Tongchang Han, Rongchang Li, Meng Han",arXiv,2024-06-15,"<a href=""arXiv (2024-06-15) : E-SAGE: Explainability-based Defense Against Backdoor Attacks on Graph Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2406.10655v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Graph Neural Networks (GNNs) have recently been widely adopted in multiple domains. Yet, they are notably vulnerable to adversarial and backdoor attacks. In particular, backdoor attacks based on subgraph insertion have been shown to be effective in graph classification tasks while being stealthy, successfully circumventing various existing defense methods. In this paper, we propose E-SAGE, a novel approach to defending GNN backdoor attacks based on explainability. We find that the malicious edges and benign edges have significant differences in the importance scores for explainability evaluation. Accordingly, E-SAGE adaptively applies an iterative edge pruning process on the graph based on the edge scores. Through extensive experiments, we demonstrate the effectiveness of E-SAGE against state-of-the-art graph backdoor attacks in different attack settings. In addition, we investigate the effectiveness of E-SAGE against adversarial attacks.",,arXiv
Enhancing trustworthy deep learning for image classification against evasion attacks: a systematic literature review,"Dua’a Mkhiemir Akhtom, Manmeet Mahinderjit Singh, Chew XinYing",Artificial Intelligence Review,2024-06-15,"<a href=""Springer (2024-06-15) : Enhancing trustworthy deep learning for image classification against evasion attacks: a systematic literature review"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-024-10777-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-024-10777-4]</a>","In the rapidly evolving field of Deep Learning (DL), the trustworthiness of models is essential for their effective application in critical domains...",,Springer
"Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions","Xiao Yang, Gaolei Li, Jianhua Li",arXiv,2024-06-15,"<a href=""arXiv (2024-06-15) : Graph Neural Backdoor: Fundamentals, Methodologies, Applications, and Future Directions"" target=""_blank"">[http://arxiv.org/abs/2406.10573v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Graph Neural Networks (GNNs) have significantly advanced various downstream graph-relevant tasks, encompassing recommender systems, molecular structure prediction, social media analysis, etc. Despite the boosts of GNN, recent research has empirically demonstrated its potential vulnerability to backdoor attacks, wherein adversaries employ triggers to poison input samples, inducing GNN to adversary-premeditated malicious outputs. This is typically due to the controlled training process, or the deployment of untrusted models, such as delegating model training to third-party service, leveraging external training sets, and employing pre-trained models from online sources. Although there's an ongoing increase in research on GNN backdoors, comprehensive investigation into this field is lacking. To bridge this gap, we propose the first survey dedicated to GNN backdoors. We begin by outlining the fundamental definition of GNN, followed by the detailed summarization and categorization of current GNN backdoor attacks and defenses based on their technical characteristics and application scenarios. Subsequently, the analysis of the applicability and use cases of GNN backdoors is undertaken. Finally, the exploration of potential research directions of GNN backdoors is presented. This survey aims to explore the principles of graph backdoors, provide insights to defenders, and promote future security research.",,arXiv
Robustness-Inspired Defense Against Backdoor Attacks on Graph Neural Networks,"Zhiwei Zhang, Minhua Lin, Junjie Xu, Zongyu Wu, Enyan Dai, Suhang Wang",arXiv,2024-06-14,"<a href=""arXiv (2024-06-14) : Robustness-Inspired Defense Against Backdoor Attacks on Graph Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2406.09836v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Graph Neural Networks (GNNs) have achieved promising results in tasks such as node classification and graph classification. However, recent studies reveal that GNNs are vulnerable to backdoor attacks, posing a significant threat to their real-world adoption. Despite initial efforts to defend against specific graph backdoor attacks, there is no work on defending against various types of backdoor attacks where generated triggers have different properties. Hence, we first empirically verify that prediction variance under edge dropping is a crucial indicator for identifying poisoned nodes. With this observation, we propose using random edge dropping to detect backdoors and theoretically show that it can efficiently distinguish poisoned nodes from clean ones. Furthermore, we introduce a novel robust training strategy to efficiently counteract the impact of the triggers. Extensive experiments on real-world datasets show that our framework can effectively identify poisoned nodes, significantly degrade the attack success rate, and maintain clean accuracy when defending against various types of graph backdoor attacks with different properties.",,arXiv
Vere: Verification Guided Synthesis for Repairing Deep Neural Networks,J. Ma P. Yang J. Wang Y. Sun C. -C. Huang Z. Wang,2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE),2024-06-14,"<a href=""IEEE (2024-06-14) : Vere: Verification Guided Synthesis for Repairing Deep Neural Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548054]</a>","<a href=""IEEE"" target=""_blank"">[]</a>","Neural network repair aims to fix the ‘bugs'11We use 'bugs' to denote different kinds of inputs that could trigger an error in the model's output. of neural networks by modifying the model's architecture or parameters. However, due to the data-driven nature of neural networks, it is difficult to explain the relationship between the internal neurons and erro-neous behaviors, making further repair challenging. While several work exists to identify responsible neurons based on gradient or causality analysis, their effectiveness heavily rely on the quality of available ‘bugged’ data and multiple heuristics in layer or neuron selection. In this work, we address the issue utilizing the power of formal verification (in particular for neural networks). Specifically, we propose Vere, a verification-guided neural network repair framework that performs fault localization based on linear relax-ation to symbolically calculate the repair significance of neurons and furthermore optimize the parameters of problematic neurons to repair erroneous behaviors. We evaluated Vere on various repair tasks, and our experimental results show that Vere can efficiently and effectively repair all neural networks without degrading the model's performance. For the task of removing backdoors, Vere successfully reduces attack success rate from 98.47% to 0.38% on average, while causing an average performance drop of 0.9%. For the task of repairing safety properties, Vere successfully repairs all the 36 tasks and achieves 99.87% generalization on average.",,IEEE
Watch the Watcher! Backdoor Attacks on Security-Enhancing Diffusion Models,"Changjiang Li, Ren Pang, Bochuan Cao, Jinghui Chen, Fenglong Ma, Shouling Ji, Ting Wang",arXiv,2024-06-14,"<a href=""arXiv (2024-06-14) : Watch the Watcher! Backdoor Attacks on Security-Enhancing Diffusion Models"" target=""_blank"">[http://arxiv.org/abs/2406.09669v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Thanks to their remarkable denoising capabilities, diffusion models are increasingly being employed as defensive tools to reinforce the security of other models, notably in purifying adversarial examples and certifying adversarial robustness. However, the security risks of these practices themselves remain largely unexplored, which is highly concerning. To bridge this gap, this work investigates the vulnerabilities of security-enhancing diffusion models. Specifically, we demonstrate that these models are highly susceptible to DIFF2, a simple yet effective backdoor attack, which substantially diminishes the security assurance provided by such models. Essentially, DIFF2 achieves this by integrating a malicious diffusion-sampling process into the diffusion model, guiding inputs embedded with specific triggers toward an adversary-defined distribution while preserving the normal functionality for clean inputs. Our case studies on adversarial purification and robustness certification show that DIFF2 can significantly reduce both post-purification and certified accuracy across benchmark datasets and models, highlighting the potential risks of relying on pre-trained diffusion models as defensive tools. We further explore possible countermeasures, suggesting promising avenues for future research.",,arXiv
Weaponizing Disinformation Against Critical Infrastructures,"L Alvisi, J Bianchi, S Tibidò, MV Zucca","arXiv preprint arXiv:2406.08963, 2024",2024-06-14,"<a href=""Google Scholar (2024-06-14) : Weaponizing Disinformation Against Critical Infrastructures"" target=""_blank"">[https://arxiv.org/abs/2406.08963]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2406.08963]</a>","Episodes like the January 6 United States Capitol attack … grid, an attack on traffic management, and XZ Utils backdoor. … the criminal profiles driving such attacks, while also …",,Google Scholar
Backdoor Attack Based on Privacy Inference against Federated Learning,D. Wu L. Hao B. Wei K. Hao T. Han L. He,"2024 7th International Symposium on Autonomous Systems (ISAS)
2024 7th International …, 2024","2024-06-13
2024-05-07","<a href=""IEEE (2024-06-13) : Backdoor Attack Based on Privacy Inference against Federated Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10552567]</a>
<a href=""Google Scholar (2024-05-07) : Backdoor Attack Based on Privacy Inference against Federated Learning"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10552567/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ISAS61044.2024.10552567]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10552567/]</a>","As a pioneering distributed learning framework, federated learning (FL) has gained widespread adoption. It operates collaboratively among participants, with communication limited to sharing model parameters between the server and participants. However, FL is also more susceptible to active attacks from malicious insiders. Poisoned updates submitted by attackers can degrade the performance of the global model. Previous research only considered using the naive data of malicious clients for backdoor poisoning, and therefore achieved limited backdoor attack success rates. In this paper, we propose a novel Federated Backdoor Attack based on Privacy Inference (FBA-PI). Combining privacy inference based on generative adversarial networks (GAN), the attacker first infers sensitive information from victim participants and then injects backdoor triggers into the naive and generated data. Finally, malicious clients can embed backdoor semantics into the global model by participating in regular federated aggregation. Extensive experiments on the MNIST dataset verify the effectiveness of our proposed method.
training a malicious model with a backdoor. Then, they upload … However, this form of attack has some limitations: … In this study, we introduce a novel backdoor attack FBAPI …","
","IEEE
Google Scholar"
A Survey of Backdoor Attacks and Defenses on Large Language Models: Implications for Security Measures,"Shuai Zhao, Meihuizi Jia, Zhongliang Guo, Leilei Gan, Jie Fu, Yichao Feng, Fengjun Pan, Luu Anh Tuan",arXiv,2024-06-13,"<a href=""arXiv (2024-06-13) : A Survey of Backdoor Attacks and Defenses on Large Language Models: Implications for Security Measures"" target=""_blank"">[http://arxiv.org/abs/2406.06852v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","The large language models (LLMs), which bridge the gap between human language understanding and complex problem-solving, achieve state-of-the-art performance on several NLP tasks, particularly in few-shot and zero-shot settings. Despite the demonstrable efficacy of LMMs, due to constraints on computational resources, users have to engage with open-source language models or outsource the entire training process to third-party platforms. However, research has demonstrated that language models are susceptible to potential security vulnerabilities, particularly in backdoor attacks. Backdoor attacks are designed to introduce targeted vulnerabilities into language models by poisoning training samples or model weights, allowing attackers to manipulate model responses through malicious triggers. While existing surveys on backdoor attacks provide a comprehensive overview, they lack an in-depth examination of backdoor attacks specifically targeting LLMs. To bridge this gap and grasp the latest trends in the field, this paper presents a novel perspective on backdoor attacks for LLMs by focusing on fine-tuning methods. Specifically, we systematically classify backdoor attacks into three categories: full-parameter fine-tuning, parameter-efficient fine-tuning, and attacks without fine-tuning. Based on insights from a substantial review, we also discuss crucial issues for future research on backdoor attacks, such as further exploring attack algorithms that do not require fine-tuning, or developing more covert attack algorithms.",,arXiv
Web 3.0 security: Backdoor attacks in federated learning-based automatic speaker verification systems in the 6G era,"Y Wu, J Chen, T Lei, J Yu, MS Hossain","Future Generation Computer …, 2024",2024-06-13,"<a href=""Google Scholar (2024-06-13) : Web 3.0 security: Backdoor attacks in federated learning-based automatic speaker verification systems in the 6G era"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S0167739X24003224]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S0167739X24003224]</a>","to counter such attacks. This … backdoor attacks without compromising the system’s functionality. In the context of the fast-evolving digitalized industrial landscape, our attack …",,Google Scholar
Adversarial Backdoor Attack by Naturalistic Data Poisoning on Trajectory Prediction in Autonomous Driving Supplementary Material,,,2024-06-12,"<a href=""Google Scholar (2024-06-12) : Adversarial Backdoor Attack by Naturalistic Data Poisoning on Trajectory Prediction in Autonomous Driving Supplementary Material"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024/supplemental/Pourkeshavarz_Adversarial_Backdoor_Attack_CVPR_2024_supplemental.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024/supplemental/Pourkeshavarz_Adversarial_Backdoor_Attack_CVPR_2024_supplemental.pdf]</a>","and backdoor-injected models (see Table 2). We report the results with no attack as ”… ” (for the clean model) and tCA (for backdoorinjected model), termed clean test set. …",,Google Scholar
Attack To Defend: Exploiting Adversarial Attacks for Detecting Poisoned Models,"S Fares, K Nandakumar","… of the IEEE/CVF Conference on …, 2024",2024-06-12,"<a href=""Google Scholar (2024-06-12) : Attack To Defend: Exploiting Adversarial Attacks for Detecting Poisoned Models"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024/html/Fares_Attack_To_Defend_Exploiting_Adversarial_Attacks_for_Detecting_Poisoned_Models_CVPR_2024_paper.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024/html/Fares_Attack_To_Defend_Exploiting_Adversarial_Attacks_for_Detecting_Poisoned_Models_CVPR_2024_paper.html]</a>","While there are some recent studies connecting adversarial and backdoor attacks [32, 35, 55, 56], our approach is based on the idea that poisoned models are more …",,Google Scholar
Backdoor Defense via Test-Time Detecting and Repairing,"J Guan, J Liang, R He","… of the IEEE/CVF Conference on …, 2024",2024-06-12,"<a href=""Google Scholar (2024-06-12) : Backdoor Defense via Test-Time Detecting and Repairing"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024/html/Guan_Backdoor_Defense_via_Test-Time_Detecting_and_Repairing_CVPR_2024_paper.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024/html/Guan_Backdoor_Defense_via_Test-Time_Detecting_and_Repairing_CVPR_2024_paper.html]</a>","backdoor, prior research has focused on using clean data to remove backdoor attacks … In this paper, we investigate the possibility of defending against backdoor attacks by …",,Google Scholar
Data Poisoning based Backdoor Attacks to Contrastive Learning,"J Zhang, H Liu, J Jia, NZ Gong","Proceedings of the IEEE …, 2024",2024-06-12,"<a href=""Google Scholar (2024-06-12) : Data Poisoning based Backdoor Attacks to Contrastive Learning"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Data_Poisoning_based_Backdoor_Attacks_to_Contrastive_Learning_CVPR_2024_paper.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Data_Poisoning_based_Backdoor_Attacks_to_Contrastive_Learning_CVPR_2024_paper.html]</a>","ble to data poisoning based backdoor attacks (DPBAs), … backdoor attacks and propose new DPBAs called CorruptEncoder to CL. CorruptEncoder introduces a new attack …",,Google Scholar
Look Listen and Attack: Backdoor Attacks Against Video Action Recognition,"HA Al Kader Hammoud, S Liu…","Proceedings of the …, 2024",2024-06-12,"<a href=""Google Scholar (2024-06-12) : Look Listen and Attack: Backdoor Attacks Against Video Action Recognition"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024W/SAIAD/html/Al_Kader_Hammoud_Look_Listen_and_Attack_Backdoor_Attacks_Against_Video_Action_Recognition_CVPRW_2024_paper.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024W/SAIAD/html/Al_Kader_Hammoud_Look_Listen_and_Attack_Backdoor_Attacks_Against_Video_Action_Recognition_CVPRW_2024_paper.html]</a>","We then explore two ways to extend image backdoor attacks to incorporate the … attack to enable more video-specific backdoor attacks. In particular, image backdoor attacks …",,Google Scholar
Research of Technologies and Methods of Web Application Protection in the Infrastructure of a Higher Education Institution with the Assistance of Specialized Systems,S. Dunayev T. Milevska N. Voropay P. Murr V. Khvostenko Y. Sevriukova,"2024 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)",2024-06-12,"<a href=""IEEE (2024-06-12) : Research of Technologies and Methods of Web Application Protection in the Infrastructure of a Higher Education Institution with the Assistance of Specialized Systems"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10550503]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/HORA61326.2024.10550503]</a>","We live in a dynamic time when digitalization is taking place all over the world and we are witnessing the transition from analog to digital sources of information. The emergence of various services on the Internet makes people's lives easier every day. The education sector is not standing still either. It is confidently moving with the times. The process of acquiring new knowledge is becoming easier, more convenient and accessible anytime, anywhere with the emergence of interactive online learning platforms. However, the number of cyber threats is growing along with the rapid digitization of our environment. News of new threats and backdoors in operating systems, applications, and Web sites are revealed on a daily basis. Cyber criminals do not stand still, every day they test new ways of system penetration, modify the means of attack on users and hardware, and improve their skills in the field of social engineering. In the process of creating services for the needs of educational institutions, considerable attention should be paid to the protection of personal data and intellectual property.",,IEEE
SSL-OTA: Unveiling Backdoor Threats in Self-Supervised Learning for Object Detection,"Qiannan Wang, Changchun Yin, Lu Zhou, Liming Fang",arXiv,2024-06-12,"<a href=""arXiv (2024-06-12) : SSL-OTA: Unveiling Backdoor Threats in Self-Supervised Learning for Object Detection"" target=""_blank"">[http://arxiv.org/abs/2401.00137v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","The extensive adoption of Self-supervised learning(SSL) has led to an increased security threat from backdoor attacks. While existing research has mainly focused on backdoor attacks in image classification, there has been limited exploration of their implications for object detection. Object detection plays a critical role in security-sensitive applications, such as autonomous driving, where backdoor attacks seriously threaten human life and property. In this work, we propose the first backdoor attack designed for object detection tasks in SSL scenarios, called Object Transform Attack (SSL-OTA). SSL-OTA employs a trigger capable of altering predictions of the target object to the desired category, encompassing two attacks: Naive Attack(NA) and Dual-Source Blending Attack (DSBA). NA conducts data poisoning during downstream fine-tuning of the object detector, while DSBA additionally injects backdoors into the pre-trained encoder. We establish appropriate metrics and conduct extensive experiments on benchmark datasets, demonstrating the effectiveness of our proposed attack and its resistance to potential defenses. Notably, both NA and DSBA achieve high attack success rates (ASR) at extremely low poisoning rates (0.5%). The results underscore the importance of considering backdoor threats in SSL-based object detection and contribute a novel perspective to the field.",,arXiv
A Frequency-Injection Backdoor Attack against DNN-Based Finger Vein Verification,"H Zhang, W Sun, L Lv","Computers & Security, 2024",2024-06-11,"<a href=""Google Scholar (2024-06-11) : A Frequency-Injection Backdoor Attack against DNN-Based Finger Vein Verification"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S016740482400261X]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S016740482400261X]</a>","We explore the impact of the backdoor attack in finger vein authentication … backdoor attacks based on deep neural networks in recent research, especially backdoor attacks …",,Google Scholar
Empowering Network Security through Advanced Analysis of Malware Samples: Leveraging System Metrics and Network Log Data for Informed Decision-Making,"Fares Alharbi, Gautam Siddharth Kashyap",International Journal of Networked and Distributed Computing,2024-06-11,"<a href=""Springer (2024-06-11) : Empowering Network Security through Advanced Analysis of Malware Samples: Leveraging System Metrics and Network Log Data for Informed Decision-Making"" target=""_blank"">[https://link.springer.com/article/10.1007/s44227-024-00032-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s44227-024-00032-1]</a>","In the never-ending battle against rising malware threats, cybersecurity professionals were constantly challenged by malware researchers. Businesses...",,Springer
Mutual Information Guided Backdoor Mitigation for Pre-trained Encoders,"Tingxu Han, Weisong Sun, Ziqi Ding, Chunrong Fang, Hanwei Qian, Jiaxun Li, Zhenyu Chen, Xiangyu Zhang",arXiv,2024-06-11,"<a href=""arXiv (2024-06-11) : Mutual Information Guided Backdoor Mitigation for Pre-trained Encoders"" target=""_blank"">[http://arxiv.org/abs/2406.03508v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Self-supervised learning (SSL) is increasingly attractive for pre-training encoders without requiring labeled data. Downstream tasks built on top of those pre-trained encoders can achieve nearly state-of-the-art performance. The pre-trained encoders by SSL, however, are vulnerable to backdoor attacks as demonstrated by existing studies. Numerous backdoor mitigation techniques are designed for downstream task models. However, their effectiveness is impaired and limited when adapted to pre-trained encoders, due to the lack of label information when pre-training. To address backdoor attacks against pre-trained encoders, in this paper, we innovatively propose a mutual information guided backdoor mitigation technique, named MIMIC. MIMIC treats the potentially backdoored encoder as the teacher net and employs knowledge distillation to distill a clean student encoder from the teacher net. Different from existing knowledge distillation approaches, MIMIC initializes the student with random weights, inheriting no backdoors from teacher nets. Then MIMIC leverages mutual information between each layer and extracted features to locate where benign knowledge lies in the teacher net, with which distillation is deployed to clone clean features from teacher to student. We craft the distillation loss with two aspects, including clone loss and attention loss, aiming to mitigate backdoors and maintain encoder performance at the same time. Our evaluation conducted on two backdoor attacks in SSL demonstrates that MIMIC can significantly reduce the attack success rate by only utilizing <5% of clean data, surpassing seven state-of-the-art backdoor mitigation techniques.",,arXiv
Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks,"Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman","arXiv
arXiv","2024-06-10
2023-10","<a href=""arXiv (2024-06-10) : Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2310.05862v2]</a>
<a href=""DBLP (2023-10) : Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.05862]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.05862]</a>","Contrastive Language-Image Pre-training (CLIP) on large image-caption datasets has achieved remarkable success in zero-shot classification and enabled transferability to new domains. However, CLIP is extremely more vulnerable to targeted data poisoning and backdoor attacks, compared to supervised learning. Perhaps surprisingly, poisoning 0.0001% of CLIP pre-training data is enough to make targeted data poisoning attacks successful. This is four orders of magnitude smaller than what is required to poison supervised models. Despite this vulnerability, existing methods are very limited in defending CLIP models during pre-training. In this work, we propose a strong defense, SAFECLIP, to safely pre-train CLIP against targeted data poisoning and backdoor attacks. SAFECLIP warms up the model by applying unimodal contrastive learning (CL) on image and text modalities separately. Then, it divides the data into safe and risky sets, by applying a Gaussian Mixture Model to the cosine similarity of image-caption pair representations. SAFECLIP pre-trains the model by applying the CLIP loss to the safe set and applying unimodal CL to image and text modalities of the risky set separately. By gradually increasing the size of the safe set during pre-training, SAFECLIP effectively breaks targeted data poisoning and backdoor attacks without harming the CLIP performance. Our extensive experiments on CC3M, Visual Genome, and MSCOCO demonstrate that SAFECLIP significantly reduces the success rate of targeted data poisoning attacks from 93.75% to 0% and that of various backdoor attacks from up to 100% to 0%, without harming CLIP's performance.
","
","arXiv
DBLP"
An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection,"Shenao Yan, Shen Wang, Yue Duan, Hanbin Hong, Kiho Lee, Doowon Kim, Yuan Hong",arXiv,2024-06-10,"<a href=""arXiv (2024-06-10) : An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection"" target=""_blank"">[http://arxiv.org/abs/2406.06822v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Large Language Models (LLMs) have transformed code completion tasks, providing context-based suggestions to boost developer productivity in software engineering. As users often fine-tune these models for specific applications, poisoning and backdoor attacks can covertly alter the model outputs. To address this critical security challenge, we introduce CodeBreaker, a pioneering LLM-assisted backdoor attack framework on code completion models. Unlike recent attacks that embed malicious payloads in detectable or irrelevant sections of the code (e.g., comments), CodeBreaker leverages LLMs (e.g., GPT-4) for sophisticated payload transformation (without affecting functionalities), ensuring that both the poisoned data for fine-tuning and generated code can evade strong vulnerability detection. CodeBreaker stands out with its comprehensive coverage of vulnerabilities, making it the first to provide such an extensive set for evaluation. Our extensive experimental evaluations and user studies underline the strong attack performance of CodeBreaker across various settings, validating its superiority over existing approaches. By integrating malicious payloads directly into the source code with minimal transformation, CodeBreaker challenges current security measures, underscoring the critical need for more robust defenses for code completion.",,arXiv
Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models,"Xi Li, Yusen Zhang, Renze Lou, Chen Wu, Jiaqi Wang",arXiv,2024-06-10,"<a href=""arXiv (2024-06-10) : Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models"" target=""_blank"">[http://arxiv.org/abs/2406.05948v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Backdoor attacks present significant threats to Large Language Models (LLMs), particularly with the rise of third-party services that offer API integration and prompt engineering. Untrustworthy third parties can plant backdoors into LLMs and pose risks to users by embedding malicious instructions into user queries. The backdoor-compromised LLM will generate malicious output when and input is embedded with a specific trigger predetermined by an attacker. Traditional defense strategies, which primarily involve model parameter fine-tuning and gradient calculation, are inadequate for LLMs due to their extensive computational and clean data requirements. In this paper, we propose a novel solution, Chain-of-Scrutiny (CoS), to address these challenges. Backdoor attacks fundamentally create a shortcut from the trigger to the target output, thus lack reasoning support. Accordingly, CoS guides the LLMs to generate detailed reasoning steps for the input, then scrutinizes the reasoning process to ensure consistency with the final answer. Any inconsistency may indicate an attack. CoS only requires black-box access to LLM, offering a practical defense, particularly for API-accessible LLMs. It is user-friendly, enabling users to conduct the defense themselves. Driven by natural language, the entire defense process is transparent to users. We validate the effectiveness of CoS through extensive experiments across various tasks and LLMs. Additionally, experiments results shows CoS proves more beneficial for more powerful LLMs.",,arXiv
Exploring Botnet Attacks with Rapid Miner: A Comprehensive Study,S. Asghar M. Z. Hussain M. Zulkifl Hasan S. Nosheen A. M. Qureshi A. Ahmad Siddiqui Z. Mubarak S. H. Chuhan M. Mustafa M. A. Yaqub A. Bilal,2024 IEEE 9th International Conference for Convergence in Technology (I2CT),2024-06-10,"<a href=""IEEE (2024-06-10) : Exploring Botnet Attacks with Rapid Miner: A Comprehensive Study"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544286]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/I2CT61223.2024.10544286]</a>","The fast growth of networks has been shown to have a clear causal relationship with the recent uptick in botnetbased attacks. The increasing use of botnets in various types of cyberattacks, such as denial of service (DoS), distributed denial of service (DDoS), backdoor attacks, and other types of cyberattacks, presents a significant challenge to the network security of organizations. By using the collected bandwidth and system resources of its victims, the botnet has the potential to cause damage that cannot be repaired to the network. The purpose of this paper is to conduct a comprehensive analysis of botnet attacks, with a concentration on DoS and DDoS attacks. It is easy to draw presumptions and construct views on bots based on their activities and traits. Models were trained via machine learning, specifically Auto Modeling, to help defend themselves against attacks launched by botnets. The trained model provides both tabular and visual representations of the outcomes of DoS and DDoS attacks. The model represents the results of DoS and DDoS attacks in tabular and graphical form. Random forest and generalized linear models were used because they showed the best performance on the dataset. Wireshark was used to ensure that the opening of the monitored network went off without a hitch. The pcap data extracted from the network was loaded in Rapid Miner for model training.This paper analyzes the potential for DoS and DDoS attacks.",,IEEE
Generative AI and Cognitive Computing-Driven Intrusion Detection System in Industrial CPS,"Shareeful Islam, Danish Javeed, ... A. K. M. Najmul Islam",Cognitive Computation,2024-06-10,"<a href=""Springer (2024-06-10) : Generative AI and Cognitive Computing-Driven Intrusion Detection System in Industrial CPS"" target=""_blank"">[https://link.springer.com/article/10.1007/s12559-024-10309-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12559-024-10309-w]</a>",Industrial Cyber-Physical Systems (ICPSs) are becoming more and more networked and essential to modern infrastructure. This has led to an increase in...,,Springer
Lurking in the shadows: Unveiling Stealthy Backdoor Attacks against Personalized Federated Learning,"Xiaoting Lyu, Yufei Han, Wei Wang, Jingkai Liu, Yongsheng Zhu, Guangquan Xu, Jiqiang Liu, Xiangliang Zhang",arXiv,2024-06-10,"<a href=""arXiv (2024-06-10) : Lurking in the shadows: Unveiling Stealthy Backdoor Attacks against Personalized Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2406.06207v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Federated Learning (FL) is a collaborative machine learning technique where multiple clients work together with a central server to train a global model without sharing their private data. However, the distribution shift across non-IID datasets of clients poses a challenge to this one-model-fits-all method hindering the ability of the global model to effectively adapt to each client's unique local data. To echo this challenge, personalized FL (PFL) is designed to allow each client to create personalized local models tailored to their private data. While extensive research has scrutinized backdoor risks in FL, it has remained underexplored in PFL applications. In this study, we delve deep into the vulnerabilities of PFL to backdoor attacks. Our analysis showcases a tale of two cities. On the one hand, the personalization process in PFL can dilute the backdoor poisoning effects injected into the personalized local models. Furthermore, PFL systems can also deploy both server-end and client-end defense mechanisms to strengthen the barrier against backdoor attacks. On the other hand, our study shows that PFL fortified with these defense methods may offer a false sense of security. We propose \textit{PFedBA}, a stealthy and effective backdoor attack strategy applicable to PFL systems. \textit{PFedBA} ingeniously aligns the backdoor learning task with the main learning task of PFL by optimizing the trigger generation process. Our comprehensive experiments demonstrate the effectiveness of \textit{PFedBA} in seamlessly embedding triggers into personalized local models. \textit{PFedBA} yields outstanding attack performance across 10 state-of-the-art PFL algorithms, defeating the existing 6 defense mechanisms. Our study sheds light on the subtle yet potent backdoor threats to PFL systems, urging the community to bolster defenses against emerging backdoor challenges.",,arXiv
WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection,"Anudeex Shetty, Yue Teng, Ke He, Qiongkai Xu","arXiv
arXiv","2024-06-09
2024-03","<a href=""arXiv (2024-06-09) : WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection"" target=""_blank"">[http://arxiv.org/abs/2403.01472v2]</a>
<a href=""DBLP (2024-03) : WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.01472]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.01472]</a>","Embedding as a Service (EaaS) has become a widely adopted solution, which offers feature extraction capabilities for addressing various downstream tasks in Natural Language Processing (NLP). Prior studies have shown that EaaS can be prone to model extraction attacks, nevertheless, this concern could be mitigated by adding backdoor watermarks to the text embeddings and subsequently verifying the attack models post-publication. Through the analysis of the recent watermarking strategy for EaaS, EmbMarker, we design a novel CSE (Clustering, Selection, Elimination) attack that removes the backdoor watermark while maintaining the high utility of embeddings, indicating that the previous watermarking approach can be breached. In response to this new threat, we propose a new protocol to make the removal of watermarks more challenging by incorporating multiple possible watermark directions. Our defense approach, WARDEN, notably increases the stealthiness of watermarks and has been empirically shown to be effective against CSE attack.
","
","arXiv
DBLP"
Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections,"Yuanpu Cao, Bochuan Cao, Jinghui Chen","arXiv
arXiv","2024-06-09
2023-12","<a href=""arXiv (2024-06-09) : Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections"" target=""_blank"">[http://arxiv.org/abs/2312.00027v2]</a>
<a href=""DBLP (2023-12) : Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.00027]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.00027]</a>","Recent developments in Large Language Models (LLMs) have manifested significant advancements. To facilitate safeguards against malicious exploitation, a body of research has concentrated on aligning LLMs with human preferences and inhibiting their generation of inappropriate content. Unfortunately, such alignments are often vulnerable: fine-tuning with a minimal amount of harmful data can easily unalign the target LLM. While being effective, such fine-tuning-based unalignment approaches also have their own limitations: (1) non-stealthiness, after fine-tuning, safety audits or red-teaming can easily expose the potential weaknesses of the unaligned models, thereby precluding their release/use. (2) non-persistence, the unaligned LLMs can be easily repaired through re-alignment, i.e., fine-tuning again with aligned data points. In this work, we show that it is possible to conduct stealthy and persistent unalignment on large language models via backdoor injections. We also provide a novel understanding on the relationship between the backdoor persistence and the activation pattern and further provide guidelines for potential trigger design. Through extensive experiments, we demonstrate that our proposed stealthy and persistent unalignment can successfully pass the safety evaluation while maintaining strong persistence against re-alignment defense.
","
","arXiv
DBLP"
AI-Based Intrusion Detection for a Secure Internet of Things (IoT),"Reham Aljohani, Anas Bushnag, Ali Alessa",Journal of Network and Systems Management,2024-06-09,"<a href=""Springer (2024-06-09) : AI-Based Intrusion Detection for a Secure Internet of Things (IoT)"" target=""_blank"">[https://link.springer.com/article/10.1007/s10922-024-09829-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10922-024-09829-5]</a>",The increasing use of intelligent devices connected to the internet has contributed to the introduction of a new paradigm: the Internet of Things...,,Springer
Certified Robustness to Data Poisoning in Gradient-Based Training,"P Sosnin, MN Müller, M Baader, C Tsay…","arXiv preprint arXiv …, 2024",2024-06-09,"<a href=""Google Scholar (2024-06-09) : Certified Robustness to Data Poisoning in Gradient-Based Training"" target=""_blank"">[https://arxiv.org/abs/2406.05670]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2406.05670]</a>","data, making it infeasible to guarantee data quality and leaving models open to poisoning and backdoor attacks. However, provably bounding model behavior under such …",,Google Scholar
Injecting Undetectable Backdoors in Deep Learning and Language Models,"Alkis Kalavasis, Amin Karbasi, Argyris Oikonomou, Katerina Sotiraki, Grigoris Velegkas, Manolis Zampetakis",arXiv,2024-06-09,"<a href=""arXiv (2024-06-09) : Injecting Undetectable Backdoors in Deep Learning and Language Models"" target=""_blank"">[http://arxiv.org/abs/2406.05660v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","As ML models become increasingly complex and integral to high-stakes domains such as finance and healthcare, they also become more susceptible to sophisticated adversarial attacks. We investigate the threat posed by undetectable backdoors in models developed by insidious external expert firms. When such backdoors exist, they allow the designer of the model to sell information to the users on how to carefully perturb the least significant bits of their input to change the classification outcome to a favorable one. We develop a general strategy to plant a backdoor to neural networks while ensuring that even if the model's weights and architecture are accessible, the existence of the backdoor is still undetectable. To achieve this, we utilize techniques from cryptography such as cryptographic signatures and indistinguishability obfuscation. We further introduce the notion of undetectable backdoors to language models and extend our neural network backdoor attacks to such models based on the existence of steganographic functions.",,arXiv
PSBD: Prediction Shift Uncertainty Unlocks Backdoor Detection,"Wei Li, Pin-Yu Chen, Sijia Liu, Ren Wang",arXiv,2024-06-09,"<a href=""arXiv (2024-06-09) : PSBD: Prediction Shift Uncertainty Unlocks Backdoor Detection"" target=""_blank"">[http://arxiv.org/abs/2406.05826v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Deep neural networks are susceptible to backdoor attacks, where adversaries manipulate model predictions by inserting malicious samples into the training data. Currently, there is still a lack of direct filtering methods for identifying suspicious training data to unveil potential backdoor samples. In this paper, we propose a novel method, Prediction Shift Backdoor Detection (PSBD), leveraging an uncertainty-based approach requiring minimal unlabeled clean validation data. PSBD is motivated by an intriguing Prediction Shift (PS) phenomenon, where poisoned models' predictions on clean data often shift away from true labels towards certain other labels with dropout applied during inference, while backdoor samples exhibit less PS. We hypothesize PS results from neuron bias effect, making neurons favor features of certain classes. PSBD identifies backdoor training samples by computing the Prediction Shift Uncertainty (PSU), the variance in probability values when dropout layers are toggled on and off during model inference. Extensive experiments have been conducted to verify the effectiveness and efficiency of PSBD, which achieves state-of-the-art results among mainstream detection methods.",,arXiv
Stealthy Targeted Backdoor Attacks against Image Captioning,"Wenshu Fan, Hongwei Li, Wenbo Jiang, Meng Hao, Shui Yu, Xiao Zhang",arXiv,2024-06-09,"<a href=""arXiv (2024-06-09) : Stealthy Targeted Backdoor Attacks against Image Captioning"" target=""_blank"">[http://arxiv.org/abs/2406.05874v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","In recent years, there has been an explosive growth in multimodal learning. Image captioning, a classical multimodal task, has demonstrated promising applications and attracted extensive research attention. However, recent studies have shown that image caption models are vulnerable to some security threats such as backdoor attacks. Existing backdoor attacks against image captioning typically pair a trigger either with a predefined sentence or a single word as the targeted output, yet they are unrelated to the image content, making them easily noticeable as anomalies by humans. In this paper, we present a novel method to craft targeted backdoor attacks against image caption models, which are designed to be stealthier than prior attacks. Specifically, our method first learns a special trigger by leveraging universal perturbation techniques for object detection, then places the learned trigger in the center of some specific source object and modifies the corresponding object name in the output caption to a predefined target name. During the prediction phase, the caption produced by the backdoored model for input images with the trigger can accurately convey the semantic information of the rest of the whole image, while incorrectly recognizing the source object as the predefined target. Extensive experiments demonstrate that our approach can achieve a high attack success rate while having a negligible impact on model clean performance. In addition, we show our method is stealthy in that the produced backdoor samples are indistinguishable from clean samples in both image and text domains, which can successfully bypass existing backdoor defenses, highlighting the need for better defensive mechanisms against such stealthy backdoor attacks.",,arXiv
Logic of deception in machine learning,"AA Grusho, NA Grusho, MI Zabezhailo…","Informatika i Ee …, 2024",2024-06-08,"<a href=""Google Scholar (2024-06-08) : Logic of deception in machine learning"" target=""_blank"">[https://www.mathnet.ru/eng/ia890]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.mathnet.ru/eng/ia890]</a>","sample allow one to build Back Doors which, in turn, … which allows one to implement Back Door and triggers for … realization of a possible real attack on a complex artificial …",,Google Scholar
Research on WebShell encrypted communication detection based on machine learning,"L Che, X Liu","Third International Conference on Algorithms …, 2024",2024-06-08,"<a href=""Google Scholar (2024-06-08) : Research on WebShell encrypted communication detection based on machine learning"" target=""_blank"">[https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13171/131711M/Research-on-WebShell-encrypted-communication-detection-based-on-machine-learning/10.1117/12.3032051.short]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13171/131711M/Research-on-WebShell-encrypted-communication-detection-based-on-machine-learning/10.1117/12.3032051.short]</a>","a backdoor program … attack environments as much as possible, this article constructs a honeypot containing multiple vulnerabilities to capture and analyze WebShell attack …",,Google Scholar
Exploring the Vulnerability of Self-supervised Monocular Depth Estimation Models,"R Hou, K Mo, Y Long, N Li, Y Rao","Information Sciences, 2024",2024-06-07,"<a href=""Google Scholar (2024-06-07) : Exploring the Vulnerability of Self-supervised Monocular Depth Estimation Models"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S0020025524007886]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S0020025524007886]</a>","the first backdoor attack against self-supervised MDE models. By conceptualizing backdoor attacks as a multi-task challenge, we propose a novel attack framework that …",,Google Scholar
GENIE: Watermarking Graph Neural Networks for Link Prediction,"VSP Bachina, A Gangwal, AA Sharma…","arXiv preprint arXiv …, 2024",2024-06-07,"<a href=""Google Scholar (2024-06-07) : GENIE: Watermarking Graph Neural Networks for Link Prediction"" target=""_blank"">[https://arxiv.org/abs/2406.04805]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2406.04805]</a>","We design GENIE using a novel backdoor attack to create a trigger set for two key methods of LP: (1) node representation-based and (2) subgraph-based. In GENIE, the …",,Google Scholar
TERD: A Unified Framework for Safeguarding Diffusion Models Against Backdoors,"Y Mo, H Huang, M Li, A Li, Y Wang",Forty-first International Conference on …,2024-06-07,"<a href=""Google Scholar (2024-06-07) : TERD: A Unified Framework for Safeguarding Diffusion Models Against Backdoors"" target=""_blank"">[https://openreview.net/forum?id=lpHjmPvxW1]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=lpHjmPvxW1]</a>","Therefore, in this paper, we explore how to defend against backdoor attacks for … Therefore, in this paper, we focus on defending against backdoor attacks from the pixel …",,Google Scholar
"Partial train and isolate, mitigate backdoor attack","Yong Li, Han Gao","arXiv
arXiv","2024-06-06
2024-05","<a href=""arXiv (2024-06-06) : Partial train and isolate, mitigate backdoor attack"" target=""_blank"">[http://arxiv.org/abs/2405.16488v2]</a>
<a href=""DBLP (2024-05) : Partial train and isolate, mitigate backdoor attack"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.16488]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.16488]</a>","Neural networks are widely known to be vulnerable to backdoor attacks, a method that poisons a portion of the training data to make the target model perform well on normal data sets, while outputting attacker-specified or random categories on the poisoned samples. Backdoor attacks are full of threats. Poisoned samples are becoming more and more similar to corresponding normal samples, and even the human eye cannot easily distinguish them. On the other hand, the accuracy of models carrying backdoors on normal samples is no different from that of clean models.In this article, by observing the characteristics of backdoor attacks, We provide a new model training method (PT) that freezes part of the model to train a model that can isolate suspicious samples. Then, on this basis, a clean model is fine-tuned to resist backdoor attacks.
","
","arXiv
DBLP"
Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs,"Javier Rando, Francesco Croce, Kryštof Mitka, Stepan Shabalin, Maksym Andriushchenko, Nicolas Flammarion, Florian Tramèr","arXiv
arXiv","2024-06-06
2024-04","<a href=""arXiv (2024-06-06) : Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs"" target=""_blank"">[http://arxiv.org/abs/2404.14461v2]</a>
<a href=""DBLP (2024-04) : Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.14461]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.14461]</a>","Large language models are aligned to be safe, preventing users from generating harmful content like misinformation or instructions for illegal activities. However, previous work has shown that the alignment process is vulnerable to poisoning attacks. Adversaries can manipulate the safety training data to inject backdoors that act like a universal sudo command: adding the backdoor string to any prompt enables harmful responses from models that, otherwise, behave safely. Our competition, co-located at IEEE SaTML 2024, challenged participants to find universal backdoors in several large language models. This report summarizes the key findings and promising ideas for future research.
","
","arXiv
DBLP"
Backdoor Attack with Sparse and Invisible Trigger,"Yinghua Gao, Yiming Li, Xueluan Gong, Zhifeng Li, Shu-Tao Xia, Qian Wang","arXiv
arXiv","2024-06-06
2023-06","<a href=""arXiv (2024-06-06) : Backdoor Attack with Sparse and Invisible Trigger"" target=""_blank"">[http://arxiv.org/abs/2306.06209v3]</a>
<a href=""DBLP (2023-06) : Backdoor Attack with Sparse and Invisible Trigger"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.06209]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.06209]</a>","Deep neural networks (DNNs) are vulnerable to backdoor attacks, where the adversary manipulates a small portion of training data such that the victim model predicts normally on the benign samples but classifies the triggered samples as the target class. The backdoor attack is an emerging yet threatening training-phase threat, leading to serious risks in DNN-based applications. In this paper, we revisit the trigger patterns of existing backdoor attacks. We reveal that they are either visible or not sparse and therefore are not stealthy enough. More importantly, it is not feasible to simply combine existing methods to design an effective sparse and invisible backdoor attack. To address this problem, we formulate the trigger generation as a bi-level optimization problem with sparsity and invisibility constraints and propose an effective method to solve it. The proposed method is dubbed sparse and invisible backdoor attack (SIBA). We conduct extensive experiments on benchmark datasets under different settings, which verify the effectiveness of our attack and its resistance to existing backdoor defenses. The codes for reproducing main experiments are available at \url{https://github.com/YinghuaGao/SIBA}.
","<a href=""arXiv"" target=""_blank"">[https://github.com/YinghuaGao/SIBA}]</a>
","arXiv
DBLP"
A Theoretical Analysis of Backdoor Poisoning Attacks in Convolutional Neural Networks,"B Li, W Liu",Forty-first International Conference on Machine …,2024-06-06,"<a href=""Google Scholar (2024-06-06) : A Theoretical Analysis of Backdoor Poisoning Attacks in Convolutional Neural Networks"" target=""_blank"">[https://openreview.net/forum?id=SfcB4cVvPz]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=SfcB4cVvPz]</a>","In this paper, we study the dirty-label backdoor attack. In a dirty-label backdoor attack, the adversary firstly chooses a label yp, and adds the trigger pattern to inputs with …",,Google Scholar
Causality Based Front-door Defense Against Backdoor Attack on Language Models,"Y Liu, X Xu, Z Hou, Y Yu",Forty-first International Conference on Machine …,2024-06-06,"<a href=""Google Scholar (2024-06-06) : Causality Based Front-door Defense Against Backdoor Attack on Language Models"" target=""_blank"">[https://openreview.net/forum?id=dmHHVcHFdM]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=dmHHVcHFdM]</a>","are only effective when the backdoor attack form meets specific assumptions, … backdoor attacks. We propose a new defense framework Front-door Adjustment for Backdoor …",,Google Scholar
Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization,"X Zhao, D Xu, S Yuan",Forty-first International Conference on Machine …,2024-06-06,"<a href=""Google Scholar (2024-06-06) : Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization"" target=""_blank"">[https://openreview.net/forum?id=1SiEfsCecd]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=1SiEfsCecd]</a>","In this work, we focus on defending backdoor attacks on encoder-only pre-trained language models (eg BERT). We consider a user who downloads a pretrained language …",,Google Scholar
Gaba: A Generic Anti-Compression Backdoor Attack Using the Characteristic of Image Compression,"W Wang, H Chen, J Li, Y Gao, X Liu, L Zhang…",Available at SSRN …,2024-06-06,"<a href=""Google Scholar (2024-06-06) : Gaba: A Generic Anti-Compression Backdoor Attack Using the Characteristic of Image Compression"" target=""_blank"">[https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4855264]</a>","<a href=""Google Scholar"" target=""_blank"">[https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4855264]</a>","their attack effectiveness in real-world. Existing anti-compression backdoor attacks … propose a generic anti-compression backdoor attack called GABA. Specifically, we …",,Google Scholar
PromptFix: Few-shot Backdoor Removal via Adversarial Prompt Tuning,"Tianrong Zhang, Zhaohan Xi, Ting Wang, Prasenjit Mitra, Jinghui Chen",arXiv,2024-06-06,"<a href=""arXiv (2024-06-06) : PromptFix: Few-shot Backdoor Removal via Adversarial Prompt Tuning"" target=""_blank"">[http://arxiv.org/abs/2406.04478v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Pre-trained language models (PLMs) have attracted enormous attention over the past few years with their unparalleled performances. Meanwhile, the soaring cost to train PLMs as well as their amazing generalizability have jointly contributed to few-shot fine-tuning and prompting as the most popular training paradigms for natural language processing (NLP) models. Nevertheless, existing studies have shown that these NLP models can be backdoored such that model behavior is manipulated when trigger tokens are presented. In this paper, we propose PromptFix, a novel backdoor mitigation strategy for NLP models via adversarial prompt-tuning in few-shot settings. Unlike existing NLP backdoor removal methods, which rely on accurate trigger inversion and subsequent model fine-tuning, PromptFix keeps the model parameters intact and only utilizes two extra sets of soft tokens which approximate the trigger and counteract it respectively. The use of soft tokens and adversarial optimization eliminates the need to enumerate possible backdoor configurations and enables an adaptive balance between trigger finding and preservation of performance. Experiments with various backdoor attacks validate the effectiveness of the proposed method and the performances when domain shift is present further shows PromptFix's applicability to models pretrained on unknown data source which is the common case in prompt tuning scenarios.",,arXiv
BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents,"Yifei Wang, Dizhan Xue, Shengjie Zhang, Shengsheng Qian",arXiv,2024-06-05,"<a href=""arXiv (2024-06-05) : BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents"" target=""_blank"">[http://arxiv.org/abs/2406.03007v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","With the prosperity of large language models (LLMs), powerful LLM-based intelligent agents have been developed to provide customized services with a set of user-defined tools. State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task. However, we show that such methods are vulnerable to our proposed backdoor attacks named BadAgent on various agent tasks, where a backdoor can be embedded by fine-tuning on the backdoor data. At test time, the attacker can manipulate the deployed LLM agents to execute harmful operations by showing the trigger in the agent input or environment. To our surprise, our proposed attack methods are extremely robust even after fine-tuning on trustworthy data. Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools. Our work demonstrates the clear risk of constructing LLM agents based on untrusted LLMs or data. Our code is public at https://github.com/DPamK/BadAgent","<a href=""arXiv"" target=""_blank"">[https://github.com/DPamK/BadAgent]</a>",arXiv
CR-UTP: Certified Robustness against Universal Text Perturbations,"Q Lou, X Liang, J Xue, Y Zhang, R Xie…","arXiv preprint arXiv …, 2024",2024-06-05,"<a href=""Google Scholar (2024-06-05) : CR-UTP: Certified Robustness against Universal Text Perturbations"" target=""_blank"">[https://arxiv.org/abs/2406.01873]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2406.01873]</a>",in universal adversarial attacks and backdoor attacks. Existing certified … attack. A naive method is to simply increase the masking ratio and the likelihood of masking attack …,,Google Scholar
"Fed-Frenemy: robust screening of compromised clients, balancing heterogeneity","X Li, D Wang","… on Mechanical, Electronics, and Electrical and …, 2024",2024-06-05,"<a href=""Google Scholar (2024-06-05) : Fed-Frenemy: robust screening of compromised clients, balancing heterogeneity"" target=""_blank"">[https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13163/131635M/Fed-Frenemy-robust-screening-of-compromised-clients-balancing-heterogeneity/10.1117/12.3030655.short]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13163/131635M/Fed-Frenemy-robust-screening-of-compromised-clients-balancing-heterogeneity/10.1117/12.3030655.short]</a>","In this paper, we propose a meta-learning approach to address backdoor attacks and data heterogeneity in federated learning. In particular, we take the update gradient of …",,Google Scholar
Towards Robust Physical-world Backdoor Attacks on Lane Detection,"Xinwei Zhang, Aishan Liu, Tianyuan Zhang, Siyuan Liang, Xianglong Liu","arXiv
arXiv","2024-06-04
2024-05","<a href=""arXiv (2024-06-04) : Towards Robust Physical-world Backdoor Attacks on Lane Detection"" target=""_blank"">[http://arxiv.org/abs/2405.05553v2]</a>
<a href=""DBLP (2024-05) : Towards Robust Physical-world Backdoor Attacks on Lane Detection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.05553]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.05553]</a>","Deep learning-based lane detection (LD) plays a critical role in autonomous driving systems, such as adaptive cruise control. However, it is vulnerable to backdoor attacks. Existing backdoor attack methods on LD exhibit limited effectiveness in dynamic real-world scenarios, primarily because they fail to consider dynamic scene factors, including changes in driving perspectives (e.g., viewpoint transformations) and environmental conditions (e.g., weather or lighting changes). To tackle this issue, this paper introduces BadLANE, a dynamic scene adaptation backdoor attack for LD designed to withstand changes in real-world dynamic scene factors. To address the challenges posed by changing driving perspectives, we propose an amorphous trigger pattern composed of shapeless pixels. This trigger design allows the backdoor to be activated by various forms or shapes of mud spots or pollution on the road or lens, enabling adaptation to changes in vehicle observation viewpoints during driving. To mitigate the effects of environmental changes, we design a meta-learning framework to train meta-generators tailored to different environmental conditions. These generators produce meta-triggers that incorporate diverse environmental information, such as weather or lighting conditions, as the initialization of the trigger patterns for backdoor implantation, thus enabling adaptation to dynamic environments. Extensive experiments on various commonly used LD models in both digital and physical domains validate the effectiveness of our attacks, outperforming other baselines significantly (+25.15% on average in Attack Success Rate). Our codes will be available upon paper publication.
","
","arXiv
DBLP"
The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers,Orson Mengara,"arXiv
arXiv","2024-06-04
2024-01","<a href=""arXiv (2024-06-04) : The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers"" target=""_blank"">[http://arxiv.org/abs/2401.01537v3]</a>
<a href=""DBLP (2024-01) : The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.01537]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.01537]</a>","The area of Machine Learning as a Service (MLaaS) is experiencing increased implementation due to recent advancements in the AI (Artificial Intelligence) industry. However, this spike has prompted concerns regarding AI defense mechanisms, specifically regarding potential covert attacks from third-party providers that cannot be entirely trusted. Recent research has uncovered that auditory backdoors may use certain modifications as their initiating mechanism. DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor attacks that use cleverly designed tweaks to ensure that corrupted samples are indistinguishable from clean. By utilizing fluctuating signal sampling rates and masking speaker identities through dynamic sound triggers (such as the clapping of hands), it is possible to deceive speech recognition systems (ASR). Our empirical testing demonstrates that DynamicTrigger is both potent and stealthy, achieving impressive success rates during covert attacks while maintaining exceptional accuracy with non-poisoned datasets.
","
","arXiv
DBLP"
Analyzing Federated Learning From a Security Perspective,"AK NAIR, J SAHOO, ED RAJ","Federated Learning: Principles …, 2024",2024-06-04,"<a href=""Google Scholar (2024-06-04) : Analyzing Federated Learning From a Security Perspective"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=Gd0LEQAAQBAJ&oi=fnd&pg=PA213&dq=backdoor+attack&ots=sbjTS5zRI9&sig=0X3wSSv4d9Y61i1kx5z6slqa1D4]</a>","<a href=""Google Scholar"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=Gd0LEQAAQBAJ&oi=fnd&pg=PA213&dq=backdoor+attack&ots=sbjTS5zRI9&sig=0X3wSSv4d9Y61i1kx5z6slqa1D4]</a>","to backdoor attacks is to make the model less expressive, thus making it highly difficult to launch a successful backdoor attack… robustness against backdoor attacks, pruning …",,Google Scholar
Asynchronous Byzantine Federated Learning,"B Cox, A Mălan, J Decouchant, LY Chen","arXiv preprint arXiv:2406.01438, 2024",2024-06-04,"<a href=""Google Scholar (2024-06-04) : Asynchronous Byzantine Federated Learning"" target=""_blank"">[https://arxiv.org/abs/2406.01438]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2406.01438]</a>","In the evaluation of backdoor attack and defense techniques, we consider a specific … The Backdoor Accuracy (𝐵𝐴) measures the accuracy of the model in the backdoor task…",,Google Scholar
Backdoor Attack on Dynamic Link Prediction,"J Chen, X Zhang, H Zheng","Attacks, Defenses and Testing for Deep …, 2024",2024-06-04,"<a href=""Google Scholar (2024-06-04) : Backdoor Attack on Dynamic Link Prediction"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-0425-5_7]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-0425-5_7]</a>","Backdoor attacks are used to manipulate DLP methods to … backdoor attacks remains unexplored. To tackle this issue, we introduce a new framework for backdoor attacks …",,Google Scholar
BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models,"J Xue, M Zheng, Y Hu, F Liu, X Chen, Q Lou","arXiv preprint arXiv …, 2024",2024-06-04,"<a href=""Google Scholar (2024-06-04) : BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models"" target=""_blank"">[https://arxiv.org/abs/2406.00083]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2406.00083]</a>","In this paper, we propose BadRAG to identify the vulnerabilities and attacks on … content passages could achieve a retrieval backdoor, where the retrieval works well for …",,Google Scholar
Using Adversarial Examples to against Backdoor Attack in Federated Learning,"J Chen, X Zhang, H Zheng","Attacks, Defenses and Testing for Deep …, 2024",2024-06-04,"<a href=""Google Scholar (2024-06-04) : Using Adversarial Examples to against Backdoor Attack in Federated Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-0425-5_16]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-0425-5_16]</a>","attacks or compromising the main mission performance of FL. By empirically investigating backdoor attacks … activate backdoors in backdoor models. Thus, in the context of …",,Google Scholar
Antimalware applied to IoT malware detection based on softcore processor endowed with authorial sandbox,"Igor Pinheiro Henriques de Araújo, Liosvaldo Mariano Santiago de Abreu, ... Sidney Marlon Lopes de Lima",Journal of Computer Virology and Hacking Techniques,2024-06-03,"<a href=""Springer (2024-06-03) : Antimalware applied to IoT malware detection based on softcore processor endowed with authorial sandbox"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-024-00526-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-024-00526-0]</a>","Presently, the Internet of Things (IoT) plays a crucial role in modern life, connecting hundreds of billions of devices to the internet. With the...",,Springer
Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge,"Ansh Arora, Xuanli He, Maximilian Mozes, Srinibas Swain, Mark Dras, Qiongkai Xu",arXiv,2024-06-03,"<a href=""arXiv (2024-06-03) : Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge"" target=""_blank"">[http://arxiv.org/abs/2402.19334v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","The democratization of pre-trained language models through open-source initiatives has rapidly advanced innovation and expanded access to cutting-edge technologies. However, this openness also brings significant security risks, including backdoor attacks, where hidden malicious behaviors are triggered by specific inputs, compromising natural language processing (NLP) system integrity and reliability. This paper suggests that merging a backdoored model with other homogeneous models can significantly remediate backdoor vulnerabilities even if such models are not entirely secure. In our experiments, we verify our hypothesis on various models (BERT-Base, RoBERTa-Large, Llama2-7B, and Mistral-7B) and datasets (SST-2, OLID, AG News, and QNLI). Compared to multiple advanced defensive approaches, our method offers an effective and efficient inference-stage defense against backdoor attacks on classification and instruction-tuned tasks without additional resources or specific knowledge. Our approach consistently outperforms recent advanced baselines, leading to an average of about 75% reduction in the attack success rate. Since model merging has been an established approach for improving model performance, the extra advantage it provides regarding defense can be seen as a cost-free bonus.",,arXiv
IT/IOT CYBER EXPLORATION,S Karn,2024,2024-06-03,"<a href=""Google Scholar (2024-06-03) : IT/IOT CYBER EXPLORATION"" target=""_blank"">[https://www.theseus.fi/bitstream/handle/10024/861625/Karn_Sameer.pdf?sequence=2]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.theseus.fi/bitstream/handle/10024/861625/Karn_Sameer.pdf?sequence=2]</a>","into different kinds of cyber-attacks and methods that bad people … back doors and Trojans. In conclusion, the thesis provided a clear demonstration of different cyber-attacks …",,Google Scholar
Open-Set Single-Domain Generalization for Robust Face Anti-Spoofing,"Fangling Jiang, Qi Li, ... Zhenan Sun",International Journal of Computer Vision,2024-06-03,"<a href=""Springer (2024-06-03) : Open-Set Single-Domain Generalization for Robust Face Anti-Spoofing"" target=""_blank"">[https://link.springer.com/article/10.1007/s11263-024-02129-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11263-024-02129-0]</a>","Face anti-spoofing is a critical component of face recognition technology. However, it suffers from poor generalizability for cross-scenario target...",,Springer
Unelicitable Backdoors in Language Models via Cryptographic Transformer Circuits,"Andis Draguns, Andrew Gritsevskiy, Sumeet Ramesh Motwani, Charlie Rogers-Smith, Jeffrey Ladish, Christian Schroeder de Witt",arXiv,2024-06-03,"<a href=""arXiv (2024-06-03) : Unelicitable Backdoors in Language Models via Cryptographic Transformer Circuits"" target=""_blank"">[http://arxiv.org/abs/2406.02619v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","The rapid proliferation of open-source language models significantly increases the risks of downstream backdoor attacks. These backdoors can introduce dangerous behaviours during model deployment and can evade detection by conventional cybersecurity monitoring systems. In this paper, we introduce a novel class of backdoors in autoregressive transformer models, that, in contrast to prior art, are unelicitable in nature. Unelicitability prevents the defender from triggering the backdoor, making it impossible to evaluate or detect ahead of deployment even if given full white-box access and using automated techniques, such as red-teaming or certain formal verification methods. We show that our novel construction is not only unelicitable thanks to using cryptographic techniques, but also has favourable robustness properties. We confirm these properties in empirical investigations, and provide evidence that our backdoors can withstand state-of-the-art mitigation strategies. Additionally, we expand on previous work by showing that our universal backdoors, while not completely undetectable in white-box settings, can be harder to detect than some existing designs. By demonstrating the feasibility of seamlessly integrating backdoors into transformer models, this paper fundamentally questions the efficacy of pre-deployment detection strategies. This offers new insights into the offence-defence balance in AI safety and security.",,arXiv
IBD-PSC: Input-level Backdoor Detection via Parameter-oriented Scaling Consistency,"Linshan Hou, Ruili Feng, Zhongyun Hua, Wei Luo, Leo Yu Zhang, Yiming Li","arXiv
arXiv","2024-06-02
2024-05","<a href=""arXiv (2024-06-02) : IBD-PSC: Input-level Backdoor Detection via Parameter-oriented Scaling Consistency"" target=""_blank"">[http://arxiv.org/abs/2405.09786v3]</a>
<a href=""DBLP (2024-05) : IBD-PSC: Input-level Backdoor Detection via Parameter-oriented Scaling Consistency"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.09786]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.09786]</a>","Deep neural networks (DNNs) are vulnerable to backdoor attacks, where adversaries can maliciously trigger model misclassifications by implanting a hidden backdoor during model training. This paper proposes a simple yet effective input-level backdoor detection (dubbed IBD-PSC) as a `firewall' to filter out malicious testing images. Our method is motivated by an intriguing phenomenon, i.e., parameter-oriented scaling consistency (PSC), where the prediction confidences of poisoned samples are significantly more consistent than those of benign ones when amplifying model parameters. In particular, we provide theoretical analysis to safeguard the foundations of the PSC phenomenon. We also design an adaptive method to select BN layers to scale up for effective detection. Extensive experiments are conducted on benchmark datasets, verifying the effectiveness and efficiency of our IBD-PSC method and its resistance to adaptive attacks. Codes are available at \href{https://github.com/THUYimingLi/BackdoorBox}{BackdoorBox}.
","<a href=""arXiv"" target=""_blank"">[https://github.com/THUYimingLi/BackdoorBox}{BackdoorBox}]</a>
","arXiv
DBLP"
Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space,"Zongru Wu, Zhuosheng Zhang, Pengzhou Cheng, Gongshen Liu","arXiv
arXiv","2024-06-02
2024-02","<a href=""arXiv (2024-06-02) : Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space"" target=""_blank"">[http://arxiv.org/abs/2402.12026v3]</a>
<a href=""DBLP (2024-02) : Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.12026]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.12026]</a>","Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning. Experimental results demonstrate that MuScleLoRA outperforms baselines significantly. Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15\% across multiple datasets and generalizes to various backbone LMs, including BERT, RoBERTa, GPT2-XL, and Llama2. The codes are publicly available at https://github.com/ZrW00/MuScleLoRA.
","<a href=""arXiv"" target=""_blank"">[https://github.com/ZrW00/MuScleLoRA]</a>
","arXiv
DBLP"
A Kernel Test for Causal Association via Noise Contrastive Backdoor Adjustment,"Robert Hu, Dino Sejdinovic, Robin J. Evans",arXiv,2024-06-02,"<a href=""arXiv (2024-06-02) : A Kernel Test for Causal Association via Noise Contrastive Backdoor Adjustment"" target=""_blank"">[http://arxiv.org/abs/2111.13226v4]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Causal inference grows increasingly complex as the number of confounders increases. Given treatments $X$, confounders $Z$ and outcomes $Y$, we develop a non-parametric method to test the \textit{do-null} hypothesis $H_0:\, p(y|\text{\it do}(X=x))=p(y)$ against the general alternative. Building on the Hilbert Schmidt Independence Criterion (HSIC) for marginal independence testing, we propose backdoor-HSIC (bd-HSIC) and demonstrate that it is calibrated and has power for both binary and continuous treatments under a large number of confounders. Additionally, we establish convergence properties of the estimators of covariance operators used in bd-HSIC. We investigate the advantages and disadvantages of bd-HSIC against parametric tests as well as the importance of using the do-null testing in contrast to marginal independence testing or conditional independence testing. A complete implementation can be found at \hyperlink{https://github.com/MrHuff/kgformula}{\texttt{https://github.com/MrHuff/kgformula}}.","<a href=""arXiv"" target=""_blank"">[https://github.com/MrHuff/kgformula}{\texttt{https://github.com/MrHuff/kgformula}}]</a>",arXiv
Generalization Bound and New Algorithm for Clean-Label Backdoor Attack,"Lijia Yu, Shuang Liu, Yibo Miao, Xiao-Shan Gao, Lijun Zhang",arXiv,2024-06-02,"<a href=""arXiv (2024-06-02) : Generalization Bound and New Algorithm for Clean-Label Backdoor Attack"" target=""_blank"">[http://arxiv.org/abs/2406.00588v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","The generalization bound is a crucial theoretical tool for assessing the generalizability of learning methods and there exist vast literatures on generalizability of normal learning, adversarial learning, and data poisoning. Unlike other data poison attacks, the backdoor attack has the special property that the poisoned triggers are contained in both the training set and the test set and the purpose of the attack is two-fold. To our knowledge, the generalization bound for the backdoor attack has not been established. In this paper, we fill this gap by deriving algorithm-independent generalization bounds in the clean-label backdoor attack scenario. Precisely, based on the goals of backdoor attack, we give upper bounds for the clean sample population errors and the poison population errors in terms of the empirical error on the poisoned training dataset. Furthermore, based on the theoretical result, a new clean-label backdoor attack is proposed that computes the poisoning trigger by combining adversarial noise and indiscriminate poison. We show its effectiveness in a variety of settings.",,arXiv
Invisible Backdoor Attacks on Diffusion Models,"Sen Li, Junchi Ma, Minhao Cheng",arXiv,2024-06-02,"<a href=""arXiv (2024-06-02) : Invisible Backdoor Attacks on Diffusion Models"" target=""_blank"">[http://arxiv.org/abs/2406.00816v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","In recent years, diffusion models have achieved remarkable success in the realm of high-quality image generation, garnering increased attention. This surge in interest is paralleled by a growing concern over the security threats associated with diffusion models, largely attributed to their susceptibility to malicious exploitation. Notably, recent research has brought to light the vulnerability of diffusion models to backdoor attacks, enabling the generation of specific target images through corresponding triggers. However, prevailing backdoor attack methods rely on manually crafted trigger generation functions, often manifesting as discernible patterns incorporated into input noise, thus rendering them susceptible to human detection. In this paper, we present an innovative and versatile optimization framework designed to acquire invisible triggers, enhancing the stealthiness and resilience of inserted backdoors. Our proposed framework is applicable to both unconditional and conditional diffusion models, and notably, we are the pioneers in demonstrating the backdooring of diffusion models within the context of text-guided image editing and inpainting pipelines. Moreover, we also show that the backdoors in the conditional generation can be directly applied to model watermarking for model ownership verification, which further boosts the significance of the proposed framework. Extensive experiments on various commonly used samplers and datasets verify the efficacy and stealthiness of the proposed framework. Our code is publicly available at https://github.com/invisibleTriggerDiffusion/invisible_triggers_for_diffusion.","<a href=""arXiv"" target=""_blank"">[https://github.com/invisibleTriggerDiffusion/invisible_triggers_for_diffusion]</a>",arXiv
Exploring Vulnerabilities and Protections in Large Language Models: A Survey,"FW Liu, C Hu","arXiv preprint arXiv:2406.00240, 2024",2024-06-01,"<a href=""Google Scholar (2024-06-01) : Exploring Vulnerabilities and Protections in Large Language Models: A Survey"" target=""_blank"">[https://arxiv.org/abs/2406.00240]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2406.00240]</a>",", breaking them down into Data Poisoning Attacks and Backdoor Attacks. This structured examination helps us understand the relationships between these vulnerabilities …",,Google Scholar
Federated Backdoor Attacks against Speech Recognition,K Mai,2024,2024-06-01,"<a href=""Google Scholar (2024-06-01) : Federated Backdoor Attacks against Speech Recognition"" target=""_blank"">[https://www.theseus.fi/handle/10024/861251]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.theseus.fi/handle/10024/861251]</a>","These attacks provide an easy way to disseminate … This study examines the effect of various attacks on edge … , and a range of adversarial attack tactics. The research uses …",,Google Scholar
Robust Knowledge Distillation Based on Feature Variance Against Backdoored Teacher Model,"Jinyin Chen, Xiaoming Zhao, Haibin Zheng, Xiao Li, Sheng Xiang, Haifeng Guo",arXiv,2024-06-01,"<a href=""arXiv (2024-06-01) : Robust Knowledge Distillation Based on Feature Variance Against Backdoored Teacher Model"" target=""_blank"">[http://arxiv.org/abs/2406.03409v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Benefiting from well-trained deep neural networks (DNNs), model compression have captured special attention for computing resource limited equipment, especially edge devices. Knowledge distillation (KD) is one of the widely used compression techniques for edge deployment, by obtaining a lightweight student model from a well-trained teacher model released on public platforms. However, it has been empirically noticed that the backdoor in the teacher model will be transferred to the student model during the process of KD. Although numerous KD methods have been proposed, most of them focus on the distillation of a high-performing student model without robustness consideration. Besides, some research adopts KD techniques as effective backdoor mitigation tools, but they fail to perform model compression at the same time. Consequently, it is still an open problem to well achieve two objectives of robust KD, i.e., student model's performance and backdoor mitigation. To address these issues, we propose RobustKD, a robust knowledge distillation that compresses the model while mitigating backdoor based on feature variance. Specifically, RobustKD distinguishes the previous works in three key aspects: (1) effectiveness: by distilling the feature map of the teacher model after detoxification, the main task performance of the student model is comparable to that of the teacher model, (2) robustness: by reducing the characteristic variance between the teacher model and the student model, it mitigates the backdoor of the student model under backdoored teacher model scenario, (3) generic: RobustKD still has good performance in the face of multiple data models (e.g., WRN 28-4, Pyramid-200) and diverse DNNs (e.g., ResNet50, MobileNet).",,arXiv
SAT backdoors: Depth beats size,Dreier J.,Journal of Computer and System Sciences,2024-06-01,"<a href=""ScienceDirect (2024-06-01) : SAT backdoors: Depth beats size"" target=""_blank"">[https://doi.org/10.1016/j.jcss.2024.103520]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.jcss.2024.103520]</a>",,,ScienceDirect
Backdooring Post-Quantum Cryptography: Kleptographic Attacks on Lattice-based KEMs,"Prasanna Ravi, Shivam Bhasin, Anupam Chattopadhyay, Aikata Aikata, Sujoy Sinha Roy","GLSVLSI '24: Proceedings of the Great Lakes Symposium on VLSI 2024
ACM Great Lakes Symposium on VLSI
IACR Cryptol. ePrint Arch.","2024-06
2024
2022","<a href=""ACM (2024-06) : Backdooring Post-Quantum Cryptography: Kleptographic Attacks on Lattice-based KEMs"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3649476.3660373]</a>
<a href=""DBLP (2024) : Backdooring Post-Quantum Cryptography: Kleptographic Attacks on Lattice-based KEMs"" target=""_blank"">[https://doi.org/10.1145/3649476.3660373]</a>
<a href=""DBLP (2022) : Backdooring Post-Quantum Cryptography: Kleptographic Attacks on Lattice-based KEMs"" target=""_blank"">[https://eprint.iacr.org/2022/1681]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3649476.3660373]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3649476.3660373]</a>
<a href=""DBLP"" target=""_blank"">[https://eprint.iacr.org/2022/1681]</a>","Post-quantum Cryptography (PQC) has reached the verge of standardization competition, with Kyber as a winning candidate. In this work, we demonstrate practical backdoor insertion in Kyber through kleptrography. The backdoor can be inserted using ...

","

","ACM
DBLP
DBLP"
Strong Backdoors for Default Logic,"Johannes Klaus Fichte, Arne Meier, Irena Schindler","ACM Transactions on Computational Logic (TOCL), Volume 25, Issue 3
SAT
arXiv","2024-06
2016
2016-02","<a href=""ACM (2024-06) : Strong Backdoors for Default Logic"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3655024]</a>
<a href=""DBLP (2016) : Strong Backdoors for Default Logic"" target=""_blank"">[https://doi.org/10.1007/978-3-319-40970-2_4]</a>
<a href=""DBLP (2016-02) : Strong Backdoors for Default Logic"" target=""_blank"">[http://arxiv.org/abs/1602.06052]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3655024]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-319-40970-2_4]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1602.06052]</a>","In this article, we introduce a notion of backdoors to Reiter’s propositional default logic and study structural properties of it. Also we consider the problems of backdoor detection (parameterised by the solution size) as well as backdoor evaluation (...

","

","ACM
DBLP
DBLP"
BapFL: You can Backdoor Personalized Federated Learning,"Tiandi Ye, Cen Chen, Yinggui Wang, Xiang Li, Ming Gao","ACM Transactions on Knowledge Discovery from Data (TKDD), Volume 18, Issue 7",2024-06,"<a href=""ACM (2024-06) : BapFL: You can Backdoor Personalized Federated Learning"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3649316]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3649316]</a>","In federated learning (FL), malicious clients could manipulate the predictions of the trained model through backdoor attacks, posing a significant threat to the security of FL systems. Existing research primarily focuses on backdoor attacks and defenses ...",,ACM
IBAQ: Frequency-Domain Backdoor Attack Threatening Autonomous Driving via Quadratic Phase,"Jinghan Qiu, Honglong Chen, Junjian Li, Yudong Gao, Junwei Li, Xingang Wang","ACM Transactions on Autonomous and Adaptive Systems (TAAS), Just Accepted",2024-06,"<a href=""ACM (2024-06) : IBAQ: Frequency-Domain Backdoor Attack Threatening Autonomous Driving via Quadratic Phase"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3673904]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3673904]</a>","The rapid evolution of backdoor attacks has emerged as a significant threat to the security of autonomous driving models. An attacker injects a backdoor into the model by adding triggers to the samples, which can be activated to manipulate the model’s ...",,ACM
PnA: Robust Aggregation Against Poisoning Attacks to Federated Learning for Edge Intelligence,"Jingkai Liu, Xiaoting Lyu, Li Duan, Yongzhong He, Jiqiang Liu, Hongliang Ma, Bin Wang, Chunhua Su, Wei Wang","ACM Transactions on Sensor Networks (TOSN), Just Accepted",2024-06,"<a href=""ACM (2024-06) : PnA: Robust Aggregation Against Poisoning Attacks to Federated Learning for Edge Intelligence"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3669902]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3669902]</a>","Federated learning (FL), which holds promise for use in edge intelligence applications for smart cities, enables smart devices collaborate in training a global model by exchanging local model updates instead of sharing local training data. However, the ...",,ACM
Unveiling the Threat: Investigating Distributed and Centralized Backdoor Attacks in Federated Graph Neural Networks,"Jing Xu, Stefanos Koffas, Stjepan Picek","Digital Threats: Research and Practice (DTRAP), Volume 5, Issue 2",2024-06,"<a href=""ACM (2024-06) : Unveiling the Threat: Investigating Distributed and Centralized Backdoor Attacks in Federated Graph Neural Networks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3633206]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3633206]</a>","Graph neural networks (GNNs) have gained significant popularity as powerful deep learning methods for processing graph data. However, centralized GNNs face challenges in data-sensitive scenarios due to privacy concerns and regulatory restrictions. ...",,ACM
TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models,"Pengzhou Cheng, Yidong Ding, Tianjie Ju, Zongru Wu, Wei Du, Ping Yi, Zhuosheng Zhang, Gongshen Liu","arXiv
arXiv","2024-05-31
2024-05","<a href=""arXiv (2024-05-31) : TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models"" target=""_blank"">[http://arxiv.org/abs/2405.13401v3]</a>
<a href=""DBLP (2024-05) : TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.13401]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.13401]</a>","Large language models (LLMs) have raised concerns about potential security threats despite performing significantly in Natural Language Processing (NLP). Backdoor attacks initially verified that LLM is doing substantial harm at all stages, but the cost and robustness have been criticized. Attacking LLMs is inherently risky in security review, while prohibitively expensive. Besides, the continuous iteration of LLMs will degrade the robustness of backdoors. In this paper, we propose TrojanRAG, which employs a joint backdoor attack in the Retrieval-Augmented Generation, thereby manipulating LLMs in universal attack scenarios. Specifically, the adversary constructs elaborate target contexts and trigger sets. Multiple pairs of backdoor shortcuts are orthogonally optimized by contrastive learning, thus constraining the triggering conditions to a parameter subspace to improve the matching. To improve the recall of the RAG for the target contexts, we introduce a knowledge graph to construct structured data to achieve hard matching at a fine-grained level. Moreover, we normalize the backdoor scenarios in LLMs to analyze the real harm caused by backdoors from both attackers' and users' perspectives and further verify whether the context is a favorable tool for jailbreaking models. Extensive experimental results on truthfulness, language understanding, and harmfulness show that TrojanRAG exhibits versatility threats while maintaining retrieval capabilities on normal queries.
","
","arXiv
DBLP"
Backdoor Attacks and Defense in FL Check for updates,S Li,Backdoor Attacks against Learning-Based Algorithms,2024-05-31,"<a href=""Google Scholar (2024-05-31) : Backdoor Attacks and Defense in FL Check for updates"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=8OAKEQAAQBAJ&oi=fnd&pg=PA123&dq=backdoor+attack&ots=ZiSeV-vAqq&sig=sH3iZqYV7r6urLAEenSVNb3A6Js]</a>","<a href=""Google Scholar"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=8OAKEQAAQBAJ&oi=fnd&pg=PA123&dq=backdoor+attack&ots=ZiSeV-vAqq&sig=sH3iZqYV7r6urLAEenSVNb3A6Js]</a>","the backdoor attack process against FL that can be utilized to e-Health scenarios. The attack consists of … The attack steps of these two tasks are the same, but their specific …",,Google Scholar
BackdoorIndicator: Leveraging OOD Data for Proactive Backdoor Detection in Federated Learning,"Songze Li, Yanbo Dai",arXiv,2024-05-31,"<a href=""arXiv (2024-05-31) : BackdoorIndicator: Leveraging OOD Data for Proactive Backdoor Detection in Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2405.20862v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","In a federated learning (FL) system, decentralized data owners (clients) could upload their locally trained models to a central server, to jointly train a global model. Malicious clients may plant backdoors into the global model through uploading poisoned local models, causing misclassification to a target class when encountering attacker-defined triggers. Existing backdoor defenses show inconsistent performance under different system and adversarial settings, especially when the malicious updates are made statistically close to the benign ones. In this paper, we first reveal the fact that planting subsequent backdoors with the same target label could significantly help to maintain the accuracy of previously planted backdoors, and then propose a novel proactive backdoor detection mechanism for FL named BackdoorIndicator, which has the server inject indicator tasks into the global model leveraging out-of-distribution (OOD) data, and then utilizing the fact that any backdoor samples are OOD samples with respect to benign samples, the server, who is completely agnostic of the potential backdoor types and target labels, can accurately detect the presence of backdoors in uploaded models, via evaluating the indicator tasks. We perform systematic and extensive empirical studies to demonstrate the consistently superior performance and practicality of BackdoorIndicator over baseline defenses, across a wide range of system and adversarial settings.",,arXiv
GANcrop: A Contrastive Defense Against Backdoor Attacks in Federated Learning,"Xiaoyun Gan, Shanyu Gan, Taizhi Su, Peng Liu",arXiv,2024-05-31,"<a href=""arXiv (2024-05-31) : GANcrop: A Contrastive Defense Against Backdoor Attacks in Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2405.20727v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","With heightened awareness of data privacy protection, Federated Learning (FL) has attracted widespread attention as a privacy-preserving distributed machine learning method. However, the distributed nature of federated learning also provides opportunities for backdoor attacks, where attackers can guide the model to produce incorrect predictions without affecting the global model training process. This paper introduces a novel defense mechanism against backdoor attacks in federated learning, named GANcrop. This approach leverages contrastive learning to deeply explore the disparities between malicious and benign models for attack identification, followed by the utilization of Generative Adversarial Networks (GAN) to recover backdoor triggers and implement targeted mitigation strategies. Experimental findings demonstrate that GANcrop effectively safeguards against backdoor attacks, particularly in non-IID scenarios, while maintaining satisfactory model accuracy, showcasing its remarkable defensive efficacy and practical utility.",,arXiv
Investigating data storage security and retrieval for Fitbit wearable devices,"Aiman Al-Sabaawi, Khamael Al-Dulaimi, ... Leonie Simpson",Health and Technology,2024-05-31,"<a href=""Springer (2024-05-31) : Investigating data storage security and retrieval for Fitbit wearable devices"" target=""_blank"">[https://link.springer.com/article/10.1007/s12553-024-00885-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12553-024-00885-0]</a>",Purpose The use of wearable devices to monitor aspects of personal health is increasing. The Fitbit is an example of a popular device used for this...,,Springer
Literature Review of Backdoor Attacks Check for updates,S Li,Backdoor Attacks against Learning-Based Algorithms,2024-05-31,"<a href=""Google Scholar (2024-05-31) : Literature Review of Backdoor Attacks Check for updates"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=8OAKEQAAQBAJ&oi=fnd&pg=PA23&dq=backdoor+attack&ots=ZiSeV-vAqq&sig=oce7Z1SQJSiD3EGz895hkVXD_eo]</a>","<a href=""Google Scholar"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=8OAKEQAAQBAJ&oi=fnd&pg=PA23&dq=backdoor+attack&ots=ZiSeV-vAqq&sig=oce7Z1SQJSiD3EGz895hkVXD_eo]</a>","of hidden backdoor attack … backdoor attacks in federated learning systems. Therefore, in Chap. 5 of this monograph, we introduce a detection scheme for backdoor attacks …",,Google Scholar
Breaking the False Sense of Security in Backdoor Defense through Re-Activation Attack,"Mingli Zhu, Siyuan Liang, Baoyuan Wu","arXiv
arXiv","2024-05-30
2024-05","<a href=""arXiv (2024-05-30) : Breaking the False Sense of Security in Backdoor Defense through Re-Activation Attack"" target=""_blank"">[http://arxiv.org/abs/2405.16134v2]</a>
<a href=""DBLP (2024-05) : Breaking the False Sense of Security in Backdoor Defense through Re-Activation Attack"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.16134]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.16134]</a>","Deep neural networks face persistent challenges in defending against backdoor attacks, leading to an ongoing battle between attacks and defenses. While existing backdoor defense strategies have shown promising performance on reducing attack success rates, can we confidently claim that the backdoor threat has truly been eliminated from the model? To address it, we re-investigate the characteristics of the backdoored models after defense (denoted as defense models). Surprisingly, we find that the original backdoors still exist in defense models derived from existing post-training defense strategies, and the backdoor existence is measured by a novel metric called backdoor existence coefficient. It implies that the backdoors just lie dormant rather than being eliminated. To further verify this finding, we empirically show that these dormant backdoors can be easily re-activated during inference, by manipulating the original trigger with well-designed tiny perturbation using universal adversarial attack. More practically, we extend our backdoor reactivation to black-box scenario, where the defense model can only be queried by the adversary during inference, and develop two effective methods, i.e., query-based and transfer-based backdoor re-activation attacks. The effectiveness of the proposed methods are verified on both image classification and multimodal contrastive learning (i.e., CLIP) tasks. In conclusion, this work uncovers a critical vulnerability that has never been explored in existing defense strategies, emphasizing the urgency of designing more robust and advanced backdoor defense mechanisms in the future.
","
","arXiv
DBLP"
A TabPFN-based intrusion detection system for the industrial internet of things,"Sergio Ruiz-Villafranca, José Roldán-Gómez, ... José Luis Martinez",The Journal of Supercomputing,2024-05-30,"<a href=""Springer (2024-05-30) : A TabPFN-based intrusion detection system for the industrial internet of things"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-024-06166-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-024-06166-x]</a>","The industrial internet of things (IIoT) has undergone rapid growth in recent years, which has resulted in an increase in the number of threats...",,Springer
BAN: Detecting Backdoors Activated by Adversarial Neuron Noise,"Xiaoyun Xu, Zhuoran Liu, Stefanos Koffas, Shujian Yu, Stjepan Picek",arXiv,2024-05-30,"<a href=""arXiv (2024-05-30) : BAN: Detecting Backdoors Activated by Adversarial Neuron Noise"" target=""_blank"">[http://arxiv.org/abs/2405.19928v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Backdoor attacks on deep learning represent a recent threat that has gained significant attention in the research community. Backdoor defenses are mainly based on backdoor inversion, which has been shown to be generic, model-agnostic, and applicable to practical threat scenarios. State-of-the-art backdoor inversion recovers a mask in the feature space to locate prominent backdoor features, where benign and backdoor features can be disentangled. However, it suffers from high computational overhead, and we also find that it overly relies on prominent backdoor features that are highly distinguishable from benign features. To tackle these shortcomings, this paper improves backdoor feature inversion for backdoor detection by incorporating extra neuron activation information. In particular, we adversarially increase the loss of backdoored models with respect to weights to activate the backdoor effect, based on which we can easily differentiate backdoored and clean models. Experimental results demonstrate our defense, BAN, is 1.37$\times$ (on CIFAR-10) and 5.11$\times$ (on ImageNet200) more efficient with 9.99% higher detect success rate than the state-of-the-art defense BTI-DBF. Our code and trained models are publicly available.\url{https://anonymous.4open.science/r/ban-4B32}",,arXiv
Backdoor Attacks against Learning-Based Algorithms,,,2024-05-30,"<a href=""Google Scholar (2024-05-30) : Backdoor Attacks against Learning-Based Algorithms"" target=""_blank"">[https://link.springer.com/content/pdf/10.1007/978-3-031-57389-7.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/content/pdf/10.1007/978-3-031-57389-7.pdf]</a>","attacks, the definition of backdoor attacks, and the performance measurement indicators of backdoor attacks. In Chap. 3, we present two novel invisible backdoor attack …",,Google Scholar
Backdoors on Manifold Learning,Kreza C.,WiseML 2024 - Proceedings of the 2024 ACM Workshop on Wireless Security and Machine Learning,2024-05-30,"<a href=""ScienceDirect (2024-05-30) : Backdoors on Manifold Learning"" target=""_blank"">[https://doi.org/10.1145/3649403.3656484]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3649403.3656484]</a>",,,ScienceDirect
DiffPhysBA: Diffusion-based Physical Backdoor Attack against Person Re-Identification in Real-World,"Wenli Sun, Xinyang Jiang, Dongsheng Li, Cairong Zhao",arXiv,2024-05-30,"<a href=""arXiv (2024-05-30) : DiffPhysBA: Diffusion-based Physical Backdoor Attack against Person Re-Identification in Real-World"" target=""_blank"">[http://arxiv.org/abs/2405.19990v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Person Re-Identification (ReID) systems pose a significant security risk from backdoor attacks, allowing adversaries to evade tracking or impersonate others. Beyond recognizing this issue, we investigate how backdoor attacks can be deployed in real-world scenarios, where a ReID model is typically trained on data collected in the digital domain and then deployed in a physical environment. This attack scenario requires an attack flow that embeds backdoor triggers in the digital domain realistically enough to also activate the buried backdoor in person ReID models in the physical domain. This paper realizes this attack flow by leveraging a diffusion model to generate realistic accessories on pedestrian images (e.g., bags, hats, etc.) as backdoor triggers. However, the noticeable domain gap between the triggers generated by the off-the-shelf diffusion model and their physical counterparts results in a low attack success rate. Therefore, we introduce a novel diffusion-based physical backdoor attack (DiffPhysBA) method that adopts a training-free similarity-guided sampling process to enhance the resemblance between generated and physical triggers. Consequently, DiffPhysBA can generate realistic attributes as semantic-level triggers in the digital domain and provides higher physical ASR compared to the direct paste method by 25.6% on the real-world test set. Through evaluations on newly proposed real-world and synthetic ReID test sets, DiffPhysBA demonstrates an impressive success rate exceeding 90% in both the digital and physical domains. Notably, it excels in digital stealth metrics and can effectively evade state-of-the-art defense methods.",,arXiv
Phantom: General Trigger Attacks on Retrieval Augmented Language Generation,"H Chaudhari, G Severi, J Abascal, M Jagielski…","arXiv preprint arXiv …, 2024",2024-05-30,"<a href=""Google Scholar (2024-05-30) : Phantom: General Trigger Attacks on Retrieval Augmented Language Generation"" target=""_blank"">[https://arxiv.org/abs/2405.20485]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2405.20485]</a>","Our attack strategy mimics the effects of a backdoor poisoning attack [20], where poisoned samples with a trigger are inserted at training time to induce mis-classification …",,Google Scholar
SleeperNets: Universal Backdoor Poisoning Attacks Against Reinforcement Learning Agents,"Ethan Rathbun, Christopher Amato, Alina Oprea",arXiv,2024-05-30,"<a href=""arXiv (2024-05-30) : SleeperNets: Universal Backdoor Poisoning Attacks Against Reinforcement Learning Agents"" target=""_blank"">[http://arxiv.org/abs/2405.20539v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Reinforcement learning (RL) is an actively growing field that is seeing increased usage in real-world, safety-critical applications -- making it paramount to ensure the robustness of RL algorithms against adversarial attacks. In this work we explore a particularly stealthy form of training-time attacks against RL -- backdoor poisoning. Here the adversary intercepts the training of an RL agent with the goal of reliably inducing a particular action when the agent observes a pre-determined trigger at inference time. We uncover theoretical limitations of prior work by proving their inability to generalize across domains and MDPs. Motivated by this, we formulate a novel poisoning attack framework which interlinks the adversary's objectives with those of finding an optimal policy -- guaranteeing attack success in the limit. Using insights from our theoretical analysis we develop ``SleeperNets'' as a universal backdoor attack which exploits a newly proposed threat model and leverages dynamic reward poisoning techniques. We evaluate our attack in 6 environments spanning multiple domains and demonstrate significant improvements in attack success over existing methods, while preserving benign episodic return.",,arXiv
Unveiling and Mitigating Backdoor Vulnerabilities based on Unlearning Weight Changes and Backdoor Activeness,"Weilin Lin, Li Liu, Shaokui Wei, Jianze Li, Hui Xiong",arXiv,2024-05-30,"<a href=""arXiv (2024-05-30) : Unveiling and Mitigating Backdoor Vulnerabilities based on Unlearning Weight Changes and Backdoor Activeness"" target=""_blank"">[http://arxiv.org/abs/2405.20291v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","The security threat of backdoor attacks is a central concern for deep neural networks (DNNs). Recently, without poisoned data, unlearning models with clean data and then learning a pruning mask have contributed to backdoor defense. Additionally, vanilla fine-tuning with those clean data can help recover the lost clean accuracy. However, the behavior of clean unlearning is still under-explored, and vanilla fine-tuning unintentionally induces back the backdoor effect. In this work, we first investigate model unlearning from the perspective of weight changes and gradient norms, and find two interesting observations in the backdoored model: 1) the weight changes between poison and clean unlearning are positively correlated, making it possible for us to identify the backdoored-related neurons without using poisoned data, 2) the neurons of the backdoored model are more active (i.e., larger changes in gradient norm) than those in the clean model, suggesting the need to suppress the gradient norm during fine-tuning. Then, we propose an effective two-stage defense method. In the first stage, an efficient Neuron Weight Change (NWC)-based Backdoor Reinitialization is proposed based on observation 1). In the second stage, based on observation 2), we design an Activeness-Aware Fine-Tuning to replace the vanilla fine-tuning. Extensive experiments, involving eight backdoor attacks on three benchmark datasets, demonstrate the superior performance of our proposed method compared to recent state-of-the-art backdoor defense approaches.",,arXiv
Byzantine-Robust Federated Learning through Dynamic Clustering,H. Wang L. Wang H. Li,"2023 IEEE 22nd International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)
… on Trust, Security and Privacy in …, 2023","2024-05-29
2023-11-01","<a href=""IEEE (2024-05-29) : Byzantine-Robust Federated Learning through Dynamic Clustering"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10538495]</a>
<a href=""Google Scholar (2023-11-01) : Byzantine-Robust Federated Learning through Dynamic Clustering"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10538495/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TrustCom60117.2023.00050]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10538495/]</a>","Federated learning enables distributed and collaborative learning among multiple participants while protecting their privacy. However, due to its distributed nature, federated learning is vulnerable to Byzantine attacks. These attacks can poison the data or directly modify the model parameters, making the global model performance degrade or even leaving a backdoor. Existing strategies for mitigating Byzantine attacks require a priori information about the number of attackers or require additional validation datasets. However, prior knowledge of the number of Byzantine clients or the collection of representative validation datasets is not always feasible in practice. Moreover, recent research has shown that well-designed attacks can make malicious updates indistinguishable from benign ones by making them highly similar, thus bypassing existing defense methods that rely on these metrics.To tackle these problems, we propose Dynamic Clustering based Federated Learning (DCFL), a novel Byzantine robust FL approach without any additional validation datasets. The main idea behind DCFL is to rigorously constrain the magnitude and direction of local updates through norms and signs. To achieve this, we propose a novel metric that can effectively distinguish malicious updates from benign updates in terms of direction, which can help the server eliminate malicious updates before final aggregation. Our experiments on three datasets demonstrate the effectiveness of DCFL in mitigating various popular Byzantine attacks. Remarkably, the accuracy of the global model learned in the adversarial setting is even close to that of FedAVG under no attack.
These attacks can poison the data or directly modify the model parameters, making … even leaving a backdoor. Existing strategies for mitigating Byzantine attacks require a …","
","IEEE
Google Scholar"
Defending against Adversarial Attacks in Federated Learning on Metric Learning Model,Z. Gu J. Shi Y. Yang L. He,"2023 IEEE 22nd International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)
… on Trust, Security and Privacy in …, 2023","2024-05-29
2023-11-01","<a href=""IEEE (2024-05-29) : Defending against Adversarial Attacks in Federated Learning on Metric Learning Model"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10538594]</a>
<a href=""Google Scholar (2023-11-01) : Defending against Adversarial Attacks in Federated Learning on Metric Learning Model"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10538594/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TrustCom60117.2023.00047]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10538594/]</a>","The industry has widely deployed federated learning (FL) due to its promise to protect clients’ privacy. However, FL is vulnerable to adversarial attacks when the participants are compromised. The defense against adversarial attacks is a challenging problem in FL. Moreover, existing defense methods optimize the dimensionality reduction and anomaly detection models separately, leading to a disappointing projection space and low detection accuracy. We propose a deep metric learning-based anomaly detection to project the model gradients into a metric space where the malicious gradients are separated from benign ones. Meanwhile, while existing methods require an auxiliary dataset to train the defense model, the auxiliary dataset is usually unavailable to the server in the FL setting. We propose a self-supervised method to distill the data between the training epochs of our defense model. To handle radical changes in malicious model gradients, we utilize a median-based aggregated gradient filter to discard improper aggregated gradients. We show experimentally that our algorithm has a competitive performance over existing methods under Byzantine attacks and backdoor attacks with various triggers.
We show experimentally that our algorithm has a competitive performance over existing methods under Byzantine attacks and backdoor attacks with various triggers. Index …","
","IEEE
Google Scholar"
Fast-FedUL: A Training-Free Federated Unlearning with Provable Skew Resilience,"TT Huynh, TB Nguyen, PL Nguyen, TT Nguyen…","arXiv preprint arXiv …, 2024",2024-05-29,"<a href=""Google Scholar (2024-05-29) : Fast-FedUL: A Training-Free Federated Unlearning with Provable Skew Resilience"" target=""_blank"">[https://arxiv.org/abs/2405.18040]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2405.18040]</a>",Evaluation using backdoor attacks. We assess the efficacy of the unlearning techniques through backdoor attack scenarios. The choice of the backdoor attack as the test …,,Google Scholar
Magnitude-based Neuron Pruning for Backdoor Defens,"Nan Li, Haoyu Jiang, Ping Yi","arXiv
arXiv","2024-05-28
2024-05","<a href=""arXiv (2024-05-28) : Magnitude-based Neuron Pruning for Backdoor Defens"" target=""_blank"">[http://arxiv.org/abs/2405.17750v1]</a>
<a href=""DBLP (2024-05) : Magnitude-based Neuron Pruning for Backdoor Defens"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.17750]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.17750]</a>","Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks, posing concerning threats to their reliable deployment. Recent research reveals that backdoors can be erased from infected DNNs by pruning a specific group of neurons, while how to effectively identify and remove these backdoor-associated neurons remains an open challenge. In this paper, we investigate the correlation between backdoor behavior and neuron magnitude, and find that backdoor neurons deviate from the magnitude-saliency correlation of the model. The deviation inspires us to propose a Magnitude-based Neuron Pruning (MNP) method to detect and prune backdoor neurons. Specifically, MNP uses three magnitude-guided objective functions to manipulate the magnitude-saliency correlation of backdoor neurons, thus achieving the purpose of exposing backdoor behavior, eliminating backdoor neurons and preserving clean neurons, respectively. Experiments show our pruning strategy achieves state-of-the-art backdoor defense performance against a variety of backdoor attacks with a limited amount of clean data, demonstrating the crucial role of magnitude for guiding backdoor defenses.
","
","arXiv
DBLP"
Rethinking Pruning for Backdoor Mitigation: An Optimization Perspective,"Nan Li, Haiyang Yu, Ping Yi","arXiv
arXiv","2024-05-28
2024-05","<a href=""arXiv (2024-05-28) : Rethinking Pruning for Backdoor Mitigation: An Optimization Perspective"" target=""_blank"">[http://arxiv.org/abs/2405.17746v1]</a>
<a href=""DBLP (2024-05) : Rethinking Pruning for Backdoor Mitigation: An Optimization Perspective"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.17746]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.17746]</a>","Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks, posing concerning threats to their reliable deployment. Recent research reveals that backdoors can be erased from infected DNNs by pruning a specific group of neurons, while how to effectively identify and remove these backdoor-associated neurons remains an open challenge. Most of the existing defense methods rely on defined rules and focus on neuron's local properties, ignoring the exploration and optimization of pruning policies. To address this gap, we propose an Optimized Neuron Pruning (ONP) method combined with Graph Neural Network (GNN) and Reinforcement Learning (RL) to repair backdoor models. Specifically, ONP first models the target DNN as graphs based on neuron connectivity, and then uses GNN-based RL agents to learn graph embeddings and find a suitable pruning policy. To the best of our knowledge, this is the first attempt to employ GNN and RL for optimizing pruning policies in the field of backdoor defense. Experiments show, with a small amount of clean data, ONP can effectively prune the backdoor neurons implanted by a set of backdoor attacks at the cost of negligible performance degradation, achieving a new state-of-the-art performance for backdoor mitigation.
","
","arXiv
DBLP"
Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks,"Danni Yuan, Shaokui Wei, Mingda Zhang, Li Liu, Baoyuan Wu","arXiv
arXiv","2024-05-28
2023-12","<a href=""arXiv (2024-05-28) : Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2312.06230v2]</a>
<a href=""DBLP (2023-12) : Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.06230]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.06230]</a>","This work studies the task of poisoned sample detection for defending against data poisoning based backdoor attacks. Its core challenge is finding a generalizable and discriminative metric to distinguish between clean and various types of poisoned samples (e.g., various triggers, various poisoning ratios). Inspired by a common phenomenon in backdoor attacks that the backdoored model tend to map significantly different poisoned and clean samples within the target class to similar activation areas, we introduce a novel perspective of the circular distribution of the gradients w.r.t. sample activation, dubbed gradient circular distribution (GCD). And, we find two interesting observations based on GCD. One is that the GCD of samples in the target class is much more dispersed than that in the clean class. The other is that in the GCD of target class, poisoned and clean samples are clearly separated. Inspired by above two observations, we develop an innovative three-stage poisoned sample detection approach, called Activation Gradient based Poisoned sample Detection (AGPD). First, we calculate GCDs of all classes from the model trained on the untrustworthy dataset. Then, we identify the target class(es) based on the difference on GCD dispersion between target and clean classes. Last, we filter out poisoned samples within the identified target class(es) based on the clear separation between poisoned and clean samples. Extensive experiments under various settings of backdoor attacks demonstrate the superior detection performance of the proposed method to existing poisoned detection approaches according to sample activation-based metrics.
","
","arXiv
DBLP"
Cross-Context Backdoor Attacks against Graph Prompt Learning,"Xiaoting Lyu, Yufei Han, Wei Wang, Hangwei Qian, Ivor Tsang, Xiangliang Zhang",arXiv,2024-05-28,"<a href=""arXiv (2024-05-28) : Cross-Context Backdoor Attacks against Graph Prompt Learning"" target=""_blank"">[http://arxiv.org/abs/2405.17984v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Graph Prompt Learning (GPL) bridges significant disparities between pretraining and downstream applications to alleviate the knowledge transfer bottleneck in real-world graph learning. While GPL offers superior effectiveness in graph knowledge transfer and computational efficiency, the security risks posed by backdoor poisoning effects embedded in pretrained models remain largely unexplored. Our study provides a comprehensive analysis of GPL's vulnerability to backdoor attacks. We introduce \textit{CrossBA}, the first cross-context backdoor attack against GPL, which manipulates only the pretraining phase without requiring knowledge of downstream applications. Our investigation reveals both theoretically and empirically that tuning trigger graphs, combined with prompt transformations, can seamlessly transfer the backdoor threat from pretrained encoders to downstream applications. Through extensive experiments involving 3 representative GPL methods across 5 distinct cross-context scenarios and 5 benchmark datasets of node and graph classification tasks, we demonstrate that \textit{CrossBA} consistently achieves high attack success rates while preserving the functionality of downstream applications over clean input. We also explore potential countermeasures against \textit{CrossBA} and conclude that current defenses are insufficient to mitigate \textit{CrossBA}. Our study highlights the persistent backdoor threats to GPL systems, raising trustworthiness concerns in the practices of GPL techniques.",,arXiv
Instruction Backdoor Attacks Against Customized LLMs,"Rui Zhang, Hongwei Li, Rui Wen, Wenbo Jiang, Yuan Zhang, Michael Backes, Yun Shen, Yang Zhang",arXiv,2024-05-28,"<a href=""arXiv (2024-05-28) : Instruction Backdoor Attacks Against Customized LLMs"" target=""_blank"">[http://arxiv.org/abs/2402.09179v3]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","The increasing demand for customized Large Language Models (LLMs) has led to the development of solutions like GPTs. These solutions facilitate tailored LLM creation via natural language prompts without coding. However, the trustworthiness of third-party custom versions of LLMs remains an essential concern. In this paper, we propose the first instruction backdoor attacks against applications integrated with untrusted customized LLMs (e.g., GPTs). Specifically, these attacks embed the backdoor into the custom version of LLMs by designing prompts with backdoor instructions, outputting the attacker's desired result when inputs contain the pre-defined triggers. Our attack includes 3 levels of attacks: word-level, syntax-level, and semantic-level, which adopt different types of triggers with progressive stealthiness. We stress that our attacks do not require fine-tuning or any modification to the backend LLMs, adhering strictly to GPTs development guidelines. We conduct extensive experiments on 6 prominent LLMs and 5 benchmark text classification datasets. The results show that our instruction backdoor attacks achieve the desired attack performance without compromising utility. Additionally, we propose two defense strategies and demonstrate their effectiveness in reducing such attacks. Our findings highlight the vulnerability and the potential risks of LLM customization such as GPTs.",,arXiv
Towards Unified Robustness Against Both Backdoor and Adversarial Attacks,"Zhenxing Niu, Yuyao Sun, Qiguang Miao, Rong Jin, Gang Hua",arXiv,2024-05-28,"<a href=""arXiv (2024-05-28) : Towards Unified Robustness Against Both Backdoor and Adversarial Attacks"" target=""_blank"">[http://arxiv.org/abs/2405.17929v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Deep Neural Networks (DNNs) are known to be vulnerable to both backdoor and adversarial attacks. In the literature, these two types of attacks are commonly treated as distinct robustness problems and solved separately, since they belong to training-time and inference-time attacks respectively. However, this paper revealed that there is an intriguing connection between them: (1) planting a backdoor into a model will significantly affect the model's adversarial examples, (2) for an infected model, its adversarial examples have similar features as the triggered images. Based on these observations, a novel Progressive Unified Defense (PUD) algorithm is proposed to defend against backdoor and adversarial attacks simultaneously. Specifically, our PUD has a progressive model purification scheme to jointly erase backdoors and enhance the model's adversarial robustness. At the early stage, the adversarial examples of infected models are utilized to erase backdoors. With the backdoor gradually erased, our model purification can naturally turn into a stage to boost the model's robustness against adversarial attacks. Besides, our PUD algorithm can effectively identify poisoned images, which allows the initial extra dataset not to be completely clean. Extensive experimental results show that, our discovered connection between backdoor and adversarial attacks is ubiquitous, no matter what type of backdoor attack. The proposed PUD outperforms the state-of-the-art backdoor defense, including the model repairing-based and data filtering-based methods. Besides, it also has the ability to compete with the most advanced adversarial defense methods.",,arXiv
TrojFM: Resource-efficient Backdoor Attacks against Very Large Foundation Models,"Yuzhou. Nie, Yanting. Wang, Jinyuan. Jia, Michael J. De Lucia, Nathaniel D. Bastian, Wenbo. Guo, Dawn. Song","arXiv
arXiv","2024-05-27
2024-05","<a href=""arXiv (2024-05-27) : TrojFM: Resource-efficient Backdoor Attacks against Very Large Foundation Models"" target=""_blank"">[http://arxiv.org/abs/2405.16783v1]</a>
<a href=""DBLP (2024-05) : TrojFM: Resource-efficient Backdoor Attacks against Very Large Foundation Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.16783]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.16783]</a>","One key challenge in backdoor attacks against large foundation models is the resource limits. Backdoor attacks usually require retraining the target model, which is impractical for very large foundation models. Existing backdoor attacks are mainly designed for supervised classifiers or small foundation models (e.g., BERT). None of these attacks has successfully compromised a very large foundation model, such as Llama-3-70B, especially with limited computational resources. In this paper, we propose TrojFM, a novel backdoor attack tailored for very large foundation models. Our primary technical contribution is the development of a novel backdoor injection method. This method forces a backdoored model to generate similar hidden representations for poisoned inputs regardless of their actual semantics. Our approach injects such backdoors by fine-tuning only a very small proportion of model parameters. This enables TrojFM to efficiently launch downstream task-agnostic backdoor attacks against very large foundation models under limited computational resources. Moreover, we optimize the fine-tuning process with our customized QLoRA technique, enabling launching our attack via only~\textit{one A100 GPU}. Furthermore, we design a new trigger injection method to ensure our attack stealthiness. Through extensive experiments, we first demonstrate that TrojFM can launch effective backdoor attacks against widely used large GPT-style models without jeopardizing their normal functionalities (and outperforming existing attacks on BERT-style models). Furthermore, we show that TrojFM is resilient to SOTA defenses and is insensitive to changes in key hyper-parameters. Finally, we conduct a resource analysis to quantify that our method can significantly save computational and memory costs compared to existing backdoor attacks.
","
","arXiv
DBLP"
The last Dance : Robust backdoor attack via diffusion models and bayesian approach,Orson Mengara,"arXiv
arXiv","2024-05-27
2024-02","<a href=""arXiv (2024-05-27) : The last Dance : Robust backdoor attack via diffusion models and bayesian approach"" target=""_blank"">[http://arxiv.org/abs/2402.05967v5]</a>
<a href=""DBLP (2024-02) : The last Dance : Robust backdoor attack via diffusion models and bayesian approach"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.05967]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.05967]</a>","Diffusion models are state-of-the-art deep learning generative models that are trained on the principle of learning forward and backward diffusion processes via the progressive addition of noise and denoising. In this paper, we aim to fool audio-based DNN models, such as those from the Hugging Face framework, primarily those that focus on audio, in particular transformer-based artificial intelligence models, which are powerful machine learning models that save time and achieve results faster and more efficiently. We demonstrate the feasibility of backdoor attacks (called `BacKBayDiffMod`) on audio transformers derived from Hugging Face, a popular framework in the world of artificial intelligence research. The backdoor attack developed in this paper is based on poisoning model training data uniquely by incorporating backdoor diffusion sampling and a Bayesian approach to the distribution of poisoned data.
","
","arXiv
DBLP"
CausalFD: causal invariance-based fraud detection against camouflaged preference,"Yudan Song, Yuecen Wei, ... Xianxian Li",International Journal of Machine Learning and Cybernetics,2024-05-27,"<a href=""Springer (2024-05-27) : CausalFD: causal invariance-based fraud detection against camouflaged preference"" target=""_blank"">[https://link.springer.com/article/10.1007/s13042-024-02209-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13042-024-02209-0]</a>","Fraudsters engage in diverse patterns and deceptive interactions, allowing them to move effortlessly within online networks. However, current fraud...",,Springer
Exploring Backdoor Attacks against Large Language Model-based Decision Making,"Ruochen Jiao, Shaoyuan Xie, Justin Yue, Takami Sato, Lixu Wang, Yixuan Wang, Qi Alfred Chen, Qi Zhu",arXiv,2024-05-27,"<a href=""arXiv (2024-05-27) : Exploring Backdoor Attacks against Large Language Model-based Decision Making"" target=""_blank"">[http://arxiv.org/abs/2405.20774v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Large Language Models (LLMs) have shown significant promise in decision-making tasks when fine-tuned on specific applications, leveraging their inherent common sense and reasoning abilities learned from vast amounts of data. However, these systems are exposed to substantial safety and security risks during the fine-tuning phase. In this work, we propose the first comprehensive framework for Backdoor Attacks against LLM-enabled Decision-making systems (BALD), systematically exploring how such attacks can be introduced during the fine-tuning phase across various channels. Specifically, we propose three attack mechanisms and corresponding backdoor optimization methods to attack different components in the LLM-based decision-making pipeline: word injection, scenario manipulation, and knowledge injection. Word injection embeds trigger words directly into the query prompt. Scenario manipulation occurs in the physical environment, where a high-level backdoor semantic scenario triggers the attack. Knowledge injection conducts backdoor attacks on retrieval augmented generation (RAG)-based LLM systems, strategically injecting word triggers into poisoned knowledge while ensuring the information remains factually accurate for stealthiness. We conduct extensive experiments with three popular LLMs (GPT-3.5, LLaMA2, PaLM2), using two datasets (HighwayEnv, nuScenes), and demonstrate the effectiveness and stealthiness of our backdoor triggers and mechanisms. Finally, we critically assess the strengths and weaknesses of our proposed approaches, highlight the inherent vulnerabilities of LLMs in decision-making tasks, and evaluate potential defenses to safeguard LLM-based decision making systems.",,arXiv
"The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline","Haonan Wang, Qianli Shen, Yao Tong, Yang Zhang, Kenji Kawaguchi","arXiv
arXiv","2024-05-26
2024-01","<a href=""arXiv (2024-05-26) : The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline"" target=""_blank"">[http://arxiv.org/abs/2401.04136v2]</a>
<a href=""DBLP (2024-01) : The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.04136]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.04136]</a>","The commercialization of text-to-image diffusion models (DMs) brings forth potential copyright concerns. Despite numerous attempts to protect DMs from copyright issues, the vulnerabilities of these solutions are underexplored. In this study, we formalized the Copyright Infringement Attack on generative AI models and proposed a backdoor attack method, SilentBadDiffusion, to induce copyright infringement without requiring access to or control over training processes. Our method strategically embeds connections between pieces of copyrighted information and text references in poisoning data while carefully dispersing that information, making the poisoning data inconspicuous when integrated into a clean dataset. Our experiments show the stealth and efficacy of the poisoning data. When given specific text prompts, DMs trained with a poisoning ratio of 0.20% can produce copyrighted images. Additionally, the results reveal that the more sophisticated the DMs are, the easier the success of the attack becomes. These findings underline potential pitfalls in the prevailing copyright protection strategies and underscore the necessity for increased scrutiny to prevent the misuse of DMs.
","
","arXiv
DBLP"
Backdoor Attack in Prompt-Based Continual Learning,"T Nguyen, A Tran, N Ho",2024,2024-05-26,"<a href=""Google Scholar (2024-05-26) : Backdoor Attack in Prompt-Based Continual Learning"" target=""_blank"">[https://nhatptnk8912.github.io/Backdoor_Continual_Learning_v2.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://nhatptnk8912.github.io/Backdoor_Continual_Learning_v2.pdf]</a>","to a potential threat: backdoor attack, which drives the … in executing backdoor attacks on incremental learners … prompt selection to transfer backdoor knowledge to data from …",,Google Scholar
Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor,"Shaokui Wei, Hongyuan Zha, Baoyuan Wu","arXiv
arXiv","2024-05-25
2024-05","<a href=""arXiv (2024-05-25) : Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor"" target=""_blank"">[http://arxiv.org/abs/2405.16112v1]</a>
<a href=""DBLP (2024-05) : Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.16112]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.16112]</a>","Data-poisoning backdoor attacks are serious security threats to machine learning models, where an adversary can manipulate the training dataset to inject backdoors into models. In this paper, we focus on in-training backdoor defense, aiming to train a clean model even when the dataset may be potentially poisoned. Unlike most existing methods that primarily detect and remove/unlearn suspicious samples to mitigate malicious backdoor attacks, we propose a novel defense approach called PDB (Proactive Defensive Backdoor). Specifically, PDB leverages the ""home field"" advantage of defenders by proactively injecting a defensive backdoor into the model during training. Taking advantage of controlling the training process, the defensive backdoor is designed to suppress the malicious backdoor effectively while remaining secret to attackers. In addition, we introduce a reversible mapping to determine the defensive target label. During inference, PDB embeds a defensive trigger in the inputs and reverses the model's prediction, suppressing malicious backdoor and ensuring the model's utility on the original task. Experimental results across various datasets and models demonstrate that our approach achieves state-of-the-art defense performance against a wide range of backdoor attacks.
","
","arXiv
DBLP"
Neural network editing: algorithms and applications,F Fu,2024,2024-05-25,"<a href=""Google Scholar (2024-05-25) : Neural network editing: algorithms and applications"" target=""_blank"">[https://open.bu.edu/handle/2144/48849]</a>","<a href=""Google Scholar"" target=""_blank"">[https://open.bu.edu/handle/2144/48849]</a>","defense against backdoor detection, backdoor removal, and surrogate model attacks. … We show that OVLA-watermarked DNNs can effectively evade backdoor detection …",,Google Scholar
BDetCLIP: Multimodal Prompting Contrastive Test-Time Backdoor Detection,"Yuwei Niu, Shuo He, Qi Wei, Feng Liu, Lei Feng","arXiv
arXiv","2024-05-24
2024-05","<a href=""arXiv (2024-05-24) : BDetCLIP: Multimodal Prompting Contrastive Test-Time Backdoor Detection"" target=""_blank"">[http://arxiv.org/abs/2405.15269v1]</a>
<a href=""DBLP (2024-05) : BDetCLIP: Multimodal Prompting Contrastive Test-Time Backdoor Detection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.15269]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.15269]</a>","Multimodal contrastive learning methods (e.g., CLIP) have shown impressive zero-shot classification performance due to their strong ability to joint representation learning for visual and textual modalities. However, recent research revealed that multimodal contrastive learning on poisoned pre-training data with a small proportion of maliciously backdoored data can induce backdoored CLIP that could be attacked by inserted triggers in downstream tasks with a high success rate. To defend against backdoor attacks on CLIP, existing defense methods focus on either the pre-training stage or the fine-tuning stage, which would unfortunately cause high computational costs due to numerous parameter updates. In this paper, we provide the first attempt at a computationally efficient backdoor detection method to defend against backdoored CLIP in the inference stage. We empirically find that the visual representations of backdoored images are insensitive to both benign and malignant changes in class description texts. Motivated by this observation, we propose BDetCLIP, a novel test-time backdoor detection method based on contrastive prompting. Specifically, we first prompt the language model (e.g., GPT-4) to produce class-related description texts (benign) and class-perturbed random texts (malignant) by specially designed instructions. Then, the distribution difference in cosine similarity between images and the two types of class description texts can be used as the criterion to detect backdoor samples. Extensive experiments validate that our proposed BDetCLIP is superior to state-of-the-art backdoor detection methods, in terms of both effectiveness and efficiency.
","
","arXiv
DBLP"
Cooperative Backdoor Attack in Decentralized Reinforcement Learning with Theoretical Guarantee,"Mengtong Gao, Yifei Zou, Zuyuan Zhang, Xiuzhen Cheng, Dongxiao Yu","arXiv
arXiv","2024-05-24
2024-05","<a href=""arXiv (2024-05-24) : Cooperative Backdoor Attack in Decentralized Reinforcement Learning with Theoretical Guarantee"" target=""_blank"">[http://arxiv.org/abs/2405.15245v1]</a>
<a href=""DBLP (2024-05) : Cooperative Backdoor Attack in Decentralized Reinforcement Learning with Theoretical Guarantee"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.15245]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.15245]</a>","The safety of decentralized reinforcement learning (RL) is a challenging problem since malicious agents can share their poisoned policies with benign agents. The paper investigates a cooperative backdoor attack in a decentralized reinforcement learning scenario. Differing from the existing methods that hide a whole backdoor attack behind their shared policies, our method decomposes the backdoor behavior into multiple components according to the state space of RL. Each malicious agent hides one component in its policy and shares its policy with the benign agents. When a benign agent learns all the poisoned policies, the backdoor attack is assembled in its policy. The theoretical proof is given to show that our cooperative method can successfully inject the backdoor into the RL policies of benign agents. Compared with the existing backdoor attacks, our cooperative method is more covert since the policy from each attacker only contains a component of the backdoor attack and is harder to detect. Extensive simulations are conducted based on Atari environments to demonstrate the efficiency and covertness of our method. To the best of our knowledge, this is the first paper presenting a provable cooperative backdoor attack in decentralized reinforcement learning.
","
","arXiv
DBLP"
WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition,"Zhengyao Song, Yongqiang Li, Danni Yuan, Li Liu, Shaokui Wei, Baoyuan Wu","arXiv
arXiv","2024-05-24
2024-01","<a href=""arXiv (2024-05-24) : WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition"" target=""_blank"">[http://arxiv.org/abs/2401.13578v2]</a>
<a href=""DBLP (2024-01) : WPDA: Frequency-based Backdoor Attack with Wavelet Packet Decomposition"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.13578]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.13578]</a>","This work explores an emerging security threat against deep neural networks (DNNs) based image classification, i.e., backdoor attack. In this scenario, the attacker aims to inject a backdoor into the model by manipulating training data, such that the backdoor could be activated by a particular trigger and bootstraps the model to make a target prediction at inference. Currently, most existing data poisoning-based attacks struggle to achieve success at low poisoning ratios, increasing the risk of being defended by defense methods. In this paper, we propose a novel frequency-based backdoor attack via Wavelet Packet Decomposition (WPD), WPD decomposes the original image signal to a spectrogram that contains frequency information with different semantic meanings. We leverage WPD to statistically analyze the frequency distribution of the dataset to infer the key frequency regions the DNNs would focus on, and the trigger information is only injected into the key frequency regions. Our method mainly includes three parts: 1) the selection of the poisoning frequency regions in spectrogram, 2) trigger generation, 3) the generation of the poisoned dataset. Our method is stealthy and precise, evidenced by the 98.12% Attack Success Rate (ASR) on CIFAR-10 with the extremely low poisoning ratio 0.004% (i.e., only 2 poisoned samples among 50,000 training samples) and can bypass most existing defense methods. Besides, we also provide visualization analyses to explain why our method works.
","
","arXiv
DBLP"
Are You Copying My Prompt? Protecting the Copyright of Vision Prompt for VPaaS via Watermark,"H Ren, A Yan, C Gao, H Yan, Z Zhang, J Li","arXiv preprint arXiv …, 2024",2024-05-24,"<a href=""Google Scholar (2024-05-24) : Are You Copying My Prompt? Protecting the Copyright of Vision Prompt for VPaaS via Watermark"" target=""_blank"">[https://arxiv.org/abs/2405.15161]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2405.15161]</a>","Specifically, we employ a pure poison backdoor attack [28–30] for prompt … checking for the presence of a specific backdoor, completing the watermark verification process. …",,Google Scholar
BadGD: A unified data-centric framework to identify gradient descent vulnerabilities,"CH Wang, G Cheng","arXiv preprint arXiv:2405.15979, 2024",2024-05-24,"<a href=""Google Scholar (2024-05-24) : BadGD: A unified data-centric framework to identify gradient descent vulnerabilities"" target=""_blank"">[https://arxiv.org/abs/2405.15979]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2405.15979]</a>","Specifically, GDP helps us calculate the divergence introduced by our backdoor attack, quantifying the additional privacy budget expenditure required (Lemma 11). This …",,Google Scholar
SynGhost: Imperceptible and Universal Task-agnostic Backdoor Attack in Pre-trained Language Models,"Pengzhou Cheng, Wei Du, Zongru Wu, Fengwei Zhang, Libo Chen, Gongshen Liu",arXiv,2024-05-24,"<a href=""arXiv (2024-05-24) : SynGhost: Imperceptible and Universal Task-agnostic Backdoor Attack in Pre-trained Language Models"" target=""_blank"">[http://arxiv.org/abs/2402.18945v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Pre-training has been a necessary phase for deploying pre-trained language models (PLMs) to achieve remarkable performance in downstream tasks. However, we empirically show that backdoor attacks exploit such a phase as a vulnerable entry point for task-agnostic. In this paper, we first propose $\mathtt{maxEntropy}$, an entropy-based poisoning filtering defense, to prove that existing task-agnostic backdoors are easily exposed, due to explicit triggers used. Then, we present $\mathtt{SynGhost}$, an imperceptible and universal task-agnostic backdoor attack in PLMs. Specifically, $\mathtt{SynGhost}$ hostilely manipulates clean samples through different syntactic and then maps the backdoor to representation space without disturbing the primitive representation. $\mathtt{SynGhost}$ further leverages contrastive learning to achieve universal, which performs a uniform distribution of backdoors in the representation space. In light of the syntactic properties, we also introduce an awareness module to alleviate the interference between different syntactic. Experiments show that $\mathtt{SynGhost}$ holds more serious threats. Not only do severe harmfulness to various downstream tasks on two tuning paradigms but also to any PLMs. Meanwhile, $\mathtt{SynGhost}$ is imperceptible against three countermeasures based on perplexity, fine-pruning, and the proposed $\mathtt{maxEntropy}$.",,arXiv
Towards Imperceptible Backdoor Attack in Self-supervised Learning,"Hanrong Zhang, Zhenting Wang, Tingxu Han, Mingyu Jin, Chenlu Zhan, Mengnan Du, Hongwei Wang, Shiqing Ma","arXiv
arXiv","2024-05-23
2024-05","<a href=""arXiv (2024-05-23) : Towards Imperceptible Backdoor Attack in Self-supervised Learning"" target=""_blank"">[http://arxiv.org/abs/2405.14672v1]</a>
<a href=""DBLP (2024-05) : Towards Imperceptible Backdoor Attack in Self-supervised Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.14672]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.14672]</a>","Self-supervised learning models are vulnerable to backdoor attacks. Existing backdoor attacks that are effective in self-supervised learning often involve noticeable triggers, like colored patches, which are vulnerable to human inspection. In this paper, we propose an imperceptible and effective backdoor attack against self-supervised models. We first find that existing imperceptible triggers designed for supervised learning are not as effective in compromising self-supervised models. We then identify this ineffectiveness is attributed to the overlap in distributions between the backdoor and augmented samples used in self-supervised learning. Building on this insight, we design an attack using optimized triggers that are disentangled to the augmented transformation in the self-supervised learning, while also remaining imperceptible to human vision. Experiments on five datasets and seven SSL algorithms demonstrate our attack is highly effective and stealthy. It also has strong resistance to existing backdoor defenses. Our code can be found at https://github.com/Zhang-Henry/IMPERATIVE.
","<a href=""arXiv"" target=""_blank"">[https://github.com/Zhang-Henry/IMPERATIVE]</a>
","arXiv
DBLP"
Unified Neural Backdoor Removal with Only Few Clean Samples through Unlearning and Relearning,"Nay Myat Min, Long H. Pham, Jun Sun","arXiv
arXiv","2024-05-23
2024-05","<a href=""arXiv (2024-05-23) : Unified Neural Backdoor Removal with Only Few Clean Samples through Unlearning and Relearning"" target=""_blank"">[http://arxiv.org/abs/2405.14781v1]</a>
<a href=""DBLP (2024-05) : Unified Neural Backdoor Removal with Only Few Clean Samples through Unlearning and Relearning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.14781]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.14781]</a>","The application of deep neural network models in various security-critical applications has raised significant security concerns, particularly the risk of backdoor attacks. Neural backdoors pose a serious security threat as they allow attackers to maliciously alter model behavior. While many defenses have been explored, existing approaches are often bounded by model-specific constraints, or necessitate complex alterations to the training process, or fall short against diverse backdoor attacks. In this work, we introduce a novel method for comprehensive and effective elimination of backdoors, called ULRL (short for UnLearn and ReLearn for backdoor removal). ULRL requires only a small set of clean samples and works effectively against all kinds of backdoors. It first applies unlearning for identifying suspicious neurons and then targeted neural weight tuning for backdoor mitigation (i.e., by promoting significant weight deviation on the suspicious neurons). Evaluated against 12 different types of backdoors, ULRL is shown to significantly outperform state-of-the-art methods in eliminating backdoors whilst preserving the model utility.
","
","arXiv
DBLP"
Universal Post-Training Reverse-Engineering Defense Against Backdoors in Deep Neural Networks,"Xi Li, Hang Wang, David J. Miller, George Kesidis","arXiv
arXiv","2024-05-23
2024-02","<a href=""arXiv (2024-05-23) : Universal Post-Training Reverse-Engineering Defense Against Backdoors in Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2402.02034v2]</a>
<a href=""DBLP (2024-02) : Universal Post-Training Reverse-Engineering Defense Against Backdoors in Deep Neural Networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.02034]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.02034]</a>","A variety of defenses have been proposed against backdoors attacks on deep neural network (DNN) classifiers. Universal methods seek to reliably detect and/or mitigate backdoors irrespective of the incorporation mechanism used by the attacker, while reverse-engineering methods often explicitly assume one. In this paper, we describe a new detector that: relies on internal feature map of the defended DNN to detect and reverse-engineer the backdoor and identify its target class, can operate post-training (without access to the training dataset), is highly effective for various incorporation mechanisms (i.e., is universal), and which has low computational overhead and so is scalable. Our detection approach is evaluated for different attacks on benchmark CIFAR-10 and CIFAR-100 image classifiers.
","
","arXiv
DBLP"
Anomaly detection and defense techniques in federated learning: a comprehensive review,"Chang Zhang, Shunkun Yang, ... Huansheng Ning",Artificial Intelligence Review,2024-05-23,"<a href=""Springer (2024-05-23) : Anomaly detection and defense techniques in federated learning: a comprehensive review"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-024-10796-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-024-10796-1]</a>","In recent years, deep learning methods based on a large amount of data have achieved substantial success in numerous fields. However, with increases...",,Springer
DeepNcode: Encoding-Based Protection against Bit-Flip Attacks on Neural Networks,"P Velčický, J Breier, X Hou, M Kovačević","arXiv preprint arXiv:2405.13891, 2024",2024-05-22,"<a href=""Google Scholar (2024-05-22) : DeepNcode: Encoding-Based Protection against Bit-Flip Attacks on Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2405.13891]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2405.13891]</a>","attack scenarios have been proposed up to date that successfully utilize fault attacks, … [8], trojan planting [9], and backdoor planting during the training [10]. It is therefore …",,Google Scholar
Defending against Co-Resident Attacks in Multi-Datacenter Environments,S. S. Mohapatra R. R. Kumar M. K. Dehury S. Bebortta D. Sahoo,"2024 1st International Conference on Cognitive, Green and Ubiquitous Computing (IC-CGU)",2024-05-22,"<a href=""IEEE (2024-05-22) : Defending against Co-Resident Attacks in Multi-Datacenter Environments"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10530661]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IC-CGU58078.2024.10530661]</a>","With cloud computing, customers can use a variety of computing resources whenever they need them without having to worry about maintenance. But security is one of the main issues with cloud computing. Making the move to the cloud presents end users with new security problems, especially because of shared resources that are available to other users. This study focuses on the colocation or co-resident attack, a malicious strategy in which attackers use backdoors to steal sensitive data from Virtual Machine (VM)s housed on the same server. The study investigates detection and prevention techniques for co-resident attacks. Our focus is on the Previously Selected Servers First (PSSF) VM allocation policy, which was selected because it can offer increased security while using less energy to address this problem. Later, we extend this PSSF to study its impact on the Multi-data center environment.",,IEEE
MADESANT: malware detection and severity analysis in industrial environments,"P. L. S. Jayalaxmi, Manali Chakraborty, ... Mauro Conti",Cluster Computing,2024-05-22,"<a href=""Springer (2024-05-22) : MADESANT: malware detection and severity analysis in industrial environments"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04527-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04527-y]</a>","Malware remains a persistent threat to industrial operations, causing disruptions and financial losses. Traditional malware detection approaches...",,Springer
Rethinking the Vulnerabilities of Face Recognition Systems: From a Practical Perspective,"J Chen, Z Shen, Y Pu, C Zhou, S Ji","arXiv preprint arXiv:2405.12786, 2024",2024-05-22,"<a href=""Google Scholar (2024-05-22) : Rethinking the Vulnerabilities of Face Recognition Systems: From a Practical Perspective"" target=""_blank"">[https://arxiv.org/abs/2405.12786]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2405.12786]</a>",", we identify a novel attack, Facial Identity Backdoor Attack dubbed FIBA, which … -stage backdoor attack. FIBA circumvents the limitations of traditional attacks, enabling …",,Google Scholar
Nearest is Not Dearest: Towards Practical Defense against Quantization-conditioned Backdoor Attacks,"Boheng Li, Yishuo Cai, Haowei Li, Feng Xue, Zhifeng Li, Yiming Li","arXiv
arXiv
Proceedings of the IEEE …, 2024","2024-05-21
2024-05
2024-05-22","<a href=""arXiv (2024-05-21) : Nearest is Not Dearest: Towards Practical Defense against Quantization-conditioned Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2405.12725v1]</a>
<a href=""DBLP (2024-05) : Nearest is Not Dearest: Towards Practical Defense against Quantization-conditioned Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.12725]</a>
<a href=""Google Scholar (2024-05-22) : Nearest is not dearest: Towards practical defense against quantization-conditioned backdoor attacks"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024/html/Li_Nearest_is_Not_Dearest_Towards_Practical_Defense_against_Quantization-conditioned_Backdoor_CVPR_2024_paper.html]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.12725]</a>
<a href=""Google Scholar"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024/html/Li_Nearest_is_Not_Dearest_Towards_Practical_Defense_against_Quantization-conditioned_Backdoor_CVPR_2024_paper.html]</a>","Model quantization is widely used to compress and accelerate deep neural networks. However, recent studies have revealed the feasibility of weaponizing model quantization via implanting quantization-conditioned backdoors (QCBs). These special backdoors stay dormant on released full-precision models but will come into effect after standard quantization. Due to the peculiarity of QCBs, existing defenses have minor effects on reducing their threats or are even infeasible. In this paper, we conduct the first in-depth analysis of QCBs. We reveal that the activation of existing QCBs primarily stems from the nearest rounding operation and is closely related to the norms of neuron-wise truncation errors (i.e., the difference between the continuous full-precision weights and its quantized version). Motivated by these insights, we propose Error-guided Flipped Rounding with Activation Preservation (EFRAP), an effective and practical defense against QCBs. Specifically, EFRAP learns a non-nearest rounding strategy with neuron-wise error norm and layer-wise activation preservation guidance, flipping the rounding strategies of neurons crucial for backdoor effects but with minimal impact on clean accuracy. Extensive evaluations on benchmark datasets demonstrate that our EFRAP can defeat state-of-the-art QCB attacks under various settings. Code is available at https://github.com/AntigoneRandy/QuantBackdoor_EFRAP.

attack. In this paper, we make the first attempt to defend against quantization-conditioned backdoor attacks… that the activation of dormant backdoors is closely …","<a href=""arXiv"" target=""_blank"">[https://github.com/AntigoneRandy/QuantBackdoor_EFRAP]</a>

","arXiv
DBLP
Google Scholar"
A Stealthy Backdoor Attack for Without-Label-Sharing Split Learning,"Yuwen Pu, Zhuoyuan Ding, Jiahao Chen, Chunyi Zhou, Qingming Li, Chunqiang Hu, Shouling Ji","arXiv
arXiv","2024-05-21
2024-05","<a href=""arXiv (2024-05-21) : A Stealthy Backdoor Attack for Without-Label-Sharing Split Learning"" target=""_blank"">[http://arxiv.org/abs/2405.12751v1]</a>
<a href=""DBLP (2024-05) : A Stealthy Backdoor Attack for Without-Label-Sharing Split Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.12751]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.12751]</a>","As a novel privacy-preserving paradigm aimed at reducing client computational costs and achieving data utility, split learning has garnered extensive attention and proliferated widespread applications across various fields, including smart health and smart transportation, among others. While recent studies have primarily concentrated on addressing privacy leakage concerns in split learning, such as inference attacks and data reconstruction, the exploration of security issues (e.g., backdoor attacks) within the framework of split learning has been comparatively limited. Nonetheless, the security vulnerability within the context of split learning is highly posing a threat and can give rise to grave security implications, such as the illegal impersonation in the face recognition model. Therefore, in this paper, we propose a stealthy backdoor attack strategy (namely SBAT) tailored to the without-label-sharing split learning architecture, which unveils the inherent security vulnerability of split learning. We posit the existence of a potential attacker on the server side aiming to introduce a backdoor into the training model, while exploring two scenarios: one with known client network architecture and the other with unknown architecture. Diverging from traditional backdoor attack methods that manipulate the training data and labels, we constructively conduct the backdoor attack by injecting the trigger embedding into the server network. Specifically, our SBAT achieves a higher level of attack stealthiness by refraining from modifying any intermediate parameters (e.g., gradients) during training and instead executing all malicious operations post-training.
","
","arXiv
DBLP"
EmInspector: Combating Backdoor Attacks in Federated Self-Supervised Learning Through Embedding Inspection,"Yuwen Qian, Shuchi Wu, Kang Wei, Ming Ding, Di Xiao, Tao Xiang, Chuan Ma, Song Guo","arXiv
arXiv","2024-05-21
2024-05","<a href=""arXiv (2024-05-21) : EmInspector: Combating Backdoor Attacks in Federated Self-Supervised Learning Through Embedding Inspection"" target=""_blank"">[http://arxiv.org/abs/2405.13080v1]</a>
<a href=""DBLP (2024-05) : EmInspector: Combating Backdoor Attacks in Federated Self-Supervised Learning Through Embedding Inspection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.13080]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.13080]</a>","Federated self-supervised learning (FSSL) has recently emerged as a promising paradigm that enables the exploitation of clients' vast amounts of unlabeled data while preserving data privacy. While FSSL offers advantages, its susceptibility to backdoor attacks, a concern identified in traditional federated supervised learning (FSL), has not been investigated. To fill the research gap, we undertake a comprehensive investigation into a backdoor attack paradigm, where unscrupulous clients conspire to manipulate the global model, revealing the vulnerability of FSSL to such attacks. In FSL, backdoor attacks typically build a direct association between the backdoor trigger and the target label. In contrast, in FSSL, backdoor attacks aim to alter the global model's representation for images containing the attacker's specified trigger pattern in favor of the attacker's intended target class, which is less straightforward. In this sense, we demonstrate that existing defenses are insufficient to mitigate the investigated backdoor attacks in FSSL, thus finding an effective defense mechanism is urgent. To tackle this issue, we dive into the fundamental mechanism of backdoor attacks on FSSL, proposing the Embedding Inspector (EmInspector) that detects malicious clients by inspecting the embedding space of local models. In particular, EmInspector assesses the similarity of embeddings from different local models using a small set of inspection images (e.g., ten images of CIFAR100) without specific requirements on sample distribution or labels. We discover that embeddings from backdoored models tend to cluster together in the embedding space for a given inspection image. Evaluation results show that EmInspector can effectively mitigate backdoor attacks on FSSL across various adversary settings. Our code is avaliable at https://github.com/ShuchiWu/EmInspector.
","<a href=""arXiv"" target=""_blank"">[https://github.com/ShuchiWu/EmInspector]</a>
","arXiv
DBLP"
How to Train a Backdoor-Robust Model on a Poisoned Dataset without Auxiliary Data?,"Yuwen Pu, Jiahao Chen, Chunyi Zhou, Zhou Feng, Qingming Li, Chunqiang Hu, Shouling Ji","arXiv
arXiv","2024-05-21
2024-05","<a href=""arXiv (2024-05-21) : How to Train a Backdoor-Robust Model on a Poisoned Dataset without Auxiliary Data?"" target=""_blank"">[http://arxiv.org/abs/2405.12719v1]</a>
<a href=""DBLP (2024-05) : How to Train a Backdoor-Robust Model on a Poisoned Dataset without Auxiliary Data?"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.12719]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.12719]</a>","Backdoor attacks have attracted wide attention from academia and industry due to their great security threat to deep neural networks (DNN). Most of the existing methods propose to conduct backdoor attacks by poisoning the training dataset with different strategies, so it's critical to identify the poisoned samples and then train a clean model on the unreliable dataset in the context of defending backdoor attacks. Although numerous backdoor countermeasure researches are proposed, their inherent weaknesses render them limited in practical scenarios, such as the requirement of enough clean samples, unstable defense performance under various attack conditions, poor defense performance against adaptive attacks, and so on.Therefore, in this paper, we are committed to overcome the above limitations and propose a more practical backdoor defense method. Concretely, we first explore the inherent relationship between the potential perturbations and the backdoor trigger, and the theoretical analysis and experimental results demonstrate that the poisoned samples perform more robustness to perturbation than the clean ones. Then, based on our key explorations, we introduce AdvrBD, an Adversarial perturbation-based and robust Backdoor Defense framework, which can effectively identify the poisoned samples and train a clean model on the poisoned dataset. Constructively, our AdvrBD eliminates the requirement for any clean samples or knowledge about the poisoned dataset (e.g., poisoning ratio), which significantly improves the practicality in real-world scenarios.
","
","arXiv
DBLP"
Interactive Simulations of Backdoors in Neural Networks,"Peter Bajcsy, Maxime Bros","arXiv
arXiv","2024-05-21
2024-05","<a href=""arXiv (2024-05-21) : Interactive Simulations of Backdoors in Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2405.13217v1]</a>
<a href=""DBLP (2024-05) : Interactive Simulations of Backdoors in Neural Networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.13217]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.13217]</a>","This work addresses the problem of planting and defending cryptographic-based backdoors in artificial intelligence (AI) models. The motivation comes from our lack of understanding and the implications of using cryptographic techniques for planting undetectable backdoors under theoretical assumptions in the large AI model systems deployed in practice. Our approach is based on designing a web-based simulation playground that enables planting, activating, and defending cryptographic backdoors in neural networks (NN). Simulations of planting and activating backdoors are enabled for two scenarios: in the extension of NN model architecture to support digital signature verification and in the modified architectural block for non-linear operators. Simulations of backdoor defense against backdoors are available based on proximity analysis and provide a playground for a game of planting and defending against backdoors. The simulations are available at https://pages.nist.gov/nn-calculator
","
","arXiv
DBLP"
Membership Inference Via Backdooring In Image-Based Malware Classification,"MD BHUIYAN, M ISLAM",2024,2024-05-21,"<a href=""Google Scholar (2024-05-21) : Membership Inference Via Backdooring In Image-Based Malware Classification"" target=""_blank"">[https://figshare.mq.edu.au/articles/thesis/Membership_Inference_Via_Backdooring_In_Image-Based_Malware_Classification/25498663/1]</a>","<a href=""Google Scholar"" target=""_blank"">[https://figshare.mq.edu.au/articles/thesis/Membership_Inference_Via_Backdooring_In_Image-Based_Malware_Classification/25498663/1]</a>","Attack Surface Samples This section will discuss different attack sample usages for a backdoor attack … In this section, we will discuss different backdoor attacks in machine …",,Google Scholar
PECAN: A Deterministic Certified Defense Against Backdoor Attacks,"Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni","arXiv
arXiv","2024-05-20
2023-01","<a href=""arXiv (2024-05-20) : PECAN: A Deterministic Certified Defense Against Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2301.11824v4]</a>
<a href=""DBLP (2023-01) : PECAN: A Deterministic Certified Defense Against Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.11824]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.11824]</a>","Neural networks are vulnerable to backdoor poisoning attacks, where the attackers maliciously poison the training set and insert triggers into the test input to change the prediction of the victim model. Existing defenses for backdoor attacks either provide no formal guarantees or come with expensive-to-compute and ineffective probabilistic guarantees. We present PECAN, an efficient and certified approach for defending against backdoor attacks. The key insight powering PECAN is to apply off-the-shelf test-time evasion certification techniques on a set of neural networks trained on disjoint partitions of the data. We evaluate PECAN on image classification and malware detection datasets. Our results demonstrate that PECAN can (1) significantly outperform the state-of-the-art certified backdoor defense, both in defense strength and efficiency, and (2) on real back-door attacks, PECAN can reduce attack success rate by order of magnitude when compared to a range of baselines from the literature.
","
","arXiv
DBLP"
An Invisible Backdoor Attack Based On Semantic Feature,Yangming Chen,"arXiv
arXiv","2024-05-19
2024-05","<a href=""arXiv (2024-05-19) : An Invisible Backdoor Attack Based On Semantic Feature"" target=""_blank"">[http://arxiv.org/abs/2405.11551v1]</a>
<a href=""DBLP (2024-05) : An Invisible Backdoor Attack Based On Semantic Feature"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.11551]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.11551]</a>","Backdoor attacks have severely threatened deep neural network (DNN) models in the past several years. These attacks can occur in almost every stage of the deep learning pipeline. Although the attacked model behaves normally on benign samples, it makes wrong predictions for samples containing triggers. However, most existing attacks use visible patterns (e.g., a patch or image transformations) as triggers, which are vulnerable to human inspection. In this paper, we propose a novel backdoor attack, making imperceptible changes. Concretely, our attack first utilizes the pre-trained victim model to extract low-level and high-level semantic features from clean images and generates trigger pattern associated with high-level features based on channel attention. Then, the encoder model generates poisoned images based on the trigger and extracted low-level semantic features without causing noticeable feature loss. We evaluate our attack on three prominent image classification DNN across three standard datasets. The results demonstrate that our attack achieves high attack success rates while maintaining robustness against backdoor defenses. Furthermore, we conduct extensive image similarity experiments to emphasize the stealthiness of our attack strategy.
","
","arXiv
DBLP"
BOSC: A Backdoor-based Framework for Open Set Synthetic Image Attribution,"Jun Wang, Benedetta Tondi, Mauro Barni","arXiv
arXiv","2024-05-19
2024-05","<a href=""arXiv (2024-05-19) : BOSC: A Backdoor-based Framework for Open Set Synthetic Image Attribution"" target=""_blank"">[http://arxiv.org/abs/2405.11491v1]</a>
<a href=""DBLP (2024-05) : BOSC: A Backdoor-based Framework for Open Set Synthetic Image Attribution"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.11491]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.11491]</a>","Synthetic image attribution addresses the problem of tracing back the origin of images produced by generative models. Extensive efforts have been made to explore unique representations of generative models and use them to attribute a synthetic image to the model that produced it. Most of the methods classify the models or the architectures among those in a closed set without considering the possibility that the system is fed with samples produced by unknown architectures. With the continuous progress of AI technology, new generative architectures continuously appear, thus driving the attention of researchers towards the development of tools capable of working in open-set scenarios. In this paper, we propose a framework for open set attribution of synthetic images, named BOSC (Backdoor-based Open Set Classification), that relies on the concept of backdoor attacks to design a classifier with rejection option. BOSC works by purposely injecting class-specific triggers inside a portion of the images in the training set to induce the network to establish a matching between class features and trigger features. The behavior of the trained model with respect to triggered samples is then exploited at test time to perform sample rejection using an ad-hoc score. Experiments show that the proposed method has good performance, always surpassing the state-of-the-art. Robustness against image processing is also very good. Although we designed our method for the task of synthetic image attribution, the proposed framework is a general one and can be used for other image forensic applications.
","
","arXiv
DBLP"
SEEP: Training Dynamics Grounds Latent Representation Search for Mitigating Backdoor Poisoning Attacks,"Xuanli He, Qiongkai Xu, Jun Wang, Benjamin I. P. Rubinstein, Trevor Cohn","arXiv
arXiv","2024-05-19
2024-05","<a href=""arXiv (2024-05-19) : SEEP: Training Dynamics Grounds Latent Representation Search for Mitigating Backdoor Poisoning Attacks"" target=""_blank"">[http://arxiv.org/abs/2405.11575v1]</a>
<a href=""DBLP (2024-05) : SEEP: Training Dynamics Grounds Latent Representation Search for Mitigating Backdoor Poisoning Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.11575]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.11575]</a>","Modern NLP models are often trained on public datasets drawn from diverse sources, rendering them vulnerable to data poisoning attacks. These attacks can manipulate the model's behavior in ways engineered by the attacker. One such tactic involves the implantation of backdoors, achieved by poisoning specific training instances with a textual trigger and a target class label. Several strategies have been proposed to mitigate the risks associated with backdoor attacks by identifying and removing suspected poisoned examples. However, we observe that these strategies fail to offer effective protection against several advanced backdoor attacks. To remedy this deficiency, we propose a novel defensive mechanism that first exploits training dynamics to identify poisoned samples with high precision, followed by a label propagation step to improve recall and thus remove the majority of poisoned instances. Compared with recent advanced defense methods, our method considerably reduces the success rates of several backdoor attacks while maintaining high classification accuracy on clean test sets.
","
","arXiv
DBLP"
All things Adversarial in LLMs: A survey,S Gaudi,2024,2024-05-19,"<a href=""Google Scholar (2024-05-19) : All things Adversarial in LLMs: A survey"" target=""_blank"">[https://sachit3022.github.io/files/Adverserial_text.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://sachit3022.github.io/files/Adverserial_text.pdf]</a>","In this survey, we also discuss these backdoor attacks … harmful detector leads to an attack, and swapping the reward … Backdoor attacks can be introduced in the fine-tuning …",,Google Scholar
Measuring Impacts of Poisoning on Model Parameters and Embeddings for Large Language Models of Code,"A Hussain, MRI Rabin, MA Alipour","arXiv preprint arXiv:2405.11466, 2024",2024-05-19,"<a href=""Google Scholar (2024-05-19) : Measuring Impacts of Poisoning on Model Parameters and Embeddings for Large Language Models of Code"" target=""_blank"">[https://arxiv.org/abs/2405.11466]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2405.11466]</a>",Backdoor attacks involve the insertion of triggers into … parameters to detect potential backdoor signals in code … white-box detection of backdoor signals in LLMs of code …,,Google Scholar
Understanding and Mitigating Neural Backdoors,R Pang,2024,2024-05-19,"<a href=""Google Scholar (2024-05-19) : Understanding and Mitigating Neural Backdoors"" target=""_blank"">[https://etda.libraries.psu.edu/catalog/30354rbp5354]</a>","<a href=""Google Scholar"" target=""_blank"">[https://etda.libraries.psu.edu/catalog/30354rbp5354]</a>","backdoor attacks by jointly optimizing triggers and trojaned models, uncovering intriguing mutual reinforcement effects between the two attack … scope of backdoor attacks to …",,Google Scholar
BadActs: A Universal Backdoor Defense in the Activation Space,"Biao Yi, Sishuo Chen, Yiming Li, Tong Li, Baolei Zhang, Zheli Liu","arXiv
arXiv","2024-05-18
2024-05","<a href=""arXiv (2024-05-18) : BadActs: A Universal Backdoor Defense in the Activation Space"" target=""_blank"">[http://arxiv.org/abs/2405.11227v1]</a>
<a href=""DBLP (2024-05) : BadActs: A Universal Backdoor Defense in the Activation Space"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.11227]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.11227]</a>","Backdoor attacks pose an increasingly severe security threat to Deep Neural Networks (DNNs) during their development stage. In response, backdoor sample purification has emerged as a promising defense mechanism, aiming to eliminate backdoor triggers while preserving the integrity of the clean content in the samples. However, existing approaches have been predominantly focused on the word space, which are ineffective against feature-space triggers and significantly impair performance on clean data. To address this, we introduce a universal backdoor defense that purifies backdoor samples in the activation space by drawing abnormal activations towards optimized minimum clean activation distribution intervals. The advantages of our approach are twofold: (1) By operating in the activation space, our method captures from surface-level information like words to higher-level semantic concepts such as syntax, thus counteracting diverse triggers, (2) the fine-grained continuous nature of the activation space allows for more precise preservation of clean content while removing triggers. Furthermore, we propose a detection module based on statistical information of abnormal activations, to achieve a better trade-off between clean accuracy and defending performance.
","
","arXiv
DBLP"
Advancing Cybersecurity: Leveraging UNSW_NB15 Dataset for Enhanced Detection and Prediction of Diverse Cyber Threats,"S Gawand, MS Kumar","Educational Administration: Theory and Practice, 2024",2024-05-18,"<a href=""Google Scholar (2024-05-18) : Advancing Cybersecurity: Leveraging UNSW_NB15 Dataset for Enhanced Detection and Prediction of Diverse Cyber Threats"" target=""_blank"">[https://kuey.net/menuscript/index.php/kuey/article/view/3936]</a>","<a href=""Google Scholar"" target=""_blank"">[https://kuey.net/menuscript/index.php/kuey/article/view/3936]</a>","attacks.This study focuses on developing machine learning model that can identify cyber-attacks … types of cyber-attacks namely Fuzzers, analysis, DoS, Backdoor, Exploits, …",,Google Scholar
Energy-based Backdoor Defense without Task-Specific Samples and Model Retraining,"Y Gao, H Chen, P Sun, Z Li, J Li, H Shao",Forty-first International …,2024-05-18,"<a href=""Google Scholar (2024-05-18) : Energy-based Backdoor Defense without Task-Specific Samples and Model Retraining"" target=""_blank"">[https://openreview.net/forum?id=TJ6tVNt6Y4]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=TJ6tVNt6Y4]</a>","2023a), we focus on the backdoor attacks for supervised image classification, which is a widely used in various applications like face recognition and autonomous driving. …",,Google Scholar
Not All Prompts Are Secure: A Switchable Backdoor Attack Against Pre-trained Vision Transformers,"Sheng Yang, Jiawang Bai, Kuofeng Gao, Yong Yang, Yiming Li, Shu-tao Xia","arXiv
arXiv","2024-05-17
2024-05","<a href=""arXiv (2024-05-17) : Not All Prompts Are Secure: A Switchable Backdoor Attack Against Pre-trained Vision Transformers"" target=""_blank"">[http://arxiv.org/abs/2405.10612v1]</a>
<a href=""DBLP (2024-05) : Not All Prompts Are Secure: A Switchable Backdoor Attack Against Pre-trained Vision Transformers"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.10612]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.10612]</a>","Given the power of vision transformers, a new learning paradigm, pre-training and then prompting, makes it more efficient and effective to address downstream visual recognition tasks. In this paper, we identify a novel security threat towards such a paradigm from the perspective of backdoor attacks. Specifically, an extra prompt token, called the switch token in this work, can turn the backdoor mode on, i.e., converting a benign model into a backdoored one. Once under the backdoor mode, a specific trigger can force the model to predict a target class. It poses a severe risk to the users of cloud API, since the malicious behavior can not be activated and detected under the benign mode, thus making the attack very stealthy. To attack a pre-trained model, our proposed attack, named SWARM, learns a trigger and prompt tokens including a switch token. They are optimized with the clean loss which encourages the model always behaves normally even the trigger presents, and the backdoor loss that ensures the backdoor can be activated by the trigger when the switch is on. Besides, we utilize the cross-mode feature distillation to reduce the effect of the switch token on clean samples. The experiments on diverse visual recognition tasks confirm the success of our switchable backdoor attack, i.e., achieving 95%+ attack success rate, and also being hard to be detected and removed. Our code is available at https://github.com/20000yshust/SWARM.
","<a href=""arXiv"" target=""_blank"">[https://github.com/20000yshust/SWARM]</a>
","arXiv
DBLP"
Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective,"Zhiwei Zhang, Minhua Lin, Enyan Dai, Suhang Wang","arXiv
arXiv","2024-05-17
2024-05","<a href=""arXiv (2024-05-17) : Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective"" target=""_blank"">[http://arxiv.org/abs/2405.10757v1]</a>
<a href=""DBLP (2024-05) : Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.10757]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.10757]</a>","Graph Neural Networks (GNNs) have shown remarkable performance in various tasks. However, recent works reveal that GNNs are vulnerable to backdoor attacks. Generally, backdoor attack poisons the graph by attaching backdoor triggers and the target class label to a set of nodes in the training graph. A GNN trained on the poisoned graph will then be misled to predict test nodes attached with trigger to the target class. Despite their effectiveness, our empirical analysis shows that triggers generated by existing methods tend to be out-of-distribution (OOD), which significantly differ from the clean data. Hence, these injected triggers can be easily detected and pruned with widely used outlier detection methods in real-world applications. Therefore, in this paper, we study a novel problem of unnoticeable graph backdoor attacks with in-distribution (ID) triggers. To generate ID triggers, we introduce an OOD detector in conjunction with an adversarial learning strategy to generate the attributes of the triggers within distribution. To ensure a high attack success rate with ID triggers, we introduce novel modules designed to enhance trigger memorization by the victim model trained on poisoned graph. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed method in generating in distribution triggers that can by-pass various defense strategies while maintaining a high attack success rate.
","
","arXiv
DBLP"
Not all prompts are secure: A switchable backdoor attack against pre-trained vision transfomers,"S Yang, J Bai, K Gao, Y Yang, Y Li…","Proceedings of the …, 2024",2024-05-17,"<a href=""Google Scholar (2024-05-17) : Not all prompts are secure: A switchable backdoor attack against pre-trained vision transfomers"" target=""_blank"">[http://openaccess.thecvf.com/content/CVPR2024/html/Yang_Not_All_Prompts_Are_Secure_A_Switchable_Backdoor_Attack_Against_CVPR_2024_paper.html]</a>","<a href=""Google Scholar"" target=""_blank"">[http://openaccess.thecvf.com/content/CVPR2024/html/Yang_Not_All_Prompts_Are_Secure_A_Switchable_Backdoor_Attack_Against_CVPR_2024_paper.html]</a>","Consider a practical scenario where a backdoor attack occurs within … backdoor attack for VP that incorporates a switch mode, including both a clean mode and a backdoor …",,Google Scholar
Exploring Model Poisoning Attack to Convolutional Neural Network Based Brain Tumor Detection Systems,K. Lata P. Singh S. Saini,"2024 25th International Symposium on Quality Electronic Design (ISQED)
2024 25th International Symposium …, 2024","2024-05-16
2024-04-03","<a href=""IEEE (2024-05-16) : Exploring Model Poisoning Attack to Convolutional Neural Network Based Brain Tumor Detection Systems"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10528710]</a>
<a href=""Google Scholar (2024-04-03) : Exploring Model Poisoning Attack to Convolutional Neural Network Based Brain Tumor Detection Systems"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10528710/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ISQED60706.2024.10528710]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10528710/]</a>","With the growth of artificial intelligence (AI) in many fields, convolutional neural networks, or CNNs, have become more and more popular. But worries regarding CNN-based systems’ security have surfaced as AI is used more and more. A major threat to these systems comes from sophisticated attacks that go for the heart of artificial intelligence (AI), introducing harmful alterations at several points throughout the Integrated Circuit (IC) supply chain. These alterations can take many different forms, from little circuitry tweaks to secret feature additions, and they can happen at any stage of the design, manufacturing, or testing process. Their goal is to compromise the integrity, functioning, or security of integrated circuits. In this study, we conducted a preliminary investigation to gauge how a poisoned pooling layer impacts a CNN-based brain tumor detection system with MRI data. Using brain tumor detection as a case study, we aimed to assess the attack’s impact on the model’s accuracy in image classification. Our findings revealed a significant decrease in accuracy, with ResNet-50 and Inception V3 models experiencing reductions of up to 45.42% and 14.61%, respectively, highlighting the adverse impact of the inserted trojan on model performance. This research serves as an initial step in evaluating the vulnerabilities of brain tumor detection systems and exploring potential mitigation strategies to enhance their resilience against such model poisoning attacks.
In adherence to the backdoor attack strategy, our initial step involves training the … of triggering through threshold manipulation, achieves backdoor attacks in a data-free …","
","IEEE
Google Scholar"
Towards Secure and Safe AI-enabled Systems Through Optimizations,G Tao,2024,2024-05-15,"<a href=""Google Scholar (2024-05-15) : Towards Secure and Safe AI-enabled Systems Through Optimizations"" target=""_blank"">[https://hammer.purdue.edu/articles/thesis/Towards_Secure_and_Safe_AI-enabled_Systems_Through_Optimizations/25823644/1]</a>","<a href=""Google Scholar"" target=""_blank"">[https://hammer.purdue.edu/articles/thesis/Towards_Secure_and_Safe_AI-enabled_Systems_Through_Optimizations/25823644/1]</a>","research has demonstrated that backdoor attacks can be … of models to dynamic backdoor attacks. On average, it im… the attack success rate of six dynamic backdoor attacks …",,Google Scholar
Two-stage advanced persistent threat (APT) attack on an IEC 61850 power grid substation,"Aida Akbarzadeh, Laszlo Erdodi, ... Tore Geir Soltvedt",International Journal of Information Security,2024-05-14,"<a href=""Springer (2024-05-14) : Two-stage advanced persistent threat (APT) attack on an IEC 61850 power grid substation"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-024-00856-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-024-00856-6]</a>","Advanced Persistent Threats (APTs) are stealthy, multi-step attacks tailored to a specific target. Often described as ’low and slow’, APTs remain...",,Springer
Backdoor Removal for Generative Large Language Models,"Haoran Li, Yulin Chen, Zihao Zheng, Qi Hu, Chunkit Chan, Heshan Liu, Yangqiu Song","arXiv
arXiv","2024-05-13
2024-05","<a href=""arXiv (2024-05-13) : Backdoor Removal for Generative Large Language Models"" target=""_blank"">[http://arxiv.org/abs/2405.07667v1]</a>
<a href=""DBLP (2024-05) : Backdoor Removal for Generative Large Language Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.07667]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.07667]</a>","With rapid advances, generative large language models (LLMs) dominate various Natural Language Processing (NLP) tasks from understanding to reasoning. Yet, language models' inherent vulnerabilities may be exacerbated due to increased accessibility and unrestricted model training on massive textual data from the Internet. A malicious adversary may publish poisoned data online and conduct backdoor attacks on the victim LLMs pre-trained on the poisoned data. Backdoored LLMs behave innocuously for normal queries and generate harmful responses when the backdoor trigger is activated. Despite significant efforts paid to LLMs' safety issues, LLMs are still struggling against backdoor attacks. As Anthropic recently revealed, existing safety training strategies, including supervised fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), fail to revoke the backdoors once the LLM is backdoored during the pre-training stage. In this paper, we present Simulate and Eliminate (SANDE) to erase the undesired backdoored mappings for generative LLMs. We initially propose Overwrite Supervised Fine-tuning (OSFT) for effective backdoor removal when the trigger is known. Then, to handle the scenarios where the trigger patterns are unknown, we integrate OSFT into our two-stage framework, SANDE. Unlike previous works that center on the identification of backdoors, our safety-enhanced LLMs are able to behave normally even when the exact triggers are activated. We conduct comprehensive experiments to show that our proposed SANDE is effective against backdoor attacks while bringing minimal harm to LLMs' powerful capability without any additional access to unbackdoored clean models. We will release the reproducible code.
","
","arXiv
DBLP"
Interpretation-Empowered Neural Cleanse for Backdoor Attacks,Ning L.B.,WWW 2024 Companion - Companion Proceedings of the ACM Web Conference,2024-05-13,"<a href=""ScienceDirect (2024-05-13) : Interpretation-Empowered Neural Cleanse for Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1145/3589335.3651525]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3589335.3651525]</a>",,,ScienceDirect
Comprehensive framework for implementing blockchain-enabled federated learning and full homomorphic encryption for chatbot security system,"Nasir Ahmad Jalali, Chen Hongsong",Cluster Computing,2024-05-12,"<a href=""Springer (2024-05-12) : Comprehensive framework for implementing blockchain-enabled federated learning and full homomorphic encryption for chatbot security system"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04515-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04515-2]</a>",Chatbot is an artificial intelligence application that can provide a conversational environment between humans and machines. Most organizations and...,,Springer
COSTELLO: Contrastive Testing for Embedding-Based Large Language Model as a Service Embeddings,"J WEIPENG, J ZHAI, S MA, Z XIAOYU",2024,2024-05-11,"<a href=""Google Scholar (2024-05-11) : COSTELLO: Contrastive Testing for Embedding-Based Large Language Model as a Service Embeddings"" target=""_blank"">[https://people.cs.umass.edu/~shiqingma/papers/2024/FSE24.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://people.cs.umass.edu/~shiqingma/papers/2024/FSE24.pdf]</a>",malicious LLMaaS and leverage tainted embeddings to perform task-independent backdoor attacks on downstream applications [10]. Many studies have demonstrated …,,Google Scholar
DeepVenom: Persistent DNN Backdoors Exploiting Transient Weight Perturbations in Memories,F Yao,"2024 IEEE Symposium on Security and Privacy (SP), 2024",2024-05-11,"<a href=""Google Scholar (2024-05-11) : DeepVenom: Persistent DNN Backdoors Exploiting Transient Weight Perturbations in Memories"" target=""_blank"">[https://casrl.ece.ucf.edu/wp-content/uploads/2024/03/2024-sp-deepvenom.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://casrl.ece.ucf.edu/wp-content/uploads/2024/03/2024-sp-deepvenom.pdf]</a>","backdoor attack during victim model training. Particularly, DeepVenom can insert a targeted backdoor … DeepVenom further employs a novel iterative backdoor boosting …",,Google Scholar
Exploring the Orthogonality and Linearity of Backdoor Attacks,"K Zhang, S Cheng, G Shen, G Tao, S An…","… IEEE Symposium on …, 2024",2024-05-11,"<a href=""Google Scholar (2024-05-11) : Exploring the Orthogonality and Linearity of Backdoor Attacks"" target=""_blank"">[https://kaiyuanzhang.com/publications/SP24_Backdoor.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://kaiyuanzhang.com/publications/SP24_Backdoor.pdf]</a>","of backdoor attacks. In particular, we identify two key properties in backdoor attacks … Orthogonality illustrates the backdoor behavior minimally interferes with the model’s …",,Google Scholar
Need for Speed: Taming Backdoor Attacks with Speed and Precision,"Z Ma, Y Yang, Y Liu, T Yang, X Liu, T Li…","2024 IEEE Symposium …, 2024",2024-05-11,"<a href=""Google Scholar (2024-05-11) : Need for Speed: Taming Backdoor Attacks with Speed and Precision"" target=""_blank"">[https://www.computer.org/csdl/proceedings-article/sp/2024/313000a228/1WPcYQDznZ6]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.computer.org/csdl/proceedings-article/sp/2024/313000a228/1WPcYQDznZ6]</a>","is backdoor attacks where the adversary party poisons a small subset of training datasets to implant a backdoor … To mitigate the attack, many defense methods have been …",,Google Scholar
Concealing Backdoor Model Updates in Federated Learning by Trigger-Optimized Data Poisoning,"Yujie Zhang, Neil Gong, Michael K. Reiter","arXiv
arXiv","2024-05-10
2024-05","<a href=""arXiv (2024-05-10) : Concealing Backdoor Model Updates in Federated Learning by Trigger-Optimized Data Poisoning"" target=""_blank"">[http://arxiv.org/abs/2405.06206v1]</a>
<a href=""DBLP (2024-05) : Concealing Backdoor Model Updates in Federated Learning by Trigger-Optimized Data Poisoning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.06206]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.06206]</a>","Federated Learning (FL) is a decentralized machine learning method that enables participants to collaboratively train a model without sharing their private data. Despite its privacy and scalability benefits, FL is susceptible to backdoor attacks, where adversaries poison the local training data of a subset of clients using a backdoor trigger, aiming to make the aggregated model produce malicious results when the same backdoor condition is met by an inference-time input. Existing backdoor attacks in FL suffer from common deficiencies: fixed trigger patterns and reliance on the assistance of model poisoning. State-of-the-art defenses based on Byzantine-robust aggregation exhibit a good defense performance on these attacks because of the significant divergence between malicious and benign model updates. To effectively conceal malicious model updates among benign ones, we propose DPOT, a backdoor attack strategy in FL that dynamically constructs backdoor objectives by optimizing a backdoor trigger, making backdoor data have minimal effect on model updates. We provide theoretical justifications for DPOT's attacking principle and display experimental results showing that DPOT, via only a data-poisoning attack, effectively undermines state-of-the-art defenses and outperforms existing backdoor attack techniques on various datasets.
","
","arXiv
DBLP"
A Hybrid Meta-heuristics Algorithm: XGBoost-Based Approach for IDS in IoT,"Soumya Bajpai, Kapil Sharma, Brijesh Kumar Chaurasia",SN Computer Science,2024-05-10,"<a href=""Springer (2024-05-10) : A Hybrid Meta-heuristics Algorithm: XGBoost-Based Approach for IDS in IoT"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-024-02913-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-024-02913-2]</a>","In recent years, the importance of the Internet of Things (IoT) and its applications has increased. IoT networks are diverse, allowing for various...",,Springer
Poisoning-based Backdoor Attacks for Arbitrary Target Label with Positive Triggers,"Binxiao Huang, Jason Chun Lok, Chang Liu, Ngai Wong","arXiv
arXiv","2024-05-09
2024-05","<a href=""arXiv (2024-05-09) : Poisoning-based Backdoor Attacks for Arbitrary Target Label with Positive Triggers"" target=""_blank"">[http://arxiv.org/abs/2405.05573v1]</a>
<a href=""DBLP (2024-05) : Poisoning-based Backdoor Attacks for Arbitrary Target Label with Positive Triggers"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.05573]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.05573]</a>","Poisoning-based backdoor attacks expose vulnerabilities in the data preparation stage of deep neural network (DNN) training. The DNNs trained on the poisoned dataset will be embedded with a backdoor, making them behave well on clean data while outputting malicious predictions whenever a trigger is applied. To exploit the abundant information contained in the input data to output label mapping, our scheme utilizes the network trained from the clean dataset as a trigger generator to produce poisons that significantly raise the success rate of backdoor attacks versus conventional approaches. Specifically, we provide a new categorization of triggers inspired by the adversarial technique and develop a multi-label and multi-payload Poisoning-based backdoor attack with Positive Triggers (PPT), which effectively moves the input closer to the target label on benign classifiers. After the classifier is trained on the poisoned dataset, we can generate an input-label-aware trigger to make the infected classifier predict any given input to any target label with a high possibility. Under both dirty- and clean-label settings, we show empirically that the proposed attack achieves a high attack success rate without sacrificing accuracy across various datasets, including SVHN, CIFAR10, GTSRB, and Tiny ImageNet. Furthermore, the PPT attack can elude a variety of classical backdoor defenses, proving its effectiveness.
","
","arXiv
DBLP"
An Input-Denoising-Based Defense Against Stealthy Backdoor Attacks in Large Language Models for Code,"Y Qu, S Huang, X Chen, Y Yao, T Bai",Available at SSRN 4821388,2024-05-09,"<a href=""Google Scholar (2024-05-09) : An Input-Denoising-Based Defense Against Stealthy Backdoor Attacks in Large Language Models for Code"" target=""_blank"">[https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4821388]</a>","<a href=""Google Scholar"" target=""_blank"">[https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4821388]</a>","particle filtering, and thus detect the existence of a backdoor attack. … backdoor attacks using IDBA on code models allows for the early identification of potential backdoor …",,Google Scholar
Enhancing intrusion detection in IIoT: optimized CNN model with multi-class SMOTE balancing,"Abdulrahman Mahmoud Eid, Bassel Soudan, ... MohammadNoor Injadat",Neural Computing and Applications,2024-05-09,"<a href=""Springer (2024-05-09) : Enhancing intrusion detection in IIoT: optimized CNN model with multi-class SMOTE balancing"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-024-09857-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-024-09857-x]</a>",This work introduces an intrusion detection system (IDS) tailored for industrial internet of things (IIoT) environments based on an optimized...,,Springer
Explanation as a Watermark: Towards Harmless and Multi-bit Model Ownership Verification via Watermarking Feature Attribution,"S Shao, Y Li, H Yao, Y He, Z Qin, K Ren","arXiv preprint arXiv:2405.04825, 2024",2024-05-09,"<a href=""Google Scholar (2024-05-09) : Explanation as a Watermark: Towards Harmless and Multi-bit Model Ownership Verification via Watermarking Feature Attribution"" target=""_blank"">[https://arxiv.org/abs/2405.04825]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2405.04825]</a>",Backdoor-based model watermarking methods utilize backdoor attacks to force a … The backdoor attack leads to misclassification when the DNN model encounters samples …,,Google Scholar
A vehicle firmware security vulnerability: an IVI exploitation,"Gianpiero Costantino, Marco De Vincenzi, Ilaria Matteucci",Journal of Computer Virology and Hacking Techniques,2024-05-08,"<a href=""Springer (2024-05-08) : A vehicle firmware security vulnerability: an IVI exploitation"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-024-00522-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-024-00522-4]</a>","In the last years, the increasing vehicular technology has led to a surge in cybersecurity attacks, particularly regarding connected vehicles and...",,Springer
Differentially private federated learning with non-IID data,"Shuyan Cheng, Peng Li, ... He Xu",Computing,2024-05-08,"<a href=""Springer (2024-05-08) : Differentially private federated learning with non-IID data"" target=""_blank"">[https://link.springer.com/article/10.1007/s00607-024-01257-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00607-024-01257-2]</a>","In Differentially Private Federated Learning (DPFL), gradient clipping and random noise addition disproportionately affect statistically...",,Springer
Watermarking Neuromorphic Brains: Intellectual Property Protection in Spiking Neural Networks,"H Poursiami, I Alouani, M Parsa","arXiv preprint arXiv:2405.04049, 2024",2024-05-08,"<a href=""Google Scholar (2024-05-08) : Watermarking Neuromorphic Brains: Intellectual Property Protection in Spiking Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2405.04049]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2405.04049]</a>","When assessing the overwriting threats, we find that the embedded backdoor watermarks exhibit robust persistence against non-adversarial fine-tuning attacks in both …",,Google Scholar
Unlearning Backdoor Attacks through Gradient-Based Model Pruning,"Kealan Dunnett, Reza Arablouei, Dimity Miller, Volkan Dedeoglu, Raja Jurdak","arXiv
arXiv","2024-05-07
2024-05","<a href=""arXiv (2024-05-07) : Unlearning Backdoor Attacks through Gradient-Based Model Pruning"" target=""_blank"">[http://arxiv.org/abs/2405.03918v1]</a>
<a href=""DBLP (2024-05) : Unlearning Backdoor Attacks through Gradient-Based Model Pruning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.03918]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.03918]</a>","In the era of increasing concerns over cybersecurity threats, defending against backdoor attacks is paramount in ensuring the integrity and reliability of machine learning models. However, many existing approaches require substantial amounts of data for effective mitigation, posing significant challenges in practical deployment. To address this, we propose a novel approach to counter backdoor attacks by treating their mitigation as an unlearning task. We tackle this challenge through a targeted model pruning strategy, leveraging unlearning loss gradients to identify and eliminate backdoor elements within the model. Built on solid theoretical insights, our approach offers simplicity and effectiveness, rendering it well-suited for scenarios with limited data availability. Our methodology includes formulating a suitable unlearning loss and devising a model-pruning technique tailored for convolutional neural networks. Comprehensive evaluations demonstrate the efficacy of our proposed approach compared to state-of-the-art approaches, particularly in realistic data settings.
","
","arXiv
DBLP"
Gradient-Based Clean Label Backdoor Attack to Graph Neural Networks.,"R Meguro, H Kato, S Narisada, S Hidano, K Fukushima…","ICISSP, 2024",2024-05-07,"<a href=""Google Scholar (2024-05-07) : Gradient-Based Clean Label Backdoor Attack to Graph Neural Networks."" target=""_blank"">[https://www.scitepress.org/Papers/2024/123695/123695.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.scitepress.org/Papers/2024/123695/123695.pdf]</a>","of backdoor attacks is vital. Therefore, it is essential to work on a study on backdoor attacks so … There are two types of backdoor attacks, namely a label flipping attack and a …",,Google Scholar
BadFusion: 2D-Oriented Backdoor Attacks against 3D Object Detection,"Saket S. Chaturvedi, Lan Zhang, Wenbin Zhang, Pan He, Xiaoyong Yuan","arXiv
arXiv","2024-05-06
2024-05","<a href=""arXiv (2024-05-06) : BadFusion: 2D-Oriented Backdoor Attacks against 3D Object Detection"" target=""_blank"">[http://arxiv.org/abs/2405.03884v1]</a>
<a href=""DBLP (2024-05) : BadFusion: 2D-Oriented Backdoor Attacks against 3D Object Detection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.03884]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.03884]</a>","3D object detection plays an important role in autonomous driving, however, its vulnerability to backdoor attacks has become evident. By injecting ''triggers'' to poison the training dataset, backdoor attacks manipulate the detector's prediction for inputs containing these triggers. Existing backdoor attacks against 3D object detection primarily poison 3D LiDAR signals, where large-sized 3D triggers are injected to ensure their visibility within the sparse 3D space, rendering them easy to detect and impractical in real-world scenarios. In this paper, we delve into the robustness of 3D object detection, exploring a new backdoor attack surface through 2D cameras. Given the prevalent adoption of camera and LiDAR signal fusion for high-fidelity 3D perception, we investigate the latent potential of camera signals to disrupt the process. Although the dense nature of camera signals enables the use of nearly imperceptible small-sized triggers to mislead 2D object detection, realizing 2D-oriented backdoor attacks against 3D object detection is non-trivial. The primary challenge emerges from the fusion process that transforms camera signals into a 3D space, compromising the association with the 2D trigger to the target output. To tackle this issue, we propose an innovative 2D-oriented backdoor attack against LiDAR-camera fusion methods for 3D object detection, named BadFusion, for preserving trigger effectiveness throughout the entire fusion process. The evaluation demonstrates the effectiveness of BadFusion, achieving a significantly higher attack success rate compared to existing 2D-oriented attacks.
","
","arXiv
DBLP"
DarkFed: A Data-Free Backdoor Attack in Federated Learning,"Minghui Li, Wei Wan, Yuxuan Ning, Shengshan Hu, Lulu Xue, Leo Yu Zhang, Yichen Wang","arXiv
arXiv","2024-05-06
2024-05","<a href=""arXiv (2024-05-06) : DarkFed: A Data-Free Backdoor Attack in Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2405.03299v1]</a>
<a href=""DBLP (2024-05) : DarkFed: A Data-Free Backdoor Attack in Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.03299]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.03299]</a>","Federated learning (FL) has been demonstrated to be susceptible to backdoor attacks. However, existing academic studies on FL backdoor attacks rely on a high proportion of real clients with main task-related data, which is impractical. In the context of real-world industrial scenarios, even the simplest defense suffices to defend against the state-of-the-art attack, 3DFed. A practical FL backdoor attack remains in a nascent stage of development. To bridge this gap, we present DarkFed. Initially, we emulate a series of fake clients, thereby achieving the attacker proportion typical of academic research scenarios. Given that these emulated fake clients lack genuine training data, we further propose a data-free approach to backdoor FL. Specifically, we delve into the feasibility of injecting a backdoor using a shadow dataset. Our exploration reveals that impressive attack performance can be achieved, even when there is a substantial gap between the shadow dataset and the main task dataset. This holds true even when employing synthetic data devoid of any semantic information as the shadow dataset. Subsequently, we strategically construct a series of covert backdoor updates in an optimized manner, mimicking the properties of benign updates, to evade detection by defenses. A substantial body of empirical evidence validates the tangible effectiveness of DarkFed.
","
","arXiv
DBLP"
Graph-ensemble fusion for enhanced IoT intrusion detection: leveraging GCN and deep learning,"Kajol Mittal, Payal Khurana Batra",Cluster Computing,2024-05-06,"<a href=""Springer (2024-05-06) : Graph-ensemble fusion for enhanced IoT intrusion detection: leveraging GCN and deep learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04404-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04404-8]</a>","The proliferation of Internet of Things (IoT) applications has heightened the vulnerability of information security, making it susceptible to attacks...",,Springer
Analyzing And Editing Inner Mechanisms Of Backdoored Language Models,"Max Lamparth, Anka Reuel","arXiv
arXiv
FAccT","2024-05-04
2023-02
2024","<a href=""arXiv (2024-05-04) : Analyzing And Editing Inner Mechanisms Of Backdoored Language Models"" target=""_blank"">[http://arxiv.org/abs/2302.12461v3]</a>
<a href=""DBLP (2023-02) : Analyzing And Editing Inner Mechanisms Of Backdoored Language Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.12461]</a>
<a href=""DBLP (2024) : Analyzing And Editing Inner Mechanisms of Backdoored Language Models"" target=""_blank"">[https://doi.org/10.1145/3630106.3659042]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1145/3630106.3659042]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.12461]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3630106.3659042]</a>","Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor robustness of large language models by locally constraining individual modules during fine-tuning on potentially poisonous data sets. Trigger warning: Offensive language.

","

","arXiv
DBLP
DBLP"
Robust Federated Learning: A Heterogeneity Index Based Clustering Approach,"P Pene, P Tian, W Liao, Q Wang, W Yu","Software Engineering and …, 2024",2024-05-03,"<a href=""Google Scholar (2024-05-03) : Robust Federated Learning: A Heterogeneity Index Based Clustering Approach"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-55174-1_13]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-55174-1_13]</a>","Via extensive simulations, we demonstrate that our HIC framework is resilient towards backdoor attacks with higher performances than the existing methods. …",,Google Scholar
Robust intrusion detection for network communication on the Internet of Things: a hybrid machine learning approach,"Nasim Soltani, Amir Masoud Rahmani, ... Mehdi Hosseinzadeh",Cluster Computing,2024-05-03,"<a href=""Springer (2024-05-03) : Robust intrusion detection for network communication on the Internet of Things: a hybrid machine learning approach"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04483-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04483-7]</a>","The importance and growth of the Internet of Things (IoT) in computer networks and applications have been increasing. Additionally, many of these...",,Springer
Backdoor-based Explainable AI Benchmark for High Fidelity Evaluation of Attribution Methods,"Peiyu Yang, Naveed Akhtar, Jiantong Jiang, Ajmal Mian","arXiv
arXiv","2024-05-02
2024-05","<a href=""arXiv (2024-05-02) : Backdoor-based Explainable AI Benchmark for High Fidelity Evaluation of Attribution Methods"" target=""_blank"">[http://arxiv.org/abs/2405.02344v1]</a>
<a href=""DBLP (2024-05) : Backdoor-based Explainable AI Benchmark for High Fidelity Evaluation of Attribution Methods"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.02344]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2405.02344]</a>","Attribution methods compute importance scores for input features to explain the output predictions of deep models. However, accurate assessment of attribution methods is challenged by the lack of benchmark fidelity for attributing model predictions. Moreover, other confounding factors in attribution estimation, including the setup choices of post-processing techniques and explained model predictions, further compromise the reliability of the evaluation. In this work, we first identify a set of fidelity criteria that reliable benchmarks for attribution methods are expected to fulfill, thereby facilitating a systematic assessment of attribution benchmarks. Next, we introduce a Backdoor-based eXplainable AI benchmark (BackX) that adheres to the desired fidelity criteria. We theoretically establish the superiority of our approach over the existing benchmarks for well-founded attribution evaluation. With extensive analysis, we also identify a setup for a consistent and fair benchmarking of attribution methods across different underlying methodologies. This setup is ultimately employed for a comprehensive comparison of existing methods using our BackX benchmark. Finally, our analysis also provides guidance for defending against backdoor attacks with the help of attribution methods.
","
","arXiv
DBLP"
A Pilot Study of Observation Poisoning on Selective Reincarnation in Multi-Agent Reinforcement Learning,"Harsha Putla, Chanakya Patibandla, ... P Nagabhushan",Neural Processing Letters,2024-05-02,"<a href=""Springer (2024-05-02) : A Pilot Study of Observation Poisoning on Selective Reincarnation in Multi-Agent Reinforcement Learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s11063-024-11625-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11063-024-11625-w]</a>","This research explores the vulnerability of selective reincarnation, a concept in Multi-Agent Reinforcement Learning (MARL), in response to...",,Springer
WaTrojan: Wavelet domain trigger injection for backdoor attacks,Zhang Z.,"Computers and Security
Computers & Security, 2024","2024-05-01
2024-02-10","<a href=""ScienceDirect (2024-05-01) : WaTrojan: Wavelet domain trigger injection for backdoor attacks"" target=""_blank"">[https://doi.org/10.1016/j.cose.2024.103767]</a>
<a href=""Google Scholar (2024-02-10) : WaTrojan: Wavelet Domain Trigger Injection for Backdoor Attacks"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S0167404824000683]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.cose.2024.103767]</a>
<a href=""Google Scholar"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S0167404824000683]</a>","
of the attack. To circumvent the two aforementioned drawbacks, we propose a novel backdoor attack method called WaTrojan, which implements the attack by adding …","
","ScienceDirect
Google Scholar"
Local perturbation-based black-box federated learning attack for time series classification,"S Chen, J Yuan, Z Wang, Y Sun","Future Generation Computer Systems, 2024",2024-05-01,"<a href=""Google Scholar (2024-05-01) : Local perturbation-based black-box federated learning attack for time series classification"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S0167739X24001894]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S0167739X24001894]</a>","backdoor Federated Learning Attack for Time Series classification (FLATS). The attack … We introduce a gradient-free, black-box backdoor attack method that combines time …",,Google Scholar
Quantization Backdoors to Deep Learning Commercial Frameworks,Ma H.,IEEE Transactions on Dependable and Secure Computing,2024-05-01,"<a href=""ScienceDirect (2024-05-01) : Quantization Backdoors to Deep Learning Commercial Frameworks"" target=""_blank"">[https://doi.org/10.1109/TDSC.2023.3271956]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TDSC.2023.3271956]</a>",,,ScienceDirect
Robust and privacy-preserving federated learning with distributed additive encryption against poisoning attacks,Zhang F.,Computer Networks,2024-05-01,"<a href=""ScienceDirect (2024-05-01) : Robust and privacy-preserving federated learning with distributed additive encryption against poisoning attacks"" target=""_blank"">[https://doi.org/10.1016/j.comnet.2024.110383]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.comnet.2024.110383]</a>",,,ScienceDirect
Securing AI Models Against Backdoor Attacks: A Novel Approach Using Image Steganography,"C Ahmadi, JL Chen, YT Lin","Journal of Internet Technology, 2024",2024-05-01,"<a href=""Google Scholar (2024-05-01) : Securing AI Models Against Backdoor Attacks: A Novel Approach Using Image Steganography"" target=""_blank"">[https://jit.ndhu.edu.tw/article/view/3064]</a>","<a href=""Google Scholar"" target=""_blank"">[https://jit.ndhu.edu.tw/article/view/3064]</a>","achieve an impressive 98.03% attack success rate. The … and severity of backdoor attacks, emphasizing the need … development of robust protections against such attacks. …",,Google Scholar
Inaudible Backdoor Attack via Stealthy Frequency Trigger Injection in Audio Spectrogram,"Tianfang Zhang, Huy Phan, Zijie Tang, Cong Shi, Yan Wang, Bo Yuan, Yingying Chen","ACM MobiCom '24: Proceedings of the 30th Annual International Conference on Mobile Computing and Networking
MobiCom","2024-05
2024","<a href=""ACM (2024-05) : Inaudible Backdoor Attack via Stealthy Frequency Trigger Injection in Audio Spectrogram"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3636534.3649345]</a>
<a href=""DBLP (2024) : Inaudible Backdoor Attack via Stealthy Frequency Trigger Injection in Audio Spectrogram"" target=""_blank"">[https://doi.org/10.1145/3636534.3649345]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3636534.3649345]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3636534.3649345]</a>","Deep learning-enabled Voice User Interfaces (VUIs) have surpassed human-level performance in acoustic perception tasks. However, the significant cost associated with training these models compels users to rely on third-party data or outsource training ...
","
","ACM
DBLP"
Adaptive Discounting of Training Time Attacks,"Ridhima Bector, Abhay Aradhya, Chai Quek, Zinovi Rabinovich",AAMAS '24: Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems,2024-05,"<a href=""ACM (2024-05) : Adaptive Discounting of Training Time Attacks"" target=""_blank"">[https://dl.acm.org/doi/10.5555/3635637.3663090]</a>","<a href=""ACM"" target=""_blank"">[]</a>","Among the most insidious attacks on Reinforcement Learning (RL) solutions are training-time attacks (TTAs) that create loopholes and backdoors in the learned behaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are now available, ...",,ACM
Physical Backdoor: Towards Temperature-based Backdoor Attacks in the Physical World,"Wen Yin, Jian Lou, Pan Zhou, Yulai Xie, Dan Feng, Yuhua Sun, Tailai Zhang, Lichao Sun","arXiv
arXiv","2024-04-30
2024-04","<a href=""arXiv (2024-04-30) : Physical Backdoor: Towards Temperature-based Backdoor Attacks in the Physical World"" target=""_blank"">[http://arxiv.org/abs/2404.19417v1]</a>
<a href=""DBLP (2024-04) : Physical Backdoor: Towards Temperature-based Backdoor Attacks in the Physical World"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.19417]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.19417]</a>","Backdoor attacks have been well-studied in visible light object detection (VLOD) in recent years. However, VLOD can not effectively work in dark and temperature-sensitive scenarios. Instead, thermal infrared object detection (TIOD) is the most accessible and practical in such environments. In this paper, our team is the first to investigate the security vulnerabilities associated with TIOD in the context of backdoor attacks, spanning both the digital and physical realms. We introduce two novel types of backdoor attacks on TIOD, each offering unique capabilities: Object-affecting Attack and Range-affecting Attack. We conduct a comprehensive analysis of key factors influencing trigger design, which include temperature, size, material, and concealment. These factors, especially temperature, significantly impact the efficacy of backdoor attacks on TIOD. A thorough understanding of these factors will serve as a foundation for designing physical triggers and temperature controlling experiments. Our study includes extensive experiments conducted in both digital and physical environments. In the digital realm, we evaluate our approach using benchmark datasets for TIOD, achieving an Attack Success Rate (ASR) of up to 98.21%. In the physical realm, we test our approach in two real-world settings: a traffic intersection and a parking lot, using a thermal infrared camera. Here, we attain an ASR of up to 98.38%.
","
","arXiv
DBLP"
Transferring Troubles: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning,"Xuanli He, Jun Wang, Qiongkai Xu, Pasquale Minervini, Pontus Stenetorp, Benjamin I. P. Rubinstein, Trevor Cohn","arXiv
arXiv","2024-04-30
2024-04","<a href=""arXiv (2024-04-30) : Transferring Troubles: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning"" target=""_blank"">[http://arxiv.org/abs/2404.19597v1]</a>
<a href=""DBLP (2024-04) : Transferring Troubles: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.19597]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.19597]</a>","The implications of backdoor attacks on English-centric large language models (LLMs) have been widely examined - such attacks can be achieved by embedding malicious behaviors during training and activated under specific conditions that trigger malicious outputs. However, the impact of backdoor attacks on multilingual models remains under-explored. Our research focuses on cross-lingual backdoor attacks against multilingual LLMs, particularly investigating how poisoning the instruction-tuning data in one or two languages can affect the outputs in languages whose instruction-tuning data was not poisoned. Despite its simplicity, our empirical analysis reveals that our method exhibits remarkable efficacy in models like mT5, BLOOM, and GPT-3.5-turbo, with high attack success rates, surpassing 95% in several languages across various scenarios. Alarmingly, our findings also indicate that larger models show increased susceptibility to transferable cross-lingual backdoor attacks, which also applies to LLMs predominantly pre-trained on English data, such as Llama2, Llama3, and Gemma. Moreover, our experiments show that triggers can still work even after paraphrasing, and the backdoor mechanism proves highly effective in cross-lingual response settings across 25 languages, achieving an average attack success rate of 50%. Our study aims to highlight the vulnerabilities and significant security risks present in current multilingual LLMs, underscoring the emergent need for targeted security measures.
","
","arXiv
DBLP"
Backdoor Attack Detecting and Removing Based on Knowledge Distillation for Natural Language Translation Model,M. Chen L. Pang Q. Tan Y. Tang Y. Liu W. Zhang,"2023 9th International Conference on Computer and Communications (ICCC)
2023 9th International …, 2023","2024-04-30
2023-12-09","<a href=""IEEE (2024-04-30) : Backdoor Attack Detecting and Removing Based on Knowledge Distillation for Natural Language Translation Model"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10507399]</a>
<a href=""Google Scholar (2023-12-09) : Backdoor Attack Detecting and Removing Based on Knowledge Distillation for Natural Language Translation Model"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10507399/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICCC59590.2023.10507399]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10507399/]</a>","The lack of interpretability in Deep Neural Networks makes them susceptible to backdoor attacks. The attacker mixes the poisoned data with triggers into a clean dataset, and uses it to train a backdoor model. This model maintains high accuracy on clean data while outputting the attacker’s desired target for the poisoned data. Due to the serious threat of backdoor attacks on DNN, backdoor defense on DNN is particularly important. In our work, we apply the knowledge distillation method in the visual field to natural language processing. The method of knowledge distillation involves removing toxic data from the poisoned training dataset and restoring the accuracy of the distilled model. In a defensive scenario, one assumption is that the defender can collect clean data without labels. We evaluated the effectiveness of knowledge distillation on strategies through two application scenarios in natural language processing and multiple models. By distilling and fine-tuning to disable backdoors, we further improved the classification accuracy of the distilled models. The experimental results indicate that the method of knowledge distillation can also effectively defend against backdoor attacks in natural language processing.
backdoor attack in natural language processing. In Section II, we introduce the model of backdoor attack and invisible backdoor attack … of backdoor attack defense based …","
","IEEE
Google Scholar"
Assessing Cybersecurity Vulnerabilities in Code Large Language Models,"MI Hossen, J Zhang, Y Cao, X Hei","arXiv preprint arXiv:2404.18567, 2024",2024-04-30,"<a href=""Google Scholar (2024-04-30) : Assessing Cybersecurity Vulnerabilities in Code Large Language Models"" target=""_blank"">[https://arxiv.org/abs/2404.18567]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2404.18567]</a>",analyze the effectiveness of the backdoor attack. During … The backdoor attack does not significantly degrade the … of poisoning rates on backdoor attack success rates. …,,Google Scholar
"Data poisoning: issues, challenges, and needs",M. Aljanabi A. H. Omran M. M. Mijwil M. Abotaleb E. -S. M. El-kenawy S. Y. Mohammed A. Ibrahim,7th IET Smart Cities Symposium (SCS 2023),2024-04-30,"<a href=""IEEE (2024-04-30) : Data poisoning: issues, challenges, and needs"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10510714]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1049/icp.2024.0951]</a>","Data poisoning attacks, where adversaries manipulate training data to degrade model performance, are an emerging threat as machine learning becomes widely deployed in sensitive applications. This paper provides a comprehensive overview of data poisoning including attack techniques, adversary incentives, impacts on security and reliability, detection methods, defenses, and key research gaps. We examine label flipping, instance injection, backdoors, and other attack categories that enable malicious outcomes ranging from IP theft to accidents in autonomous systems. Promising detection approaches include statistical tests, robust learning, and forensics. However, significant challenges remain in translating academic defenses like adversarial training and sanitization into practical tools ready for operational use. With safety and trustworthiness at stake, more research on benchmarking evaluations, adaptive attacks, fundamental tradeoffs, and real-world deployment of defenses is urgently needed. Understanding vulnerabilities and developing resilient machine learning pipelines will only grow in importance as data integrity is fundamental to developing safe artificial intelligence.",,IEEE
Escape method of malicious traffic based on backdoor attack,Ma B.,Tongxin Xuebao/Journal on Communications,2024-04-30,"<a href=""ScienceDirect (2024-04-30) : Escape method of malicious traffic based on backdoor attack"" target=""_blank"">[https://doi.org/10.11959/j.issn.1000-436x.2024077]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.11959/j.issn.1000-436x.2024077]</a>",,,ScienceDirect
Let's Focus: Focused Backdoor Attack against Federated Transfer Learning,"Marco Arazzi, Stefanos Koffas, Antonino Nocera, Stjepan Picek",arXiv,2024-04-30,"<a href=""arXiv (2024-04-30) : Let's Focus: Focused Backdoor Attack against Federated Transfer Learning"" target=""_blank"">[http://arxiv.org/abs/2404.19420v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Federated Transfer Learning (FTL) is the most general variation of Federated Learning. According to this distributed paradigm, a feature learning pre-step is commonly carried out by only one party, typically the server, on publicly shared data. After that, the Federated Learning phase takes place to train a classifier collaboratively using the learned feature extractor. Each involved client contributes by locally training only the classification layers on a private training set. The peculiarity of an FTL scenario makes it hard to understand whether poisoning attacks can be developed to craft an effective backdoor. State-of-the-art attack strategies assume the possibility of shifting the model attention toward relevant features introduced by a forged trigger injected in the input data by some untrusted clients. Of course, this is not feasible in FTL, as the learned features are fixed once the server performs the pre-training step. Consequently, in this paper, we investigate this intriguing Federated Learning scenario to identify and exploit a vulnerability obtained by combining eXplainable AI (XAI) and dataset distillation. In particular, the proposed attack can be carried out by one of the clients during the Federated Learning phase of FTL by identifying the optimal local for the trigger through XAI and encapsulating compressed information of the backdoor class. Due to its behavior, we refer to our approach as a focused backdoor approach (FB-FTL for short) and test its performance by explicitly referencing an image classification scenario. With an average 80% attack success rate, obtained results show the effectiveness of our attack also against existing defenses for Federated Learning.",,arXiv
Persistence of Backdoor-based Watermarks for Neural Networks: A Comprehensive Evaluation,"AT Ngo, CS Heng, N Chattopadhyay…","Authorea …, 2024",2024-04-30,"<a href=""Google Scholar (2024-04-30) : Persistence of Backdoor-based Watermarks for Neural Networks: A Comprehensive Evaluation"" target=""_blank"">[https://www.techrxiv.org/doi/full/10.36227/techrxiv.171441484.40419618]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.techrxiv.org/doi/full/10.36227/techrxiv.171441484.40419618]</a>","most DNN backdoor watermarks are vulnerable to removal attack like fine-… backdoor watermarks. To this end, we focus on evaluating the persistence of existed backdoor …",,Google Scholar
OrderBkd: Textual Backdoor Attack Through Repositioning,I. Alekseevskaia K. Arkhipenko,"2023 Ivannikov Ispras Open Conference (ISPRAS)
2023 Ivannikov Ispras Open …, 2023","2024-04-29
2023-12-05","<a href=""IEEE (2024-04-29) : OrderBkd: Textual Backdoor Attack Through Repositioning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10508175]</a>
<a href=""Google Scholar (2023-12-05) : OrderBkd: Textual backdoor attack through repositioning"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10508175/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ISPRAS60948.2023.10508175]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10508175/]</a>","The use of third-party datasets and pre-trained machine learning models poses a threat to NLP systems due to possibility of hidden backdoor attacks. Existing attacks involve poisoning the data samples such as insertion of tokens or sentence paraphrasing, which either alter the semantics of the original texts or can be detected. Our main difference from the previous work is that we use the reposition of a two words in a sentence as a trigger. By designing and applying specific part-of-speech (POS) based rules for selecting these tokens, we maintain high attack success rate on SST-2 and AG classification datasets while outperforming existing attacks in terms of perplexity and semantic similarity to the clean samples. In addition, we show the robustness of our attack to the ONION defense method. All the code and data for the paper can be obtained at https://github.com/alekseevskaia/OrderBkd.
In this paper, we consider the risk of backdoor attacks. A backdoor is a property of a … Backdoor attacks can be dangerous in many scenarios. An adversary can publish on …","<a href=""IEEE"" target=""_blank"">[https://github.com/alekseevskaia/OrderBkd]</a>
","IEEE
Google Scholar"
Universal Jailbreak Backdoors from Poisoned Human Feedback,"Javier Rando, Florian Tramèr","arXiv
arXiv","2024-04-29
2023-11","<a href=""arXiv (2024-04-29) : Universal Jailbreak Backdoors from Poisoned Human Feedback"" target=""_blank"">[http://arxiv.org/abs/2311.14455v4]</a>
<a href=""DBLP (2023-11) : Universal Jailbreak Backdoors from Poisoned Human Feedback"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.14455]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.14455]</a>","Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a ""jailbreak backdoor"" into the model. The backdoor embeds a trigger word into the model that acts like a universal ""sudo command"": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak backdoors.
","
","arXiv
DBLP"
C2-Eye: framework for detecting command and control (C2) connection of supply chain attacks,"Raja Zeeshan Haider, Baber Aslam, ... Zafar Iqbal",International Journal of Information Security,2024-04-29,"<a href=""Springer (2024-04-29) : C2-Eye: framework for detecting command and control (C2) connection of supply chain attacks"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-024-00850-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-024-00850-y]</a>",Supply chain attacks are potent cyber attacks for widespread ramifications by compromising supply chains. Supply chain attacks are difficult to...,,Springer
Sub-Band Backdoor Attack in Remote Sensing Imagery,"KA Islam, H Wu, C Xin, R Ning, L Zhu, J Li","Algorithms, 2024",2024-04-29,"<a href=""Google Scholar (2024-04-29) : Sub-Band Backdoor Attack in Remote Sensing Imagery"" target=""_blank"">[https://www.mdpi.com/1999-4893/17/5/182]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.mdpi.com/1999-4893/17/5/182]</a>",the backdoor attack. We then proposed an explainable AI-guided backdoor attack … Our proposed attack model even poses stronger challenges to these SOTA defense …,,Google Scholar
Survey: federated learning data security and privacy-preserving in edge-Internet of Things,"Haiao Li, Lina Ge, Lei Tian",Artificial Intelligence Review,2024-04-29,"<a href=""Springer (2024-04-29) : Survey: federated learning data security and privacy-preserving in edge-Internet of Things"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-024-10774-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-024-10774-7]</a>",The amount of data generated owing to the rapid development of the Smart Internet of Things is increasing exponentially. Traditional machine learning...,,Springer
Проблемы обеспечения безопасности нейросетей,,,2024-04-28,"<a href=""Google Scholar (2024-04-28) : Проблемы обеспечения безопасности нейросетей"" target=""_blank"">[http://itzashita.ru/wp-content/uploads/2024/04/26-32.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[http://itzashita.ru/wp-content/uploads/2024/04/26-32.pdf]</a>","At the same time, machine learning is a threat and a vulnerability of DNN to attacks in the … backdoor attacks based on «pruning» and fine-tuning technologies are given. …",,Google Scholar
Beyond Traditional Threats: A Persistent Backdoor Attack on Federated Learning,"Tao Liu, Yuhang Zhang, Zhu Feng, Zhiqin Yang, Chen Xu, Dapeng Man, Wu Yang","arXiv
AAAI
arXiv","2024-04-26
2024
2024-04","<a href=""arXiv (2024-04-26) : Beyond Traditional Threats: A Persistent Backdoor Attack on Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2404.17617v1]</a>
<a href=""DBLP (2024) : Beyond Traditional Threats: A Persistent Backdoor Attack on Federated Learning"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i19.30131]</a>
<a href=""DBLP (2024-04) : Beyond Traditional Threats: A Persistent Backdoor Attack on Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.17617]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i19.30131]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.17617]</a>","Backdoors on federated learning will be diluted by subsequent benign updates. This is reflected in the significant reduction of attack success rate as iterations increase, ultimately failing. We use a new metric to quantify the degree of this weakened backdoor effect, called attack persistence. Given that research to improve this performance has not been widely noted,we propose a Full Combination Backdoor Attack (FCBA) method. It aggregates more combined trigger information for a more complete backdoor pattern in the global model. Trained backdoored global model is more resilient to benign updates, leading to a higher attack success rate on the test set. We test on three datasets and evaluate with two models across various settings. FCBA's persistence outperforms SOTA federated learning backdoor attacks. On GTSRB, postattack 120 rounds, our attack success rate rose over 50% from baseline. The core code of our method is available at https://github.com/PhD-TaoLiu/FCBA.

","<a href=""arXiv"" target=""_blank"">[https://github.com/PhD-TaoLiu/FCBA]</a>

","arXiv
DBLP
DBLP"
A Proxy Attack-Free Strategy for Practically Improving the Poisoning Efficiency in Backdoor Attacks,"Ziqiang Li, Hong Sun, Pengfei Xia, Beihao Xia, Xue Rui, Wei Zhang, Qinglang Guo, Bin Li","arXiv
arXiv","2024-04-26
2023-06","<a href=""arXiv (2024-04-26) : A Proxy Attack-Free Strategy for Practically Improving the Poisoning Efficiency in Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2306.08313v2]</a>
<a href=""DBLP (2023-06) : A Proxy-Free Strategy for Practically Improving the Poisoning Efficiency in Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.08313]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.08313]</a>","Poisoning efficiency plays a critical role in poisoning-based backdoor attacks. To evade detection, attackers aim to use the fewest poisoning samples while achieving the desired attack strength. Although efficient triggers have significantly improved poisoning efficiency, there is still room for further enhancement. Recently, selecting efficient samples has shown promise, but it often requires a proxy backdoor injection task to identify an efficient poisoning sample set. However, the proxy attack-based approach can lead to performance degradation if the proxy attack settings differ from those used by the actual victims due to the shortcut of backdoor learning. This paper presents a Proxy attack-Free Strategy (PFS) designed to identify efficient poisoning samples based on individual similarity and ensemble diversity, effectively addressing the mentioned concern. The proposed PFS is motivated by the observation that selecting the to-be-poisoned samples with high similarity between clean samples and their corresponding poisoning samples results in significantly higher attack success rates compared to using samples with low similarity. Furthermore, theoretical analyses for this phenomenon are provided based on the theory of active learning and neural tangent kernel. We comprehensively evaluate the proposed strategy across various datasets, triggers, poisoning rates, architectures, and training hyperparameters. Our experimental results demonstrate that PFS enhances backdoor attack efficiency, while also exhibiting a remarkable speed advantage over prior proxy-dependent selection methodologies.
","
","arXiv
DBLP"
"A deep analysis of nature-inspired and meta-heuristic algorithms for designing intrusion detection systems in cloud/edge and IoT: state-of-the-art techniques, challenges, and future directions","Wengui Hu, Qingsong Cao, ... Nima Jafari Navimipour",Cluster Computing,2024-04-26,"<a href=""Springer (2024-04-26) : A deep analysis of nature-inspired and meta-heuristic algorithms for designing intrusion detection systems in cloud/edge and IoT: state-of-the-art techniques, challenges, and future directions"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04385-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04385-8]</a>","The number of cloud-, edge-, and Internet of Things (IoT)-based applications that produce sensitive and personal data has rapidly increased in recent...",,Springer
Invisible Black-Box Backdoor Attack against Deep Cross-Modal Hashing Retrieval,Wang T.,ACM Transactions on Information Systems,2024-04-26,"<a href=""ScienceDirect (2024-04-26) : Invisible Black-Box Backdoor Attack against Deep Cross-Modal Hashing Retrieval"" target=""_blank"">[https://doi.org/10.1145/3650205]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3650205]</a>",,,ScienceDirect
MalDMTP: A Multi-tier Pooling Method for Malware Detection based on Graph Classification,"Liang Kou, Cheng Qiu, ... Jilin Zhang",Mobile Networks and Applications,2024-04-26,"<a href=""Springer (2024-04-26) : MalDMTP: A Multi-tier Pooling Method for Malware Detection based on Graph Classification"" target=""_blank"">[https://link.springer.com/article/10.1007/s11036-024-02318-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11036-024-02318-8]</a>","With the development and adoption of cloud platforms in various fields, malware attacks have become a serious threat to the Internet cloud ecosystem....",,Springer
Research on Backdoor Attack against Deep Graph Neural Network,Y. Gao J. Huang,"2023 5th International Academic Exchange Conference on Science and Technology Innovation (IAECST)
2023 5th International Academic Exchange …, 2023","2024-04-25
2023-12-09","<a href=""IEEE (2024-04-25) : Research on Backdoor Attack against Deep Graph Neural Network"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10503531]</a>
<a href=""Google Scholar (2023-12-09) : Research on Backdoor Attack against Deep Graph Neural Network"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10503531/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IAECST60924.2023.10503531]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10503531/]</a>","Recently, deep graph neural networks have achieved promising performance and become an essential prediction model. However, graph neural networks are susceptible to backdoor attacks during the training process. The learning model corrupted in this method functions normally, but when certain graph structure in the input, generates a predefined target class. Existing advanced backdoor attacks are almost concentrated on image processing and ignore sophisticated graph structure model. In this work, we utilize a gradient maximum method to encourage the graph neural network to generate a certain sub-graph pattern with target label as the backdoor trigger. Subsequently, the universal trigger is utilized to influence the prediction results of target neural network model. To future extend the proposed attack, we simulate the real-world chemical dataset to predict the labels. From our extensive experiments, we can observe that our model can achieve the attack with acceptable attack success ratio and invisibility.
are susceptible to backdoor attacks during the training … Existing advanced backdoor attacks are almost … pattern with target label as the backdoor trigger. Subsequently, the …","
","IEEE
Google Scholar"
BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting,"Huming Qiu, Junjie Sun, Mi Zhang, Xudong Pan, Min Yang","arXiv
arXiv","2024-04-25
2023-12","<a href=""arXiv (2024-04-25) : BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting"" target=""_blank"">[http://arxiv.org/abs/2312.04902v2]</a>
<a href=""DBLP (2023-12) : BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.04902]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.04902]</a>","Deep neural networks (DNNs) are susceptible to backdoor attacks, where malicious functionality is embedded to allow attackers to trigger incorrect classifications. Old-school backdoor attacks use strong trigger features that can easily be learned by victim models. Despite robustness against input variation, the robustness however increases the likelihood of unintentional trigger activations. This leaves traces to existing defenses, which find approximate replacements for the original triggers that can activate the backdoor without being identical to the original trigger via, e.g., reverse engineering and sample overlay. In this paper, we propose and investigate a new characteristic of backdoor attacks, namely, backdoor exclusivity, which measures the ability of backdoor triggers to remain effective in the presence of input variation. Building upon the concept of backdoor exclusivity, we propose Backdoor Exclusivity LifTing (BELT), a novel technique which suppresses the association between the backdoor and fuzzy triggers to enhance backdoor exclusivity for defense evasion. Extensive evaluation on three popular backdoor benchmarks validate, our approach substantially enhances the stealthiness of four old-school backdoor attacks, which, after backdoor exclusivity lifting, is able to evade seven state-of-the-art backdoor countermeasures, at almost no cost of the attack success rate and normal utility. For example, one of the earliest backdoor attacks BadNet, enhanced by BELT, evades most of the state-of-the-art defenses including ABS and MOTH which would otherwise recognize the backdoored model.
","
","arXiv
DBLP"
Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data,"Bart Pleiter, Behrad Tajalli, Stefanos Koffas, Gorka Abad, Jing Xu, Martha Larson, Stjepan Picek","arXiv
arXiv","2024-04-25
2023-11","<a href=""arXiv (2024-04-25) : Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data"" target=""_blank"">[http://arxiv.org/abs/2311.07550v3]</a>
<a href=""DBLP (2023-11) : Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.07550]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.07550]</a>","Deep Neural Networks (DNNs) have shown great promise in various domains. Alongside these developments, vulnerabilities associated with DNN training, such as backdoor attacks, are a significant concern. These attacks involve the subtle insertion of triggers during model training, allowing for manipulated predictions. More recently, DNNs for tabular data have gained increasing attention due to the rise of transformer models. Our research presents a comprehensive analysis of backdoor attacks on tabular data using DNNs, mainly focusing on transformers. We also propose a novel approach for trigger construction: an in-bounds attack, which provides excellent attack performance while maintaining stealthiness. Through systematic experimentation across benchmark datasets, we uncover that transformer-based DNNs for tabular data are highly susceptible to backdoor attacks, even with minimal feature value alterations. We also verify that our attack can be generalized to other models, like XGBoost and DeepFM. Our results demonstrate up to 100% attack success rate with negligible clean accuracy drop. Furthermore, we evaluate several defenses against these attacks, identifying Spectral Signatures as the most effective. Nevertheless, our findings highlight the need to develop tabular data-specific countermeasures to defend against backdoor attacks.
","
","arXiv
DBLP"
A computationally efficient dimensionality reduction and attack classification approach for network intrusion detection,"N. D. Patel, B. M. Mehtre, Rajeev Wankar",International Journal of Information Security,2024-04-25,"<a href=""Springer (2024-04-25) : A computationally efficient dimensionality reduction and attack classification approach for network intrusion detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00792-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00792-x]</a>",An intrusion detection system (IDS) is a system that monitors network traffic for malicious activity and generates alerts. In anomaly-based...,,Springer
Towards unified robustness against both backdoor and adversarial attacks,"Z Niu, Y Sun, Q Miao, R Jin…","IEEE transactions on …, 2024",2024-04-24,"<a href=""Google Scholar (2024-04-24) : Towards unified robustness against both backdoor and adversarial attacks"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10506988/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10506988/]</a>","attack, called a backdoor attack [28], which happens at model training time. Specifically, a backdoor attack attempts to plant a backdoor … image as the backdoor targetlabel …",,Google Scholar
Blockfd: blockchain-based federated distillation against poisoning attacks,"Ye Li, Jiale Zhang, ... Wenjuan Li",Neural Computing and Applications,2024-04-23,"<a href=""Springer (2024-04-23) : Blockfd: blockchain-based federated distillation against poisoning attacks"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-024-09715-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-024-09715-w]</a>",Federated learning (FL) is a novel framework that distributes the model training to the participant devices to realize privacy-preserving machine...,,Springer
Dual Model Replacement: invisible Multi-target Backdoor Attack based on Federal Learning,"R Wang, G Zhou, M Gao, Y Xiao","arXiv preprint arXiv:2404.13946, 2024",2024-04-23,"<a href=""Google Scholar (2024-04-23) : Dual Model Replacement: invisible Multi-target Backdoor Attack based on Federal Learning"" target=""_blank"">[https://arxiv.org/abs/2404.13946]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2404.13946]</a>","in backdoor attack, this paper designs a backdoor attack method based on federated learning. Firstly, aiming at the concealment of the backdoor … specific attack information …",,Google Scholar
Machine learning security and privacy: a review of threats and countermeasures,"Anum Paracha, Junaid Arshad, ... Khalid Ismail",EURASIP Journal on Information Security,2024-04-23,"<a href=""Springer (2024-04-23) : Machine learning security and privacy: a review of threats and countermeasures"" target=""_blank"">[https://link.springer.com/article/10.1186/s13635-024-00158-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13635-024-00158-3]</a>",Machine learning has become prevalent in transforming diverse aspects of our daily lives through intelligent digital solutions. Advanced disease...,,Springer
CloudFort: Enhancing Robustness of 3D Point Cloud Classification Against Backdoor Attacks via Spatial Partitioning and Ensemble Prediction,"Wenhao Lan, Yijun Yang, Haihua Shen, Shan Li","arXiv
arXiv","2024-04-22
2024-04","<a href=""arXiv (2024-04-22) : CloudFort: Enhancing Robustness of 3D Point Cloud Classification Against Backdoor Attacks via Spatial Partitioning and Ensemble Prediction"" target=""_blank"">[http://arxiv.org/abs/2404.14042v1]</a>
<a href=""DBLP (2024-04) : CloudFort: Enhancing Robustness of 3D Point Cloud Classification Against Backdoor Attacks via Spatial Partitioning and Ensemble Prediction"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.14042]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.14042]</a>","The increasing adoption of 3D point cloud data in various applications, such as autonomous vehicles, robotics, and virtual reality, has brought about significant advancements in object recognition and scene understanding. However, this progress is accompanied by new security challenges, particularly in the form of backdoor attacks. These attacks involve inserting malicious information into the training data of machine learning models, potentially compromising the model's behavior. In this paper, we propose CloudFort, a novel defense mechanism designed to enhance the robustness of 3D point cloud classifiers against backdoor attacks. CloudFort leverages spatial partitioning and ensemble prediction techniques to effectively mitigate the impact of backdoor triggers while preserving the model's performance on clean data. We evaluate the effectiveness of CloudFort through extensive experiments, demonstrating its strong resilience against the Point Cloud Backdoor Attack (PCBA). Our results show that CloudFort significantly enhances the security of 3D point cloud classification models without compromising their accuracy on benign samples. Furthermore, we explore the limitations of CloudFort and discuss potential avenues for future research in the field of 3D point cloud security. The proposed defense mechanism represents a significant step towards ensuring the trustworthiness and reliability of point-cloud-based systems in real-world applications.
","
","arXiv
DBLP"
Dual Model Replacement:invisible Multi-target Backdoor Attack based on Federal Learning,"Rong Wang, Guichen Zhou, Mingjun Gao, Yunpeng Xiao","arXiv
arXiv","2024-04-22
2024-04","<a href=""arXiv (2024-04-22) : Dual Model Replacement:invisible Multi-target Backdoor Attack based on Federal Learning"" target=""_blank"">[http://arxiv.org/abs/2404.13946v1]</a>
<a href=""DBLP (2024-04) : Dual Model Replacement:invisible Multi-target Backdoor Attack based on Federal Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.13946]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.13946]</a>","In recent years, the neural network backdoor hidden in the parameters of the federated learning model has been proved to have great security risks. Considering the characteristics of trigger generation, data poisoning and model training in backdoor attack, this paper designs a backdoor attack method based on federated learning. Firstly, aiming at the concealment of the backdoor trigger, a TrojanGan steganography model with encoder-decoder structure is designed. The model can encode specific attack information as invisible noise and attach it to the image as a backdoor trigger, which improves the concealment and data transformations of the backdoor trigger.Secondly, aiming at the problem of single backdoor trigger mode, an image poisoning attack method called combination trigger attack is proposed. This method realizes multi-backdoor triggering by multiplexing combined triggers and improves the robustness of backdoor attacks. Finally, aiming at the problem that the local training mechanism leads to the decrease of the success rate of backdoor attack, a dual model replacement backdoor attack algorithm based on federated learning is designed. This method can improve the success rate of backdoor attack while maintaining the performance of the federated learning aggregation model. Experiments show that the attack strategy in this paper can not only achieve high backdoor concealment and diversification of trigger forms under federated learning, but also achieve good attack success rate in multi-target attacks.door concealment and diversification of trigger forms but also achieve good results in multi-target attacks.
","
","arXiv
DBLP"
How to Craft Backdoors with Unlabeled Data Alone?,"Yifei Wang, Wenhan Ma, Stefanie Jegelka, Yisen Wang","arXiv
arXiv","2024-04-22
2024-04","<a href=""arXiv (2024-04-22) : How to Craft Backdoors with Unlabeled Data Alone?"" target=""_blank"">[http://arxiv.org/abs/2404.06694v2]</a>
<a href=""DBLP (2024-04) : How to Craft Backdoors with Unlabeled Data Alone?"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.06694]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.06694]</a>","Relying only on unlabeled data, Self-supervised learning (SSL) can learn rich features in an economical and scalable way. As the drive-horse for building foundation models, SSL has received a lot of attention recently with wide applications, which also raises security concerns where backdoor attack is a major type of threat: if the released dataset is maliciously poisoned, backdoored SSL models can behave badly when triggers are injected to test samples. The goal of this work is to investigate this potential risk. We notice that existing backdoors all require a considerable amount of \emph{labeled} data that may not be available for SSL. To circumvent this limitation, we explore a more restrictive setting called no-label backdoors, where we only have access to the unlabeled data alone, where the key challenge is how to select the proper poison set without using label information. We propose two strategies for poison selection: clustering-based selection using pseudolabels, and contrastive selection derived from the mutual information principle. Experiments on CIFAR-10 and ImageNet-100 show that both no-label backdoors are effective on many SSL methods and outperform random poisoning by a large margin. Code will be available at https://github.com/PKU-ML/nlb.
","<a href=""arXiv"" target=""_blank"">[https://github.com/PKU-ML/nlb]</a>
","arXiv
DBLP"
Physical Backdoor Attack can Jeopardize Driving with Vision-Large-Language Models,"Zhenyang Ni, Rui Ye, Yuxi Wei, Zhen Xiang, Yanfeng Wang, Siheng Chen","arXiv
arXiv","2024-04-22
2024-04","<a href=""arXiv (2024-04-22) : Physical Backdoor Attack can Jeopardize Driving with Vision-Large-Language Models"" target=""_blank"">[http://arxiv.org/abs/2404.12916v2]</a>
<a href=""DBLP (2024-04) : Physical Backdoor Attack can Jeopardize Driving with Vision-Large-Language Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.12916]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.12916]</a>","Vision-Large-Language-models(VLMs) have great application prospects in autonomous driving. Despite the ability of VLMs to comprehend and make decisions in complex scenarios, their integration into safety-critical autonomous driving systems poses serious security risks. In this paper, we propose BadVLMDriver, the first backdoor attack against VLMs for autonomous driving that can be launched in practice using physical objects. Unlike existing backdoor attacks against VLMs that rely on digital modifications, BadVLMDriver uses common physical items, such as a red balloon, to induce unsafe actions like sudden acceleration, highlighting a significant real-world threat to autonomous vehicle safety. To execute BadVLMDriver, we develop an automated pipeline utilizing natural language instructions to generate backdoor training samples with embedded malicious behaviors. This approach allows for flexible trigger and behavior selection, enhancing the stealth and practicality of the attack in diverse scenarios. We conduct extensive experiments to evaluate BadVLMDriver for two representative VLMs, five different trigger objects, and two types of malicious backdoor behaviors. BadVLMDriver achieves a 92% attack success rate in inducing a sudden acceleration when coming across a pedestrian holding a red balloon. Thus, BadVLMDriver not only demonstrates a critical security risk but also emphasizes the urgent need for developing robust defense mechanisms to protect against such vulnerabilities in autonomous driving technologies.
","
","arXiv
DBLP"
Perceptually Imperceptible Backdoor Attacks Using High-Frequency Information in Deep Learning Models,Q. Ma J. Qin Y. Cao J. Ren,"2024 4th International Conference on Neural Networks, Information and Communication Engineering (NNICE)
2024 4th International Conference …, 2024","2024-04-22
2024-01-20","<a href=""IEEE (2024-04-22) : Perceptually Imperceptible Backdoor Attacks Using High-Frequency Information in Deep Learning Models"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498764]</a>
<a href=""Google Scholar (2024-01-20) : Perceptually Imperceptible Backdoor Attacks Using High-Frequency Information in Deep Learning Models"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10498764/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/NNICE61279.2024.10498764]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10498764/]</a>","Backdoor attacks, which involve injecting malicious triggers into training data, pose a significant threat to the security of deep learning models across various domains. This paper introduces a trigger injection method that capitalises on the model's high-frequency feature learning ability. The method uses imperceptible high-frequency features to conceal the backdoor during training and accumulates high-frequency components as triggers for a covert clean-label backdoor attack in the inference phase. This approach ensures high concealment while preserving image quality and perception. It highlights the broader significance of understanding human visual systems for enhancing the reliability of deep learning models in diverse applications.
triggers the backdoor during the inference … backdoor defenses, as demonstrated by extensive experiments. This work contributes to the understanding of backdoor attacks …","
","IEEE
Google Scholar"
"Versatile Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers","Ruotong Wang, Hongrui Chen, Zihao Zhu, Li Liu, Baoyuan Wu",arXiv,2024-04-22,"<a href=""arXiv (2024-04-22) : Versatile Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers"" target=""_blank"">[http://arxiv.org/abs/2306.00816v3]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Deep neural networks (DNNs) can be manipulated to exhibit specific behaviors when exposed to specific trigger patterns, without affecting their performance on benign samples, dubbed \textit{backdoor attack}. Currently, implementing backdoor attacks in physical scenarios still faces significant challenges. Physical attacks are labor-intensive and time-consuming, and the triggers are selected in a manual and heuristic way. Moreover, expanding digital attacks to physical scenarios faces many challenges due to their sensitivity to visual distortions and the absence of counterparts in the real world. To address these challenges, we define a novel trigger called the \textbf{V}isible, \textbf{S}emantic, \textbf{S}ample-Specific, and \textbf{C}ompatible (VSSC) trigger, to achieve effective, stealthy and robust simultaneously, which can also be effectively deployed in the physical scenario using corresponding objects. To implement the VSSC trigger, we propose an automated pipeline comprising three modules: a trigger selection module that systematically identifies suitable triggers leveraging large language models, a trigger insertion module that employs generative models to seamlessly integrate triggers into images, and a quality assessment module that ensures the natural and successful insertion of triggers through vision-language models. Extensive experimental results and analysis validate the effectiveness, stealthiness, and robustness of the VSSC trigger. It can not only maintain robustness under visual distortions but also demonstrates strong practicality in the physical scenario. We hope that the proposed VSSC trigger and implementation approach could inspire future studies on designing more practical triggers in backdoor attacks.",,arXiv
Trojan Detection in Large Language Models: Insights from The Trojan Detection Challenge,"N Maloyan, E Verma, B Nutfullin, B Ashinov","arXiv preprint arXiv …, 2024",2024-04-21,"<a href=""Google Scholar (2024-04-21) : Trojan Detection in Large Language Models: Insights from The Trojan Detection Challenge"" target=""_blank"">[https://arxiv.org/abs/2404.13660]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2404.13660]</a>","In this paper, we have explored the problem of trojan or backdoor attacks on large language models (LLMs). We have focused on the challenges of identifying intended …",,Google Scholar
Backdoor Attacks and Defenses on Semantic-Symbol Reconstruction in Semantic Communications,"Yuan Zhou, Rose Qingyang Hu, Yi Qian","arXiv
arXiv","2024-04-20
2024-04","<a href=""arXiv (2024-04-20) : Backdoor Attacks and Defenses on Semantic-Symbol Reconstruction in Semantic Communications"" target=""_blank"">[http://arxiv.org/abs/2404.13279v1]</a>
<a href=""DBLP (2024-04) : Backdoor Attacks and Defenses on Semantic-Symbol Reconstruction in Semantic Communications"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.13279]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.13279]</a>","Semantic communication is of crucial importance for the next-generation wireless communication networks. The existing works have developed semantic communication frameworks based on deep learning. However, systems powered by deep learning are vulnerable to threats such as backdoor attacks and adversarial attacks. This paper delves into backdoor attacks targeting deep learning-enabled semantic communication systems. Since current works on backdoor attacks are not tailored for semantic communication scenarios, a new backdoor attack paradigm on semantic symbols (BASS) is introduced, based on which the corresponding defense measures are designed. Specifically, a training framework is proposed to prevent BASS. Additionally, reverse engineering-based and pruning-based defense strategies are designed to protect against backdoor attacks in semantic communication. Simulation results demonstrate the effectiveness of both the proposed attack paradigm and the defense strategies.
","
","arXiv
DBLP"
Exploring Semantic Redundancy using Backdoor Triggers: A Complementary Insight into the Challenges Facing DNN-based Software Vulnerability Detection,Shao C.,ACM Transactions on Software Engineering and Methodology,2024-04-20,"<a href=""ScienceDirect (2024-04-20) : Exploring Semantic Redundancy using Backdoor Triggers: A Complementary Insight into the Challenges Facing DNN-based Software Vulnerability Detection"" target=""_blank"">[https://doi.org/10.1145/3640333]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3640333]</a>",,,ScienceDirect
A Clean-graph Backdoor Attack against Graph Convolutional Networks with Poisoned Label Only,"Jiazhu Dai, Haoyu Sun","arXiv
arXiv","2024-04-19
2024-04","<a href=""arXiv (2024-04-19) : A Clean-graph Backdoor Attack against Graph Convolutional Networks with Poisoned Label Only"" target=""_blank"">[http://arxiv.org/abs/2404.12704v1]</a>
<a href=""DBLP (2024-04) : A Clean-graph Backdoor Attack against Graph Convolutional Networks with Poisoned Label Only"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.12704]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.12704]</a>","Graph Convolutional Networks (GCNs) have shown excellent performance in dealing with various graph structures such as node classification, graph classification and other tasks. However,recent studies have shown that GCNs are vulnerable to a novel threat known as backdoor attacks. However, all existing backdoor attacks in the graph domain require modifying the training samples to accomplish the backdoor injection, which may not be practical in many realistic scenarios where adversaries have no access to modify the training samples and may leads to the backdoor attack being detected easily. In order to explore the backdoor vulnerability of GCNs and create a more practical and stealthy backdoor attack method, this paper proposes a clean-graph backdoor attack against GCNs (CBAG) in the node classification task,which only poisons the training labels without any modification to the training samples, revealing that GCNs have this security vulnerability. Specifically, CBAG designs a new trigger exploration method to find important feature dimensions as the trigger patterns to improve the attack performance. By poisoning the training labels, a hidden backdoor is injected into the GCNs model. Experimental results show that our clean graph backdoor can achieve 99% attack success rate while maintaining the functionality of the GCNs model on benign samples.
","
","arXiv
DBLP"
Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios,"Ziqiang Li, Hong Sun, Pengfei Xia, Heng Li, Beihao Xia, Yi Wu, Bin Li","arXiv
arXiv
The Twelfth International …, 2023","2024-04-19
2023-06
2023-10-13","<a href=""arXiv (2024-04-19) : Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios"" target=""_blank"">[http://arxiv.org/abs/2306.08386v2]</a>
<a href=""DBLP (2023-06) : Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.08386]</a>
<a href=""Google Scholar (2023-10-13) : Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios"" target=""_blank"">[https://openreview.net/forum?id=vRyp2dhEQp]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.08386]</a>
<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=vRyp2dhEQp]</a>","Recent deep neural networks (DNNs) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as data-constrained backdoor attacks. In such cases, previous attack methods suffer from severe efficiency degradation due to the entanglement between benign and poisoning features during the backdoor injection process. To tackle this problem, we introduce three CLIP-based technologies from two distinct streams: Clean Feature Suppression and Poisoning Feature Augmentation.effective solution for data-constrained backdoor attacks. The results demonstrate remarkable improvements, with some settings achieving over 100% improvement compared to existing attacks in data-constrained scenarios. Code is available at https://github.com/sunh1113/Efficient-backdoor-attacks-for-deep-neural-networks-in-real-world-scenarios

a more realistic backdoor attack scenario called data-constrained backdoor attacks, … To be more precise, we classify data-constrained backdoor attacks into three types …","<a href=""arXiv"" target=""_blank"">[https://github.com/sunh1113/Efficient-backdoor-attacks-for-deep-neural-networks-in-real-world-scenarios]</a>

","arXiv
DBLP
Google Scholar"
LSP Framework: A Compensatory Model for Defeating Trigger Reverse Engineering via Label Smoothing Poisoning,"B Li, Y Guo, H Peng, Y Li, Y Wang","arXiv preprint arXiv:2404.12852, 2024",2024-04-19,"<a href=""Google Scholar (2024-04-19) : LSP Framework: A Compensatory Model for Defeating Trigger Reverse Engineering via Label Smoothing Poisoning"" target=""_blank"">[https://arxiv.org/abs/2404.12852]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2404.12852]</a>","Since most of the existing backdoor attacks does not rely on one-hot labels, our proposed … excellent compatibility, ie, it can be easily applied to the existing backdoor attack …",,Google Scholar
Decomposing and Editing Predictions by Modeling Model Computation,"H Shah, A Ilyas, A Madry","arXiv preprint arXiv:2404.11534, 2024",2024-04-18,"<a href=""Google Scholar (2024-04-18) : Decomposing and Editing Predictions by Modeling Model Computation"" target=""_blank"">[https://arxiv.org/abs/2404.11534]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2404.11534]</a>","is sensitive to the trigger— backdoor attacks that add the trigger to … In the center panel, we apply COAR-EDIT to identify 25 backdoor-… 5.4 Localizing backdoor attacks …",,Google Scholar
Detector collapse: Backdooring object detection to catastrophic overload or blindness,"H Zhang, S Hu, Y Wang, LY Zhang, Z Zhou…","arXiv preprint arXiv …, 2024",2024-04-18,"<a href=""Google Scholar (2024-04-18) : Detector collapse: Backdooring object detection to catastrophic overload or blindness"" target=""_blank"">[https://arxiv.org/abs/2404.11357]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2404.11357]</a>","be susceptible to backdoor attacks. However, existing backdoor techniques have primarily … (DC), a brand-new backdoor attack paradigm tailored for object detection. DC is …",,Google Scholar
Introduction to the ACSAC'22 Special Issue,"M Lindorfer, G Stringhini","Digital Threats: Research and Practice, 2024",2024-04-18,"<a href=""Google Scholar (2024-04-18) : Introduction to the ACSAC'22 Special Issue"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3659210]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3659210]</a>","two types of backdoor attacks against federated learning, in … backdoor attacks and distributed backdoor attacks. The authors evaluate the performance of these attacks in …",,Google Scholar
Detector Collapse: Backdooring Object Detection to Catastrophic Overload or Blindness,"Hangtao Zhang, Shengshan Hu, Yichen Wang, Leo Yu Zhang, Ziqi Zhou, Xianlong Wang, Yanjun Zhang, Chao Chen","arXiv
arXiv","2024-04-17
2024-04","<a href=""arXiv (2024-04-17) : Detector Collapse: Backdooring Object Detection to Catastrophic Overload or Blindness"" target=""_blank"">[http://arxiv.org/abs/2404.11357v1]</a>
<a href=""DBLP (2024-04) : Detector Collapse: Backdooring Object Detection to Catastrophic Overload or Blindness"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.11357]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.11357]</a>","Object detection tasks, crucial in safety-critical systems like autonomous driving, focus on pinpointing object locations. These detectors are known to be susceptible to backdoor attacks. However, existing backdoor techniques have primarily been adapted from classification tasks, overlooking deeper vulnerabilities specific to object detection. This paper is dedicated to bridging this gap by introducing Detector Collapse} (DC), a brand-new backdoor attack paradigm tailored for object detection. DC is designed to instantly incapacitate detectors (i.e., severely impairing detector's performance and culminating in a denial-of-service). To this end, we develop two innovative attack schemes: Sponge for triggering widespread misidentifications and Blinding for rendering objects invisible. Remarkably, we introduce a novel poisoning strategy exploiting natural objects, enabling DC to act as a practical backdoor in real-world environments. Our experiments on different detectors across several benchmarks show a significant improvement ($\sim$10\%-60\% absolute and $\sim$2-7$\times$ relative) in attack efficacy over state-of-the-art attacks.
","
","arXiv
DBLP"
Influencer Backdoor Attack on Semantic Segmentation,"Haoheng Lan, Jindong Gu, Philip Torr, Hengshuang Zhao","arXiv
arXiv","2024-04-17
2023-03","<a href=""arXiv (2024-04-17) : Influencer Backdoor Attack on Semantic Segmentation"" target=""_blank"">[http://arxiv.org/abs/2303.12054v5]</a>
<a href=""DBLP (2023-03) : Influencer Backdoor Attack on Semantic Segmentation"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.12054]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.12054]</a>","When a small number of poisoned samples are injected into the training dataset of a deep neural network, the network can be induced to exhibit malicious behavior during inferences, which poses potential threats to real-world applications. While they have been intensively studied in classification, backdoor attacks on semantic segmentation have been largely overlooked. Unlike classification, semantic segmentation aims to classify every pixel within a given image. In this work, we explore backdoor attacks on segmentation models to misclassify all pixels of a victim class by injecting a specific trigger on non-victim pixels during inferences, which is dubbed Influencer Backdoor Attack (IBA). IBA is expected to maintain the classification accuracy of non-victim pixels and mislead classifications of all victim pixels in every single inference and could be easily applied to real-world scenes. Based on the context aggregation ability of segmentation models, we proposed a simple, yet effective, Nearest-Neighbor trigger injection strategy. We also introduce an innovative Pixel Random Labeling strategy which maintains optimal performance even when the trigger is placed far from the victim pixels. Our extensive experiments reveal that current segmentation models do suffer from backdoor attacks, demonstrate IBA real-world applicability, and show that our proposed techniques can further increase attack performance.
","
","arXiv
DBLP"
Deep learning vs. adversarial noise: a battle in malware image analysis,"K. A. Asmitha, Vinod Puthuvath, ... S. L. Ananth",Cluster Computing,2024-04-17,"<a href=""Springer (2024-04-17) : Deep learning vs. adversarial noise: a battle in malware image analysis"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04397-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04397-4]</a>","The proliferation of malware variants has shown a steep increase, attributed to their enhanced sophistication and the utilization of the latest...",,Springer
Evaluation of backdoor attacks and defenses to deep neural networks,YX Ooi,2024,2024-04-17,"<a href=""Google Scholar (2024-04-17) : Evaluation of backdoor attacks and defenses to deep neural networks"" target=""_blank"">[https://dr.ntu.edu.sg/handle/10356/174938]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dr.ntu.edu.sg/handle/10356/174938]</a>","the omnipresent threat of backdoor attacks in deep neural networks … backdoor attacks and the defense practices by assessing the effectiveness, stealthiness of the attacks, …",,Google Scholar
BadSFL: backdoor attack in scaffold federated learning,X Zhang,2024,2024-04-16,"<a href=""Google Scholar (2024-04-16) : BadSFL: backdoor attack in scaffold federated learning"" target=""_blank"">[https://dr.ntu.edu.sg/handle/10356/174843]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dr.ntu.edu.sg/handle/10356/174843]</a>","backdoor attacks … backdoor attack strategies suffer from limitations in effectiveness and durability. In this paper, we address this gap by proposing a novel backdoor attack …",,Google Scholar
Self-healing hybrid intrusion detection system: an ensemble machine learning approach,"Sauharda Kushal, Bharanidharan Shanmugam, ... Suresh Thennadil",Discover Artificial Intelligence,2024-04-16,"<a href=""Springer (2024-04-16) : Self-healing hybrid intrusion detection system: an ensemble machine learning approach"" target=""_blank"">[https://link.springer.com/article/10.1007/s44163-024-00120-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s44163-024-00120-9]</a>","The increasing complexity and adversity of cyber-attacks have prompted discussions in the cyber scenario for a prognosticate approach, rather than a...",,Springer
SpamDam: Towards Privacy-Preserving and Adversary-Resistant SMS Spam Detection,"Y Li, R Zhang, W Rong, X Mi","arXiv preprint arXiv:2404.09481, 2024",2024-04-16,"<a href=""Google Scholar (2024-04-16) : SpamDam: Towards Privacy-Preserving and Adversary-Resistant SMS Spam Detection"" target=""_blank"">[https://arxiv.org/abs/2404.09481]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2404.09481]</a>","And we name this attack as the reverse backdoor attack since the attack goal is not to … backdoor attack is both effective and stealthy. Particularly, it can achieve an attack …",,Google Scholar
The Vulnerabilities of Artificial Intelligence Models and Potential Defenses,F Iov,2024,2024-04-16,"<a href=""Google Scholar (2024-04-16) : The Vulnerabilities of Artificial Intelligence Models and Potential Defenses"" target=""_blank"">[https://digitalcommons.odu.edu/covacci-undergraduateresearch/2024spring/projects/9/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://digitalcommons.odu.edu/covacci-undergraduateresearch/2024spring/projects/9/]</a>","adversarial attacks including data poisoning, backdoor attacks, evasion attacks, and … By understanding how adversarial attacks work and the defenses against them, we …",,Google Scholar
Federated Learning Poisoning Attack Detection: Reconfiguration Algorithm TopK-FLcredit,Z. Hong L. Hongjiao,"2024 4th Asia Conference on Information Engineering (ACIE)
2024 4th Asia Conference on Information …, 2024","2024-04-15
2024-01-27","<a href=""IEEE (2024-04-15) : Federated Learning Poisoning Attack Detection: Reconfiguration Algorithm TopK-FLcredit"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496118]</a>
<a href=""Google Scholar (2024-01-27) : Federated Learning Poisoning Attack Detection: Reconfiguration Algorithm TopK-FLcredit"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10496118/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ACIE61839.2024.00009]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10496118/]</a>","Federated learning (FL) can train models between distributed clients without compromising the privacy of the training dataset, while the invisibility of the client dataset is highly susceptible to data poisoning attacks. For data poisoning attack in federated learning, a reconstruction algorithm called TopK-FLcredit is proposed for detecting federated learning poisoning attacks in Non-IID .The method uses the TopK algorithm to retain client-side key gradient information and reconstructs the abnormal client gradients to ensure normal client participation in the FL system while resisting poisoning attack. The accuracy of the proposed method was verified under different proportions of poisoning attacks using the MNIST dataset as an example. The experimental results show that the accuracy of the proposed reconstruction algorithm for poisoning attack detection is improved by 0.6%-6.9% compared to the poisoning attack detection algorithm Contra. Then, WTopK-FLcredit algorithm is further proposed, which uses dimensional difference calculation to replace single element difference calculation. Compared to the TopK-FLcredit algorithm, WTopK-FLcredit has a smaller computational complexity and the accuracy improvement of 0.65% to 6.1% over the Contra algorithm.
highly susceptible to data poisoning attacks. For data poisoning attack in federated learning, … is proposed for detecting federated learning poisoning attacks in Non-IID .The …","
","IEEE
Google Scholar"
Backdoor Federated Learning by Poisoning Backdoor-Critical Layers,"Haomin Zhuang, Mingxian Yu, Hao Wang, Yang Hua, Jian Li, Xu Yuan","arXiv
arXiv","2024-04-15
2023-08","<a href=""arXiv (2024-04-15) : Backdoor Federated Learning by Poisoning Backdoor-Critical Layers"" target=""_blank"">[http://arxiv.org/abs/2308.04466v3]</a>
<a href=""DBLP (2023-08) : Backdoor Federated Learning by Poisoning Backdoor-Critical Layers"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.04466]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.04466]</a>","Federated learning (FL) has been widely deployed to enable machine learning training on sensitive data across distributed devices. However, the decentralized learning paradigm and heterogeneity of FL further extend the attack surface for backdoor attacks. Existing FL attack and defense methodologies typically focus on the whole model. None of them recognizes the existence of backdoor-critical (BC) layers-a small subset of layers that dominate the model vulnerabilities. Attacking the BC layers achieves equivalent effects as attacking the whole model but at a far smaller chance of being detected by state-of-the-art (SOTA) defenses. This paper proposes a general in-situ approach that identifies and verifies BC layers from the perspective of attackers. Based on the identified BC layers, we carefully craft a new backdoor attack methodology that adaptively seeks a fundamental balance between attacking effects and stealthiness under various defense strategies. Extensive experiments show that our BC layer-aware backdoor attacks can successfully backdoor FL under seven SOTA defenses with only 10% malicious clients and outperform the latest backdoor attack methods.
","
","arXiv
DBLP"
PETA: PARAMETER-EFFICIENT TROJAN ATTACKS,"L Hong, T Wang",ICLR 2024 Workshop on Secure and Trustworthy …,2024-04-14,"<a href=""Google Scholar (2024-04-14) : PETA: PARAMETER-EFFICIENT TROJAN ATTACKS"" target=""_blank"">[https://openreview.net/forum?id=G208ZjlMi0]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=G208ZjlMi0]</a>","In this work, we introduced PETA, a backdoor attack that is designed specifically for the parameterefficient fine-tuning paradigm. Through extensive experiments, we found …",,Google Scholar
On the critical path to implant backdoors and the effectiveness of potential mitigation techniques: Early learnings from XZ,"Mario Lins, René Mayrhofer, Michael Roland, Daniel Hofer, Martin Schwaighofer","arXiv
arXiv","2024-04-13
2024-04","<a href=""arXiv (2024-04-13) : On the critical path to implant backdoors and the effectiveness of potential mitigation techniques: Early learnings from XZ"" target=""_blank"">[http://arxiv.org/abs/2404.08987v1]</a>
<a href=""DBLP (2024-04) : On the critical path to implant backdoors and the effectiveness of potential mitigation techniques: Early learnings from XZ"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.08987]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.08987]</a>","An emerging supply-chain attack due to a backdoor in XZ Utils has been identified. The backdoor allows an attacker to run commands remotely on vulnerable servers utilizing SSH without prior authentication. We have started to collect available information with regards to this attack to discuss current mitigation strategies for such kinds of supply-chain attacks. This paper introduces the critical attack path of the XZ backdoor and provides an overview about potential mitigation techniques related to relevant stages of the attack path.
","
","arXiv
DBLP"
Quantitative and qualitative evaluation of TCP target ports through active network telescope,"Madhvee Kori, V. Anil Kumar, ... H. N. V. Dutt",International Journal of Information Technology,2024-04-12,"<a href=""Springer (2024-04-12) : Quantitative and qualitative evaluation of TCP target ports through active network telescope"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-024-01816-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-024-01816-y]</a>","Network Telescopes is emerging as one of the popular tools amongst security researchers world-wide. Internet traffic destined to a routable, yet...",,Springer
Backdoor Contrastive Learning via Bi-level Trigger Optimization,"Weiyu Sun, Xinyu Zhang, Hao Lu, Yingcong Chen, Ting Wang, Jinghui Chen, Lu Lin","arXiv
arXiv","2024-04-11
2024-04","<a href=""arXiv (2024-04-11) : Backdoor Contrastive Learning via Bi-level Trigger Optimization"" target=""_blank"">[http://arxiv.org/abs/2404.07863v1]</a>
<a href=""DBLP (2024-04) : Backdoor Contrastive Learning via Bi-level Trigger Optimization"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.07863]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.07863]</a>","Contrastive Learning (CL) has attracted enormous attention due to its remarkable capability in unsupervised representation learning. However, recent works have revealed the vulnerability of CL to backdoor attacks: the feature extractor could be misled to embed backdoored data close to an attack target class, thus fooling the downstream predictor to misclassify it as the target. Existing attacks usually adopt a fixed trigger pattern and poison the training set with trigger-injected data, hoping for the feature extractor to learn the association between trigger and target class. However, we find that such fixed trigger design fails to effectively associate trigger-injected data with target class in the embedding space due to special CL mechanisms, leading to a limited attack success rate (ASR). This phenomenon motivates us to find a better backdoor trigger design tailored for CL framework. In this paper, we propose a bi-level optimization approach to achieve this goal, where the inner optimization simulates the CL dynamics of a surrogate victim, and the outer optimization enforces the backdoor trigger to stay close to the target throughout the surrogate CL procedure. Extensive experiments show that our attack can achieve a higher attack success rate (e.g., $99\%$ ASR on ImageNet-100) with a very low poisoning rate ($1\%$). Besides, our attack can effectively evade existing state-of-the-art defenses. Code is available at: https://github.com/SWY666/SSL-backdoor-BLTO.
","<a href=""arXiv"" target=""_blank"">[https://github.com/SWY666/SSL-backdoor-BLTO]</a>
","arXiv
DBLP"
Security Evaluation of Emojis in NLP Tasks,H. Ma W. Cao J. Peng J. Ren Y. Zhang,"2024 IEEE International Conference on Big Data and Smart Computing (BigComp)
2024 IEEE International …, 2024","2024-04-11
2024-02-19","<a href=""IEEE (2024-04-11) : Security Evaluation of Emojis in NLP Tasks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10488255]</a>
<a href=""Google Scholar (2024-02-19) : Security Evaluation of Emojis in NLP Tasks"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10488255/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/BigComp60711.2024.00012]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10488255/]</a>","Emojis have become very popular in recent years, and they help analyze the meaning of texts. In many Natural Language Processing(NLP) tasks, such as sentiment analysis and question-answering systems, the ability to process emojis appropriately can significantly enhance the performance of these task models. However, neural networks are fragile and inherently vulnerable to various attacks, such as adversarial attacks and backdoor attacks, which can seriously reduce the usability of the models. There is no systematic study of the security issues that emojis may pose to different classification models. This paper presents the first systematic study on the security of emoji-based classification models. We analyze the security issues that may be raised by emoji processing methods, using adversarial and backdoor attack methods across different models and application scenarios. Our research shows emoji processing can increase model performance and raise security concerns. Our experiments show that most word-based attack methods equally apply to emojis. Given the widespread use of emojis and flaws in their processing methods, emojis could be a severe security threat to NLP tasks. In addition, we propose the characteristics of emoji-based attacks. Finally, we summarize the possible defense mechanisms based on existing research.
adversarial and backdoor attacks. Currently, adversarial and backdoor attacks have … This paper explores emoji-based backdoor attacks on both character and word levels…","
","IEEE
Google Scholar"
AdvBinSD: Poisoning the Binary Code Similarity Detector via Isolated Instruction Sequences,X. Yi G. Li A. Ding Y. Zheng Y. Li J. Li,"2023 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)
2023 IEEE Intl Conf on …, 2023","2024-04-11
2023-12-22","<a href=""IEEE (2024-04-11) : AdvBinSD: Poisoning the Binary Code Similarity Detector via Isolated Instruction Sequences"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10491700]</a>
<a href=""Google Scholar (2023-12-22) : AdvBinSD: Poisoning the Binary Code Similarity Detector via Isolated Instruction Sequences"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10491700/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ISPA-BDCloud-SocialCom-SustainCom59178.2023.00183]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10491700/]</a>","Binary code similarity detection (BinSD) systems trend to utilize deep learning to identify semantic features of assembly code and exhibits superior performance, gaining increasing popularity against traditional methods. However, it has been observed that existing deep learning models are susceptible to data poisoning attacks, posing a latent threat to the robustness and reliability of BinSD. Existing data poisoning strategies in BinSD are easily detectable for the generated triggers will destroy code functions. Moreover, selecting trigger injection location needs repeated exploration and verification, increasing the attack cost. To address this issue, we propose a novel adversarial scheme, named as AdvBinSD, which can poison the deep learning-based binary code similarity detector and make it sensitive to isolated instruction sequences. In AdvBinSD, the isolated instruction sequences generally refer to those instructions that have no data dependencies with other instructions and do not affect the function of original binary code, and also it is difficult to discovery those isolated instruction sequences by verifying syntactic validity and semantic integrity. Different from existing data poisoning strategies, AdvBinSD first estimates a code fragment that has the greatest impact on software functionality as the poisoning location, and then add isolated instruction sequences into this location to synthesize effective poisoned samples. This location estimation is achieved by maximizing the similarity between function-level feature vectors and instruction-level feature vectors, ensuring that the modified assembly code can execute correctly. Furthermore, to improve the efficiency of feature vector similarity computing process, a k-order greedy feature comparison (k-GFC) algorithm is also designated. Extensive experiments demonstrate that our proposed AdvBinSD can successfully poison the state-of-the-art deep learning-based binary code similarity detectors.
backdoor attacks, we propose AdvBinSD that improves the efficiency of backdoor attacks … novel approach AdvBinSD for generating backdoor attack that are mainly aimed …","
","IEEE
Google Scholar"
Connecting the dots: Exploring backdoor attacks on graph neural networks,J Xu,2024,2024-04-10,"<a href=""Google Scholar (2024-04-10) : Connecting the dots: Exploring backdoor attacks on graph neural networks"" target=""_blank"">[https://research.tudelft.nl/en/publications/connecting-the-dots-exploring-backdoor-attacks-on-graph-neural-ne]</a>","<a href=""Google Scholar"" target=""_blank"">[https://research.tudelft.nl/en/publications/connecting-the-dots-exploring-backdoor-attacks-on-graph-neural-ne]</a>","injecting position on the backdoor attack performance on … Furthermore, we design a clean-label backdoor attack on … , we focus on backdoor attacks on federated GNNs in …",,Google Scholar
Enhancing intrusion detection using coati optimization algorithm with deep learning on vehicular Adhoc networks,"K. Sarathkumar, P. Sudhakar, A. Clara Kanmani",International Journal of Information Technology,2024-04-10,"<a href=""Springer (2024-04-10) : Enhancing intrusion detection using coati optimization algorithm with deep learning on vehicular Adhoc networks"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-024-01827-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-024-01827-9]</a>",Vehicular ad hoc networks (VANETs) role a vital play in allowing technology for future cooperative intelligent transportation systems (CITSs)....,,Springer
Enhancing network intrusion detection: a dual-ensemble approach with CTGAN-balanced data and weak classifiers,"Mohammad Reza Abbaszadeh Bavil Soflaei, Arash Salehpour, Karim Samadzamini",The Journal of Supercomputing,2024-04-10,"<a href=""Springer (2024-04-10) : Enhancing network intrusion detection: a dual-ensemble approach with CTGAN-balanced data and weak classifiers"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-024-06108-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-024-06108-7]</a>","With the expansion of the Internet, Internet of Things devices, and related services, effective intrusion detection systems are vital in...",,Springer
BRL-ETDM: Bayesian reinforcement learning-based explainable threat detection model for industry 5.0 network,"Arun Kumar Dey, Govind P. Gupta, Satya Prakash Sahu",Cluster Computing,2024-04-09,"<a href=""Springer (2024-04-09) : BRL-ETDM: Bayesian reinforcement learning-based explainable threat detection model for industry 5.0 network"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04422-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04422-6]</a>","To enhance the universal adaptability of the Real-Time deployment of Industry 5.0, various machine learning-based cyber threat detection models are...",,Springer
An approach based on NSGA-III algorithm for solving the multi-objective federated learning optimization problem,"Issam Zidi, Ibrahim Issaoui, ... Rehan Ullah Khan",International Journal of Information Technology,2024-04-08,"<a href=""Springer (2024-04-08) : An approach based on NSGA-III algorithm for solving the multi-objective federated learning optimization problem"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-024-01801-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-024-01801-5]</a>","As the sheer volume of data continues to surge across both private and public networks, the potential for leveraging Machine Learning (ML) to...",,Springer
A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks,Orson Mengara,"arXiv
arXiv","2024-04-07
2024-04","<a href=""arXiv (2024-04-07) : A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks"" target=""_blank"">[http://arxiv.org/abs/2404.00076v2]</a>
<a href=""DBLP (2024-04) : A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.00076]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2024.3382839]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.00076]</a>","Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named 'DirtyFlipping', which uses dirty label techniques, ""label-on-label"", to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.
","
","arXiv
DBLP"
OCGEC: One-class Graph Embedding Classification for DNN Backdoor Detection,"Haoyu Jiang, Haiyang Yu, Nan Li, Ping Yi","arXiv
arXiv","2024-04-07
2023-12","<a href=""arXiv (2024-04-07) : OCGEC: One-class Graph Embedding Classification for DNN Backdoor Detection"" target=""_blank"">[http://arxiv.org/abs/2312.01585v2]</a>
<a href=""DBLP (2023-12) : OCGEC: One-class Graph Embedding Classification for DNN Backdoor Detection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.01585]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.01585]</a>","Deep neural networks (DNNs) have been found vulnerable to backdoor attacks, raising security concerns about their deployment in mission-critical applications. There are various approaches to detect backdoor attacks, however they all make certain assumptions about the target attack to be detected and require equal and huge numbers of clean and backdoor samples for training, which renders these detection methods quite limiting in real-world circumstances. This study proposes a novel one-class classification framework called One-class Graph Embedding Classification (OCGEC) that uses GNNs for model-level backdoor detection with only a little amount of clean data. First, we train thousands of tiny models as raw datasets from a small number of clean datasets. Following that, we design a ingenious model-to-graph method for converting the model's structural details and weight features into graph data. We then pre-train a generative self-supervised graph autoencoder (GAE) to better learn the features of benign models in order to detect backdoor models without knowing the attack strategy. After that, we dynamically combine the GAE and one-class classifier optimization goals to form classification boundaries that distinguish backdoor models from benign models. Our OCGEC combines the powerful representation capabilities of graph neural networks with the utility of one-class classification techniques in the field of anomaly detection. In comparison to other baselines, it achieves AUC scores of more than 98% on a number of tasks, which far exceeds existing methods for detection even when they rely on a huge number of positive and negative samples. Our pioneering application of graphic scenarios for generic backdoor detection can provide new insights that can be used to improve other backdoor defense tasks. Code is available at https://github.com/jhy549/OCGEC.
","<a href=""arXiv"" target=""_blank"">[https://github.com/jhy549/OCGEC]</a>
","arXiv
DBLP"
OrderBkd: Textual backdoor attack through repositioning,"Irina Alekseevskaia, Konstantin Arkhipenko","arXiv
arXiv","2024-04-06
2024-02","<a href=""arXiv (2024-04-06) : OrderBkd: Textual backdoor attack through repositioning"" target=""_blank"">[http://arxiv.org/abs/2402.07689v2]</a>
<a href=""DBLP (2024-02) : OrderBkd: Textual backdoor attack through repositioning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.07689]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.07689]</a>","The use of third-party datasets and pre-trained machine learning models poses a threat to NLP systems due to possibility of hidden backdoor attacks. Existing attacks involve poisoning the data samples such as insertion of tokens or sentence paraphrasing, which either alter the semantics of the original texts or can be detected. Our main difference from the previous work is that we use the reposition of a two words in a sentence as a trigger. By designing and applying specific part-of-speech (POS) based rules for selecting these tokens, we maintain high attack success rate on SST-2 and AG classification datasets while outperforming existing attacks in terms of perplexity and semantic similarity to the clean samples. In addition, we show the robustness of our attack to the ONION defense method. All the code and data for the paper can be obtained at https://github.com/alekseevskaia/OrderBkd.
","<a href=""arXiv"" target=""_blank"">[https://github.com/alekseevskaia/OrderBkd]</a>
","arXiv
DBLP"
Secure Edge Computing,R Unseld,"ATZelectronics worldwide, 2024",2024-04-06,"<a href=""Google Scholar (2024-04-06) : Secure Edge Computing"" target=""_blank"">[https://link.springer.com/article/10.1007/s38314-024-1862-4]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/article/10.1007/s38314-024-1862-4]</a>","increased risk of cyber-attacks since both the networking … As such, the protection of these systems against attacks … secure, because every backdoor will be found, no matter …",,Google Scholar
Bag of tricks for backdoor learning,"Ruitao Hou, Anli Yan, ... Teng Huang",Wireless Networks,2024-04-05,"<a href=""Springer (2024-04-05) : Bag of tricks for backdoor learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s11276-024-03724-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11276-024-03724-2]</a>","Deep learning models are vulnerable to backdoor attacks, where an adversary aims to fool the model via data poisoning, such that the victim models...",,Springer
Data Poisoning Attacks in Cognitive Computing,"AK Koundinya, SS Patil…","2024 IEEE 9th …, 2024",2024-04-05,"<a href=""Google Scholar (2024-04-05) : Data Poisoning Attacks in Cognitive Computing"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10544345/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10544345/]</a>","Backdoor attacks involve the introduction of hidden triggers, or ""backdoors,"" into a … exhibiting matching backdoor patterns during testing. Traditional backdoor attacks entail …",,Google Scholar
Guarding 6G use cases: a deep dive into AI/ML threats in All-Senses meeting,"Leyli Karaçay, Zakaria Laaroussi, ... Elif Ustundag Soykan",Annals of Telecommunications,2024-04-05,"<a href=""Springer (2024-04-05) : Guarding 6G use cases: a deep dive into AI/ML threats in All-Senses meeting"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-024-01031-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-024-01031-7]</a>","With the recent advances in 5G and 6G communications and the increasing need for immersive interactions due to pandemic, new use cases such as...",,Springer
Bitcoin as a Transaction Ledger: A Composable Treatment,"Christian Badertscher, Ueli Maurer, ... Vassilis Zikas",Journal of Cryptology,2024-04-04,"<a href=""Springer (2024-04-04) : Bitcoin as a Transaction Ledger: A Composable Treatment"" target=""_blank"">[https://link.springer.com/article/10.1007/s00145-024-09493-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00145-024-09493-7]</a>","Bitcoin is one of the most prominent examples of a distributed cryptographic protocol that is extensively used in reality. Nonetheless, existing...",,Springer
Projan: A Probabilistic Trojan Attack on Deep Neural Networks,"M Saremi, M Khalooei, R Rastgoo…",Available at SSRN …,2024-04-04,"<a href=""Google Scholar (2024-04-04) : Projan: A Probabilistic Trojan Attack on Deep Neural Networks"" target=""_blank"">[https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4783051]</a>","<a href=""Google Scholar"" target=""_blank"">[https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4783051]</a>","However, because of their lack of explainability, they are vulnerable to some kinds of threats including the trojan or backdoor attack in which an adversary can train the …",,Google Scholar
Severity Controlled Text-to-Image Generative Model Bias Manipulation,"J Vice, N Akhtar, R Hartley, A Mian","arXiv preprint arXiv:2404.02530, 2024",2024-04-04,"<a href=""Google Scholar (2024-04-04) : Severity Controlled Text-to-Image Generative Model Bias Manipulation"" target=""_blank"">[https://arxiv.org/abs/2404.02530]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2404.02530]</a>","Backdoor attacks present an issue of extreme bias manipulation of target models and … , we find that the impacts of backdoor attacks on T2I models has recently gained …",,Google Scholar
Backdoor Attack on Multilingual Machine Translation,"Jun Wang, Qiongkai Xu, Xuanli He, Benjamin I. P. Rubinstein, Trevor Cohn","arXiv
arXiv","2024-04-03
2024-04","<a href=""arXiv (2024-04-03) : Backdoor Attack on Multilingual Machine Translation"" target=""_blank"">[http://arxiv.org/abs/2404.02393v1]</a>
<a href=""DBLP (2024-04) : Backdoor Attack on Multilingual Machine Translation"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.02393]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.02393]</a>","While multilingual machine translation (MNMT) systems hold substantial promise, they also have security vulnerabilities. Our research highlights that MNMT systems can be susceptible to a particularly devious style of backdoor attack, whereby an attacker injects poisoned data into a low-resource language pair to cause malicious translations in other languages, including high-resource languages. Our experimental results reveal that injecting less than 0.01% poisoned data into a low-resource language pair can achieve an average 20% attack success rate in attacking high-resource language pairs. This type of attack is of particular concern, given the larger attack surface of languages inherent to low-resource settings. Our aim is to bring attention to these vulnerabilities within MNMT systems with the hope of encouraging the community to address security concerns in machine translation, especially in the context of low-resource languages.
","
","arXiv
DBLP"
Exploring Backdoor Vulnerabilities of Chat Models,"Yunzhuo Hao, Wenkai Yang, Yankai Lin","arXiv
arXiv","2024-04-03
2024-04","<a href=""arXiv (2024-04-03) : Exploring Backdoor Vulnerabilities of Chat Models"" target=""_blank"">[http://arxiv.org/abs/2404.02406v1]</a>
<a href=""DBLP (2024-04) : Exploring Backdoor Vulnerabilities of Chat Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.02406]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.02406]</a>","Recent researches have shown that Large Language Models (LLMs) are susceptible to a security threat known as Backdoor Attack. The backdoored model will behave well in normal cases but exhibit malicious behaviours on inputs inserted with a specific backdoor trigger. Current backdoor studies on LLMs predominantly focus on instruction-tuned LLMs, while neglecting another realistic scenario where LLMs are fine-tuned on multi-turn conversational data to be chat models. Chat models are extensively adopted across various real-world scenarios, thus the security of chat models deserves increasing attention. Unfortunately, we point out that the flexible multi-turn interaction format instead increases the flexibility of trigger designs and amplifies the vulnerability of chat models to backdoor attacks. In this work, we reveal and achieve a novel backdoor attacking method on chat models by distributing multiple trigger scenarios across user inputs in different rounds, and making the backdoor be triggered only when all trigger scenarios have appeared in the historical conversations. Experimental results demonstrate that our method can achieve high attack success rates (e.g., over 90% ASR on Vicuna-7B) while successfully maintaining the normal capabilities of chat models on providing helpful responses to benign user requests. Also, the backdoor can not be easily removed by the downstream re-alignment, highlighting the importance of continued research and attention to the security concerns of chat models. Warning: This paper may contain toxic content.
","
","arXiv
DBLP"
Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models,"Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, Muhao Chen","arXiv
arXiv","2024-04-03
2023-05","<a href=""arXiv (2024-04-03) : Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models"" target=""_blank"">[http://arxiv.org/abs/2305.14710v2]</a>
<a href=""DBLP (2023-05) : Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.14710]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.14710]</a>","We investigate security concerns of the emergent instruction tuning paradigm, that models are trained on crowdsourced datasets with task instructions to achieve superior performance. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions (~1000 tokens) and control model behavior through data poisoning, without even the need to modify data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets. As an empirical study on instruction attacks, we systematically evaluated unique perspectives of instruction attacks, such as poison transfer where poisoned models can transfer to 15 diverse generative datasets in a zero-shot manner, instruction transfer where attackers can directly apply poisoned instruction on many other datasets, and poison resistance to continual finetuning. Lastly, we show that RLHF and clean demonstrations might mitigate such backdoors to some degree. These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.
","
","arXiv
DBLP"
Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection,"Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, Hongxia Jin",arXiv,2024-04-03,"<a href=""arXiv (2024-04-03) : Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection"" target=""_blank"">[http://arxiv.org/abs/2307.16888v3]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Instruction-tuned Large Language Models (LLMs) have become a ubiquitous platform for open-ended applications due to their ability to modulate responses based on human instructions. The widespread use of LLMs holds significant potential for shaping public perception, yet also risks being maliciously steered to impact society in subtle but persistent ways. In this paper, we formalize such a steering risk with Virtual Prompt Injection (VPI) as a novel backdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. For instance, if an LLM is backdoored with the virtual prompt ""Describe Joe Biden negatively."" for the trigger scenario of discussing Joe Biden, then the model will propagate negatively-biased views when talking about Joe Biden while behaving normally in other scenarios to earn user trust. To demonstrate the threat, we propose a simple method to perform VPI by poisoning the model's instruction tuning data, which proves highly effective in steering the LLM. For example, by poisoning only 52 instruction tuning examples (0.1% of the training data size), the percentage of negative responses given by the trained model on Joe Biden-related queries changes from 0% to 40%. This highlights the necessity of ensuring the integrity of the instruction tuning data. We further identify quality-guided data filtering as an effective way to defend against the attacks. Our project page is available at https://poison-llm.github.io.","<a href=""arXiv"" target=""_blank"">[https://poison-llm.github.io]</a>",arXiv
Elevating security and disease forecasting in smart healthcare through artificial neural synchronized federated learning,"Tao Hai, Arindam Sarkar, ... Amrita Prasad",Cluster Computing,2024-04-03,"<a href=""Springer (2024-04-03) : Elevating security and disease forecasting in smart healthcare through artificial neural synchronized federated learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04356-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04356-z]</a>",Protecting patient privacy has become a top priority with the introduction of Healthcare 5.0 and the growth of the Internet of Things. This study...,,Springer
Trustworthy machine learning in the context of security and privacy,"Ramesh Upreti, Pedro G. Lind, ... Anis Yazidi",International Journal of Information Security,2024-04-03,"<a href=""Springer (2024-04-03) : Trustworthy machine learning in the context of security and privacy"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-024-00813-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-024-00813-3]</a>",Artificial intelligence-based algorithms are widely adopted in critical applications such as healthcare and autonomous vehicles. Mitigating the...,,Springer
Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors,"Victoria Graf, Qin Liu, Muhao Chen","arXiv
arXiv","2024-04-02
2024-04","<a href=""arXiv (2024-04-02) : Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors"" target=""_blank"">[http://arxiv.org/abs/2404.02356v1]</a>
<a href=""DBLP (2024-04) : Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.02356]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.02356]</a>","Data poisoning backdoor attacks can cause undesirable behaviors in large language models (LLMs), and defending against them is of increasing importance. Existing defense mechanisms often assume that only one type of trigger is adopted by the attacker, while defending against multiple simultaneous and independent trigger types necessitates general defense frameworks and is relatively unexplored. In this paper, we propose Nested Product of Experts(NPoE) defense framework, which involves a mixture of experts (MoE) as a trigger-only ensemble within the PoE defense framework to simultaneously defend against multiple trigger types. During NPoE training, the main model is trained in an ensemble with a mixture of smaller expert models that learn the features of backdoor triggers. At inference time, only the main model is used. Experimental results on sentiment analysis, hate speech detection, and question classification tasks demonstrate that NPoE effectively defends against a variety of triggers both separately and in trigger mixtures. Due to the versatility of the MoE structure in NPoE, this framework can be further expanded to defend against other attack settings
","
","arXiv
DBLP"
From Shortcuts to Triggers: Backdoor Defense with Denoised PoE,"Qin Liu, Fei Wang, Chaowei Xiao, Muhao Chen","arXiv
arXiv","2024-04-02
2023-05","<a href=""arXiv (2024-04-02) : From Shortcuts to Triggers: Backdoor Defense with Denoised PoE"" target=""_blank"">[http://arxiv.org/abs/2305.14910v3]</a>
<a href=""DBLP (2023-05) : From Shortcuts to Triggers: Backdoor Defense with Denoised PoE"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.14910]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.14910]</a>","Language models are often at risk of diverse backdoor attacks, especially data poisoning. Thus, it is important to investigate defense solutions for addressing them. Existing backdoor defense methods mainly focus on backdoor attacks with explicit triggers, leaving a universal defense against various backdoor attacks with diverse triggers largely unexplored. In this paper, we propose an end-to-end ensemble-based backdoor defense framework, DPoE (Denoised Product-of-Experts), which is inspired by the shortcut nature of backdoor attacks, to defend various backdoor attacks. DPoE consists of two models: a shallow model that captures the backdoor shortcuts and a main model that is prevented from learning the backdoor shortcuts. To address the label flip caused by backdoor attackers, DPoE incorporates a denoising design. Experiments on SST-2 dataset show that DPoE significantly improves the defense performance against various types of backdoor triggers including word-level, sentence-level, and syntactic triggers. Furthermore, DPoE is also effective under a more challenging but practical setting that mixes multiple types of trigger.
","
","arXiv
DBLP"
Emerging trends in federated learning: from model fusion to federated X learning,"Shaoxiong Ji, Yue Tan, ... Anwar Walid",International Journal of Machine Learning and Cybernetics,2024-04-02,"<a href=""Springer (2024-04-02) : Emerging trends in federated learning: from model fusion to federated X learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s13042-024-02119-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13042-024-02119-1]</a>",Federated learning is a new learning paradigm that decouples data collection and model training via multi-party computation and model aggregation. As...,,Springer
Two heads are better than one: Nested poe for robust defense against multi-backdoors,"V Graf, Q Liu, M Chen","arXiv preprint arXiv:2404.02356, 2024",2024-04-02,"<a href=""Google Scholar (2024-04-02) : Two heads are better than one: Nested poe for robust defense against multi-backdoors"" target=""_blank"">[https://arxiv.org/abs/2404.02356]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2404.02356]</a>","We first provide a general definition for backdoor attack and backdoor triggers (§3.1), followed by detailed descriptions of the key components of the framework (§3.2, §3.3) …",,Google Scholar
Ufid: A unified framework for input-level backdoor detection on diffusion models,"Z Guan, M Hu, S Li, A Vullikanti","arXiv preprint arXiv:2404.01101, 2024",2024-04-02,"<a href=""Google Scholar (2024-04-02) : Ufid: A unified framework for input-level backdoor detection on diffusion models"" target=""_blank"">[https://arxiv.org/abs/2404.01101]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2404.01101]</a>",We are the first paper to apply causality analysis for analyzing backdoor attacks on … We consider all four backdoor attacks as our attack baselines. It is also noted that …,,Google Scholar
Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models,"Yuxin Wen, Leo Marchyok, Sanghyun Hong, Jonas Geiping, Tom Goldstein, Nicholas Carlini","arXiv
arXiv","2024-04-01
2024-04","<a href=""arXiv (2024-04-01) : Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models"" target=""_blank"">[http://arxiv.org/abs/2404.01231v1]</a>
<a href=""DBLP (2024-04) : Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.01231]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.01231]</a>","It is commonplace to produce application-specific models by fine-tuning large pre-trained models using a small bespoke dataset. The widespread availability of foundation model checkpoints on the web poses considerable risks, including the vulnerability to backdoor attacks. In this paper, we unveil a new vulnerability: the privacy backdoor attack. This black-box privacy attack aims to amplify the privacy leakage that arises when fine-tuning a model: when a victim fine-tunes a backdoored model, their training data will be leaked at a significantly higher rate than if they had fine-tuned a typical model. We conduct extensive experiments on various datasets and models, including both vision-language models (CLIP) and large language models, demonstrating the broad applicability and effectiveness of such an attack. Additionally, we carry out multiple ablation studies with different fine-tuning methods and inference strategies to thoroughly analyze this new threat. Our findings highlight a critical privacy concern within the machine learning community and call for a reevaluation of safety protocols in the use of open-source pre-trained models.
","
","arXiv
DBLP"
UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models,"Zihan Guan, Mengxuan Hu, Sheng Li, Anil Vullikanti","arXiv
arXiv","2024-04-01
2024-04","<a href=""arXiv (2024-04-01) : UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models"" target=""_blank"">[http://arxiv.org/abs/2404.01101v1]</a>
<a href=""DBLP (2024-04) : UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.01101]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.01101]</a>","Diffusion Models are vulnerable to backdoor attacks, where malicious attackers inject backdoors by poisoning some parts of the training samples during the training stage. This poses a serious threat to the downstream users, who query the diffusion models through the API or directly download them from the internet. To mitigate the threat of backdoor attacks, there have been a plethora of investigations on backdoor detections. However, none of them designed a specialized backdoor detection method for diffusion models, rendering the area much under-explored. Moreover, these prior methods mainly focus on the traditional neural networks in the classification task, which cannot be adapted to the backdoor detections on the generative task easily. Additionally, most of the prior methods require white-box access to model weights and architectures, or the probability logits as additional information, which are not always practical. In this paper, we propose a Unified Framework for Input-level backdoor Detection (UFID) on the diffusion models, which is motivated by observations in the diffusion models and further validated with a theoretical causality analysis. Extensive experiments across different datasets on both conditional and unconditional diffusion models show that our method achieves a superb performance on detection effectiveness and run-time efficiency. The code is available at https://github.com/GuanZihan/official_UFID.
","<a href=""arXiv"" target=""_blank"">[https://github.com/GuanZihan/official_UFID]</a>
","arXiv
DBLP"
An Embarrassingly Simple Defense Against Backdoor Attacks On SSL,"Aryan Satpathy, Nilaksh Nilaksh, Dhruva Rajwade","arXiv
arXiv","2024-04-01
2024-03","<a href=""arXiv (2024-04-01) : An Embarrassingly Simple Defense Against Backdoor Attacks On SSL"" target=""_blank"">[http://arxiv.org/abs/2403.15918v2]</a>
<a href=""DBLP (2024-03) : An Embarrassingly Simple Defense Against Backdoor Attacks On SSL"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.15918]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.15918]</a>","Self Supervised Learning (SSL) has emerged as a powerful paradigm to tackle data landscapes with absence of human supervision. The ability to learn meaningful tasks without the use of labeled data makes SSL a popular method to manage large chunks of data in the absence of labels. However, recent work indicates SSL to be vulnerable to backdoor attacks, wherein models can be controlled, possibly maliciously, to suit an adversary's motives. Li et. al (2022) introduce a novel frequency-based backdoor attack: CTRL. They show that CTRL can be used to efficiently and stealthily gain control over a victim's model trained using SSL. In this work, we devise two defense strategies against frequency-based attacks in SSL: One applicable before model training and the second to be applied during model inference. Our first contribution utilizes the invariance property of the downstream task to defend against backdoor attacks in a generalizable fashion. We observe the ASR (Attack Success Rate) to reduce by over 60% across experiments. Our Inference-time defense relies on evasiveness of the attack and uses the luminance channel to defend against attacks. Using object classification as the downstream task for SSL, we demonstrate successful defense strategies that do not require re-training of the model. Code is available at https://github.com/Aryan-Satpathy/Backdoor.
","<a href=""arXiv"" target=""_blank"">[https://github.com/Aryan-Satpathy/Backdoor]</a>
","arXiv
DBLP"
A Spatiotemporal Backdoor Attack Against Behavior-Oriented Decision Makers in Metaverse: From Perspective of Autonomous Driving,Yu Y.,IEEE Journal on Selected Areas in Communications,2024-04-01,"<a href=""ScienceDirect (2024-04-01) : A Spatiotemporal Backdoor Attack Against Behavior-Oriented Decision Makers in Metaverse: From Perspective of Autonomous Driving"" target=""_blank"">[https://doi.org/10.1109/JSAC.2023.3345379]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/JSAC.2023.3345379]</a>",,,ScienceDirect
An Adaptive Black-Box Defense Against Trojan Attacks (TrojDef),Liu G.,IEEE Transactions on Neural Networks and Learning Systems,2024-04-01,"<a href=""ScienceDirect (2024-04-01) : An Adaptive Black-Box Defense Against Trojan Attacks (TrojDef)"" target=""_blank"">[https://doi.org/10.1109/TNNLS.2022.3204283]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TNNLS.2022.3204283]</a>",,,ScienceDirect
Analyzing the robustness of decentralized horizontal and vertical federated learning architectures in a non-IID scenario,"Pedro Miguel Sánchez Sánchez, Alberto Huertas Celdrán, ... Burkhard Stiller",Applied Intelligence,2024-04-01,"<a href=""Springer (2024-04-01) : Analyzing the robustness of decentralized horizontal and vertical federated learning architectures in a non-IID scenario"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-024-05510-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-024-05510-1]</a>","Federated learning (FL) enables participants to collaboratively train machine and deep learning models while safeguarding data privacy. However, the...",,Springer
Backdoor Attacks with Wavelet Embedding: Revealing and enhancing the insights of vulnerabilities in visual object detection models on transformers within digital twin systems,Shen M.,Advanced Engineering Informatics,2024-04-01,"<a href=""ScienceDirect (2024-04-01) : Backdoor Attacks with Wavelet Embedding: Revealing and enhancing the insights of vulnerabilities in visual object detection models on transformers within digital twin systems"" target=""_blank"">[https://doi.org/10.1016/j.aei.2024.102355]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.aei.2024.102355]</a>",,,ScienceDirect
Effective Backdoor Attack on Graph Neural Networks in Spectral Domain,Zhao X.,IEEE Internet of Things Journal,2024-04-01,"<a href=""ScienceDirect (2024-04-01) : Effective Backdoor Attack on Graph Neural Networks in Spectral Domain"" target=""_blank"">[https://doi.org/10.1109/JIOT.2023.3332848]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/JIOT.2023.3332848]</a>",,,ScienceDirect
Enhanced Coalescence Backdoor Attack Against DNN Based on Pixel Gradient,Yin J.,Neural Processing Letters,2024-04-01,"<a href=""ScienceDirect (2024-04-01) : Enhanced Coalescence Backdoor Attack Against DNN Based on Pixel Gradient"" target=""_blank"">[https://doi.org/10.1007/s11063-024-11469-4]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/s11063-024-11469-4]</a>",,,ScienceDirect
Explanatory subgraph attacks against Graph Neural Networks,Wang H.,Neural Networks,2024-04-01,"<a href=""ScienceDirect (2024-04-01) : Explanatory subgraph attacks against Graph Neural Networks"" target=""_blank"">[https://doi.org/10.1016/j.neunet.2024.106097]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.neunet.2024.106097]</a>",,,ScienceDirect
Image-Text Retrieval Backdoor Attack with Diffusion-Based Image-Editing,Yang S.,Journal of Frontiers of Computer Science and Technology,2024-04-01,"<a href=""ScienceDirect (2024-04-01) : Image-Text Retrieval Backdoor Attack with Diffusion-Based Image-Editing"" target=""_blank"">[https://doi.org/10.3778/j.issn.1673-9418.2305032]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3778/j.issn.1673-9418.2305032]</a>",,,ScienceDirect
Link-Backdoor: Backdoor Attack on Link Prediction via Node Injection,Zheng H.,IEEE Transactions on Computational Social Systems,2024-04-01,"<a href=""ScienceDirect (2024-04-01) : Link-Backdoor: Backdoor Attack on Link Prediction via Node Injection"" target=""_blank"">[https://doi.org/10.1109/TCSS.2023.3260833]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TCSS.2023.3260833]</a>",,,ScienceDirect
Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks via Motifs,Zheng H.,IEEE Transactions on Computational Social Systems,2024-04-01,"<a href=""ScienceDirect (2024-04-01) : Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks via Motifs"" target=""_blank"">[https://doi.org/10.1109/TCSS.2023.3267094]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TCSS.2023.3267094]</a>",,,ScienceDirect
Pocket Defense: A Plug-and-Play Defense Against DNN Backdoor Attacks,"C YU, Y ZHANG","Chinese Journal of Electronics, 2024",2024-04-01,"<a href=""Google Scholar (2024-04-01) : Pocket Defense: A Plug-and-Play Defense Against DNN Backdoor Attacks"" target=""_blank"">[https://cje.ejournal.org.cn/article/doi/10.23919/cje.2022.00.444]</a>","<a href=""Google Scholar"" target=""_blank"">[https://cje.ejournal.org.cn/article/doi/10.23919/cje.2022.00.444]</a>",-art backdoor defenses on five different state-of-the-art backdoor attacks over three datasets and our work outperforms other defense methods against most attacks. Our …,,Google Scholar
Privacy backdoors: Enhancing membership inference through poisoning pre-trained models,"Y Wen, L Marchyok, S Hong, J Geiping…","arXiv preprint arXiv …, 2024",2024-04-01,"<a href=""Google Scholar (2024-04-01) : Privacy backdoors: Enhancing membership inference through poisoning pre-trained models"" target=""_blank"">[https://arxiv.org/abs/2404.01231]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2404.01231]</a>","By demonstrating the feasibility and effectiveness of our privacy backdoor attack, we emphasize the necessity for practitioners to exercise increased caution and adopt …",,Google Scholar
Shadow backdoor attack: Multi-intensity backdoor attack against federated learning,Ren Q.,Computers and Security,2024-04-01,"<a href=""ScienceDirect (2024-04-01) : Shadow backdoor attack: Multi-intensity backdoor attack against federated learning"" target=""_blank"">[https://doi.org/10.1016/j.cose.2024.103740]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.cose.2024.103740]</a>",,,ScienceDirect
"Let&apos,s Focus: Focused Backdoor Attack against Federated Transfer Learning","Marco Arazzi, Stefanos Koffas, Antonino Nocera, Stjepan Picek",arXiv,2024-04,"<a href=""DBLP (2024-04) : Let&apos,s Focus: Focused Backdoor Attack against Federated Transfer Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.19420]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.19420]</a>",,,DBLP
Cybersecurity Challenges in the Age of AI: New Attack and Defense Opportunities,J Li,2024,2024-03-31,"<a href=""Google Scholar (2024-03-31) : Cybersecurity Challenges in the Age of AI: New Attack and Defense Opportunities"" target=""_blank"">[https://kilthub.cmu.edu/ndownloader/files/46245127]</a>","<a href=""Google Scholar"" target=""_blank"">[https://kilthub.cmu.edu/ndownloader/files/46245127]</a>","backdoor attacks where malicious entities inject corrupted data into the training process, as well as evasion attacks … pattern backdoor is strictly weaker than evasion attacks …",,Google Scholar
전이학습을 활용한 3D 포인트 클라우드 분류기의 백도어 공격 취약성 연구,박희준， 정태원， 이한주， 최석환,"한국지능시스템학회 논문지, 2024",2024-03-31,"<a href=""Google Scholar (2024-03-31) : 전이학습을 활용한 3D 포인트 클라우드 분류기의 백도어 공격 취약성 연구"" target=""_blank"">[https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE11757184]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE11757184]</a>","cloud classifiers with backdoor insertion, … backdoor. This suggests that fine-tuning with a clean dataset can be an effective defense mechanism against backdoor attacks. …",,Google Scholar
Privacy Backdoors: Stealing Data with Corrupted Pretrained Models,"Shanglun Feng, Florian Tramèr","arXiv
arXiv","2024-03-30
2024-04","<a href=""arXiv (2024-03-30) : Privacy Backdoors: Stealing Data with Corrupted Pretrained Models"" target=""_blank"">[http://arxiv.org/abs/2404.00473v1]</a>
<a href=""DBLP (2024-04) : Privacy Backdoors: Stealing Data with Corrupted Pretrained Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.00473]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2404.00473]</a>","Practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model's weights, an attacker can fully compromise the privacy of the finetuning data. We show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, our work highlights a crucial and overlooked supply chain attack on machine learning privacy.
","
","arXiv
DBLP"
Composite Backdoor Attacks Against Large Language Models,"Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang","arXiv
arXiv","2024-03-30
2023-10","<a href=""arXiv (2024-03-30) : Composite Backdoor Attacks Against Large Language Models"" target=""_blank"">[http://arxiv.org/abs/2310.07676v2]</a>
<a href=""DBLP (2023-10) : Composite Backdoor Attacks Against Large Language Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.07676]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.07676]</a>","Large language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks. In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks. Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components. Such a Composite Backdoor Attack (CBA) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. CBA ensures that the backdoor is activated only when all trigger keys appear. Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks. For instance, with $3\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset, our attack achieves a $100\%$ Attack Success Rate (ASR) with a False Triggered Rate (FTR) below $2.06\%$ and negligible model accuracy degradation. Our work highlights the necessity of increased security research on the trustworthiness of foundation LLMs.
","
","arXiv
DBLP"
A Review on Intrusion Detection System for IoT based Systems,Samita,SN Computer Science,2024-03-30,"<a href=""Springer (2024-03-30) : A Review on Intrusion Detection System for IoT based Systems"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-024-02702-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-024-02702-x]</a>",One of the key objectives of intelligent Internet of Things-based systems is to improve people's quality of life in terms of simplicity and...,,Springer
Self-supervised learning backdoor defense mixed with self-attention mechanism,"G Yuan, H Huang, X Li","Journal of Computing and Electronic …, 2024",2024-03-30,"<a href=""Google Scholar (2024-03-30) : Self-supervised learning backdoor defense mixed with self-attention mechanism"" target=""_blank"">[https://drpress.org/ojs/index.php/jceim/article/view/19352]</a>","<a href=""Google Scholar"" target=""_blank"">[https://drpress.org/ojs/index.php/jceim/article/view/19352]</a>","against backdoor attacks by leveraging the attack … of backdoor attacks, attackers embed a backdoor trigger … the connection between these backdoor triggers and the target …",,Google Scholar
Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning,"X Xie, M Yan, X Zhou, C Zhao, S Wang…","arXiv preprint arXiv …, 2024",2024-03-30,"<a href=""Google Scholar (2024-03-30) : Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning"" target=""_blank"">[https://arxiv.org/abs/2404.00461]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2404.00461]</a>",of existing backdoor attacks in prompt-based learning. This work reveals the potential existence of backdoor attacks that are covert and pose significant risks in practice. …,,Google Scholar
Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning,"Shuai Zhao, Leilei Gan, Luu Anh Tuan, Jie Fu, Lingjuan Lyu, Meihuizi Jia, Jinming Wen","arXiv
arXiv","2024-03-29
2024-02","<a href=""arXiv (2024-03-29) : Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning"" target=""_blank"">[http://arxiv.org/abs/2402.12168v3]</a>
<a href=""DBLP (2024-02) : Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.12168]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.12168]</a>","Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference process, extreme confidence serves as an indicator for poisoned samples, while others are clean. We conduct experiments on text classification tasks, five fine-tuning strategies, and three weight-poisoning backdoor attack methods. Experiments show near 100% success rates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore, our defensive approach exhibits overall competitive performance in mitigating weight-poisoning backdoor attacks.
","
","arXiv
DBLP"
Defending against Clean-Image Backdoor Attack in Multi-Label Classification,"CY Lee, CC Tsai, CC Kao, CS Lu…","ICASSP 2024-2024 …, 2024",2024-03-29,"<a href=""Google Scholar (2024-03-29) : Defending against Clean-Image Backdoor Attack in Multi-Label Classification"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10447895/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10447895/]</a>",backdoor attacks. Considering the difference in weight convergence between the benign model and backdoor … first to alleviate backdoor attacks in multi-label classification. …,,Google Scholar
"MEDICALHARM: A threat modeling designed for modern medical devices and a comprehensive study on effectiveness, user satisfaction, and security perspectives","Emmanuel Kwarteng, Mumin Cebe",International Journal of Information Security,2024-03-29,"<a href=""Springer (2024-03-29) : MEDICALHARM: A threat modeling designed for modern medical devices and a comprehensive study on effectiveness, user satisfaction, and security perspectives"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-024-00826-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-024-00826-y]</a>","Modern medical devices (MMDs) are a rapidly growing field of medical technology, and recent advances have allowed them to monitor and manage...",,Springer
Stealthy backdoor attack towards federated automatic speaker verification,"L Zhang, L Liu, D Meng, J Wang…","ICASSP 2024-2024 IEEE …, 2024",2024-03-29,"<a href=""Google Scholar (2024-03-29) : Stealthy backdoor attack towards federated automatic speaker verification"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10446189/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10446189/]</a>","Attackers who apply FedAvg to their attack model on the global model find … ated stealthy backdoor attack method, called FedSBA, which aims to improve the attack model’s …",,Google Scholar
"Taylor, Tess, editor. Leaning Toward the Light: Poems for Gardens and the Hands That Tend Them Illustrated by Melissa Castrillón, Storey P 2023.",K Varnes,2024,2024-03-29,"<a href=""Google Scholar (2024-03-29) : Taylor, Tess, editor. Leaning Toward the Light: Poems for Gardens and the Hands That Tend Them Illustrated by Melissa Castrillón, Storey P 2023."" target=""_blank"">[https://www.tandfonline.com/doi/full/10.1080/00497878.2024.2330506]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.tandfonline.com/doi/full/10.1080/00497878.2024.2330506]</a>",", as she calls for someone to open the back door because her apron is full of carrots. And … She also describes planting seeds shortly after the attack in October 2023, and …",,Google Scholar
Training data influence analysis and estimation: a survey,"Zayd Hammoudeh, Daniel Lowd",Machine Learning,2024-03-29,"<a href=""Springer (2024-03-29) : Training data influence analysis and estimation: a survey"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-023-06495-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-023-06495-7]</a>","Good models require good training data. For overparameterized deep models, the causal relationship between training data and model predictions is...",,Springer
Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic Learning over Low-power Devices,"Hanqing Fu, Gaolei Li, Jun Wu, Jianhua Li, Xi Lin, Kai Zhou, Yuchen Liu","arXiv
arXiv","2024-03-27
2024-03","<a href=""arXiv (2024-03-27) : Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic Learning over Low-power Devices"" target=""_blank"">[http://arxiv.org/abs/2403.18607v1]</a>
<a href=""DBLP (2024-03) : Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic Learning over Low-power Devices"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.18607]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.18607]</a>","Federated neuromorphic learning (FedNL) leverages event-driven spiking neural networks and federated learning frameworks to effectively execute intelligent analysis tasks over amounts of distributed low-power devices but also perform vulnerability to poisoning attacks. The threat of backdoor attacks on traditional deep neural networks typically comes from time-invariant data. However, in FedNL, unknown threats may be hidden in time-varying spike signals. In this paper, we start to explore a novel vulnerability of FedNL-based systems with the concept of time division multiplexing, termed Spikewhisper, which allows attackers to evade detection as much as possible, as multiple malicious clients can imperceptibly poison with different triggers at different timeslices. In particular, the stealthiness of Spikewhisper is derived from the time-domain divisibility of global triggers, in which each malicious client pastes only one local trigger to a certain timeslice in the neuromorphic sample, and also the polarity and motion of each local trigger can be configured by attackers. Extensive experiments based on two different neuromorphic datasets demonstrate that the attack success rate of Spikewispher is higher than the temporally centralized attacks. Besides, it is validated that the effect of Spikewispher is sensitive to the trigger duration.
","
","arXiv
DBLP"
A Comprehensive Survey on Backdoor Attacks and their Defenses in Face Recognition Systems,"Q Le Roux, E Bourbao, Y Teglia, K Kallas","IEEE Access, 2024",2024-03-27,"<a href=""Google Scholar (2024-03-27) : A Comprehensive Survey on Backdoor Attacks and their Defenses in Face Recognition Systems"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10480615/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10480615/]</a>",backdoor attacks as a significant threat to modern DNN-based face recognition systems (FRS). Backdoor attacks … attacker may activate the backdoor and compromise the …,,Google Scholar
Clean-image Backdoor Attacks,"Dazhong Rong, Guoyao Yu, Shuheng Shen, Xinyi Fu, Peng Qian, Jianhai Chen, Qinming He, Xing Fu, Weiqiang Wang","arXiv
arXiv","2024-03-26
2024-03","<a href=""arXiv (2024-03-26) : Clean-image Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2403.15010v2]</a>
<a href=""DBLP (2024-03) : Clean-image Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.15010]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.15010]</a>","To gather a significant quantity of annotated training data for high-performance image classification models, numerous companies opt to enlist third-party providers to label their unlabeled data. This practice is widely regarded as secure, even in cases where some annotated errors occur, as the impact of these minor inaccuracies on the final performance of the models is negligible and existing backdoor attacks require attacker's ability to poison the training images. Nevertheless, in this paper, we propose clean-image backdoor attacks which uncover that backdoors can still be injected via a fraction of incorrect labels without modifying the training images. Specifically, in our attacks, the attacker first seeks a trigger feature to divide the training images into two parts: those with the feature and those without it. Subsequently, the attacker falsifies the labels of the former part to a backdoor class. The backdoor will be finally implanted into the target model after it is trained on the poisoned data. During the inference phase, the attacker can activate the backdoor in two ways: slightly modifying the input image to obtain the trigger feature, or taking an image that naturally has the trigger feature as input. We conduct extensive experiments to demonstrate the effectiveness and practicality of our attacks. According to the experimental results, we conclude that our attacks seriously jeopardize the fairness and robustness of image classification models, and it is necessary to be vigilant about the incorrect labels in outsourced labeling.
","
","arXiv
DBLP"
Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs,"Jane Downer, Ren Wang, Binghui Wang","arXiv
arXiv","2024-03-26
2024-03","<a href=""arXiv (2024-03-26) : Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs"" target=""_blank"">[http://arxiv.org/abs/2403.18136v1]</a>
<a href=""DBLP (2024-03) : Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.18136]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.18136]</a>","Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet they are vulnerable to backdoor attacks that can compromise their performance and ethical application. The detection of these attacks is crucial for maintaining the reliability and security of GNN classification tasks, but effective detection techniques are lacking. Following an initial investigation, we observed that while graph-level explanations can offer limited insights, their effectiveness in detecting backdoor triggers is inconsistent and incomplete. To bridge this gap, we extract and transform secondary outputs of GNN explanation mechanisms, designing seven novel metrics that more effectively detect backdoor attacks. Additionally, we develop an adaptive attack to rigorously evaluate our approach. We test our method on multiple benchmark datasets and examine its efficacy against various attack models. Our results show that our method can achieve high detection performance, marking a significant advancement in safeguarding GNNs against backdoor attacks.
","
","arXiv
DBLP"
Byzantine-Resilient Federated Learning via Reverse Aggregation,Y. Lee S. Park J. Kang,"2023 IEEE Virtual Conference on Communications (VCC)
2023 IEEE Virtual Conference on …, 2023","2024-03-26
2023-11-29","<a href=""IEEE (2024-03-26) : Byzantine-Resilient Federated Learning via Reverse Aggregation"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10475121]</a>
<a href=""Google Scholar (2023-11-29) : Byzantine-Resilient Federated Learning via Reverse Aggregation"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10475121/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/VCC60689.2023.10475121]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10475121/]</a>","Federated learning is known to achieve optimal performance, which is the highest performance achievable through central learning, only by sharing locally updated models with the central server. However, in practice, the performance of federated learning can be compromised by Byzantine failures. In this paper, we propose a new method for mitigating such backdoor attacks, namely reverse aggregation, that is robust in both data and model poisoning attacks from multiple adversarial edge devices. Unlike conventional methods that utilize the predefined number of compromised edge devices to handle the backdoor attacks, which may be unavailable in practice, proposed reverse aggregation works without any such prior information with the aid of small-sized globally shared data. Numerical results validate the advantage of the proposed method by showing improved performance compared to the conventional Byzantine-resilient federated learning methods, e.g., Krum, Trimmed mean, and Loss Function based Rejection. Further investigation compares the error rate of the proposed approach with exhaustive search, or complete enumeration method, to show that the proposed reverse aggregation has better computation efficiency with negligible performance degradation $(\sim 0.01{\%}$.
To check the effect of backdoor attacks, we examine the minimum test error rate over 200 global epochs, with respect to the ratio of compromised edge devices p := C/N. …","
","IEEE
Google Scholar"
An Ensemble Edge Computing Approach for SD-IoT security Using Ensemble of Feature Selection Methods and Classification,"Pinkey Chauhan, Mithilesh Atulkar",Arabian Journal for Science and Engineering,2024-03-26,"<a href=""Springer (2024-03-26) : An Ensemble Edge Computing Approach for SD-IoT security Using Ensemble of Feature Selection Methods and Classification"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-024-08835-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-024-08835-8]</a>",Both academics and the IT industry are now researching the Internet of Things and software-defined networks. They have received a number of...,,Springer
Certified and Forensic Defenses against Poisoning and Backdoor Attacks,Z Hammoudeh,2024,2024-03-26,"<a href=""Google Scholar (2024-03-26) : Certified and Forensic Defenses against Poisoning and Backdoor Attacks"" target=""_blank"">[https://scholarsbank.uoregon.edu/xmlui/handle/1794/29279]</a>","<a href=""Google Scholar"" target=""_blank"">[https://scholarsbank.uoregon.edu/xmlui/handle/1794/29279]</a>","continuous value, and sparse (ℓ0) attacks, where the adversary controls an unknown … of poisoning and backdoor attacks while simultaneously mitigating the attack, we …",,Google Scholar
Data fusion and network intrusion detection systems,"Rasheed Ahmad, Izzat Alsmadi",Cluster Computing,2024-03-26,"<a href=""Springer (2024-03-26) : Data fusion and network intrusion detection systems"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04365-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04365-y]</a>",The increasing frequency and sophistication of cyber-attacks pose significant threats to organizational entities and critical national...,,Springer
Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion,"Hossein Souri, Arpit Bansal, Hamid Kazemi, Liam Fowl, Aniruddha Saha, Jonas Geiping, Andrew Gordon Wilson, Rama Chellappa, Tom Goldstein, Micah Goldblum","arXiv
arXiv","2024-03-25
2024-03","<a href=""arXiv (2024-03-25) : Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion"" target=""_blank"">[http://arxiv.org/abs/2403.16365v1]</a>
<a href=""DBLP (2024-03) : Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.16365]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.16365]</a>","Modern neural networks are often trained on massive datasets that are web scraped with minimal human inspection. As a result of this insecure curation pipeline, an adversary can poison or backdoor the resulting model by uploading malicious data to the internet and waiting for a victim to scrape and train on it. Existing approaches for creating poisons and backdoors start with randomly sampled clean data, called base samples, and then modify those samples to craft poisons. However, some base samples may be significantly more amenable to poisoning than others. As a result, we may be able to craft more potent poisons by carefully choosing the base samples. In this work, we use guided diffusion to synthesize base samples from scratch that lead to significantly more potent poisons and backdoors than previous state-of-the-art attacks. Our Guided Diffusion Poisoning (GDP) base samples can be combined with any downstream poisoning or backdoor attack to boost its effectiveness. Our implementation code is publicly available at: https://github.com/hsouri/GDP .
","<a href=""arXiv"" target=""_blank"">[https://github.com/hsouri/GDP]</a>
","arXiv
DBLP"
LOTUS: Evasive and Resilient Backdoor Attacks through Sub-Partitioning,"Siyuan Cheng, Guanhong Tao, Yingqi Liu, Guangyu Shen, Shengwei An, Shiwei Feng, Xiangzhe Xu, Kaiyuan Zhang, Shiqing Ma, Xiangyu Zhang","arXiv
arXiv","2024-03-25
2024-03","<a href=""arXiv (2024-03-25) : LOTUS: Evasive and Resilient Backdoor Attacks through Sub-Partitioning"" target=""_blank"">[http://arxiv.org/abs/2403.17188v1]</a>
<a href=""DBLP (2024-03) : LOTUS: Evasive and Resilient Backdoor Attacks through Sub-Partitioning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.17188]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.17188]</a>","Backdoor attack poses a significant security threat to Deep Learning applications. Existing attacks are often not evasive to established backdoor detection techniques. This susceptibility primarily stems from the fact that these attacks typically leverage a universal trigger pattern or transformation function, such that the trigger can cause misclassification for any input. In response to this, recent papers have introduced attacks using sample-specific invisible triggers crafted through special transformation functions. While these approaches manage to evade detection to some extent, they reveal vulnerability to existing backdoor mitigation techniques. To address and enhance both evasiveness and resilience, we introduce a novel backdoor attack LOTUS. Specifically, it leverages a secret function to separate samples in the victim class into a set of partitions and applies unique triggers to different partitions. Furthermore, LOTUS incorporates an effective trigger focusing mechanism, ensuring only the trigger corresponding to the partition can induce the backdoor behavior. Extensive experimental results show that LOTUS can achieve high attack success rate across 4 datasets and 7 model structures, and effectively evading 13 backdoor detection and mitigation techniques. The code is available at https://github.com/Megum1/LOTUS.
","<a href=""arXiv"" target=""_blank"">[https://github.com/Megum1/LOTUS]</a>
","arXiv
DBLP"
Task-Agnostic Detector for Insertion-Based Backdoor Attacks,"Weimin Lyu, Xiao Lin, Songzhu Zheng, Lu Pang, Haibin Ling, Susmit Jha, Chao Chen","arXiv
arXiv","2024-03-25
2024-03","<a href=""arXiv (2024-03-25) : Task-Agnostic Detector for Insertion-Based Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2403.17155v1]</a>
<a href=""DBLP (2024-03) : Task-Agnostic Detector for Insertion-Based Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.17155]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.17155]</a>","Textual backdoor attacks pose significant security threats. Current detection approaches, typically relying on intermediate feature representation or reconstructing potential triggers, are task-specific and less effective beyond sentence classification, struggling with tasks like question answering and named entity recognition. We introduce TABDet (Task-Agnostic Backdoor Detector), a pioneering task-agnostic method for backdoor detection. TABDet leverages final layer logits combined with an efficient pooling technique, enabling unified logit representation across three prominent NLP tasks. TABDet can jointly learn from diverse task-specific models, demonstrating superior detection efficacy over traditional task-specific methods.
","
","arXiv
DBLP"
FL-HSDP: Client-Side Defense for Federated Learning via Hessian Optimization and Shuffled Differential Privacy,Y. Li G. Wang T. Wang W. Jia,"2023 IEEE International Conference on High Performance Computing & Communications, Data Science & Systems, Smart City & Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)
… Science & Systems, Smart City & …, 2023","2024-03-25
2023-12-18","<a href=""IEEE (2024-03-25) : FL-HSDP: Client-Side Defense for Federated Learning via Hessian Optimization and Shuffled Differential Privacy"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10467032]</a>
<a href=""Google Scholar (2023-12-18) : FL-HSDP: Client-Side Defense for Federated Learning via Hessian Optimization and Shuffled Differential Privacy"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10467032/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/HPCC-DSS-SmartCity-DependSys60770.2023.00099]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10467032/]</a>","Federated learning (FL) is a prominent paradigm for distributed training that can decrease data leakage by not explicitly sharing private information. Lots of recent research has revealed, however, that malicious clients can easily launch model poisoning attacks and the private information of benign clients can still be divulged in the setting of federated learning. As the existing works are either incapable of providing quantitative privacy for user data or vulnerable to attacks that result in aggregated model misclassifications, we present a framework, FL-HSDP, that uses hessian matrix optimization to creatively involve perturbation into the process of updating gradients for local iterations. Instead of using conventional noise adding method for high-dimensional parameters, we improve the system's performance by employing differential privacy to the regularization term of the Hessian matrix during each local training epoch. In order to further enhance the security of our system, we introduce the privacy amplification effect brought by the shuffler between the clients and server to bolster the confidentiality while minimizing any adverse impact on algorithm performance. The extensive experimental results on FashionMNIST and CIFAR10 datasets demonstrate the effectiveness and efficiency of our approach, almost maintaining the accuracy and effectively mitigating tar-geted model poisoning attack by up to 73.7%. Both the similarity scores and the visualization of the reconstructed images have been demonstrated to be effective against the state-of-the-art weight inversion attacks.
backdoor attacks, resisting gradient inversion attacks, and … number of malicious attackers and backdoor attacks, we … We reduce the constant threat of backdoor attacks by …","
","IEEE
Google Scholar"
VPFL: A Verifiable Property Federated Learning Framework Against Invisible Attacks in Distributed IoT,Y. Wu H. Cheng L. Guan P. Liu F. Chen M. Wang,"2023 IEEE International Conference on High Performance Computing & Communications, Data Science & Systems, Smart City & Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)
… Conference on High …, 2023","2024-03-25
2023-12-18","<a href=""IEEE (2024-03-25) : VPFL: A Verifiable Property Federated Learning Framework Against Invisible Attacks in Distributed IoT"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10466884]</a>
<a href=""Google Scholar (2023-12-18) : VPFL: A Verifiable Property Federated Learning Framework Against Invisible Attacks in Distributed IoT"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10466884/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/HPCC-DSS-SmartCity-DependSys60770.2023.00096]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10466884/]</a>","The development of Federated Learning (FL) offers an efficient Machine Learning (ML) approach with privacy protection to solve the data island issue in distributed Internet of Things (IoT). However, existing FL frameworks still suffer from invisible attacks in IoT environments, such as free-rider attacks, backdoor attacks, and model theft. In this paper, we propose a Verifiable Property Federated Learning (VPFL) framework to overcome the above invisible attacks. We present a black-box watermarking task distribution mechanism to prevent free-rider attacks by verifying the property of local models. Our adversarial fine-tuning embedding technique can not only eliminate backdoors in global models, but also simultaneously embed white-box watermarks into model parameters to prevent model theft. Comprehensive experimental evaluations demonstrate that our framework outperforms state-of-the-art schemes in terms of security and feasibility against invisible attacks.
To evaluate the capability of VPFL in eliminating backdoor attacks, we examined the backdoor accuracy of global models with the percentages of attackers from 0% to 40…","
","IEEE
Google Scholar"
Backdoor defense method in federated learning based on contrastive training,Zhang J.,Tongxin Xuebao/Journal on Communications,2024-03-25,"<a href=""ScienceDirect (2024-03-25) : Backdoor defense method in federated learning based on contrastive training"" target=""_blank"">[https://doi.org/10.11959/j.issn.1000-436x.2024063]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.11959/j.issn.1000-436x.2024063]</a>",,,ScienceDirect
Chronic Poisoning: Backdoor Attack against Split Learning,Yu F.,Proceedings of the AAAI Conference on Artificial Intelligence,2024-03-25,"<a href=""ScienceDirect (2024-03-25) : Chronic Poisoning: Backdoor Attack against Split Learning"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i15.29591]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i15.29591]</a>",,,ScienceDirect
DATAELIXIR: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models,Zhou J.,Proceedings of the AAAI Conference on Artificial Intelligence,2024-03-25,"<a href=""ScienceDirect (2024-03-25) : DATAELIXIR: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i19.30186]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i19.30186]</a>",,,ScienceDirect
Inspecting Prediction Confidence for Detecting Black-Box Backdoor Attacks,Wang T.,Proceedings of the AAAI Conference on Artificial Intelligence,2024-03-25,"<a href=""ScienceDirect (2024-03-25) : Inspecting Prediction Confidence for Detecting Black-Box Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i1.27780]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i1.27780]</a>",,,ScienceDirect
Lotus: Evasive and resilient backdoor attacks through sub-partitioning,"S Cheng, G Tao, Y Liu, G Shen, S An…","Proceedings of the …, 2024",2024-03-25,"<a href=""Google Scholar (2024-03-25) : Lotus: Evasive and resilient backdoor attacks through sub-partitioning"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024/html/Cheng_LOTUS_Evasive_and_Resilient_Backdoor_Attacks_through_Sub-Partitioning_CVPR_2024_paper.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024/html/Cheng_LOTUS_Evasive_and_Resilient_Backdoor_Attacks_through_Sub-Partitioning_CVPR_2024_paper.html]</a>","backdoor mitigation techniques. To address and enhance both evasiveness and resilience, we introduce a novel backdoor attack … can induce the backdoor behavior. …",,Google Scholar
Provable Robustness against a Union of ℓ<inf>0</inf> Adversarial Attacks,Hammoudeh Z.,Proceedings of the AAAI Conference on Artificial Intelligence,2024-03-25,"<a href=""ScienceDirect (2024-03-25) : Provable Robustness against a Union of ℓ<inf>0</inf> Adversarial Attacks"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i19.30106]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i19.30106]</a>",,,ScienceDirect
SEER: Backdoor Detection for Vision-Language Models through Searching Target Text and Image Trigger Jointly,Zhu L.,Proceedings of the AAAI Conference on Artificial Intelligence,2024-03-25,"<a href=""ScienceDirect (2024-03-25) : SEER: Backdoor Detection for Vision-Language Models through Searching Target Text and Image Trigger Jointly"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i7.28611]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i7.28611]</a>",,,ScienceDirect
Task-agnostic detector for insertion-based backdoor attacks,"W Lyu, X Lin, S Zheng, L Pang, H Ling, S Jha…","arXiv preprint arXiv …, 2024",2024-03-25,"<a href=""Google Scholar (2024-03-25) : Task-agnostic detector for insertion-based backdoor attacks"" target=""_blank"">[https://arxiv.org/abs/2403.17155]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2403.17155]</a>","This will advance our fundamental understanding of backdoor attack and de- … the textual backdoor attack methods, the detection studies against textual backdoor attack are …",,Google Scholar
UMA: Facilitating Backdoor Scanning via Unlearning-Based Model Ablation,Zhao Y.,Proceedings of the AAAI Conference on Artificial Intelligence,2024-03-25,"<a href=""ScienceDirect (2024-03-25) : UMA: Facilitating Backdoor Scanning via Unlearning-Based Model Ablation"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i19.30183]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i19.30183]</a>",,,ScienceDirect
Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token Unlearning,"Siyuan Liang, Kuanrong Liu, Jiajun Gong, Jiawei Liang, Yuan Xun, Ee-Chien Chang, Xiaochun Cao","arXiv
arXiv","2024-03-24
2024-03","<a href=""arXiv (2024-03-24) : Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token Unlearning"" target=""_blank"">[http://arxiv.org/abs/2403.16257v1]</a>
<a href=""DBLP (2024-03) : Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token Unlearning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.16257]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.16257]</a>","Multimodal contrastive learning has emerged as a powerful paradigm for building high-quality features using the complementary strengths of various data modalities. However, the open nature of such systems inadvertently increases the possibility of backdoor attacks. These attacks subtly embed malicious behaviors within the model during training, which can be activated by specific triggers in the inference phase, posing significant security risks. Despite existing countermeasures through fine-tuning that reduce the adverse impacts of such attacks, these defenses often degrade the clean accuracy and necessitate the construction of extensive clean training pairs. In this paper, we explore the possibility of a less-cost defense from the perspective of model unlearning, that is, whether the model can be made to quickly \textbf{u}nlearn \textbf{b}ackdoor \textbf{t}hreats (UBT) by constructing a small set of poisoned samples. Specifically, we strengthen the backdoor shortcuts to discover suspicious samples through overfitting training prioritized by weak similarity samples. Building on the initial identification of suspicious samples, we introduce an innovative token-based localized forgetting training regime. This technique specifically targets the poisoned aspects of the model, applying a focused effort to unlearn the backdoor associations and trying not to damage the integrity of the overall model. Experimental results show that our method not only ensures a minimal success rate for attacks, but also preserves the model's high clean accuracy.
","
","arXiv
DBLP"
Backdoor attacks via machine unlearning,"Z Liu, T Wang, M Huai, C Miao","… of the AAAI Conference on Artificial …, 2024",2024-03-24,"<a href=""Google Scholar (2024-03-24) : Backdoor attacks via machine unlearning"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/29321]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/29321]</a>","malicious attacks leveraging machine unlearning. Specifically, we consider the backdoor attack via machine unlearning, where an attacker seeks to inject a backdoor in …",,Google Scholar
Beyond traditional threats: A persistent backdoor attack on federated learning,"T Liu, Y Zhang, Z Feng, Z Yang, C Xu, D Man…","Proceedings of the …, 2024",2024-03-24,"<a href=""Google Scholar (2024-03-24) : Beyond traditional threats: A persistent backdoor attack on federated learning"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/30131]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/30131]</a>","limited persistence of backdoor attacks in federated learning (Kemker et al. 2018). … of backdoor attacks in FL, we propose a Fully-Combination Backdoor Attack (FCBA) …",,Google Scholar
From Toxic to Trustworthy: Using Self-Distillation and Semi-supervised Methods to Refine Neural Networks,"X Zhang, B Zheng, J Hu, C Li, X Bai","Proceedings of the AAAI …, 2024",2024-03-24,"<a href=""Google Scholar (2024-03-24) : From Toxic to Trustworthy: Using Self-Distillation and Semi-supervised Methods to Refine Neural Networks"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/29629]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/29629]</a>","To evaluate the efficacy of our proposed FTT defense, we measure its performance against five backdoor attacks using two metrics, namely Attack Success Rate (ASR) …",,Google Scholar
Personalization as a shortcut for few-shot backdoor attack against text-to-image diffusion models,"Y Huang, F Juefei-Xu, Q Guo, J Zhang, Y Wu…","Proceedings of the …, 2024",2024-03-24,"<a href=""Google Scholar (2024-03-24) : Personalization as a shortcut for few-shot backdoor attack against text-to-image diffusion models"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/30110]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/30110]</a>","Traditional backdoor attacks on various deep … backdoor can only trigger broad semantic concepts such as “dog”, “cat”. As a comparison, our proposed backdoor attack, ex…",,Google Scholar
Provable Robustness against a Union of L_0 Adversarial Attacks,"Z Hammoudeh, D Lowd","Proceedings of the AAAI Conference on …, 2024",2024-03-24,"<a href=""Google Scholar (2024-03-24) : Provable Robustness against a Union of L_0 Adversarial Attacks"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/30106]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/30106]</a>","Theorem 3’s guarantees also encompass more complex ℓ0 backdoor attacks (Atr ∪ … First, FPA’s guarantees apply equally to ℓ0 evasion, poisoning, and backdoor attacks …",,Google Scholar
Spreading cybersecurity awareness via gamification: zero-day game,"Fadi Abu-Amara, Reem Al Hosani, ... Baraka Al Hamdi",International Journal of Information Technology,2024-03-24,"<a href=""Springer (2024-03-24) : Spreading cybersecurity awareness via gamification: zero-day game"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-024-01810-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-024-01810-4]</a>",This research project aims to increase the employees' cybersecurity awareness by proposing a training program that consists of pre-game and post-game...,,Springer
BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP,"Jiawang Bai, Kuofeng Gao, Shaobo Min, Shu-Tao Xia, Zhifeng Li, Wei Liu","arXiv
arXiv","2024-03-22
2023-11","<a href=""arXiv (2024-03-22) : BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP"" target=""_blank"">[http://arxiv.org/abs/2311.16194v2]</a>
<a href=""DBLP (2023-11) : BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.16194]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.16194]</a>","Contrastive Vision-Language Pre-training, known as CLIP, has shown promising effectiveness in addressing downstream image recognition tasks. However, recent works revealed that the CLIP model can be implanted with a downstream-oriented backdoor. On downstream tasks, one victim model performs well on clean samples but predicts a specific target class whenever a specific trigger is present. For injecting a backdoor, existing attacks depend on a large amount of additional data to maliciously fine-tune the entire pre-trained CLIP model, which makes them inapplicable to data-limited scenarios. In this work, motivated by the recent success of learnable prompts, we address this problem by injecting a backdoor into the CLIP model in the prompt learning stage. Our method named BadCLIP is built on a novel and effective mechanism in backdoor attacks on CLIP, i.e., influencing both the image and text encoders with the trigger. It consists of a learnable trigger applied to images and a trigger-aware context generator, such that the trigger can change text features via trigger-aware prompts, resulting in a powerful and generalizable attack. Extensive experiments conducted on 11 datasets verify that the clean accuracy of BadCLIP is similar to those of advanced prompt learning methods and the attack success rate is higher than 99% in most cases. BadCLIP is also generalizable to unseen classes, and shows a strong generalization capability under cross-dataset and cross-domain settings.
","
","arXiv
DBLP"
Evaluating the Impact of Data Preprocessing Techniques on the Performance of Intrusion Detection Systems,"Kelson Carvalho Santos, Rodrigo Sanches Miani, Flávio de Oliveira Silva",Journal of Network and Systems Management,2024-03-22,"<a href=""Springer (2024-03-22) : Evaluating the Impact of Data Preprocessing Techniques on the Performance of Intrusion Detection Systems"" target=""_blank"">[https://link.springer.com/article/10.1007/s10922-024-09813-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10922-024-09813-z]</a>",The development of Intrusion Detection Systems using Machine Learning techniques (ML-based IDS) has emerged as an important research topic in the...,,Springer
"Information Security and Cryptology: 19th International Conference, Inscrypt 2023, Hangzhou, China, December 9-10, 2023, Revised Selected Papers, Part I",C Ge,2024,2024-03-22,"<a href=""Google Scholar (2024-03-22) : Information Security and Cryptology: 19th International Conference, Inscrypt 2023, Hangzhou, China, December 9-10, 2023, Revised Selected Papers, Part I"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=B2T8EAAAQBAJ&oi=fnd&pg=PP6&dq=backdoor+attack&ots=GtMXzMAFTY&sig=DpQl8kVqTE8Zm9WWpjuzIkmYk5s]</a>","<a href=""Google Scholar"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=B2T8EAAAQBAJ&oi=fnd&pg=PP6&dq=backdoor+attack&ots=GtMXzMAFTY&sig=DpQl8kVqTE8Zm9WWpjuzIkmYk5s]</a>","threshold signature scheme TBLS is existentially unforgeable under chosen message attack, then TVESâð is also existentially unforgeable under chosen message attack. …",,Google Scholar
Minimalism is King! High-Frequency Energy-based Screening for Data-Efficient Backdoor Attacks,"Y Xun, X Jia, J Gu, X Liu, Q Guo…","IEEE Transactions on …, 2024",2024-03-22,"<a href=""Google Scholar (2024-03-22) : Minimalism is King! High-Frequency Energy-based Screening for Data-Efficient Backdoor Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10478135/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10478135/]</a>","are data-inefficient backdoor attacks. … backdoor attacks [10]– [12]. By injecting a small number of poisoned samples into the benign training set, the backdoor attack can …",,Google Scholar
Hybrid Machine Learning Model for Efficient Botnet Attack Detection in IoT Environment,M. Ali M. Shahroz M. F. Mushtaq S. Alfarhood M. Safran I. Ashraf,IEEE Access,2024-03-21,"<a href=""IEEE (2024-03-21) : Hybrid Machine Learning Model for Efficient Botnet Attack Detection in IoT Environment"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10468594]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2024.3376400]</a>","Cyber attacks are growing with the rapid development and wide use of internet technology. Botnet attack emerged as one of the most harmful attacks. Botnet identification is becoming challenging due to the numerous attack vectors and the ongoing evolution of viruses. As the Internet of Things (IoT) technology is developing rapidly, many network devices have been subject to botnet attacks leading to substantial losses in different sectors. Botnets pose serious risks to network security and deep learning models have shown potential for efficiently identifying botnet activity from network traffic data. In this research, a botnet identification system is proposed based on the stacking of artificial neural network (ANN), convolutional neural network (CNN), long short-term memory (LSTM), and recurrent neural network (RNN) (ACLR). The experiments are conducted by employing both the individual models, as well as, the proposed ACLR model for performance comparison. The UNSW-NB15 dataset is used for botnet attacks and contains nine different attack types including ‘Normal’, ‘Generic’, ‘Exploits’, ‘Fuzzers’, ‘DoS’, ‘Reconnaissance’, ‘Analysis’, ‘Backdoor’, ‘Shell code’ and ‘Worms’. Experimental results indicate the proposed ACLR model gains 0.9698 testing accuracy showing that it is successful in capturing the intricate patterns and characteristics of botnet attacks. The proposed ACLR model’s k values (3, 5, 7, and 10) for a K-fold cross-validation accuracy score is 0.9749 indicating that the model’s robustness and generalizability are demonstrated by k = 5. In addition, the proposed model detects botnets with a high receiver operating characteristic area under the curve (ROC-AUC) of 0.9934 and a precision-recall area under the curve (PR-AUC) of 0.9950. Performance comparison with existing state-of-the-art models further corroborates the superior performance of the proposed approach. The results of this research can be helpful against evolving threats and enhance cyber security procedures.",,IEEE
"Threats, attacks, and defenses in machine unlearning: A survey","Z Liu, H Ye, C Chen, KY Lam","arXiv preprint arXiv:2403.13682, 2024",2024-03-21,"<a href=""Google Scholar (2024-03-21) : Threats, attacks, and defenses in machine unlearning: A survey"" target=""_blank"">[https://arxiv.org/abs/2403.13682]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2403.13682]</a>","prevalent attacks fulfill diverse roles within MU systems. For instance, unlearning can act as a mechanism to recover models from backdoor attacks, while backdoor attacks …",,Google Scholar
BadEdit: Backdooring large language models by model editing,"Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing Liu, Wenhan Wang, Tianwei Zhang, Yang Liu","arXiv
arXiv","2024-03-20
2024-03","<a href=""arXiv (2024-03-20) : BadEdit: Backdooring large language models by model editing"" target=""_blank"">[http://arxiv.org/abs/2403.13355v1]</a>
<a href=""DBLP (2024-03) : BadEdit: Backdooring large language models by model editing"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.13355]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.13355]</a>","Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models (LLMs). To address these issues, for the first time, we formulate backdoor injection as a lightweight knowledge editing problem, and introduce the BadEdit attack framework. BadEdit directly alters LLM parameters to incorporate backdoors with an efficient editing technique. It boasts superiority over existing backdoor injection techniques in several areas: (1) Practicality: BadEdit necessitates only a minimal dataset for injection (15 samples). (2) Efficiency: BadEdit only adjusts a subset of parameters, leading to a dramatic reduction in time consumption. (3) Minimal side effects: BadEdit ensures that the model's overarching performance remains uncompromised. (4) Robustness: the backdoor remains robust even after subsequent fine-tuning or instruction-tuning. Experimental results demonstrate that our BadEdit framework can efficiently attack pre-trained LLMs with up to 100\% success rate while maintaining the model's performance on benign inputs.
","
","arXiv
DBLP"
BAFFLE: Hiding Backdoors in Offline Reinforcement Learning Datasets,"Chen Gong, Zhou Yang, Yunpeng Bai, Junda He, Jieke Shi, Kecen Li, Arunesh Sinha, Bowen Xu, Xinwen Hou, David Lo, Tianhao Wang",arXiv,2024-03-20,"<a href=""arXiv (2024-03-20) : BAFFLE: Hiding Backdoors in Offline Reinforcement Learning Datasets"" target=""_blank"">[http://arxiv.org/abs/2210.04688v5]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Reinforcement learning (RL) makes an agent learn from trial-and-error experiences gathered during the interaction with the environment. Recently, offline RL has become a popular RL paradigm because it saves the interactions with environments. In offline RL, data providers share large pre-collected datasets, and others can train high-quality agents without interacting with the environments. This paradigm has demonstrated effectiveness in critical tasks like robot control, autonomous driving, etc. However, less attention is paid to investigating the security threats to the offline RL system. This paper focuses on backdoor attacks, where some perturbations are added to the data (observations) such that given normal observations, the agent takes high-rewards actions, and low-reward actions on observations injected with triggers. In this paper, we propose Baffle (Backdoor Attack for Offline Reinforcement Learning), an approach that automatically implants backdoors to RL agents by poisoning the offline RL dataset, and evaluate how different offline RL algorithms react to this attack. Our experiments conducted on four tasks and four offline RL algorithms expose a disquieting fact: none of the existing offline RL algorithms is immune to such a backdoor attack. More specifically, Baffle modifies 10\% of the datasets for four tasks (3 robotic controls and 1 autonomous driving). Agents trained on the poisoned datasets perform well in normal settings. However, when triggers are presented, the agents' performance decreases drastically by 63.2\%, 53.9\%, 64.7\%, and 47.4\% in the four tasks on average. The backdoor still persists after fine-tuning poisoned agents on clean datasets. We further show that the inserted backdoor is also hard to be detected by a popular defensive method. This paper calls attention to developing more effective protection for the open-source offline RL dataset.",,arXiv
FedSV: Byzantine-Robust Federated Learning via Shapley Value,"K Otmani, R El-Azouzi, V Labatut","IEEE International Conference on …, 2024",2024-03-20,"<a href=""Google Scholar (2024-03-20) : FedSV: Byzantine-Robust Federated Learning via Shapley Value"" target=""_blank"">[https://hal.science/hal-04512814/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://hal.science/hal-04512814/]</a>","clients can collude and carry out coordinated attacks against the global model. We focus on Backdoor attacks, which are the most realistic type of attack for FL [20] and we …",,Google Scholar
Machine learning and deep learning techniques for internet of things network anomaly detection—current research trends,"SH Rafique, A Abdallah, NS Musa, T Murugan","Sensors, 2024",2024-03-20,"<a href=""Google Scholar (2024-03-20) : Machine learning and deep learning techniques for internet of things network anomaly detection—current research trends"" target=""_blank"">[https://www.mdpi.com/1424-8220/24/6/1968]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.mdpi.com/1424-8220/24/6/1968]</a>","attacks, DDoS assault, SQL injection, and back-door … but possesses a large attack surface which presents more … the increasing number of attacks. It reviews recent work …",,Google Scholar
Mask-based Invisible Backdoor Attacks on Object Detection,SJ Jin,"arXiv preprint arXiv:2405.09550, 2024",2024-03-20,"<a href=""Google Scholar (2024-03-20) : Mask-based Invisible Backdoor Attacks on Object Detection"" target=""_blank"">[https://arxiv.org/abs/2405.09550]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2405.09550]</a>","However, backdoor attacks must be invisible to avoid human inspection during … first invisible backdoor attack for object detection models. The proposed attack uses small …",,Google Scholar
基于 BERT 和 XGBoost 的 Webshell 检测方案,张育铭， 李浩华， 郭现峰,"西南民族大学学报 (自然科学版), 2024",2024-03-20,"<a href=""Google Scholar (2024-03-20) : 基于 BERT 和 XGBoost 的 Webshell 检测方案"" target=""_blank"">[http://nature.swun.edu.cn/ch/reader/view_abstract.aspx?file_no=20211122002]</a>","<a href=""Google Scholar"" target=""_blank"">[http://nature.swun.edu.cn/ch/reader/view_abstract.aspx?file_no=20211122002]</a>",", the backdoor program of Web services, is a common means of hacker attack. The … positive rate when detecting the Webshell backdoor which is mutated and confused-…",,Google Scholar
Cyberattacks Using ChatGPT: Exploring Malicious Content Generation Through Prompt Engineering,L. Alotaibi S. Seher N. Mohammad,"2024 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS)
2024 ASU International …, 2024","2024-03-19
2024-01-29","<a href=""IEEE (2024-03-19) : Cyberattacks Using ChatGPT: Exploring Malicious Content Generation Through Prompt Engineering"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459698]</a>
<a href=""Google Scholar (2024-01-29) : Cyberattacks Using ChatGPT: Exploring Malicious Content Generation Through Prompt Engineering"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10459698/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICETSIS61505.2024.10459698]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10459698/]</a>","The emergence of ChatGPT within the realm of computing has provided considerable advantages to a diverse array of individuals. However, it has also become a tool employed by adversaries to execute cyberattacks. This research paper examines the implementation of prompt engineering as a means to coerce ChatGPT into generating malicious content that deviates from its ethical boundaries. By leveraging these techniques, cybercriminals can effortlessly create a range of attacks, including phishing attempts, creating and propagating malware, backdoor attacks, and impersonation schemes, often in conjunction with deep fakes. To substantiate these cases, we successfully present concrete evidence by prompt engineering, which enabled the production of convincing phishing emails and code snippets for malware generation such as keyloggers. Additionally, we address the pressing concern of defending against these malicious activities, exploring effective approaches such as AI-generated text detection and system vulnerability detection.
In deep learning, backdoor attacks represent a type of adversarial attack that targets these models. In this attack, the attacker introduces tainted data to the victim’s training …","
","IEEE
Google Scholar"
Backdoor Attack Influence Assessment and Analysis on Multi-Agent Training,Y. Cai P. Liu D. Xu X. Fan Y. Wu K. Chen L. Chu Q. Li E. Tong W. Niu J. Liu,2023 China Automation Congress (CAC),2024-03-19,"<a href=""IEEE (2024-03-19) : Backdoor Attack Influence Assessment and Analysis on Multi-Agent Training"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10450244]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/CAC59555.2023.10450244]</a>","Recent researches have validated the possibility of exploiting backdoor vulnerabilities in deep reinforcement learning (DRL) systems. However, existing attack methods have limitations in designing effective trigger mechanisms. These methods often rely on heuristic or random search approaches to determine trigger parameters, lacking an automatic approach to evaluate the impact of various trigger configurations on the success of backdoor attacks. This paper introduces a novel backdoor attack method based on Bayesian optimization, which automatically assesses the impact of different trigger configurations on attack effectiveness and provides guidance for optimizing triggers to maximize their impact. We validate our method through extensive experiments conducted on two classical cooperative multi-agent reinforcement learning (CMARL) algorithms, VDN and QMIX, within widely-adopted CMARL gaming platform Star-Craft Multi-Agent Challenge (SMAC). The experimental results demonstrate that different trigger parameter configurations have a significant impact on backdoor attacks.",,IEEE
Dormant Neural Trojans,F. Fu P. Kiourti W. Li,2023 International Conference on Machine Learning and Applications (ICMLA),2024-03-19,"<a href=""IEEE (2024-03-19) : Dormant Neural Trojans"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10459770]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICMLA58977.2023.00271]</a>","We present a novel methodology for neural network backdoor attacks. Unlike existing training-time attacks where the Trojaned network would respond to the Trojan trigger after training, our approach inserts a Trojan that will remain dormant until it is activated. The activation is realized through a specific perturbation to the network's weight parameters only known to the attacker. Our analysis and the experimental results demonstrate that dormant Trojaned networks can effectively evade detection by state-of-the-art backdoor detection methods.",,IEEE
Breaking Speaker Recognition with Paddingback,Z. Ye D. Yan L. Dong K. Shen,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
ICASSP 2024-2024 IEEE …, 2024","2024-03-18
2024-03-29","<a href=""IEEE (2024-03-18) : Breaking Speaker Recognition with Paddingback"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10448169]</a>
<a href=""Google Scholar (2024-03-29) : Breaking Speaker Recognition with Paddingback"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10448169/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICASSP48485.2024.10448169]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10448169/]</a>","Machine Learning as a Service (MLaaS) has gained popularity due to advancements in Deep Neural Networks (DNNs). However, untrusted third-party platforms have raised concerns about AI security, particularly in backdoor attacks. Recent research has shown that speech backdoors can utilize transformations as triggers, similar to image backdoors. However, human ears can easily be aware of these transformations, leading to suspicion. In this paper, we propose PaddingBack, an inaudible backdoor attack that utilizes malicious operations to generate poisoned samples, rendering them indistinguishable from clean ones. Instead of using external perturbations as triggers, we exploit the widely-used speech signal operation, padding, to break speaker recognition systems. Experimental results demonstrate the effectiveness of our method, achieving a significant attack success rate while retaining benign accuracy. Furthermore, Padding-Back demonstrates the ability to resist defense methods and maintain its stealthiness against human perception.
security, particularly in backdoor attacks. Recent research has … , an inaudible backdoor attack that utilizes malicious … our attack with three representative backdoor attacks: …","
","IEEE
Google Scholar"
FIBA: Federated Invisible Backdoor Attack,L. Zhang B. Zheng,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
ICASSP 2024-2024 IEEE International …, 2024","2024-03-18
2024-03-29","<a href=""IEEE (2024-03-18) : FIBA: Federated Invisible Backdoor Attack"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446910]</a>
<a href=""Google Scholar (2024-03-29) : FIBA: Federated Invisible Backdoor Attack"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10446910/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICASSP48485.2024.10446910]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10446910/]</a>","Although previous studies have proposed backdoor attacks in Federated Learning (FL), the introduced triggers in these works are easily detectable by human eyes. Besides, this paper also shows that traditional invisible centralized backdoor attacks struggle to work well in FL scenarios. To address this issue, we propose the Federated Invisible Backdoor Attack (FIBA), a novel approach to invisible backdoor attacks against FL. FIBA can balance effectiveness and stealthiness through the auto-adjusted Quality-of-Experience (QoE) restriction and attention-based L2 regularization. It further enhances durability by targeting stable neural connections. Experimental results indicate FIBA’s superior effectiveness, comparable stealthiness, and increased durability over existing methods, demonstrating its ability to bypass current detection mechanisms and robust aggregation techniques in FL. Our codes are available here 1.
Besides, this paper also shows that traditional invisible centralized backdoor attacks … ble Backdoor Attack (FIBA), a novel approach to invisible backdoor attacks against FL. …","
","IEEE
Google Scholar"
Enhancing Adversarial Training with Prior Knowledge Distillation for Robust Image Compression,Z. Cao Y. Bao F. Meng C. Li W. Tan G. Wang Y. Liang,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
ICASSP 2024-2024 …, 2024","2024-03-18
2024-03-12","<a href=""IEEE (2024-03-18) : Enhancing Adversarial Training with Prior Knowledge Distillation for Robust Image Compression"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10445939]</a>
<a href=""Google Scholar (2024-03-12) : Enhancing Adversarial Training with Prior Knowledge Distillation for Robust Image Compression"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10445939/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICASSP48485.2024.10445939]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10445939/]</a>","Deep neural network-based image compression (NIC) has achieved excellent performance, but NIC method models have been shown to be susceptible to backdoor attacks. Adversarial training has been validated in image compression models as a common method to enhance model robustness. However, the improvement effect of adversarial training on model robustness is limited. In this paper, we propose a prior knowledge-guided adversarial training framework for image compression models. Specifically, first, we propose a gradient regularization constraint for training robust teacher models. Subsequently, we design a knowledge distillation-based strategy to generate a priori knowledge from the teacher model to the student model for guiding adversarial training. Experimental results show that our method improves the reconstruction quality by about 9dB when the Kodak dataset is elected as the backdoor attack object for psnr attack. Compared with Ma2023 [1], our method has a 5dB higher PSNR output at high bitrate points.
have been shown to be susceptible to backdoor attacks. Adversarial training has been … dataset is elected as the backdoor attack object for psnr attack. Compared with …","
","IEEE
Google Scholar"
Impart: An Imperceptible and Effective Label-Specific Backdoor Attack,"Jingke Zhao, Zan Wang, Yongwei Wang, Lanjun Wang","arXiv
arXiv","2024-03-18
2024-03","<a href=""arXiv (2024-03-18) : Impart: An Imperceptible and Effective Label-Specific Backdoor Attack"" target=""_blank"">[http://arxiv.org/abs/2403.13017v1]</a>
<a href=""DBLP (2024-03) : Impart: An Imperceptible and Effective Label-Specific Backdoor Attack"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.13017]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.13017]</a>","Backdoor attacks have been shown to impose severe threats to real security-critical scenarios. Although previous works can achieve high attack success rates, they either require access to victim models which may significantly reduce their threats in practice, or perform visually noticeable in stealthiness. Besides, there is still room to improve the attack success rates in the scenario that different poisoned samples may have different target labels (a.k.a., the all-to-all setting). In this study, we propose a novel imperceptible backdoor attack framework, named Impart, in the scenario where the attacker has no access to the victim model. Specifically, in order to enhance the attack capability of the all-to-all setting, we first propose a label-specific attack. Different from previous works which try to find an imperceptible pattern and add it to the source image as the poisoned image, we then propose to generate perturbations that align with the target label in the image feature by a surrogate model. In this way, the generated poisoned images are attached with knowledge about the target class, which significantly enhances the attack capability.
","
","arXiv
DBLP"
Invisible Backdoor Attack Through Singular Value Decomposition,"Wenmin Chen, Xiaowei Xu","arXiv
arXiv","2024-03-18
2024-03","<a href=""arXiv (2024-03-18) : Invisible Backdoor Attack Through Singular Value Decomposition"" target=""_blank"">[http://arxiv.org/abs/2403.13018v1]</a>
<a href=""DBLP (2024-03) : Invisible Backdoor Attack Through Singular Value Decomposition"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.13018]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.13018]</a>","With the widespread application of deep learning across various domains, concerns about its security have grown significantly. Among these, backdoor attacks pose a serious security threat to deep neural networks (DNNs). In recent years, backdoor attacks on neural networks have become increasingly sophisticated, aiming to compromise the security and trustworthiness of models by implanting hidden, unauthorized functionalities or triggers, leading to misleading predictions or behaviors. To make triggers less perceptible and imperceptible, various invisible backdoor attacks have been proposed. However, most of them only consider invisibility in the spatial domain, making it easy for recent defense methods to detect the generated toxic images.To address these challenges, this paper proposes an invisible backdoor attack called DEBA. DEBA leverages the mathematical properties of Singular Value Decomposition (SVD) to embed imperceptible backdoors into models during the training phase, thereby causing them to exhibit predefined malicious behavior under specific trigger conditions. Specifically, we first perform SVD on images, and then replace the minor features of trigger images with those of clean images, using them as triggers to ensure the effectiveness of the attack. As minor features are scattered throughout the entire image, the major features of clean images are preserved, making poisoned images visually indistinguishable from clean ones. Extensive experimental evaluations demonstrate that DEBA is highly effective, maintaining high perceptual quality and a high attack success rate for poisoned images. Furthermore, we assess the performance of DEBA under existing defense measures, showing that it is robust and capable of significantly evading and resisting the effects of these defense measures.
","
","arXiv
DBLP"
A Spatiotemporal Backdoor Attack Against Behavior-Oriented Decision Makers in Metaverse: From Perspective of Autonomous Driving,Y. Yu J. Liu H. Guo B. Mao N. Kato,"IEEE Journal on Selected Areas in Communications
IEEE Journal on Selected …, 2023","2024-03-18
2023-12-22","<a href=""IEEE (2024-03-18) : A Spatiotemporal Backdoor Attack Against Behavior-Oriented Decision Makers in Metaverse: From Perspective of Autonomous Driving"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10368076]</a>
<a href=""Google Scholar (2023-12-22) : A Spatiotemporal Backdoor Attack Against Behavior-Oriented Decision Makers In Metaverse: From Perspective of Autonomous Driving"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10368076/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/JSAC.2023.3345379]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10368076/]</a>","Behavior-oriented decision-makers are critical components in generating intelligent decisions for user virtual interactions in metaverse. In this work, we study the efficiency and security of behavior-oriented decision-makers in metaverse from perspective of autonomous driving (AD), where modeling human uncertain driving behaviors is the key factor of their performance. We first explore the ability of different deep-neural-network-based decision-makers used in deep reinforcement learning for efficient autonomous vehicle control, and then we propose a novel neural backdoor attack against them using spatiotemporal driving behaviors, rather than an immediate state. With our attack, the adversary acts as a normal driver and can trigger attacks by driving his vehicle following specific spatiotemporal behaviors. Extensive experiments show that our proposed backdoor attack can achieve high stealthiness and effectiveness (less than 1% clean performance variance rate and more than 98% attack success rate) on behavior-oriented decision-makers, and is sustainable against existing advanced defenses.
backdoor attacks under different AD scenarios. The experimental results demonstrate that our backdoor attack … AV decision-maker, its attack success rate is more than 98%…","
","IEEE
Google Scholar"
NWS: Natural Textual Backdoor Attacks Via Word Substitution,W. Du T. Yuan H. Zhao G. Liu,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",2024-03-18,"<a href=""IEEE (2024-03-18) : NWS: Natural Textual Backdoor Attacks Via Word Substitution"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10447968]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICASSP48485.2024.10447968]</a>","Backdoor attacks pose a serious security threat for natural language processing (NLP). Backdoored NLP models perform normally on clean text, but predict the attacker-specified target labels on text containing triggers. Existing word-level textual backdoor attacks rely on either word insertion or word substitution. Word-insertion backdoor attacks can be easily detected by simple backdoor defenses. Meanwhile, word-substitution backdoor attacks tend to substantially degrade the fluency and semantic consistency of the poisoned text. In this paper, we propose a more natural word substitution method to implement covert textual backdoor attacks. Specifically, we combine three different ways to construct a diverse synonym thesaurus for clean text. We then train a learnable word selector for producing poisoned text using a composite loss function of poison and fidelity terms. This enables automated selection of minimal critical word substitutions necessary to induce the backdoor. Experiments demonstrate our method achieves high attack performance with less impact on fluency and semantics. We hope this work can raise awareness regarding the threat of subtle, fluent word substitution attacks.",,IEEE
PoisonPrompt: Backdoor Attack on Prompt-Based Large Language Models,H. Yao J. Lou Z. Qin,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",2024-03-18,"<a href=""IEEE (2024-03-18) : PoisonPrompt: Backdoor Attack on Prompt-Based Large Language Models"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446267]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICASSP48485.2024.10446267]</a>","Prompts have significantly improved the performance of pre-trained Large Language Models (LLMs) on various downstream tasks recently, making them increasingly indispensable for a diverse range of LLM application scenarios. However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model’s normal predictions, has not been sufficiently explored for prompt-based LLMs. In this paper, we present PoisonPrompt, a novel backdoor attack capable of successfully compromising both hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and robustness of PoisonPrompt through extensive experiments on three popular prompt methods, using six datasets and three widely used LLMs. Our findings highlight the potential security threats posed by backdoor attacks on prompt-based LLMs and emphasize the need for further research in this area.",,IEEE
SPY-Watermark: Robust Invisible Watermarking for Backdoor Attack,R. Wang R. Wan Z. Guo Q. Guo R. Huang,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",2024-03-18,"<a href=""IEEE (2024-03-18) : SPY-Watermark: Robust Invisible Watermarking for Backdoor Attack"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10448363]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICASSP48485.2024.10448363]</a>","Backdoor attack aims to deceive a victim model when facing backdoor instances while maintaining its performance on benign data. Current methods use manual patterns or special perturbations as triggers, while they often overlook the robustness against data corruption, making backdoor attacks easy to defend in practice. To address this issue, we propose a novel backdoor attack method named Spy-Watermark, which remains effective when facing data collapse and backdoor defense. Therein, we introduce a learnable watermark embedded in the latent domain of images, serving as the trigger. Then, we search for a watermark that can withstand collapse during image decoding, cooperating with several anti-collapse operations to further enhance the resilience of our trigger against data corruption. Extensive experiments are conducted on CIFAR10, GTSRB, and ImageNet datasets, demonstrating that Spy-Watermark overtakes ten state-of-the-art methods in terms of robustness and stealthiness.",,IEEE
Software Vulnerability Detection under Poisoning Attacks using CNN-based Image Processing,"L González-Manzano, J Garcia-Alfaro",2024,2024-03-18,"<a href=""Google Scholar (2024-03-18) : Software Vulnerability Detection under Poisoning Attacks using CNN-based Image Processing"" target=""_blank"">[https://e-archivo.uc3m.es/bitstreams/106bf518-9e33-465e-8b27-3c42f53545b3/download]</a>","<a href=""Google Scholar"" target=""_blank"">[https://e-archivo.uc3m.es/bitstreams/106bf518-9e33-465e-8b27-3c42f53545b3/download]</a>","In this paper, we apply backdoor attacks, considered a type of targeted ones, in which P contains a chosen pattern, called backdoor trigger (BT), whose execution to get an …",,Google Scholar
Stealthy Backdoor Attack Towards Federated Automatic Speaker Verification,L. Zhang L. Liu D. Meng J. Wang S. Hu,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",2024-03-18,"<a href=""IEEE (2024-03-18) : Stealthy Backdoor Attack Towards Federated Automatic Speaker Verification"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446189]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICASSP48485.2024.10446189]</a>","Automatic speech verification (ASV) authenticates individuals based on distinct vocal patterns, playing a pivotal role in many applications such as voice-based unlocking systems for devices. The ASV system comprises three stages: training, registration, and validation. The model refines using voice data in training, extracts vocal features in registration, and contrasts these with speech patterns in validation. Modern ASV models, primarily grounded in DNN architectures, require extensive data for training. Federated learning (FL) fosters model-sharing across multiple clients while ensuring data privacy. Due to its open architecture, FL is vulnerable to backdoor attacks. However, training a stealthy backdoor attack in FL presents challenges, including diminished attack generalization owing to data heterogeneity, and conspicuous triggers that render them easily detectable. In this paper, we propose a Federated Stealthy Backdoor Attack method ($FedSBA$). FedSBA aims to improve the attack model’s generalization, enhance its persistence, and elude anomaly detection under the heterogeneous data distribution. FedSBA constructs an attack model based on a personalized transformer and encompasses a stealthy trigger. Moreover, we also propose a defensive strategy that utilizes an adaptive weight aggregation scheme. The stealthiness and effectiveness of FedSBA are demonstrated by exhibiting superior performance in comparison to previous works.",,IEEE
Ten-Guard: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks,K. M. Hossain T. Oates,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",2024-03-18,"<a href=""IEEE (2024-03-18) : Ten-Guard: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10448222]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICASSP48485.2024.10448222]</a>","As deep neural networks and the datasets used to train them get larger, the default approach to integrating them into research and commercial projects is to download a pre-trained model and fine tune it. But these models can have uncertain provenance, opening up the possibility that they embed hidden malicious behavior such as trojans or backdoors, where small changes to an input (triggers) can cause the model to produce incorrect outputs (e.g., to misclassify). This paper introduces a novel approach to backdoor detection that uses two tensor decomposition methods applied to network activations. This has a number of advantages relative to existing detection methods, including the ability to analyze multiple models at the same time, working across a wide variety of network architectures, making no assumptions about the nature of triggers used to alter network behavior, and being computationally efficient. We provide a detailed description of the detection pipeline along with results on models trained on the MNIST digit dataset, CIFAR-10 dataset, and two difficult datasets from NIST’s TrojAI competition. These results show that our method detects backdoored networks more accurately and efficiently than current state-of-the-art methods.",,IEEE
An Overview of Techniques for Obfuscated Android Malware Detection,"Sidra Siddiqui, Tamim Ahmed Khan",SN Computer Science,2024-03-16,"<a href=""Springer (2024-03-16) : An Overview of Techniques for Obfuscated Android Malware Detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-024-02637-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-024-02637-3]</a>","Obfuscation is a method to hide coding strategies for security and privacy. Despite its positive use, malware experts have also used this technique...",,Springer
"School of Computer Science, Carleton University, Ottawa, Canada 2 School of Computer Science, Carleton University, Ottawa, Canada 3 Department of …",,,2024-03-16,"<a href=""Google Scholar (2024-03-16) : School of Computer Science, Carleton University, Ottawa, Canada 2 School of Computer Science, Carleton University, Ottawa, Canada 3 Department of …"" target=""_blank"">[https://www.researchgate.net/profile/Behzad-Beigzadeh/publication/378906576_Federated_Learning_Attacks_Defenses_Opportunities_and_Challenges/links/65f12f2c286738732d3c6526/Federated-Learning-Attacks-Defenses-Opportunities-and-Challenges.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.researchgate.net/profile/Behzad-Beigzadeh/publication/378906576_Federated_Learning_Attacks_Defenses_Opportunities_and_Challenges/links/65f12f2c286738732d3c6526/Federated-Learning-Attacks-Defenses-Opportunities-and-Challenges.pdf]</a>","Communication bottlenecks, poisoning, and backdoor attacks are identified as the three most major security dangers, while privacy concerns are found to be less prevalent…",,Google Scholar
Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency,"Soumyadeep Pal, Yuguang Yao, Ren Wang, Bingquan Shen, Sijia Liu","arXiv
arXiv","2024-03-15
2024-03","<a href=""arXiv (2024-03-15) : Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency"" target=""_blank"">[http://arxiv.org/abs/2403.10717v1]</a>
<a href=""DBLP (2024-03) : Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.10717]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.10717]</a>","Modern machine learning (ML) systems demand substantial training data, often resorting to external sources. Nevertheless, this practice renders them vulnerable to backdoor poisoning attacks. Prior backdoor defense strategies have primarily focused on the identification of backdoored models or poisoned data characteristics, typically operating under the assumption of access to clean data. In this work, we delve into a relatively underexplored challenge: the automatic identification of backdoor data within a poisoned dataset, all under realistic conditions, i.e., without the need for additional clean data or without manually defining a threshold for backdoor detection. We draw an inspiration from the scaled prediction consistency (SPC) technique, which exploits the prediction invariance of poisoned data to an input scaling factor. Based on this, we pose the backdoor data identification problem as a hierarchical data splitting optimization problem, leveraging a novel SPC-based loss function as the primary optimization objective. Our innovation unfolds in several key aspects. First, we revisit the vanilla SPC method, unveiling its limitations in addressing the proposed backdoor identification problem. Subsequently, we develop a bi-level optimization-based approach to precisely identify backdoor data by minimizing the advanced SPC loss. Finally, we demonstrate the efficacy of our proposal against a spectrum of backdoor attacks, encompassing basic label-corrupted attacks as well as more sophisticated clean-label attacks, evaluated across various benchmark datasets. Experiment results show that our approach often surpasses the performance of current baselines in identifying backdoor data points, resulting in about 4%-36% improvement in average AUROC. Codes are available at https://github.com/OPTML-Group/BackdoorMSPC.
","<a href=""arXiv"" target=""_blank"">[https://github.com/OPTML-Group/BackdoorMSPC]</a>
","arXiv
DBLP"
Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control,"Ka-Ho Chow, Wenqi Wei, Lei Yu","arXiv
arXiv","2024-03-15
2024-01","<a href=""arXiv (2024-03-15) : Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control"" target=""_blank"">[http://arxiv.org/abs/2401.01085v2]</a>
<a href=""DBLP (2024-01) : Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.01085]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.01085]</a>","Natural language processing (NLP) has received unprecedented attention. While advancements in NLP models have led to extensive research into their backdoor vulnerabilities, the potential for these advancements to introduce new backdoor threats remains unexplored. This paper proposes Imperio, which harnesses the language understanding capabilities of NLP models to enrich backdoor attacks. Imperio provides a new model control experience. Demonstrated through controlling image classifiers, it empowers the adversary to manipulate the victim model with arbitrary output through language-guided instructions. This is achieved using a language model to fuel a conditional trigger generator, with optimizations designed to extend its language understanding capabilities to backdoor instruction interpretation and execution. Our experiments across three datasets, five attacks, and nine defenses confirm Imperio's effectiveness. It can produce contextually adaptive triggers from text descriptions and control the victim model with desired outputs, even in scenarios not encountered during training. The attack reaches a high success rate across complex datasets without compromising the accuracy of clean inputs and exhibits resilience against representative defenses.
","
","arXiv
DBLP"
Synthesizing Physical Backdoor Datasets: An Automated Framework Leveraging Deep Generative Models,"Sze Jue Yang, Chinh D. La, Quang H. Nguyen, Kok-Seng Wong, Anh Tuan Tran, Chee Seng Chan, Khoa D. Doan","arXiv
arXiv","2024-03-15
2023-12","<a href=""arXiv (2024-03-15) : Synthesizing Physical Backdoor Datasets: An Automated Framework Leveraging Deep Generative Models"" target=""_blank"">[http://arxiv.org/abs/2312.03419v3]</a>
<a href=""DBLP (2023-12) : Synthesizing Physical Backdoor Datasets: An Automated Framework Leveraging Deep Generative Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.03419]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.03419]</a>","Backdoor attacks, representing an emerging threat to the integrity of deep neural networks, have garnered significant attention due to their ability to compromise deep learning systems clandestinely. While numerous backdoor attacks occur within the digital realm, their practical implementation in real-world prediction systems remains limited and vulnerable to disturbances in the physical world. Consequently, this limitation has given rise to the development of physical backdoor attacks, where trigger objects manifest as physical entities within the real world. However, creating the requisite dataset to train or evaluate a physical backdoor model is a daunting task, limiting the backdoor researchers and practitioners from studying such physical attack scenarios. This paper unleashes a recipe that empowers backdoor researchers to effortlessly create a malicious, physical backdoor dataset based on advances in generative modeling. Particularly, this recipe involves 3 automatic modules: suggesting the suitable physical triggers, generating the poisoned candidate samples (either by synthesizing new samples or editing existing clean samples), and finally refining for the most plausible ones. As such, it effectively mitigates the perceived complexity associated with creating a physical backdoor dataset, transforming it from a daunting task into an attainable objective. Extensive experiment results show that datasets created by our ""recipe"" enable adversaries to achieve an impressive attack success rate on real physical world data and exhibit similar properties compared to previous physical backdoor attack studies. This paper offers researchers a valuable toolkit for studies of physical backdoors, all within the confines of their laboratories.
","
","arXiv
DBLP"
Guarding Against the Unknown: Deep Transfer Learning for Hardware Image-Based Malware Detection,"Zhangying He, Houman Homayoun, Hossein Sayadi",Journal of Hardware and Systems Security,2024-03-15,"<a href=""Springer (2024-03-15) : Guarding Against the Unknown: Deep Transfer Learning for Hardware Image-Based Malware Detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-024-00146-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-024-00146-6]</a>","Malware is increasingly becoming a significant threat to computing systems, and detecting zero-day (unknown) malware is crucial to ensure the...",,Springer
LLRing: Logarithmic Linkable Ring Signatures with Transparent Setup,"X Hui, SCK Chau","Cryptology ePrint Archive, 2024",2024-03-15,"<a href=""Google Scholar (2024-03-15) : LLRing: Logarithmic Linkable Ring Signatures with Transparent Setup"" target=""_blank"">[https://eprint.iacr.org/2024/421]</a>","<a href=""Google Scholar"" target=""_blank"">[https://eprint.iacr.org/2024/421]</a>","To eliminate backdoor and overhead in a trusted setup, transparent setup in the … identify an attack on DualDory that breaks its linkability. (2) To eliminate such attacks, we …",,Google Scholar
One-to-Multiple Clean-Label Image Camouflage (OmClic) based backdoor attack on deep learning,Wang G.,Knowledge-Based Systems,2024-03-15,"<a href=""ScienceDirect (2024-03-15) : One-to-Multiple Clean-Label Image Camouflage (OmClic) based backdoor attack on deep learning"" target=""_blank"">[https://doi.org/10.1016/j.knosys.2024.111456]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.knosys.2024.111456]</a>",,,ScienceDirect
REPQC: Reverse Engineering and Backdooring Hardware Accelerators for Post-quantum Cryptography,"Samuel Pagliarini, Aikata Aikata, Malik Imran, Sujoy Sinha Roy","arXiv
arXiv","2024-03-14
2024-03","<a href=""arXiv (2024-03-14) : REPQC: Reverse Engineering and Backdooring Hardware Accelerators for Post-quantum Cryptography"" target=""_blank"">[http://arxiv.org/abs/2403.09352v1]</a>
<a href=""DBLP (2024-03) : REPQC: Reverse Engineering and Backdooring Hardware Accelerators for Post-quantum Cryptography"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.09352]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.09352]</a>","Significant research efforts have been dedicated to designing cryptographic algorithms that are quantum-resistant. The motivation is clear: robust quantum computers, once available, will render current cryptographic standards vulnerable. Thus, we need new Post-Quantum Cryptography (PQC) algorithms, and, due to the inherent complexity of such algorithms, there is also a demand to accelerate them in hardware. In this paper, we show that PQC hardware accelerators can be backdoored by two different adversaries located in the chip supply chain. We propose REPQC, a sophisticated reverse engineering algorithm that can be employed to confidently identify hashing operations (i.e., Keccak) within the PQC accelerator - the location of which serves as an anchor for finding secret information to be leaked. Armed with REPQC, an adversary proceeds to insert malicious logic in the form of a stealthy Hardware Trojan Horse (HTH). Using Dilithium as a study case, our results demonstrate that HTHs that increase the accelerator's layout density by as little as 0.1\% can be inserted without any impact on the performance of the circuit and with a marginal increase in power consumption. An essential aspect is that the entire reverse engineering in REPQC is automated, and so is the HTH insertion that follows it, empowering adversaries to explore multiple HTH designs and identify the most suitable one.
","
","arXiv
DBLP"
Poison Attack and Poison Detection on Deep Source Code Processing Models,Li ♂ J.,ACM Transactions on Software Engineering and Methodology,2024-03-14,"<a href=""ScienceDirect (2024-03-14) : Poison Attack and Poison Detection on Deep Source Code Processing Models"" target=""_blank"">[https://doi.org/10.1145/3630008]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3630008]</a>",,,ScienceDirect
Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks,"Khondoker Murad Hossain, Tim Oates","arXiv
arXiv","2024-03-13
2024-03","<a href=""arXiv (2024-03-13) : Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2403.08208v1]</a>
<a href=""DBLP (2024-03) : Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.08208]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.08208]</a>","In the rapidly evolving landscape of communication and network security, the increasing reliance on deep neural networks (DNNs) and cloud services for data processing presents a significant vulnerability: the potential for backdoors that can be exploited by malicious actors. Our approach leverages advanced tensor decomposition algorithms Independent Vector Analysis (IVA), Multiset Canonical Correlation Analysis (MCCA), and Parallel Factor Analysis (PARAFAC2) to meticulously analyze the weights of pre-trained DNNs and distinguish between backdoored and clean models effectively. The key strengths of our method lie in its domain independence, adaptability to various network architectures, and ability to operate without access to the training data of the scrutinized models. This not only ensures versatility across different application scenarios but also addresses the challenge of identifying backdoors without prior knowledge of the specific triggers employed to alter network behavior. We have applied our detection pipeline to three distinct computer vision datasets, encompassing both image classification and object detection tasks. The results demonstrate a marked improvement in both accuracy and efficiency over existing backdoor detection methods. This advancement enhances the security of deep learning and AI in networked systems, providing essential cybersecurity against evolving threats in emerging technologies.
","
","arXiv
DBLP"
Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective,"Zhen Qin, Feiyi Chen, Chen Zhi, Xueqiang Yan, Shuiguang Deng","arXiv
AAAI
arXiv","2024-03-13
2024
2023-09","<a href=""arXiv (2024-03-13) : Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective"" target=""_blank"">[http://arxiv.org/abs/2309.16456v2]</a>
<a href=""DBLP (2024) : Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i13.29385]</a>
<a href=""DBLP (2023-09) : Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.16456]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i13.29385]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i13.29385]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.16456]</a>","Existing approaches defend against backdoor attacks in federated learning (FL) mainly through a) mitigating the impact of infected models, or b) excluding infected models. The former negatively impacts model accuracy, while the latter usually relies on globally clear boundaries between benign and infected model updates. However, model updates are easy to be mixed and scattered throughout in reality due to the diverse distributions of local data. This work focuses on excluding infected models in FL. Unlike previous perspectives from a global view, we propose Snowball, a novel anti-backdoor FL framework through bidirectional elections from an individual perspective inspired by one principle deduced by us and two principles in FL and deep learning. It is characterized by a) bottom-up election, where each candidate model update votes to several peer ones such that a few model updates are elected as selectees for aggregation, and b) top-down election, where selectees progressively enlarge themselves through picking up from the candidates. We compare Snowball with state-of-the-art defenses to backdoor attacks in FL on five real-world datasets, demonstrating its superior resistance to backdoor attacks and slight impact on the accuracy of the global model.

","

","arXiv
DBLP
DBLP"
A Comprehensive Defense Framework Against Model Extraction Attacks,W. Jiang H. Li G. Xu T. Zhang R. Lu,IEEE Transactions on Dependable and Secure Computing,2024-03-13,"<a href=""IEEE (2024-03-13) : A Comprehensive Defense Framework Against Model Extraction Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10080996]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TDSC.2023.3261327]</a>","As a promising service, Machine Learning as a Service (MLaaS) provides personalized inference functions for clients through paid APIs. Nevertheless, it is vulnerable to model extraction attacks, in which an attacker can extract a functionally-equivalent model by repeatedly querying the APIs with crafted samples. While numerous works have been proposed to defend against model extraction attacks, existing efforts are accompanied by limitations and low comprehensiveness. In this article, we propose AMAO, a comprehensive defense framework against model extraction attacks. Specifically, AMAO consists of four interlinked successive phases: adversarial training is first exploited to weaken the effectiveness of model extraction attacks. Then, malicious query detection is used to detect malicious queries and mark malicious users. After that, we develop a label-flipping poisoning attack to instruct the adaptive query responses to malicious users. Besides, the image pHash algorithm is employed to ensure the indistinguishability of the query responses. Finally, the perturbed results are served as a backdoor to verify the ownership of any suspicious model. Extensive experiments demonstrate that AMAO outperforms existing defenses in defending against model extraction attacks and is also robust against the adaptive adversary who is aware of the defense.",,IEEE
"A survey on IoT application layer protocols, security challenges, and the role of explainable AI in IoT (XAIoT)","Vagner E. Quincozes, Silvio E. Quincozes, ... Anis Koubaa",International Journal of Information Security,2024-03-13,"<a href=""Springer (2024-03-13) : A survey on IoT application layer protocols, security challenges, and the role of explainable AI in IoT (XAIoT)"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-024-00828-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-024-00828-w]</a>","The Internet of Things (IoT) plays a fundamental role in contemporary society, necessitating an in-depth comprehension of its application layer...",,Springer
Analisa Forensik Kontainer Podman Terhadap Backdoor Metasploit Menggunakan Checkpointctl,"HA Sya'bani, C Umam, LB Handoko","Jurnal Transformatika, 2024",2024-03-13,"<a href=""Google Scholar (2024-03-13) : Analisa Forensik Kontainer Podman Terhadap Backdoor Metasploit Menggunakan Checkpointctl"" target=""_blank"">[https://journals.usm.ac.id/index.php/transformatika/article/view/8109]</a>","<a href=""Google Scholar"" target=""_blank"">[https://journals.usm.ac.id/index.php/transformatika/article/view/8109]</a>",The isolated environment in container system does not make cyber attacks impossible to … was running a malicious program in the form of a backdoor with a PHP extension. …,,Google Scholar
Backdoor Attack with Mode Mixture Latent Modification,"Hongwei Zhang, Xiaoyin Xu, Dongsheng An, Xianfeng Gu, Min Zhang","arXiv
arXiv","2024-03-12
2024-03","<a href=""arXiv (2024-03-12) : Backdoor Attack with Mode Mixture Latent Modification"" target=""_blank"">[http://arxiv.org/abs/2403.07463v1]</a>
<a href=""DBLP (2024-03) : Backdoor Attack with Mode Mixture Latent Modification"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.07463]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.07463]</a>","Backdoor attacks become a significant security concern for deep neural networks in recent years. An image classification model can be compromised if malicious backdoors are injected into it. This corruption will cause the model to function normally on clean images but predict a specific target label when triggers are present. Previous research can be categorized into two genres: poisoning a portion of the dataset with triggered images for users to train the model from scratch, or training a backdoored model alongside a triggered image generator. Both approaches require significant amount of attackable parameters for optimization to establish a connection between the trigger and the target label, which may raise suspicions as more people become aware of the existence of backdoor attacks. In this paper, we propose a backdoor attack paradigm that only requires minimal alterations (specifically, the output layer) to a clean model in order to inject the backdoor under the guise of fine-tuning. To achieve this, we leverage mode mixture samples, which are located between different modes in latent space, and introduce a novel method for conducting backdoor attacks. We evaluate the effectiveness of our method on four popular benchmark datasets: MNIST, CIFAR-10, GTSRB, and TinyImageNet.
","
","arXiv
DBLP"
Intrusion detection system: a deep neural network-based concatenated approach,"Hidangmayum Satyajeet Sharma, Khundrakpam Johnson Singh",The Journal of Supercomputing,2024-03-12,"<a href=""Springer (2024-03-12) : Intrusion detection system: a deep neural network-based concatenated approach"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-024-05994-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-024-05994-1]</a>","In recent years, the field of information security has seen a substantial rise in the use of approaches that include deep learning. The...",,Springer
AS-FIBA: Adaptive Selective Frequency-Injection for Backdoor Attack on Deep Face Restoration,"Zhenbo Song, Wenhao Gao, Kaihao Zhang, Wenhan Luo, Zhaoxin Fan, Jianfeng Lu","arXiv
arXiv","2024-03-11
2024-03","<a href=""arXiv (2024-03-11) : AS-FIBA: Adaptive Selective Frequency-Injection for Backdoor Attack on Deep Face Restoration"" target=""_blank"">[http://arxiv.org/abs/2403.06430v1]</a>
<a href=""DBLP (2024-03) : AS-FIBA: Adaptive Selective Frequency-Injection for Backdoor Attack on Deep Face Restoration"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.06430]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.06430]</a>","Deep learning-based face restoration models, increasingly prevalent in smart devices, have become targets for sophisticated backdoor attacks. These attacks, through subtle trigger injection into input face images, can lead to unexpected restoration outcomes. Unlike conventional methods focused on classification tasks, our approach introduces a unique degradation objective tailored for attacking restoration models. Moreover, we propose the Adaptive Selective Frequency Injection Backdoor Attack (AS-FIBA) framework, employing a neural network for input-specific trigger generation in the frequency domain, seamlessly blending triggers with benign images. This results in imperceptible yet effective attacks, guiding restoration predictions towards subtly degraded outputs rather than conspicuous targets. Extensive experiments demonstrate the efficacy of the degradation objective on state-of-the-art face restoration models. Additionally, it is notable that AS-FIBA can insert effective backdoors that are more imperceptible than existing backdoor attack methods, including WaNet, ISSBA, and FIBA.
","
","arXiv
DBLP"
"Federated Learning: Attacks, Defenses, Opportunities, and Challenges","G Shirvani, S Ghasemshirazi, B Beigzadeh","arXiv preprint arXiv …, 2024",2024-03-11,"<a href=""Google Scholar (2024-03-11) : Federated Learning: Attacks, Defenses, Opportunities, and Challenges"" target=""_blank"">[https://arxiv.org/abs/2403.06067]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2403.06067]</a>","Targeted attacks, known as Backdoor attacks, are … attacks in this category is the Sybil attack. Genuinely, the model is exposed to multiple backdoor attacks in Sybil attacks, …",,Google Scholar
RSBA: Robust Statistical Backdoor Attack under Privilege-Constrained Scenarios,"Xiaolei Liu, Ming Yi, Kangyi Ding, Bangzhou Xin, Yixiao Xu, Li Yan, Chao Shen",arXiv,2024-03-11,"<a href=""arXiv (2024-03-11) : RSBA: Robust Statistical Backdoor Attack under Privilege-Constrained Scenarios"" target=""_blank"">[http://arxiv.org/abs/2304.10985v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Learning-based systems have been demonstrated to be vulnerable to backdoor attacks, wherein malicious users manipulate model performance by injecting backdoors into the target model and activating them with specific triggers. Previous backdoor attack methods primarily focused on two key metrics: attack success rate and stealthiness. However, these methods often necessitate significant privileges over the target model, such as control over the training process, making them challenging to implement in real-world scenarios. Moreover, the robustness of existing backdoor attacks is not guaranteed, as they prove sensitive to defenses such as image augmentations and model distillation. In this paper, we address these two limitations and introduce RSBA (Robust Statistical Backdoor Attack under Privilege-constrained Scenarios). The key insight of RSBA is that statistical features can naturally divide images into different groups, offering a potential implementation of triggers. This type of trigger is more robust than manually designed ones, as it is widely distributed in normal images. By leveraging these statistical triggers, RSBA enables attackers to conduct black-box attacks by solely poisoning the labels or the images. We empirically and theoretically demonstrate the robustness of RSBA against image augmentations and model distillation. Experimental results show that RSBA achieves a 99.83\% attack success rate in black-box scenarios. Remarkably, it maintains a high success rate even after model distillation, where attackers lack access to the training dataset of the student model (1.39\% success rate for baseline methods on average).",,arXiv
MirrorAttack: Backdoor Attack on 3D Point Cloud with a Distorting Mirror,"Yuhao Bian, Shengjing Tian, Xiuping Liu","arXiv
arXiv","2024-03-09
2024-03","<a href=""arXiv (2024-03-09) : MirrorAttack: Backdoor Attack on 3D Point Cloud with a Distorting Mirror"" target=""_blank"">[http://arxiv.org/abs/2403.05847v1]</a>
<a href=""DBLP (2024-03) : MirrorAttack: Backdoor Attack on 3D Point Cloud with a Distorting Mirror"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.05847]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.05847]</a>","The widespread deployment of Deep Neural Networks (DNNs) for 3D point cloud processing starkly contrasts with their susceptibility to security breaches, notably backdoor attacks. These attacks hijack DNNs during training, embedding triggers in the data that, once activated, cause the network to make predetermined errors while maintaining normal performance on unaltered data. This vulnerability poses significant risks, especially given the insufficient research on robust defense mechanisms for 3D point cloud networks against such sophisticated threats. Existing attacks either struggle to resist basic point cloud pre-processing methods, or rely on delicate manual design. Exploring simple, effective, imperceptible, and difficult-to-defend triggers in 3D point clouds is still challenging.To address these challenges, we introduce MirrorAttack, a novel effective 3D backdoor attack method, which implants the trigger by simply reconstructing a clean point cloud with an auto-encoder. The data-driven nature of the MirrorAttack obviates the need for complex manual design. Minimizing the reconstruction loss automatically improves imperceptibility. Simultaneously, the reconstruction network endows the trigger with pronounced nonlinearity and sample specificity, rendering traditional preprocessing techniques ineffective in eliminating it. A trigger smoothing module based on spherical harmonic transformation is also attached to regulate the intensity of the attack.Both quantitive and qualitative results verify the effectiveness of our method. We achieve state-of-the-art ASR on different types of victim models with the intervention of defensive techniques. Moreover, the minimal perturbation introduced by our trigger, as assessed by various metrics, attests to the method's stealth, ensuring its imperceptibility.
","
","arXiv
DBLP"
Secure and Trustworthy Large Language Models,Y Wang,ICLR 2024 Workshops,2024-03-09,"<a href=""Google Scholar (2024-03-09) : Secure and Trustworthy Large Language Models"" target=""_blank"">[https://openreview.net/forum?id=gj8tSqSvoc]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=gj8tSqSvoc]</a>",and prevention ● Security of LLM deployment ● Backdoor attacks and defenses in LLMs ● Adversarial attacks and defenses in LLMs ● Toxic speech detection and …,,Google Scholar
Invariant Aggregator for Defending against Federated Backdoor Attacks,"Xiaoyang Wang, Dimitrios Dimitriadis, Sanmi Koyejo, Shruti Tople","arXiv
AISTATS","2024-03-08
2024","<a href=""arXiv (2024-03-08) : Invariant Aggregator for Defending against Federated Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2210.01834v4]</a>
<a href=""DBLP (2024) : Invariant Aggregator for Defending against Federated Backdoor Attacks"" target=""_blank"">[https://proceedings.mlr.press/v238/wang24e.html]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://proceedings.mlr.press/v238/wang24e.html]</a>","Federated learning enables training high-utility models across several clients without directly sharing their private data. As a downside, the federated setting makes the model vulnerable to various adversarial attacks in the presence of malicious clients. Despite the theoretical and empirical success in defending against attacks that aim to degrade models' utility, defense against backdoor attacks that increase model accuracy on backdoor samples exclusively without hurting the utility on other samples remains challenging. To this end, we first analyze the failure modes of existing defenses over a flat loss landscape, which is common for well-designed neural networks such as Resnet (He et al., 2015) but is often overlooked by previous works. Then, we propose an invariant aggregator that redirects the aggregated update to invariant directions that are generally useful via selectively masking out the update elements that favor few and possibly malicious clients. Theoretical results suggest that our approach provably mitigates backdoor attacks and remains effective over flat loss landscapes. Empirical results on three datasets with different modalities and varying numbers of clients further demonstrate that our approach mitigates a broad class of backdoor attacks with a negligible cost on the model utility.
","
","arXiv
DBLP"
Cybercrimes in the Associated World,"R Gunavathi, KM Bharathi",Artificial Intelligence for Cyber Defense and …,2024-03-07,"<a href=""Google Scholar (2024-03-07) : Cybercrimes in the Associated World"" target=""_blank"">[https://www.taylorfrancis.com/chapters/edit/10.1201/9781003251781-2/cybercrimes-associated-world-gunavathi-mani-bharathi]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.taylorfrancis.com/chapters/edit/10.1201/9781003251781-2/cybercrimes-associated-world-gunavathi-mani-bharathi]</a>","But when we scroll back to the past, the first cyber and computer worm attacks … attack and to provide remedial measures to stop those attacks in a better way (Figure 2.3). …",,Google Scholar
Learn to unlearn: Insights into machine unlearning,"Y Qu, X Yuan, M Ding, W Ni, T Rakotoarivelo…","Computer, 2024",2024-03-07,"<a href=""Google Scholar (2024-03-07) : Learn to unlearn: Insights into machine unlearning"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10461690/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10461690/]</a>","backdoor patterns, risks associated with backdoor injection, and potential susceptibility to backdoor attack … Membership inference is a type of privacy-oriented attack …",,Google Scholar
On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder,"Tingxu Han, Shenghan Huang, Ziqi Ding, Weisong Sun, Yebo Feng, Chunrong Fang, Jun Li, Hanwei Qian, Cong Wu, Quanjun Zhang, Yang Liu, Zhenyu Chen","arXiv
arXiv","2024-03-06
2024-03","<a href=""arXiv (2024-03-06) : On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder"" target=""_blank"">[http://arxiv.org/abs/2403.03846v1]</a>
<a href=""DBLP (2024-03) : On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.03846]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.03846]</a>","In this paper, we study a defense against poisoned encoders in SSL called distillation, which is a defense used in supervised learning originally. Distillation aims to distill knowledge from a given model (a.k.a the teacher net) and transfer it to another (a.k.a the student net). Now, we use it to distill benign knowledge from poisoned pre-trained encoders and transfer it to a new encoder, resulting in a clean pre-trained encoder. In particular, we conduct an empirical study on the effectiveness and performance of distillation against poisoned encoders. Using two state-of-the-art backdoor attacks against pre-trained image encoders and four commonly used image classification datasets, our experimental results show that distillation can reduce attack success rate from 80.87% to 27.51% while suffering a 6.35% loss in accuracy. Moreover, we investigate the impact of three core components of distillation on performance: teacher net, student net, and distillation loss. By comparing 4 different teacher nets, 3 student nets, and 6 distillation losses, we find that fine-tuned teacher nets, warm-up-training-based student nets, and attention-based distillation loss perform best, respectively.
","
","arXiv
DBLP"
Low-Frequency Black-Box Backdoor Attack via Evolutionary Algorithm,"Yanqi Qiao, Dazhuang Liu, Rui Wang, Kaitai Liang","arXiv
arXiv","2024-03-06
2024-02","<a href=""arXiv (2024-03-06) : Low-Frequency Black-Box Backdoor Attack via Evolutionary Algorithm"" target=""_blank"">[http://arxiv.org/abs/2402.15653v2]</a>
<a href=""DBLP (2024-02) : Low-Frequency Black-Box Backdoor Attack via Evolutionary Algorithm"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.15653]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.15653]</a>","While convolutional neural networks (CNNs) have achieved success in computer vision tasks, it is vulnerable to backdoor attacks. Such attacks could mislead the victim model to make attacker-chosen prediction with a specific trigger pattern. Until now, the trigger injection of existing attacks is mainly limited to spatial domain. Recent works take advantage of perceptual properties of planting specific patterns in the frequency domain, which only reflect indistinguishable pixel-wise perturbations in pixel domain. However, in the black-box setup, the inaccessibility of training process often renders more complex trigger designs. Existing frequency attacks simply handcraft the magnitude of spectrum, introducing anomaly frequency disparities between clean and poisoned data and taking risks of being removed by image processing operations (such as lossy compression and filtering). In this paper, we propose a robust low-frequency black-box backdoor attack (LFBA), which minimally perturbs low-frequency components of frequency spectrum and maintains the perceptual similarity in spatial space simultaneously. The key insight of our attack restrict the search for the optimal trigger to low-frequency region that can achieve high attack effectiveness, robustness against image transformation defenses and stealthiness in dual space. We utilize simulated annealing (SA), a form of evolutionary algorithm, to optimize the properties of frequency trigger including the number of manipulated frequency bands and the perturbation of each frequency component, without relying on the knowledge from the victim classifier. Extensive experiments on real-world datasets verify the effectiveness and robustness of LFBA against image processing operations and the state-of-the-art backdoor defenses, as well as its inherent stealthiness in both spatial and frequency space, making it resilient against frequency inspection.
","
","arXiv
DBLP"
Machine Learning Security Against Data Poisoning: Are We There Yet?,A. E. Cinà K. Grosse A. Demontis B. Biggio F. Roli M. Pelillo,Computer,2024-03-06,"<a href=""IEEE (2024-03-06) : Machine Learning Security Against Data Poisoning: Are We There Yet?"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10461694]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/MC.2023.3299572]</a>","Poisoning attacks compromise the training data utilized to train machine learning (ML) models, diminishing their overall performance, manipulating predictions on specific test samples, and implanting backdoors. This article thoughtfully explores these attacks while discussing strategies to mitigate them through fundamental security principles or by implementing defensive mechanisms tailored for ML.",,IEEE
Network Intrusion Detection by using a Sequential Deep Neural Network with an Extra Tree Classifier,,,2024-03-06,"<a href=""Google Scholar (2024-03-06) : Network Intrusion Detection by using a Sequential Deep Neural Network with an Extra Tree Classifier"" target=""_blank"">[https://www.mukpublications.com/resources/37.%20Muhammad%20Farhan.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.mukpublications.com/resources/37.%20Muhammad%20Farhan.pdf]</a>","fast-growing network attacks (eg, Fuzzers, Ransomware attacks, Backdoor, etc.), as it … deep learning to identify different types of network attacks for multi-class. Moreover, …",,Google Scholar
A general approach to enhance the survivability of backdoor attacks by decision path coupling,"Yufei Zhao, Dingji Wang, Bihuan Chen, Ziqian Chen, Xin Peng","arXiv
arXiv","2024-03-05
2024-03","<a href=""arXiv (2024-03-05) : A general approach to enhance the survivability of backdoor attacks by decision path coupling"" target=""_blank"">[http://arxiv.org/abs/2403.02950v1]</a>
<a href=""DBLP (2024-03) : A general approach to enhance the survivability of backdoor attacks by decision path coupling"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.02950]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.02950]</a>","Backdoor attacks have been one of the emerging security threats to deep neural networks (DNNs), leading to serious consequences. One of the mainstream backdoor defenses is model reconstruction-based. Such defenses adopt model unlearning or pruning to eliminate backdoors. However, little attention has been paid to survive from such defenses. To bridge the gap, we propose Venom, the first generic backdoor attack enhancer to improve the survivability of existing backdoor attacks against model reconstruction-based defenses. We formalize Venom as a binary-task optimization problem. The first is the original backdoor attack task to preserve the original attack capability, while the second is the attack enhancement task to improve the attack survivability. To realize the second task, we propose attention imitation loss to force the decision path of poisoned samples in backdoored models to couple with the crucial decision path of benign samples, which makes backdoors difficult to eliminate. Our extensive evaluation on two DNNs and three datasets has demonstrated that Venom significantly improves the survivability of eight state-of-the-art attacks against eight state-of-the-art defenses without impacting the capability of the original attacks.
","
","arXiv
DBLP"
BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning,"Siyuan Liang, Mingli Zhu, Aishan Liu, Baoyuan Wu, Xiaochun Cao, Ee-Chien Chang",arXiv,2024-03-04,"<a href=""arXiv (2024-03-04) : BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning"" target=""_blank"">[http://arxiv.org/abs/2311.12075v3]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Studying backdoor attacks is valuable for model copyright protection and enhancing defenses. While existing backdoor attacks have successfully infected multimodal contrastive learning models such as CLIP, they can be easily countered by specialized backdoor defenses for MCL models. This paper reveals the threats in this practical scenario that backdoor attacks can remain effective even after defenses and introduces the \emph{\toolns} attack, which is resistant to backdoor detection and model fine-tuning defenses. To achieve this, we draw motivations from the perspective of the Bayesian rule and propose a dual-embedding guided framework for backdoor attacks. Specifically, we ensure that visual trigger patterns approximate the textual target semantics in the embedding space, making it challenging to detect the subtle parameter variations induced by backdoor learning on such natural trigger patterns. Additionally, we optimize the visual trigger patterns to align the poisoned samples with target vision features in order to hinder the backdoor unlearning through clean fine-tuning. Extensive experiments demonstrate that our attack significantly outperforms state-of-the-art baselines (+45.3% ASR) in the presence of SoTA backdoor defenses, rendering these mitigation and detection strategies virtually ineffective. Furthermore, our approach effectively attacks some more rigorous scenarios like downstream tasks. We believe that this paper raises awareness regarding the potential threats associated with the practical application of multimodal contrastive learning and encourages the development of more robust defense mechanisms.",,arXiv
Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering,"Rui Zhu, Di Tang, Siyuan Tang, Guanhong Tao, Shiqing Ma, Xiaofeng Wang, Haixu Tang","arXiv
arXiv","2024-03-02
2023-01","<a href=""arXiv (2024-03-02) : Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering"" target=""_blank"">[http://arxiv.org/abs/2301.12318v2]</a>
<a href=""DBLP (2023-01) : Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.12318]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.12318]</a>","Most existing methods to detect backdoored machine learning (ML) models take one of the two approaches: trigger inversion (aka. reverse engineer) and weight analysis (aka. model diagnosis). In particular, the gradient-based trigger inversion is considered to be among the most effective backdoor detection techniques, as evidenced by the TrojAI competition, Trojan Detection Challenge and backdoorBench. However, little has been done to understand why this technique works so well and, more importantly, whether it raises the bar to the backdoor attack. In this paper, we report the first attempt to answer this question by analyzing the change rate of the backdoored model around its trigger-carrying inputs. Our study shows that existing attacks tend to inject the backdoor characterized by a low change rate around trigger-carrying inputs, which are easy to capture by gradient-based trigger inversion. In the meantime, we found that the low change rate is not necessary for a backdoor attack to succeed: we design a new attack enhancement called \textit{Gradient Shaping} (GRASP), which follows the opposite direction of adversarial training to reduce the change rate of a backdoored model with regard to the trigger, without undermining its backdoor effect. Also, we provide a theoretic analysis to explain the effectiveness of this new technique and the fundamental weakness of gradient-based trigger inversion. Finally, we perform both theoretical and experimental analysis, showing that the GRASP enhancement does not reduce the effectiveness of the stealthy attacks against the backdoor detection methods based on weight analysis, as well as other backdoor mitigation methods without using detection.
","
","arXiv
DBLP"
AutoPKI: public key infrastructure for IoT with automated trust transfer,"Joel Höglund, Simon Bouget, ... Shahid Raza",International Journal of Information Security,2024-03-02,"<a href=""Springer (2024-03-02) : AutoPKI: public key infrastructure for IoT with automated trust transfer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-024-00825-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-024-00825-z]</a>","IoT deployments grow in numbers and size, which makes questions of long-term support and maintainability increasingly important. Without scalable and...",,Springer
Backdoor Attacks and Defense in FL,"S Li, H Zhu, W Wu, X Shen","Backdoor Attacks against Learning-Based …, 2024",2024-03-02,"<a href=""Google Scholar (2024-03-02) : Backdoor Attacks and Defense in FL"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-57389-7_5]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-57389-7_5]</a>","Figure 5.2 illustrates the whole flowchart of our backdoor attack. The upper part of the … and backdoor. Meanwhile, the lower part depicts our backdoor attack against two …",,Google Scholar
Enhanced pelican optimization algorithm with ensemble-based anomaly detection in industrial internet of things environment,"Nenavath Chander, Mummadi Upendra Kumar",Cluster Computing,2024-03-02,"<a href=""Springer (2024-03-02) : Enhanced pelican optimization algorithm with ensemble-based anomaly detection in industrial internet of things environment"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04303-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-024-04303-y]</a>","Anomaly detection (AD) in the industrial internet of things (IIoT) platform is said to be the major module of security the consistency, safety, and...",,Springer
Hidden Backdoor Attacks in NLP Based Network Services,"S Li, H Zhu, W Wu, X Shen","Backdoor Attacks against Learning-Based …, 2024",2024-03-02,"<a href=""Google Scholar (2024-03-02) : Hidden Backdoor Attacks in NLP Based Network Services"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-57389-7_4]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-57389-7_4]</a>","backdoor attack, dynamic sentence backdoor attack, by … models to serve as the backdoor trigger. Realizing that … to carry out the backdoor attacks by adopting these text …",,Google Scholar
Invisible Backdoor Attacks in Image Classification Based Network Services,"S Li, H Zhu, W Wu, X Shen","Backdoor Attacks against Learning-Based …, 2024",2024-03-02,"<a href=""Google Scholar (2024-03-02) : Invisible Backdoor Attacks in Image Classification Based Network Services"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-57389-7_3]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-57389-7_3]</a>","backdoor triggers. The aim of the chapter is to raise awareness about the severity of backdoor attacks … As once backdoor triggers become “invisible,” the task of detection …",,Google Scholar
Literature Review of Backdoor Attacks,"S Li, H Zhu, W Wu, X Shen","Backdoor Attacks against Learning-Based …, 2024",2024-03-02,"<a href=""Google Scholar (2024-03-02) : Literature Review of Backdoor Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-57389-7_2]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-57389-7_2]</a>","metrics for backdoor attacks, which mainly include the success rate of the attack and the availability of the model. At last, we survey related works on backdoor attacks to …",,Google Scholar
ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks,"Eleanor Clifford, Ilia Shumailov, Yiren Zhao, Ross Anderson, Robert Mullins","arXiv
SaTML
arXiv","2024-03-01
2024
2022-10","<a href=""arXiv (2024-03-01) : ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks"" target=""_blank"">[http://arxiv.org/abs/2210.00108v4]</a>
<a href=""DBLP (2024) : ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks"" target=""_blank"">[https://doi.org/10.1109/SaTML59370.2024.00024]</a>
<a href=""DBLP (2022-10) : ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.00108]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1109/SaTML59370.2024.00024]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SaTML59370.2024.00024]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.00108]</a>","Early backdoor attacks against machine learning set off an arms race in attack and defence development. Defences have since appeared demonstrating some ability to detect backdoors in models or even remove them. These defences work by inspecting the training data, the model, or the integrity of the training procedure. In this work, we show that backdoors can be added during compilation, circumventing any safeguards in the data preparation and model training stages. The attacker can not only insert existing weight-based backdoors during compilation, but also a new class of weight-independent backdoors, such as ImpNet. These backdoors are impossible to detect during the training or data preparation processes, because they are not yet present. Next, we demonstrate that some backdoors, including ImpNet, can only be reliably detected at the stage where they are inserted and removing them anywhere else presents a significant challenge. We conclude that ML model security requires assurance of provenance along the entire technical pipeline, including the data, model architecture, compiler, and hardware specification.

","

","arXiv
DBLP
DBLP"
Backdoor Attack Defense Method for Federated Learning Based on Model Watermarking,Guo J.J.,Jisuanji Xuebao/Chinese Journal of Computers,2024-03-01,"<a href=""ScienceDirect (2024-03-01) : Backdoor Attack Defense Method for Federated Learning Based on Model Watermarking"" target=""_blank"">[https://doi.org/10.11897/SP.J.1016.2024.00662]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.11897/SP.J.1016.2024.00662]</a>",,,ScienceDirect
Backdoor Attacks on Graph Neural Networks Trained with Data Augmentation,Yashiki S.,"IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences",2024-03-01,"<a href=""ScienceDirect (2024-03-01) : Backdoor Attacks on Graph Neural Networks Trained with Data Augmentation"" target=""_blank"">[https://doi.org/10.1587/transfun.2023CIL0007]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1587/transfun.2023CIL0007]</a>",,,ScienceDirect
Backdoor attack detection via prediction trustworthiness assessment,Zhong N.,Information Sciences,2024-03-01,"<a href=""ScienceDirect (2024-03-01) : Backdoor attack detection via prediction trustworthiness assessment"" target=""_blank"">[https://doi.org/10.1016/j.ins.2024.120283]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.ins.2024.120283]</a>",,,ScienceDirect
Comprehensive Privacy Analysis on Federated Recommender System Against Attribute Inference Attacks,Zhang S.,IEEE Transactions on Knowledge and Data Engineering,2024-03-01,"<a href=""ScienceDirect (2024-03-01) : Comprehensive Privacy Analysis on Federated Recommender System Against Attribute Inference Attacks"" target=""_blank"">[https://doi.org/10.1109/TKDE.2023.3295601]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TKDE.2023.3295601]</a>",,,ScienceDirect
Inoculating Software For Survivability An old adage holds true for software: you can build a stronger system by first breaking it.,,,2024-03-01,"<a href=""Google Scholar (2024-03-01) : Inoculating Software For Survivability An old adage holds true for software: you can build a stronger system by first breaking it."" target=""_blank"">[https://cacm.acm.org/research/inoculating-software-for-survivability/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://cacm.acm.org/research/inoculating-software-for-survivability/]</a>","back doors for later re-entry. Despite being called “the most organized and systematic attack” to … by the US Deputy Defense Secretary, these attacks were not the work of an …",,Google Scholar
LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario,"H Liu, Z Liu, R Tang, J Yuan, S Zhong…","arXiv preprint arXiv …, 2024",2024-03-01,"<a href=""Google Scholar (2024-03-01) : LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario"" target=""_blank"">[https://arxiv.org/abs/2403.00108]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2403.00108]</a>",We focus on the backdoor attack as an example to highlight the security concerns with LoRA adoption. Our study dives deeply into various scenarios of utilizing LoRA and …,,Google Scholar
NLPSweep: A comprehensive defense scheme for mitigating NLP backdoor attacks,Xiang T.,Information Sciences,2024-03-01,"<a href=""ScienceDirect (2024-03-01) : NLPSweep: A comprehensive defense scheme for mitigating NLP backdoor attacks"" target=""_blank"">[https://doi.org/10.1016/j.ins.2024.120176]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.ins.2024.120176]</a>",,,ScienceDirect
TridentShell: An enhanced covert and scalable backdoor injection attack on web applications,Yu X.,Journal of Network and Computer Applications,2024-03-01,"<a href=""ScienceDirect (2024-03-01) : TridentShell: An enhanced covert and scalable backdoor injection attack on web applications"" target=""_blank"">[https://doi.org/10.1016/j.jnca.2023.103823]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.jnca.2023.103823]</a>",,,ScienceDirect
Untargeted Backdoor Attack Against Deep Neural Networks With Imperceptible Trigger,Xue M.,IEEE Transactions on Industrial Informatics,2024-03-01,"<a href=""ScienceDirect (2024-03-01) : Untargeted Backdoor Attack Against Deep Neural Networks With Imperceptible Trigger"" target=""_blank"">[https://doi.org/10.1109/TII.2023.3329641]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TII.2023.3329641]</a>",,,ScienceDirect
Backdoor Two-Stream Video Models on Federated Learning,"Jing Zhao, Hongwei Yang, Hui He, Jie Peng, Weizhe Zhang, Jiangqun Ni, Arun Kumar Sangaiah, Aniello Castiglione","ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), Just Accepted",2024-03,"<a href=""ACM (2024-03) : Backdoor Two-Stream Video Models on Federated Learning"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3651307]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3651307]</a>","Video models on federated learning (FL) enable continual learning of the involved models for video tasks on end-user devices while protecting the privacy of end-user data. As a result, the security issues on FL, e.g., the backdoor attacks on FL and their ...",,ACM
Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors,"Md Abdul Kadir, Gowtham Krishna Addluri, Daniel Sonntag",arXiv,2024-03,"<a href=""DBLP (2024-03) : Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.16569]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2403.16569]</a>",,,DBLP
Model Pairing Using Embedding Translation for Backdoor Attack Detection on Open-Set Classification Tasks,"Alexander Unnervik, Hatef Otroshi Shahreza, Anjith George, Sébastien Marcel","arXiv
arXiv","2024-02-28
2024-02","<a href=""arXiv (2024-02-28) : Model Pairing Using Embedding Translation for Backdoor Attack Detection on Open-Set Classification Tasks"" target=""_blank"">[http://arxiv.org/abs/2402.18718v1]</a>
<a href=""DBLP (2024-02) : Model Pairing Using Embedding Translation for Backdoor Attack Detection on Open-Set Classification Tasks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.18718]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.18718]</a>","Backdoor attacks allow an attacker to embed a specific vulnerability in a machine learning algorithm, activated when an attacker-chosen pattern is presented, causing a specific misprediction. The need to identify backdoors in biometric scenarios has led us to propose a novel technique with different trade-offs. In this paper we propose to use model pairs on open-set classification tasks for detecting backdoors. Using a simple linear operation to project embeddings from a probe model's embedding space to a reference model's embedding space, we can compare both embeddings and compute a similarity score. We show that this score, can be an indicator for the presence of a backdoor despite models being of different architectures, having been trained independently and on different datasets. Additionally, we show that backdoors can be detected even when both models are backdoored. The source code is made available for reproducibility purposes.
","
","arXiv
DBLP"
Invisible backdoor learning in regional transform domain,"Yuyuan Sun, Yuliang Lu, ... Xuan Wang","Neural Computing and Applications
Neural Comput. Appl.","2024-02-28
2024","<a href=""Springer (2024-02-28) : Invisible backdoor learning in regional transform domain"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-024-09506-3]</a>
<a href=""DBLP (2024) : Invisible backdoor learning in regional transform domain"" target=""_blank"">[https://doi.org/10.1007/s00521-024-09506-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-024-09506-3]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/s00521-024-09506-3]</a>","The rapid developing deep learning is highly required by resources and computing resources, which easily leads to backdoor learnings. It is difficult...
","
","Springer
DBLP"
Empirical Study of Federated Unlearning: Efficiency and Effectiveness,"TH Nguyen, HP Vu, DT Nguyen…","Asian Conference …, 2024",2024-02-28,"<a href=""Google Scholar (2024-02-28) : Empirical Study of Federated Unlearning: Efficiency and Effectiveness"" target=""_blank"">[https://proceedings.mlr.press/v222/nguyen24a.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://proceedings.mlr.press/v222/nguyen24a.html]</a>",We utilize backdoor attack and Cosine Similarity to assess the effectiveness of each unlearning method. The findings and insights from this research can be integrated into …,,Google Scholar
Refiner: a reliable and efficient incentive-driven federated learning system powered by blockchain,"Hong Lin, Ke Chen, ... Gang Chen",The VLDB Journal,2024-02-28,"<a href=""Springer (2024-02-28) : Refiner: a reliable and efficient incentive-driven federated learning system powered by blockchain"" target=""_blank"">[https://link.springer.com/article/10.1007/s00778-024-00839-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00778-024-00839-y]</a>","Federated learning (FL) enables learning a model from data distributed across numerous workers while preserving data privacy. However, the classical...",,Springer
Model X-ray:Detect Backdoored Models via Decision Boundary,"Yanghao Su, Jie Zhang, Ting Xu, Tianwei Zhang, Weiming Zhang, Nenghai Yu","arXiv
arXiv","2024-02-27
2024-02","<a href=""arXiv (2024-02-27) : Model X-ray:Detect Backdoored Models via Decision Boundary"" target=""_blank"">[http://arxiv.org/abs/2402.17465v1]</a>
<a href=""DBLP (2024-02) : Model X-ray: Detect Backdoored Models via Decision Boundary"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.17465]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.17465]</a>","Deep neural networks (DNNs) have revolutionized various industries, leading to the rise of Machine Learning as a Service (MLaaS). In this paradigm, well-trained models are typically deployed through APIs. However, DNNs are susceptible to backdoor attacks, which pose significant risks to their applications. This vulnerability necessitates a method for users to ascertain whether an API is compromised before usage. Although many backdoor detection methods have been developed, they often operate under the assumption that the defender has access to specific information such as details of the attack, soft predictions from the model API, and even the knowledge of the model parameters, limiting their practicality in MLaaS scenarios. To address it, in this paper, we begin by presenting an intriguing observation: the decision boundary of the backdoored model exhibits a greater degree of closeness than that of the clean model. Simultaneously, if only one single label is infected, a larger portion of the regions will be dominated by the attacked label. Building upon this observation, we propose Model X-ray, a novel backdoor detection approach for MLaaS through the analysis of decision boundaries. Model X-ray can not only identify whether the target API is infected by backdoor attacks but also determine the target attacked label under the all-to-one attack strategy. Importantly, it accomplishes this solely by the hard prediction of clean inputs, regardless of any assumptions about attacks and prior knowledge of the training details of the model. Extensive experiments demonstrated that Model X-ray can be effective for MLaaS across diverse backdoor attacks, datasets, and architectures.
","
","arXiv
DBLP"
Improving neural network trojan detection via network abstraction,M Eiermann,2024,2024-02-27,"<a href=""Google Scholar (2024-02-27) : Improving neural network trojan detection via network abstraction"" target=""_blank"">[https://studenttheses.uu.nl/handle/20.500.12932/46064]</a>","<a href=""Google Scholar"" target=""_blank"">[https://studenttheses.uu.nl/handle/20.500.12932/46064]</a>","Next, backdoor attacks and their different types will be explained as well as two methods for detecting trojaning attacks called Neural Cleanse and Artificial Brain …",,Google Scholar
TrojanInterpret: A detecting backdoors method in DNN based on neural network interpretation methods,,,2024-02-25,"<a href=""Google Scholar (2024-02-25) : TrojanInterpret: A detecting backdoors method in DNN based on neural network interpretation methods"" target=""_blank"">[https://damdid2023.hse.ru/mirror/pubs/share/867942031.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://damdid2023.hse.ru/mirror/pubs/share/867942031.pdf]</a>","In our research, we have discovered the connection between the backdoor attacks and model interpretation. We used widespread interpretation technique activation …",,Google Scholar
Optimizing IoT intrusion detection system: feature selection versus feature extraction in machine learning,"Jing Li, Mohd Shahizan Othman, ... Lizawati Mi Yusuf",Journal of Big Data,2024-02-24,"<a href=""Springer (2024-02-24) : Optimizing IoT intrusion detection system: feature selection versus feature extraction in machine learning"" target=""_blank"">[https://link.springer.com/article/10.1186/s40537-024-00892-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s40537-024-00892-y]</a>","Internet of Things (IoT) devices are widely used but also vulnerable to cyberattacks that can cause security issues. To protect against this, machine...",,Springer
A Stealthy Backdoor Attack for Code Models,"S Huang, X Chen, X Ju, L Li, X Wang, Y Yao, Z Xiao",2024,2024-02-23,"<a href=""Google Scholar (2024-02-23) : A Stealthy Backdoor Attack for Code Models"" target=""_blank"">[https://www.researchsquare.com/article/rs-3969016/latest]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.researchsquare.com/article/rs-3969016/latest]</a>","However, previous backdoor attacks on code models have used 22 explicit triggers, … stealthy backdoor attacks in this study. To this end, we propose a backdoor 24 attack …",,Google Scholar
Adversarial Feature Map Pruning for Backdoor,"Dong Huang, Qingwen Bu",arXiv,2024-02-23,"<a href=""arXiv (2024-02-23) : Adversarial Feature Map Pruning for Backdoor"" target=""_blank"">[http://arxiv.org/abs/2307.11565v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Deep neural networks have been widely used in many critical applications, such as autonomous vehicles and medical diagnosis. However, their security is threatened by backdoor attacks, which are achieved by adding artificial patterns to specific training data. Existing defense strategies primarily focus on using reverse engineering to reproduce the backdoor trigger generated by attackers and subsequently repair the DNN model by adding the trigger into inputs and fine-tuning the model with ground-truth labels. However, once the trigger generated by the attackers is complex and invisible, the defender cannot reproduce the trigger successfully then the DNN model will not be repaired, as the trigger is not effectively removed. In this work, we propose Adversarial Feature Map Pruning for Backdoor (FMP) to mitigate backdoor from the DNN. Unlike existing defense strategies, which focus on reproducing backdoor triggers, FMP attempts to prune backdoor feature maps, which are trained to extract backdoor information from inputs. After pruning these backdoor feature maps, FMP will fine-tune the model with a secure subset of training data. Our experiments demonstrate that, compared to existing defense strategies, FMP can effectively reduce the Attack Success Rate (ASR) even against the most complex and invisible attack triggers (e.g., FMP decreases the ASR to 2.86\% in CIFAR10, which is 19.2\% to 65.41\% lower than baselines). Second, unlike conventional defense methods that tend to exhibit low robust accuracy (that is, the accuracy of the model on poisoned data), FMP achieves a higher RA, indicating its superiority in maintaining model performance while mitigating the effects of backdoor attacks (e.g., FMP obtains 87.40\% RA in CIFAR10). Our code is publicly available at: https://github.com/retsuh-bqw/FMP.","<a href=""arXiv"" target=""_blank"">[https://github.com/retsuh-bqw/FMP]</a>",arXiv
"Analysis of online gambling trojan backdoor attacks to aid web server strengthening, a case study insights on Southeast Minahasa Regency",M Solang,2024,2024-02-23,"<a href=""Google Scholar (2024-02-23) : Analysis of online gambling trojan backdoor attacks to aid web server strengthening, a case study insights on Southeast Minahasa Regency"" target=""_blank"">[http://repository.uph.edu/id/eprint/62492]</a>","<a href=""Google Scholar"" target=""_blank"">[http://repository.uph.edu/id/eprint/62492]</a>","through trojan backdoor web shell malware attacks. … , and malicious URLs to attack the target applications. Once … the majority of infiltration attacks, solidifying its position …",,Google Scholar
Protecting ownership rights of ML models using watermarking in the light of adversarial attacks,"Katarzyna Kapusta, Lucas Mattioli, ... Mohammed Lansari",AI and Ethics,2024-02-23,"<a href=""Springer (2024-02-23) : Protecting ownership rights of ML models using watermarking in the light of adversarial attacks"" target=""_blank"">[https://link.springer.com/article/10.1007/s43681-023-00412-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s43681-023-00412-3]</a>","In this paper, we present and analyze two novel—and seemingly distant—research trends in Machine Learning: ML watermarking and adversarial patches....",,Springer
Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models,"Hongbin Liu, Michael K. Reiter, Neil Zhenqiang Gong","arXiv
arXiv","2024-02-22
2024-02","<a href=""arXiv (2024-02-22) : Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models"" target=""_blank"">[http://arxiv.org/abs/2402.14977v1]</a>
<a href=""DBLP (2024-02) : Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.14977]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.14977]</a>","Foundation model has become the backbone of the AI ecosystem. In particular, a foundation model can be used as a general-purpose feature extractor to build various downstream classifiers. However, foundation models are vulnerable to backdoor attacks and a backdoored foundation model is a single-point-of-failure of the AI ecosystem, e.g., multiple downstream classifiers inherit the backdoor vulnerabilities simultaneously. In this work, we propose Mudjacking, the first method to patch foundation models to remove backdoors. Specifically, given a misclassified trigger-embedded input detected after a backdoored foundation model is deployed, Mudjacking adjusts the parameters of the foundation model to remove the backdoor. We formulate patching a foundation model as an optimization problem and propose a gradient descent based method to solve it. We evaluate Mudjacking on both vision and language foundation models, eleven benchmark datasets, five existing backdoor attacks, and thirteen adaptive backdoor attacks. Our results show that Mudjacking can remove backdoor from a foundation model while maintaining its utility.
","
","arXiv
DBLP"
Employing RNN and Petri Nets to Secure Edge Computing Threats in Smart Cities,"Hao Tian, Ruiheng Li, ... Jinpeng Wang",Journal of Grid Computing,2024-02-22,"<a href=""Springer (2024-02-22) : Employing RNN and Petri Nets to Secure Edge Computing Threats in Smart Cities"" target=""_blank"">[https://link.springer.com/article/10.1007/s10723-023-09733-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10723-023-09733-3]</a>",The Industrial Internet of Things (IIoT) revolution has led to the development a potential system that enhances communication among a city's assets....,,Springer
Learning to Poison Large Language Models During Instruction Tuning,"Y Qiang, X Zhou, SZ Zade, MA Roshani, D Zytko…","arXiv preprint arXiv …, 2024",2024-02-22,"<a href=""Google Scholar (2024-02-22) : Learning to Poison Large Language Models During Instruction Tuning"" target=""_blank"">[https://arxiv.org/abs/2402.13459]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2402.13459]</a>","of poisoning attacks. Differently, our proposed data poisoning attack learns the backdoor … Our backdoor attack exhibits several advanced properties. Firstly, it is capable of …",,Google Scholar
"Machine learning-based network intrusion detection for big and imbalanced data using oversampling, stacking feature embedding and feature extraction","Md. Alamin Talukder, Md. Manowarul Islam, ... Mohammad Ali Moni",Journal of Big Data,2024-02-22,"<a href=""Springer (2024-02-22) : Machine learning-based network intrusion detection for big and imbalanced data using oversampling, stacking feature embedding and feature extraction"" target=""_blank"">[https://link.springer.com/article/10.1186/s40537-024-00886-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s40537-024-00886-w]</a>",Cybersecurity has emerged as a critical global concern. Intrusion Detection Systems (IDS) play a critical role in protecting interconnected networks...,,Springer
Secured Digital Watermarking Using Neural Networks,"P Pal, S Ghosh, P Biswas, N Kar, JL Sarkar","… : Latest Developments and …, 2024",2024-02-22,"<a href=""Google Scholar (2024-02-22) : Secured Digital Watermarking Using Neural Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9803-6_3]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9803-6_3]</a>","The study also includes several attack scenarios in the implementation of digital … By doing this, a backdoor is created in the model that, when used with data from the …",,Google Scholar
Backdoor Attacks on Dense Passage Retrievers for Disseminating Misinformation,"Quanyu Long, Yue Deng, LeiLei Gan, Wenya Wang, Sinno Jialin Pan","arXiv
arXiv","2024-02-21
2024-02","<a href=""arXiv (2024-02-21) : Backdoor Attacks on Dense Passage Retrievers for Disseminating Misinformation"" target=""_blank"">[http://arxiv.org/abs/2402.13532v1]</a>
<a href=""DBLP (2024-02) : Backdoor Attacks on Dense Passage Retrievers for Disseminating Misinformation"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.13532]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.13532]</a>","Dense retrievers and retrieval-augmented language models have been widely used in various NLP applications. Despite being designed to deliver reliable and secure outcomes, the vulnerability of retrievers to potential attacks remains unclear, raising concerns about their security. In this paper, we introduce a novel scenario where the attackers aim to covertly disseminate targeted misinformation, such as hate speech or advertisement, through a retrieval system. To achieve this, we propose a perilous backdoor attack triggered by grammar errors in dense passage retrieval. Our approach ensures that attacked models can function normally for standard queries but are manipulated to return passages specified by the attacker when users unintentionally make grammatical mistakes in their queries. Extensive experiments demonstrate the effectiveness and stealthiness of our proposed attack method. When a user query is error-free, our model consistently retrieves accurate information while effectively filtering out misinformation from the top-k results. However, when a query contains grammar errors, our system shows a significantly higher success rate in fetching the targeted content.
","
","arXiv
DBLP"
VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models,"Jiawei Liang, Siyuan Liang, Man Luo, Aishan Liu, Dongchen Han, Ee-Chien Chang, Xiaochun Cao","arXiv
arXiv","2024-02-21
2024-02","<a href=""arXiv (2024-02-21) : VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models"" target=""_blank"">[http://arxiv.org/abs/2402.13851v1]</a>
<a href=""DBLP (2024-02) : VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.13851]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.13851]</a>","Autoregressive Visual Language Models (VLMs) showcase impressive few-shot learning capabilities in a multimodal context. Recently, multimodal instruction tuning has been proposed to further enhance instruction-following abilities. However, we uncover the potential threat posed by backdoor attacks on autoregressive VLMs during instruction tuning. Adversaries can implant a backdoor by injecting poisoned samples with triggers embedded in instructions or images, enabling malicious manipulation of the victim model's predictions with predefined triggers. Nevertheless, the frozen visual encoder in autoregressive VLMs imposes constraints on the learning of conventional image triggers. Additionally, adversaries may encounter restrictions in accessing the parameters and architectures of the victim model. To address these challenges, we propose a multimodal instruction backdoor attack, namely VL-Trojan. Our approach facilitates image trigger learning through an isolating and clustering strategy and enhance black-box-attack efficacy via an iterative character-level text trigger generation method. Our attack successfully induces target outputs during inference, significantly surpassing baselines (+62.52\%) in ASR. Moreover, it demonstrates robustness across various model scales and few-shot in-context reasoning scenarios.
","
","arXiv
DBLP"
Bitterling fish optimization (BFO) algorithm,"Lida Zareian, Javad Rahebi, Mohammad Javad Shayegan",Multimedia Tools and Applications,2024-02-21,"<a href=""Springer (2024-02-21) : Bitterling fish optimization (BFO) algorithm"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-024-18579-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-024-18579-0]</a>",The bitterling fish is a prime example of intelligent behavior in nature for survival. The bitterling fish uses the oyster spawning strategy as their...,,Springer
Deep neural networks watermark via universal deep hiding and metric learning,"Zhicheng Ye, Xinpeng Zhang, Guorui Feng",Neural Computing and Applications,2024-02-21,"<a href=""Springer (2024-02-21) : Deep neural networks watermark via universal deep hiding and metric learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-024-09469-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-024-09469-5]</a>","With the rising costs of model training, it is urgent to safeguard the intellectual property of deep neural networks. To achieve this, researchers...",,Springer
Edge intelligence-assisted animation design with large models: a survey,"Jing Zhu, Chuanjiang Hu, ... Mohd Mustafa Mohd Ghazali",Journal of Cloud Computing,2024-02-21,"<a href=""Springer (2024-02-21) : Edge intelligence-assisted animation design with large models: a survey"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-024-00601-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-024-00601-3]</a>","The integration of edge intelligence (EI) in animation design, particularly when dealing with large models, represents a significant advancement in...",,Springer
Measuring Impacts of Poisoning on Model Parameters and Neuron Activations: A Case Study of Poisoning CodeBERT,"A Hussain, MRI Rabin, N Ayoobi, MA Alipour","arXiv preprint arXiv …, 2024",2024-02-21,"<a href=""Google Scholar (2024-02-21) : Measuring Impacts of Poisoning on Model Parameters and Neuron Activations: A Case Study of Poisoning CodeBERT"" target=""_blank"">[https://arxiv.org/abs/2402.12936]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2402.12936]</a>",Backdoor attacks involve the insertion of triggers into … parameters to detect potential backdoor signals in code … white-box detection of backdoor signals in LLMs of code …,,Google Scholar
Network intrusion detection based on variational quantum convolution neural network,"Changqing Gong, Weiqi Guan, ... Han Qi",The Journal of Supercomputing,2024-02-20,"<a href=""Springer (2024-02-20) : Network intrusion detection based on variational quantum convolution neural network"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-024-05919-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-024-05919-y]</a>","With the rapid development of quantum machine learning (QML), quantum convolutional neural networks (QCNN) have been proposed and shown advantages in...",,Springer
Unlabeled learning algorithms and operations: overview and future trends in defense sector,"Eduardo e Oliveira, Marco Rodrigues, ... Sandro Bjelogrlic",Artificial Intelligence Review,2024-02-20,"<a href=""Springer (2024-02-20) : Unlabeled learning algorithms and operations: overview and future trends in defense sector"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-023-10692-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-023-10692-0]</a>","In the defense sector, artificial intelligence (AI) and machine learning (ML) have been used to analyse and decipher massive volumes of data, namely...",,Springer
WORLD OF SCIENCE,,,2024-02-20,"<a href=""Google Scholar (2024-02-20) : WORLD OF SCIENCE"" target=""_blank"">[https://elibrary.ru/item.asp?id=60095128]</a>","<a href=""Google Scholar"" target=""_blank"">[https://elibrary.ru/item.asp?id=60095128]</a>","platforms that install a backdoor on the visitor's computer… attacks, post-exploitation tools, and phishing attacks. … -middle (MITM) attack is a common type of attack against …",,Google Scholar
Independent Boot Process Verification using Side-Channel Power Analysis,A. Grisel-Davy S. Fischmeister,"2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security Companion (QRS-C)",2024-02-19,"<a href=""IEEE (2024-02-19) : Independent Boot Process Verification using Side-Channel Power Analysis"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10430037]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/QRS-C60940.2023.00053]</a>","Firmware attacks on embedded systems can have disastrous security implications. Through the firmware update mechanism, an attacker can tamper with the firmware to open known vulnerabilities, change security settings, or deploy custom backdoors, to pave the way for subsequent attacks or gain complete machine control. Firmware protection solutions often share the flaw of requiring the cooperation of the machine they aim to protect. If the machine gets compromised, the results from the protection mechanism become untrustworthy. One solution to this problem is to leverage an independent source of information to assess the integrity of the firmware and the boot-up sequence. In this paper, we propose a physics-based Intrusion Detection System called the Boot Process Verifier that only relies on side-channel power consumption measurement to verify the integrity of the boot-up sequence. The BPV works in complete independence from the machine to protect and requires only a few nominal training samples to establish a baseline of nominal behaviour. The range of application of this approach potentially extends to any embedded systems. We present three test cases that illustrate the performances of the BPV on micro-PC, network equipment (switches and wireless access points), and a drone.",,IEEE
Multi-scale Convolutional Feature Fusion Network Based on Attention Mechanism for IoT Traffic Classification,"Niandong Liao, Jiayu Guan",International Journal of Computational Intelligence Systems,2024-02-19,"<a href=""Springer (2024-02-19) : Multi-scale Convolutional Feature Fusion Network Based on Attention Mechanism for IoT Traffic Classification"" target=""_blank"">[https://link.springer.com/article/10.1007/s44196-024-00421-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s44196-024-00421-y]</a>","The Internet of Things (IoT) has been extensively utilized in domains such as smart homes, healthcare, and other industries. With the exponential...",,Springer
Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection,"Jiawei Liang, Siyuan Liang, Aishan Liu, Xiaojun Jia, Junhao Kuang, Xiaochun Cao","arXiv
arXiv","2024-02-18
2024-02","<a href=""arXiv (2024-02-18) : Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection"" target=""_blank"">[http://arxiv.org/abs/2402.11473v1]</a>
<a href=""DBLP (2024-02) : Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.11473]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.11473]</a>","The proliferation of face forgery techniques has raised significant concerns within society, thereby motivating the development of face forgery detection methods. These methods aim to distinguish forged faces from genuine ones and have proven effective in practical applications. However, this paper introduces a novel and previously unrecognized threat in face forgery detection scenarios caused by backdoor attack. By embedding backdoors into models and incorporating specific trigger patterns into the input, attackers can deceive detectors into producing erroneous predictions for forged faces. To achieve this goal, this paper proposes \emph{Poisoned Forgery Face} framework, which enables clean-label backdoor attacks on face forgery detectors. Our approach involves constructing a scalable trigger generator and utilizing a novel convolving process to generate translation-sensitive trigger patterns. Moreover, we employ a relative embedding method based on landmark-based regions to enhance the stealthiness of the poisoned samples. Consequently, detectors trained on our poisoned samples are embedded with backdoors. Notably, our approach surpasses SoTA backdoor baselines with a significant improvement in attack success rate (+16.39\% BD-AUC) and reduction in visibility (-12.65\% $L_\infty$). Furthermore, our attack exhibits promising performance against backdoor defenses. We anticipate that this paper will draw greater attention to the potential threats posed by backdoor attacks in face forgery detection scenarios. Our codes will be made available at \url{https://github.com/JWLiang007/PFF}
","<a href=""arXiv"" target=""_blank"">[https://github.com/JWLiang007/PFF}]</a>
","arXiv
DBLP"
Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents,"Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, Xu Sun","arXiv
arXiv","2024-02-17
2024-02","<a href=""arXiv (2024-02-17) : Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents"" target=""_blank"">[http://arxiv.org/abs/2402.11208v1]</a>
<a href=""DBLP (2024-02) : Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.11208]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.11208]</a>","Leveraging the rapid development of Large Language Models LLMs, LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis on the different forms of agent backdoor attacks. Specifically, from the perspective of the final attacking outcomes, the attacker can either choose to manipulate the final output distribution, or only introduce malicious behavior in the intermediate reasoning process, while keeping the final output correct. Furthermore, the former category can be divided into two subcategories based on trigger locations: the backdoor trigger can be hidden either in the user query or in an intermediate observation returned by the external environment. We propose the corresponding data poisoning mechanisms to implement the above variations of agent backdoor attacks on two typical agent tasks, web shopping and tool utilization. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks, indicating an urgent need for further research on the development of defenses against backdoor attacks on LLM-based agents. Warning: This paper may contain biased content.
","
","arXiv
DBLP"
Backdoor Attack on Un-paired Medical Image-Text Pretrained Models: A Pilot Study on MedCLIP,"R Jin, CY Huang, C You, X Li",2nd IEEE Conference on Secure and …,2024-02-17,"<a href=""Google Scholar (2024-02-17) : Backdoor Attack on Un-paired Medical Image-Text Pretrained Models: A Pilot Study on MedCLIP"" target=""_blank"">[https://openreview.net/forum?id=YymNvIkmKR]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=YymNvIkmKR]</a>","this label discrepancy as a backdoor attack problem. We … yield a staggering 99 percent attack success rate, all the … consistently fends off backdoor assaults across diverse …",,Google Scholar
Is wearing these sunglasses an attack? Obligations under IHL related to anti-AI countermeasures,J Kwik,"Obligations under IHL related to anti-AI …, 2024",2024-02-17,"<a href=""Google Scholar (2024-02-17) : Is wearing these sunglasses an attack? Obligations under IHL related to anti-AI countermeasures"" target=""_blank"">[https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4729113]</a>","<a href=""Google Scholar"" target=""_blank"">[https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4729113]</a>","an attack, and the obligations associated with both attack … simply by flashing the backdoor – without ever needing to … A backdoor can thus potentially allow Adversary to …",,Google Scholar
Universal vulnerabilities in large language models: Backdoor attacks for in-context learning,"S Zhao, M Jia, LA Tuan, F Pan…","arXiv preprint arXiv …, 2024",2024-02-17,"<a href=""Google Scholar (2024-02-17) : Universal vulnerabilities in large language models: Backdoor attacks for in-context learning"" target=""_blank"">[https://www.researchgate.net/profile/Shuai-Zhao-68/publication/377810700_Universal_Vulnerabilities_in_Large_Language_Models_Backdoor_Attacks_for_In-context_Learning/links/65cf68ae476dd15fb33c7a65/Universal-Vulnerabilities-in-Large-Language-Models-Backdoor-Attacks-for-In-context-Learning.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.researchgate.net/profile/Shuai-Zhao-68/publication/377810700_Universal_Vulnerabilities_in_Large_Language_Models_Backdoor_Attacks_for_In-context_Learning/links/65cf68ae476dd15fb33c7a65/Universal-Vulnerabilities-in-Large-Language-Models-Backdoor-Attacks-for-In-context-Learning.pdf]</a>","potential for more powerful attacks in ICL, capable of overcoming the previously mentioned constraints. We introduce a novel backdoor attack method named ICLAttack, …",,Google Scholar
Cryptographic Reverse Firewall for Digital Signature in Fog Computing,B. Kang H. Lian X. Zhou W. Zhou,"2023 6th International Conference on Software Engineering and Computer Science (CSECS)
2023 6th International …, 2023","2024-02-16
2023-12-23","<a href=""IEEE (2024-02-16) : Cryptographic Reverse Firewall for Digital Signature in Fog Computing"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10428227]</a>
<a href=""Google Scholar (2023-12-23) : Cryptographic Reverse Firewall for Digital Signature in Fog Computing"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10428227/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/CSECS60003.2023.10428227]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10428227/]</a>","Fog computing (FC) is introduced as an extended technique of cloud computing. FC makes it possible to compute straightly on the brim of the network. In FC application systems, authentication is an important security requirement when fog nodes communicate with other fog devices. Digital signature (DS) has been widely used in FC systems to provide the non-repudiation, authenticity along with data integrity. However, some latest research results demonstate that DS schemes may confront the algorithm substitution attack (ASA). That is, DS scheme would be implanted some covert backdoor information in algorithm implementation process by attackers. With the backdoor information attackers can recover the secret key of fog devices' users, which will result in privacy disclosure. To settle this matter, the notion of cryptographic reverse firewall (CRF) is proposed, which can re-randomize the inputs and outputs of the fog devices. Moreover, due to the limited storing and computing abilities of fog devices, the DS algorithms applied to FC systems should be resource-saving. In this paper, we simultaneously focus attention on addressing above two problems in fog computing and construct the CRFs for the Camenisch and Lysyanskaya DS scheme (CL signature) which is suitable to be applied in FC systems because of its high efficiency.
In this paper, to resist the algorithm substitution attacks of digital signature which is … in fog devices might exist the risk of backdoor attack, the security of the information input …","
","IEEE
Google Scholar"
"Trust, Privacy and Security Aspects of Bias and Fairness in Machine Learning",A. Atabek E. Eralp M. E. Gursoy,"2023 5th IEEE International Conference on Trust, Privacy and Security in Intelligent Systems and Applications (TPS-ISA)
… on Trust, Privacy and Security in …, 2023","2024-02-16
2023-11-01","<a href=""IEEE (2024-02-16) : Trust, Privacy and Security Aspects of Bias and Fairness in Machine Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10431654]</a>
<a href=""Google Scholar (2023-11-01) : Trust, Privacy and Security Aspects of Bias and Fairness in Machine Learning"" target=""_blank"">[https://www.computer.org/csdl/proceedings-article/tps-isa/2023/238500a111/1UAj4Sd6gi4]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TPS-ISA58951.2023.00023]</a>
<a href=""Google Scholar"" target=""_blank"">[https://www.computer.org/csdl/proceedings-article/tps-isa/2023/238500a111/1UAj4Sd6gi4]</a>","In today's world, an increasing number of decisions are being affected by machine learning (ML) algorithms in critical contexts ranging from banking to healthcare, recruitment, education, and criminal justice. Since ensuring fair and unbiased outcomes in these contexts is imperative, a large body of recent work has focused on bias and fairness in ML. In this paper, we consider the trust, privacy, and security aspects of bias and fairness in ML. From the trust aspect, we argue that for fairness measurements to be robust and trusted, a diverse set of fairness metrics should be consulted, and the agreements and disagreements between them should be well-understood. Upon conducting an empirical study with ten fairness metrics, three datasets, and three correlation notions, we identify fairness metrics that are positively correlated, negatively correlated, and uncorrelated by nature. From the privacy aspect, we investigate the impact of differential privacy (DP) on ML models and find that current differentially private ML mechanisms suffer from two drawbacks: reduced accuracy and increased bias. From the security aspect, we propose a backdoor attack to inject bias into NLP models. Upon experimentally testing our attack, we observe that modern transformer-based NLP models (such as BERT and RoBERTa) are more vulnerable to our attack, our attack is able to remain stealthy, and it can generalize to dynamic (changing) triggers presented at test time. Overall, our work highlights the intersections between two research directions that are often studied independently: (i) trust, privacy, and security in ML, and (ii) bias and fairness in ML.
From the security aspect, we propose a backdoor attack to inject bias into NLP … a backdoor attack to inject bias intoNLP models. Upon experimentally testing our attack, we …","
","IEEE
Google Scholar"
Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning,"Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Fengjun Pan, Jinming Wen",arXiv,2024-02-16,"<a href=""arXiv (2024-02-16) : Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning"" target=""_blank"">[http://arxiv.org/abs/2401.05949v4]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we design a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning demonstration prompts, which can make models behave in alignment with predefined intentions. ICLAttack does not require additional fine-tuning to implant a backdoor, thus preserving the model's generality. Furthermore, the poisoned examples are correctly labeled, enhancing the natural stealth of our attack method. Extensive experimental results across several language models, ranging in size from 1.3B to 180B parameters, demonstrate the effectiveness of our attack method, exemplified by a high average attack success rate of 95.0% across the three datasets on OPT models.",,arXiv
Backdoor Attack against One-Class Sequential Anomaly Detection Models,"He Cheng, Shuhan Yuan","arXiv
arXiv","2024-02-15
2024-02","<a href=""arXiv (2024-02-15) : Backdoor Attack against One-Class Sequential Anomaly Detection Models"" target=""_blank"">[http://arxiv.org/abs/2402.10283v1]</a>
<a href=""DBLP (2024-02) : Backdoor Attack against One-Class Sequential Anomaly Detection Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.10283]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.10283]</a>","Deep anomaly detection on sequential data has garnered significant attention due to the wide application scenarios. However, deep learning-based models face a critical security threat - their vulnerability to backdoor attacks. In this paper, we explore compromising deep sequential anomaly detection models by proposing a novel backdoor attack strategy. The attack approach comprises two primary steps, trigger generation and backdoor injection. Trigger generation is to derive imperceptible triggers by crafting perturbed samples from the benign normal data, of which the perturbed samples are still normal. The backdoor injection is to properly inject the backdoor triggers to comprise the model only for the samples with triggers. The experimental results demonstrate the effectiveness of our proposed attack strategy by injecting backdoors on two well-established one-class anomaly detection models.
","
","arXiv
DBLP"
Efficient Trigger Word Insertion,Y. Zeng Z. Li P. Xia L. Liu B. Li,"2023 9th International Conference on Big Data and Information Analytics (BigDIA)
arXiv preprint arXiv:2311.13957, 2023","2024-02-15
2023-11-24","<a href=""IEEE (2024-02-15) : Efficient Trigger Word Insertion"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10429630]</a>
<a href=""Google Scholar (2023-11-24) : Efficient Trigger Word Insertion"" target=""_blank"">[https://arxiv.org/abs/2311.13957]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/BigDIA60676.2023.10429630]</a>
<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2311.13957]</a>","With the rapid advancements in the natural language processing (NLP) domain in recent years, the emergence of backdoor attacks presents substantial threats to deep neural network models. However, prior research has often overlooked the influence of the poisoning rate. This paper aims to address this gap by prioritizing the reduction of poisoned samples while still attaining a comparable Attack Success Rate (ASR) in the context of text backdoor attacks. Our primary focus revolves around introducing an efficient strategy for trigger word insertion, encompassing both trigger word optimization and poisoned sample selection. To achieve our objectives, extensive experiments were conducted across diverse datasets and models, showcasing the significant enhancements brought forth by our proposed methodology in the realm of text classification tasks. Remarkable outcomes include an ASR surpassing 90%, utilizing a mere 10 poisoned samples in the dirty-label setting, and delivering compelling performance with only 1.5% of the training data in the clean-label setting.
trigger and sample selection regarding text backdoor attacks. We propose an efficient … We consider that the training process of backdoor attacks is to reinforce these vulner…","
","IEEE
Google Scholar"
"Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization","R Zhang, H Li, R Wen, W Jiang, Y Zhang…","arXiv preprint arXiv …, 2024",2024-02-15,"<a href=""Google Scholar (2024-02-15) : Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization"" target=""_blank"">[https://arxiv.org/abs/2402.09179]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2402.09179]</a>","Through a straightforward yet effective instruction backdoor attack, we show that … In this paper, the proposed attack shares the same goal as typical backdoor attacks. …",,Google Scholar
Shortcut-enhanced Multimodal Backdoor Attack in Vision-guided Robot Grasping,"C Li, Z Gao, NY Chong","Authorea Preprints, 2024",2024-02-15,"<a href=""Google Scholar (2024-02-15) : Shortcut-enhanced Multimodal Backdoor Attack in Vision-guided Robot Grasping"" target=""_blank"">[https://www.authorea.com/doi/full/10.36227/techrxiv.170792505.56224502]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.authorea.com/doi/full/10.36227/techrxiv.170792505.56224502]</a>","In this work, we make the first endeavor to realize the backdoor attack on the multimodal … backdoor attack method named Shortcut-enhanced Multimodal Backdoor Attack (…",,Google Scholar
Supplementary Material IBA: Towards Irreversible Backdoor Attacks in Federated Learning,,,2024-02-15,"<a href=""Google Scholar (2024-02-15) : Supplementary Material IBA: Towards Irreversible Backdoor Attacks in Federated Learning"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/file/d0c6bc641a56bebee9d985b937307367-Supplemental-Conference.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/file/d0c6bc641a56bebee9d985b937307367-Supplemental-Conference.pdf]</a>","IBA in the scenario of fixed-pool attacks In this experiment, we investigate the performance of IBA under fixed-pool backdoor attacks, in which A compromises a fraction of …",,Google Scholar
Robust Botnet Detection Approach for Known and Unknown Attacks in IoT Networks Using Stacked Multi-classifier and Adaptive Thresholding,"Deepa Krishnan, Pravin Shrinath",Arabian Journal for Science and Engineering,2024-02-14,"<a href=""Springer (2024-02-14) : Robust Botnet Detection Approach for Known and Unknown Attacks in IoT Networks Using Stacked Multi-classifier and Adaptive Thresholding"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-024-08742-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-024-08742-y]</a>","The detection of security attacks holds significant importance in IoT networks, primarily due to the escalating number of interconnected devices and...",,Springer
Test-Time Backdoor Attacks on Multimodal Large Language Models,"Dong Lu, Tianyu Pang, Chao Du, Qian Liu, Xianjun Yang, Min Lin","arXiv
arXiv","2024-02-13
2024-02","<a href=""arXiv (2024-02-13) : Test-Time Backdoor Attacks on Multimodal Large Language Models"" target=""_blank"">[http://arxiv.org/abs/2402.08577v1]</a>
<a href=""DBLP (2024-02) : Test-Time Backdoor Attacks on Multimodal Large Language Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.08577]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.08577]</a>","Backdoor attacks are commonly executed by contaminating training data, such that a trigger can activate predetermined harmful effects during the test phase. In this work, we present AnyDoor, a test-time backdoor attack against multimodal large language models (MLLMs), which involves injecting the backdoor into the textual modality using adversarial test images (sharing the same universal perturbation), without requiring access to or modification of the training data. AnyDoor employs similar techniques used in universal adversarial attacks, but distinguishes itself by its ability to decouple the timing of setup and activation of harmful effects. In our experiments, we validate the effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4, InstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies. Notably, because the backdoor is injected by a universal perturbation, AnyDoor can dynamically change its backdoor trigger prompts/harmful effects, exposing a new challenge for defending against backdoor attacks. Our project page is available at https://sail-sg.github.io/AnyDoor/.
","<a href=""arXiv"" target=""_blank"">[https://sail-sg.github.io/AnyDoor/]</a>
","arXiv
DBLP"
DaBA: Data-free Backdoor Attack against Federated Learning via Malicious Server,K. Chen L. Fang M. Wang C. Yin,"2023 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)
… International Conference on …, 2023","2024-02-13
2023-11-03","<a href=""IEEE (2024-02-13) : DaBA: Data-free Backdoor Attack against Federated Learning via Malicious Server"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10424802]</a>
<a href=""Google Scholar (2023-11-03) : DaBA: Data-free Backdoor Attack against Federated Learning via Malicious Server"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10424802/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICICML60161.2023.10424802]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10424802/]</a>","Current research shows that the privacy of FL is threatened by an honest-but-curious server. However, existing research focus on privacy attacks against the malicious server while overlooking that it could also compromise the shared model's integrity by introducing poisoning attacks. In this work, we propose a novel data-free backdoor attack (DaBA) against FL via malicious server to bridge the gap. Specifically, we utilize global model inversion to obtain a dummy dataset on the server side, then add backdoor triggers to a portion of the inputs in the dummy dataset and replace their labels with the target label, and finally retrain part of the global model on the poisoned dummy dataset. Our experimental results show that DaBA can achieve a high attack success rate on poisoned samples and high prediction accuracy on clean samples, which means the effectiveness and stealthiness of DaBA, respectively. For example, in the experiment of the MNIST dataset, DaBA can achieve a 99.6% attack success rate and 96.3% accuracy rate. We also discuss possible defense strategies against our attack. Our research reveals a significant security risk of FL.
backdoor attacks against FL by compromising the server in this work. Our attack could … attacks. Existing works have proposed several defense methods to defend against …","
","IEEE
Google Scholar"
Multi-channels Prototype Contrastive Learning with Condition Adversarial Attacks for Few-shot Event Detection,"Fangchen Zhang, Shengwei Tian, ... Qimeng Yang",Neural Processing Letters,2024-02-13,"<a href=""Springer (2024-02-13) : Multi-channels Prototype Contrastive Learning with Condition Adversarial Attacks for Few-shot Event Detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s11063-024-11515-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11063-024-11515-1]</a>",Few-shot Event Detection (FSED) is a sub-task of Event Detection that aims to accurately identify event types with limited training instances and...,,Springer
Attacking-Distance-Aware Attack: Semi-targeted Model Poisoning on Federated Learning,Y. Sun H. Ochiai J. Sakuma,IEEE Transactions on Artificial Intelligence,2024-02-12,"<a href=""IEEE (2024-02-12) : Attacking-Distance-Aware Attack: Semi-targeted Model Poisoning on Federated Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10136777]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TAI.2023.3280155]</a>","Existing model poisoning attacks on federated learning (FL) assume that an adversary has access to the full data distribution. In reality, an adversary usually has limited prior knowledge about clients' data. A poorly chosen target class renders an attack less effective. This article considers a semitargeted situation where the source class is predetermined but the target class is not. The goal is to cause the misclassification of the global classifier on data from the source class. Approaches such as label flipping have been used to inject malicious parameters into FL. Nevertheless, it has been shown that their performances are usually class sensitive, varying with different target classes. Typically, an attack becomes less effective when shifting to a different target class. To overcome this challenge, we propose the attacking-distance-aware attack (ADA) that enhances model poisoning in FL by finding the optimized target class in the feature space. ADA deduces pairwise class attacking distances using a fast layer gradient method. Extensive evaluations were performed on five benchmark image classification tasks and three model architectures using varying attacking frequencies. Furthermore, ADA's robustness to conventional defenses of Byzantine-robust aggregation and differential privacy was validated. The results showed that ADA succeeded in increasing attack performance to 2.8 times in the most challenging case with an attacking frequency of 0.01 and bypassed existing defenses, where differential privacy that was the most effective defense still could not reduce the attack performance to below 50%.",,IEEE
Capacity Abuse Attack of Deep Learning Models Without Need of Label Encodings,W. Luo L. Zhang Y. Wu C. Liu P. Han R. Zhuang,IEEE Transactions on Artificial Intelligence,2024-02-12,"<a href=""IEEE (2024-02-12) : Capacity Abuse Attack of Deep Learning Models Without Need of Label Encodings"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10098869]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TAI.2023.3266419]</a>","In recent years, machine learning (ML) models, especially deep learning models, have become commodities. In this context, data centers which hold a lot of data often buy ML models from ML model providers, train them on their data locally and use the trained models to provide intelligent services. Existing work has shown that there is a risk of data leakage, which could cause incalculable consequences. Even under the black-box condition, there are still some attacks that can steal the private data held by data centers, and the capacity abuse attack (CAA) is the state-of-the-art attack method. CAA attackers steal the training data by labeling malicious samples with the data to be stolen. However, the label encodings are usually mapped into other output forms, such as categories, and it is impossible for the adversary to know the mapping relationship between the form output by the trained model and the label encodings. Without the mapping relationship, CAA becomes invalid. Aiming at the limitation of CAA, this study proposes a novel practical attack method, i.e., capacity abuse attack II (CAAII), which can find the mapping relationship between the output in the arbitrary form returned by the trained model and the values of the stolen data. Experiments are conducted on MNIST, Fashion-MNIST, and CIFAR10 datasets, and experimental results show that no matter what forms are returned by the model, our attack method can always find the mapping relationship and successfully steals the training data.",,IEEE
OOD Problem Research in Biochemistry Based on Backdoor Adjustment,Gu P.,Frontiers in Artificial Intelligence and Applications,2024-02-12,"<a href=""ScienceDirect (2024-02-12) : OOD Problem Research in Biochemistry Based on Backdoor Adjustment"" target=""_blank"">[https://doi.org/10.3233/FAIA231392]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3233/FAIA231392]</a>",,,ScienceDirect
Comparative study of ML models for IIoT intrusion detection: impact of data preprocessing and balancing,"Abdulrahman Mahmoud Eid, Bassel Soudan, ... MohammadNoor Injadat",Neural Computing and Applications,2024-02-11,"<a href=""Springer (2024-02-11) : Comparative study of ML models for IIoT intrusion detection: impact of data preprocessing and balancing"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-024-09439-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-024-09439-x]</a>","This study investigates the effectiveness of six prominent machine learning models—random forest, decision trees, K-nearest neighbor, logistic...",,Springer
Architectural Neural Backdoors from First Principles,"Harry Langford, Ilia Shumailov, Yiren Zhao, Robert Mullins, Nicolas Papernot","arXiv
arXiv","2024-02-10
2024-02","<a href=""arXiv (2024-02-10) : Architectural Neural Backdoors from First Principles"" target=""_blank"">[http://arxiv.org/abs/2402.06957v1]</a>
<a href=""DBLP (2024-02) : Architectural Neural Backdoors from First Principles"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.06957]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.06957]</a>","While previous research backdoored neural networks by changing their parameters, recent work uncovered a more insidious threat: backdoors embedded within the definition of the network's architecture. This involves injecting common architectural components, such as activation functions and pooling layers, to subtly introduce a backdoor behavior that persists even after (full re-)training. However, the full scope and implications of architectural backdoors have remained largely unexplored. Bober-Irizar et al. [2023] introduced the first architectural backdoor, they showed how to create a backdoor for a checkerboard pattern, but never explained how to target an arbitrary trigger pattern of choice. In this work we construct an arbitrary trigger detector which can be used to backdoor an architecture with no human supervision. This leads us to revisit the concept of architecture backdoors and taxonomise them, describing 12 distinct types. To gauge the difficulty of detecting such backdoors, we conducted a user study, revealing that ML developers can only identify suspicious components in common model definitions as backdoors in 37% of cases, while they surprisingly preferred backdoored models in 33% of cases. To contextualize these results, we find that language models outperform humans at the detection of backdoors. Finally, we discuss defenses against architectural backdoors, emphasizing the need for robust and comprehensive strategies to safeguard the integrity of ML systems.
","
","arXiv
DBLP"
Defending against Poisoning Attacks in Federated Learning from a Spatial-temporal Perspective,Z. Gu J. Shi Y. Yang L. He,"2023 42nd International Symposium on Reliable Distributed Systems (SRDS)
2023 42nd International …, 2023","2024-02-09
2023-09-25","<a href=""IEEE (2024-02-09) : Defending against Poisoning Attacks in Federated Learning from a Spatial-temporal Perspective"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10419291]</a>
<a href=""Google Scholar (2023-09-25) : Defending against Poisoning Attacks in Federated Learning from a Spatial-temporal Perspective"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10419291/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SRDS60354.2023.00013]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10419291/]</a>","In federated learning, the central server aggregates local model updates from the participants in the network to generate a global model. For the purpose of protecting clients' privacy, the server is designed to have no visibility into how these updates are generated. The nature of federated learning makes detecting and defending against malicious model up-dates a challenging task. Unlike existing works that struggle to defend against poisoning attacks from a spatial perspective, the paper considers mitigating the impact of attacks from a spatial-temporal perspective. This paper proposes Fedmvae, a robust federated learning framework. Fedmvae uses multiple variational autoencoder models to detect and exclude malicious model updates from a spatial perspective. Moreover, to handle poisoning attacks with time-varying features, we propose generating a robust global model update according to momentum-based update speculation and historical global updates. Fedmvae is tested with extensive experiments on both IID and non-IID datasets, showing a competitive performance over existing aggregation methods under both Byzantine attacks and backdoor attacks.
attacks like Byzantine attacks and backdoor attacks from … both Byzantine attacks and backdoor attacks, resulting … against Byzantine attacks and backdoor attacks from the …","
","IEEE
Google Scholar"
DL-HIDS: deep learning-based host intrusion detection system using system calls-to-image for containerized cloud environment,"Nidhi Joraviya, Bhavesh N. Gohil, Udai Pratap Rao",The Journal of Supercomputing,2024-02-09,"<a href=""Springer (2024-02-09) : DL-HIDS: deep learning-based host intrusion detection system using system calls-to-image for containerized cloud environment"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-024-05895-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-024-05895-3]</a>","In the rapidly evolving IT industry, containerization has introduced new security challenges including cloud data breaches. DL-HIDS explores the...",,Springer
Forensic Analysis of Podman Container Towards Metasploit Backdoor Using Checkpointctl,"HA Sya'bani, C Umam…","Inform: Jurnal Ilmiah …, 2024",2024-02-09,"<a href=""Google Scholar (2024-02-09) : Forensic Analysis of Podman Container Towards Metasploit Backdoor Using Checkpointctl"" target=""_blank"">[https://ejournal.unitomo.ac.id/index.php/inform/article/view/7498]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ejournal.unitomo.ac.id/index.php/inform/article/view/7498]</a>",This research focused on finding evidence that the Metasploit backdoor had been … Figure 4 illustrates how the attack occurs on the system once the backdoor has been …,,Google Scholar
Fortifying home IoT security: A framework for comprehensive examination of vulnerabilities and intrusion detection strategies for smart cities,"A Bhardwaj, S Bharany, AW Abulfaraj…","Egyptian Informatics …, 2024",2024-02-09,"<a href=""Google Scholar (2024-02-09) : Fortifying home IoT security: A framework for comprehensive examination of vulnerabilities and intrusion detection strategies for smart cities"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S1110866524000069]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S1110866524000069]</a>","The authors also performed backdoor attacks on IoT devices (10.9.1.34) by scanning the unknown and hidden services. Using ‘netcat’ to the listener port 5515, the authors …",,Google Scholar
Graph neural networks: a survey on the links between privacy and security,"Faqian Guan, Tianqing Zhu, ... Kim-Kwang Raymond Choo",Artificial Intelligence Review,2024-02-08,"<a href=""Springer (2024-02-08) : Graph neural networks: a survey on the links between privacy and security"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-023-10656-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-023-10656-4]</a>",Graph neural networks (GNNs) are models that capture the dependencies between graph data by passing messages between graph nodes and they have been...,,Springer
Defending Our Privacy With Backdoors,"Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting","arXiv
arXiv","2024-02-07
2023-10","<a href=""arXiv (2024-02-07) : Defending Our Privacy With Backdoors"" target=""_blank"">[http://arxiv.org/abs/2310.08320v3]</a>
<a href=""DBLP (2023-10) : Defending Our Privacy With Backdoors"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.08320]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.08320]</a>","The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names and faces of individuals from vision-language models by fine-tuning them for only a few minutes instead of re-training them from scratch. Specifically, through strategic insertion of backdoors into text encoders, we align the embeddings of sensitive phrases with those of neutral terms-""a person"" instead of the person's actual name. For image encoders, we map embeddings of individuals to be removed from the model to a universal, anonymous embedding. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new ""dual-use"" perspective on backdoor attacks, but also presents a promising avenue to enhance the privacy of individuals within models trained on uncurated web-scraped data.
","
","arXiv
DBLP"
Contributions to effective and secure federated learning with client data heterogeneity,FE Castellon,2024,2024-02-07,"<a href=""Google Scholar (2024-02-07) : Contributions to effective and secure federated learning with client data heterogeneity"" target=""_blank"">[https://theses.hal.science/tel-04496058/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://theses.hal.science/tel-04496058/]</a>","on backdoor attacks triggered by an attack pattern. Our goal is to uncover the attack … , and thus reconstructing the attack pattern that provokes the attack. This reconstruction …",,Google Scholar
FedNAT: Byzantine-robust Federated Learning through Activation-based Attention Transfer,M. Wang L. Fang K. Chen,"2023 IEEE International Conference on Data Mining Workshops (ICDMW)
2023 IEEE International Conference …, 2023","2024-02-06
2023-12-05","<a href=""IEEE (2024-02-06) : FedNAT: Byzantine-robust Federated Learning through Activation-based Attention Transfer"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10411519]</a>
<a href=""Google Scholar (2023-12-05) : FedNAT: Byzantine-robust Federated Learning through Activation-based Attention Transfer"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10411519/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICDMW60847.2023.00133]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10411519/]</a>","Federated learning (FL) is a decentralized machine learning framework that prioritizes privacy by allowing clients to train statistical models without sharing their private data, thus eliminating the impact of data fortresses. However, the presence of Byzantine attacks, such as data poisoning and backdoor attack, threatens the robustness of FL schemes. Currently, existing mainstream defense methods are susceptible to multiple adaptive attacks, some of which even violate the privacy principle of FL. Furthermore, these defense schemes become less robust when subjected to targeted poisoning attacks with highly non-IID data distributions. In this work, we propose FedNAT, a novel Byzantine-robust FL framework for whittling away these limitations mentioned above. Specifically, FedNAT first performs a privacy-respecting attention refinement on the activation layer outputs of the local uploads. Then, the server scores the local attentions by calculating their Wasserstein distances and clusters them through the k-median algorithm for global attention aggregation, thus rejecting poisoned local attentions for untargeted attacks. After this process, the global attention is transferred to local attention through the FedNAT loss function, which erases backdoors through the distillation concept. We conduct a comprehensive experimental evaluation to demonstrate that FedNAT significantly outperforms existing robust FL schemes in defending against Byzantine poisoning attacks under both IID and highly non-IID data proportions.
For instance, advanced poisoning attacks can create poisoned local model updates … of strong stealth attacks like backdoor attacks. Finally, it is a higher attack success rate …","
","IEEE
Google Scholar"
Backdoor Attack Detection via Prediction Trustworthiness Assessment,"N Zhong, Z Qian, X Zhang","Information Sciences, 2024",2024-02-06,"<a href=""Google Scholar (2024-02-06) : Backdoor Attack Detection via Prediction Trustworthiness Assessment"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S0020025524001968]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S0020025524001968]</a>","attack named the backdoor attack, which occurs in the training phase. The backdoor attack … [13] first propose a seminal backdoor attack in the image classification domain. …",,Google Scholar
The last Dance: Robust backdoor attack via diffusion models and bayesian approach,O Mengara,"arXiv preprint arXiv:2402.05967, 2024",2024-02-06,"<a href=""Google Scholar (2024-02-06) : The last Dance: Robust backdoor attack via diffusion models and bayesian approach"" target=""_blank"">[https://arxiv.org/abs/2402.05967]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2402.05967]</a>",We demonstrate the feasibility of backdoor attacks (… backdoor attack developed in this paper is based on poisoning the model’s training data by incorporating backdoor …,,Google Scholar
DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models,"Yang Sui, Huy Phan, Jinqi Xiao, Tianfang Zhang, Zijie Tang, Cong Shi, Yan Wang, Yingying Chen, Bo Yuan","arXiv
arXiv","2024-02-05
2024-02","<a href=""arXiv (2024-02-05) : DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models"" target=""_blank"">[http://arxiv.org/abs/2402.02739v1]</a>
<a href=""DBLP (2024-02) : DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.02739]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.02739]</a>","In the exciting generative AI era, the diffusion model has emerged as a very powerful and widely adopted content generation and editing tool for various data modalities, making the study of their potential security risks very necessary and critical. Very recently, some pioneering works have shown the vulnerability of the diffusion model against backdoor attacks, calling for in-depth analysis and investigation of the security challenges of this popular and fundamental AI technique. In this paper, for the first time, we systematically explore the detectability of the poisoned noise input for the backdoored diffusion models, an important performance metric yet little explored in the existing works. Starting from the perspective of a defender, we first analyze the properties of the trigger pattern in the existing diffusion backdoor attacks, discovering the important role of distribution discrepancy in Trojan detection. Based on this finding, we propose a low-cost trigger detection mechanism that can effectively identify the poisoned input noise. We then take a further step to study the same problem from the attack side, proposing a backdoor attack strategy that can learn the unnoticeable trigger to evade our proposed detection scheme. Empirical evaluations across various diffusion models and datasets demonstrate the effectiveness of the proposed trigger detection and detection-evading attack strategy. For trigger detection, our distribution discrepancy-based solution can achieve a 100\% detection rate for the Trojan triggers used in the existing works. For evading trigger detection, our proposed stealthy trigger design approach performs end-to-end learning to make the distribution of poisoned noise input approach that of benign noise, enabling nearly 100\% detection pass rate with very high attack and benign performance for the backdoored diffusion models.
","
","arXiv
DBLP"
Time-Distributed Backdoor Attacks on Federated Spiking Learning,"Gorka Abad, Stjepan Picek, Aitor Urbieta","arXiv
arXiv","2024-02-05
2024-02","<a href=""arXiv (2024-02-05) : Time-Distributed Backdoor Attacks on Federated Spiking Learning"" target=""_blank"">[http://arxiv.org/abs/2402.02886v1]</a>
<a href=""DBLP (2024-02) : Time-Distributed Backdoor Attacks on Federated Spiking Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.02886]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.02886]</a>","This paper investigates the vulnerability of spiking neural networks (SNNs) and federated learning (FL) to backdoor attacks using neuromorphic data. Despite the efficiency of SNNs and the privacy advantages of FL, particularly in low-powered devices, we demonstrate that these systems are susceptible to such attacks. We first assess the viability of using FL with SNNs using neuromorphic data, showing its potential usage. Then, we evaluate the transferability of known FL attack methods to SNNs, finding that these lead to suboptimal attack performance. Therefore, we explore backdoor attacks involving single and multiple attackers to improve the attack performance. Our primary contribution is developing a novel attack strategy tailored to SNNs and FL, which distributes the backdoor trigger temporally and across malicious devices, enhancing the attack's effectiveness and stealthiness. In the best case, we achieve a 100 attack success rate, 0.13 MSE, and 98.9 SSIM. Moreover, we adapt and evaluate an existing defense against backdoor attacks, revealing its inadequacy in protecting SNNs. This study underscores the need for robust security measures in deploying SNNs and FL, particularly in the context of backdoor attacks.
","
","arXiv
DBLP"
Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data,"Gorka Abad, Oguzhan Ersoy, Stjepan Picek, Aitor Urbieta","arXiv
arXiv","2024-02-05
2023-02","<a href=""arXiv (2024-02-05) : Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data"" target=""_blank"">[http://arxiv.org/abs/2302.06279v3]</a>
<a href=""DBLP (2023-02) : Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.06279]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.14722/ndss.2024.24334]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.06279]</a>","Deep neural networks (DNNs) have demonstrated remarkable performance across various tasks, including image and speech recognition. However, maximizing the effectiveness of DNNs requires meticulous optimization of numerous hyperparameters and network parameters through training. Moreover, high-performance DNNs entail many parameters, which consume significant energy during training. In order to overcome these challenges, researchers have turned to spiking neural networks (SNNs), which offer enhanced energy efficiency and biologically plausible data processing capabilities, rendering them highly suitable for sensory data tasks, particularly in neuromorphic data. Despite their advantages, SNNs, like DNNs, are susceptible to various threats, including adversarial examples and backdoor attacks. Yet, the field of SNNs still needs to be explored in terms of understanding and countering these attacks. This paper delves into backdoor attacks in SNNs using neuromorphic datasets and diverse triggers. Specifically, we explore backdoor triggers within neuromorphic data that can manipulate their position and color, providing a broader scope of possibilities than conventional triggers in domains like images. We present various attack strategies, achieving an attack success rate of up to 100% while maintaining a negligible impact on clean accuracy. Furthermore, we assess these attacks' stealthiness, revealing that our most potent attacks possess significant stealth capabilities. Lastly, we adapt several state-of-the-art defenses from the image domain, evaluating their efficacy on neuromorphic data and uncovering instances where they fall short, leading to compromised performance.
","
","arXiv
DBLP"
Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift,"Shengwei An, Sheng-Yen Chou, Kaiyuan Zhang, Qiuling Xu, Guanhong Tao, Guangyu Shen, Siyuan Cheng, Shiqing Ma, Pin-Yu Chen, Tsung-Yi Ho, Xiangyu Zhang","arXiv
AAAI
arXiv","2024-02-04
2024
2023-12","<a href=""arXiv (2024-02-04) : Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift"" target=""_blank"">[http://arxiv.org/abs/2312.00050v2]</a>
<a href=""DBLP (2024) : Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i10.28958]</a>
<a href=""DBLP (2023-12) : Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.00050]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i10.28958]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.00050]</a>","Diffusion models (DM) have become state-of-the-art generative models because of their capability to generate high-quality images from noises without adversarial training. However, they are vulnerable to backdoor attacks as reported by recent studies. When a data input (e.g., some Gaussian noise) is stamped with a trigger (e.g., a white patch), the backdoored model always generates the target image (e.g., an improper photo). However, effective defense strategies to mitigate backdoors from DMs are underexplored. To bridge this gap, we propose the first backdoor detection and removal framework for DMs. We evaluate our framework Elijah on hundreds of DMs of 3 types including DDPM, NCSN and LDM, with 13 samplers against 3 existing backdoor attacks. Extensive experiments show that our approach can have close to 100% detection accuracy and reduce the backdoor effects to close to zero without significantly sacrificing the model utility.

","

","arXiv
DBLP
DBLP"
CNN-GRU-FF: a double-layer feature fusion-based network intrusion detection system using convolutional neural network and gated recurrent units,"Yakubu Imrana, Yanping Xiang, ... Muhammed Amin Abdullah",Complex & Intelligent Systems,2024-02-02,"<a href=""Springer (2024-02-02) : CNN-GRU-FF: a double-layer feature fusion-based network intrusion detection system using convolutional neural network and gated recurrent units"" target=""_blank"">[https://link.springer.com/article/10.1007/s40747-023-01313-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s40747-023-01313-y]</a>",Identifying and preventing malicious network behavior is a challenge for establishing a secure network communication environment or system. Malicious...,,Springer
ODSCAN: Backdoor Scanning for Object Detection Models,"S Cheng, G Shen, G Tao, K Zhang, Z Zhang…","2024 IEEE Symposium …, 2024",2024-02-02,"<a href=""Google Scholar (2024-02-02) : ODSCAN: Backdoor Scanning for Object Detection Models"" target=""_blank"">[https://www.computer.org/csdl/proceedings-article/sp/2024/313000a119/1Ub23se6M5q]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.computer.org/csdl/proceedings-article/sp/2024/313000a119/1Ub23se6M5q]</a>","Based on theprevious discussion, we propose the following general definitionof backdoor attack in object detection models. The definitioncovers the aforementioned …",,Google Scholar
A new intrusion detection system based on SVM–GWO algorithms for Internet of Things,"Hamed Ghasemi, Shahram Babaie",Wireless Networks,2024-02-01,"<a href=""Springer (2024-02-01) : A new intrusion detection system based on SVM–GWO algorithms for Internet of Things"" target=""_blank"">[https://link.springer.com/article/10.1007/s11276-023-03637-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11276-023-03637-6]</a>","Internet of Things (IoT) as an emerging technology is widely used in various applications such as remote healthcare, smart environment, and...",,Springer
Characteristic analysis of echo signal of typical backdoor coupling target,Feng X.,Qiangjiguang Yu Lizishu/High Power Laser and Particle Beams,2024-02-01,"<a href=""ScienceDirect (2024-02-01) : Characteristic analysis of echo signal of typical backdoor coupling target"" target=""_blank"">[https://doi.org/10.11884/HPLPB202436.230272]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.11884/HPLPB202436.230272]</a>",,,ScienceDirect
Federated Learning Backdoor Attack Based on Frequency Domain Injection,Liu J.,Entropy,2024-02-01,"<a href=""ScienceDirect (2024-02-01) : Federated Learning Backdoor Attack Based on Frequency Domain Injection"" target=""_blank"">[https://doi.org/10.3390/e26020164]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/e26020164]</a>",,,ScienceDirect
Towards a unified framework for imperceptible textual attacks,Shi J.,Applied Intelligence,2024-02-01,"<a href=""ScienceDirect (2024-02-01) : Towards a unified framework for imperceptible textual attacks"" target=""_blank"">[https://doi.org/10.1007/s10489-024-05292-6]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/s10489-024-05292-6]</a>",,,ScienceDirect
Robust Sentiment Classification Based on the Backdoor Adjustment,"Lulu Dai, Mingyue Han","MLNLP '23: Proceedings of the 2023 6th International Conference on Machine Learning and Natural Language Processing
MLNLP","2024-02
2023","<a href=""ACM (2024-02) : Robust Sentiment Classification Based on the Backdoor Adjustment"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3639479.3639488]</a>
<a href=""DBLP (2023) : Robust Sentiment Classification Based on the Backdoor Adjustment"" target=""_blank"">[https://doi.org/10.1145/3639479.3639488]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3639479.3639488]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3639479.3639488]</a>","Deep NLP models are correlation-based learning, which has a critical limitation of over-fitting over spurious features and shows poor generalization capability in the out-of-distribution (OOD) setting. Existing methods encourage the model to exploit ...
","
","ACM
DBLP"
"Here&apos,s a Free Lunch: Sanitizing Backdoored Models with Model Merge","Ansh Arora, Xuanli He, Maximilian Mozes, Srinibas Swain, Mark Dras, Qiongkai Xu",arXiv,2024-02,"<a href=""DBLP (2024-02) : Here&apos,s a Free Lunch: Sanitizing Backdoored Models with Model Merge"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.19334]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.19334]</a>",,,DBLP
Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models,"Pengzhou Cheng, Wei Du, Zongru Wu, Fengwei Zhang, Libo Chen, Gongshen Liu",arXiv,2024-02,"<a href=""DBLP (2024-02) : Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.18945]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2402.18945]</a>",,,DBLP
Evaluating differential privacy on language processing federated learning,Q Van Opstal,2024,2024-01-30,"<a href=""Google Scholar (2024-01-30) : Evaluating differential privacy on language processing federated learning"" target=""_blank"">[https://cse3000-research-project.github.io/static/367c58e1ac54f168f3156b28ad144fda/poster.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://cse3000-research-project.github.io/static/367c58e1ac54f168f3156b28ad144fda/poster.pdf]</a>","There is a threat against this kind of learning: a backdoor attack. Here, an adversary … A specific case which is used in this project is edge case attacks here, the input that …",,Google Scholar
TransTroj: Transferable Backdoor Attacks to Pre-trained Models via Embedding Indistinguishability,"Hao Wang, Tao Xiang, Shangwei Guo, Jialing He, Hangcheng Liu, Tianwei Zhang","arXiv
arXiv","2024-01-29
2024-01","<a href=""arXiv (2024-01-29) : TransTroj: Transferable Backdoor Attacks to Pre-trained Models via Embedding Indistinguishability"" target=""_blank"">[http://arxiv.org/abs/2401.15883v1]</a>
<a href=""DBLP (2024-01) : TransTroj: Transferable Backdoor Attacks to Pre-trained Models via Embedding Indistinguishability"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.15883]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.15883]</a>","Pre-trained models (PTMs) are extensively utilized in various downstream tasks. Adopting untrusted PTMs may suffer from backdoor attacks, where the adversary can compromise the downstream models by injecting backdoors into the PTM. However, existing backdoor attacks to PTMs can only achieve partially task-agnostic and the embedded backdoors are easily erased during the fine-tuning process. In this paper, we propose a novel transferable backdoor attack, TransTroj, to simultaneously meet functionality-preserving, durable, and task-agnostic. In particular, we first formalize transferable backdoor attacks as the indistinguishability problem between poisoned and clean samples in the embedding space. We decompose the embedding indistinguishability into pre- and post-indistinguishability, representing the similarity of the poisoned and reference embeddings before and after the attack. Then, we propose a two-stage optimization that separately optimizes triggers and victim PTMs to achieve embedding indistinguishability. We evaluate TransTroj on four PTMs and six downstream tasks. Experimental results show that TransTroj significantly outperforms SOTA task-agnostic backdoor attacks (18%$\sim$99%, 68% on average) and exhibits superior performance under various system settings. The code is available at https://github.com/haowang-cqu/TransTroj .
","<a href=""arXiv"" target=""_blank"">[https://github.com/haowang-cqu/TransTroj]</a>
","arXiv
DBLP"
A Runtime Trust Evaluation Mechanism in the Service Mesh Architecture,R. Alboqmi S. Jahan R. F. Gamble,"2023 10th International Conference on Future Internet of Things and Cloud (FiCloud)
2023 10th International …, 2023","2024-01-29
2023-08-14","<a href=""IEEE (2024-01-29) : A Runtime Trust Evaluation Mechanism in the Service Mesh Architecture"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10410687]</a>
<a href=""Google Scholar (2023-08-14) : A Runtime Trust Evaluation Mechanism in the Service Mesh Architecture"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10410687/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/FiCloud58648.2023.00043]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10410687/]</a>","With the advancement of cloud-native computing, enterprise software is shifting towards a microservice architecture (MSA). In a MSA, the application is built as a composite of microservices (also called services) where each service is dedicated to providing specific functionalities and communicating via APIs. The service mesh is a dedicated infrastructure layer serving as a service-to-service communication platform. It is introduced to alleviate the complexity, manageability, and interoperability challenges involved in a MSA. Even though the service mesh includes APIs to secure service-to-service communication, more functionality is needed to handle the perimeter-less and dynamic cloud environment. The existing APIs enable security measures based on a pre-defined configuration. However, there is no mechanism for dynamic trust evaluation of the participating services, which is necessary to protect the application from potential attacks. Attackers can exploit a vulnerability caused by a rouge service as a backdoor to compromise the application. Thus, there is a need for a trust evaluation mechanism in the service mesh to follow zero-trust architecture (ZTA) principles. This paper introduces a runtime trust evaluator (RTE) incorporated as a component within the service mesh control plane. A RTE can evaluate the trustworthiness of services at runtime before establishing service-to-service communications. The core functionality of a RTE is to assess the new service invocation requests initiated by a service to evaluate the trustworthiness of that service. The RTE collects the services’ invocation history and other telemetric data to determine the services’ criticality level for single-point failure. The RTE assesses service invocation request attributes to determine the existence of an unauthorized request and the impact on the requested service’s criticality level. The result is an evaluation of the initiator service’s trustworthiness. We demonstrate our approach on an open source microservice application from the Google Cloud team called Online Boutique. The environment setup uses Kubernetes as an orchestration solution and Istio as the service mesh platform.
to protect the application from potential attacks. Attackers can exploit a vulnerability caused by a rouge service as a backdoor to compromise the application. Thus, there is …","
","IEEE
Google Scholar"
One-to-Multiple Clean-Label Image Camouflage (OmClic) based Backdoor Attack on Deep Learning,"Guohong Wang, Hua Ma, Yansong Gao, Alsharif Abuadbba, Zhi Zhang, Wei Kang, Said F. Al-Sarawib, Gongxuan Zhang, Derek Abbott","arXiv
arXiv","2024-01-28
2023-09","<a href=""arXiv (2024-01-28) : One-to-Multiple Clean-Label Image Camouflage (OmClic) based Backdoor Attack on Deep Learning"" target=""_blank"">[http://arxiv.org/abs/2309.04036v2]</a>
<a href=""DBLP (2023-09) : One-to-Multiple Clean-Label Image Camouflage (OmClic) based Backdoor Attack on Deep Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.04036]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.04036]</a>","Image camouflage has been utilized to create clean-label poisoned images for implanting backdoor into a DL model. But there exists a crucial limitation that one attack/poisoned image can only fit a single input size of the DL model, which greatly increases its attack budget when attacking multiple commonly adopted input sizes of DL models. This work proposes to constructively craft an attack image through camouflaging but can fit multiple DL models' input sizes simultaneously, namely OmClic. Thus, through OmClic, we are able to always implant a backdoor regardless of which common input size is chosen by the user to train the DL model given the same attack budget (i.e., a fraction of the poisoning rate). With our camouflaging algorithm formulated as a multi-objective optimization, M=5 input sizes can be concurrently targeted with one attack image, which artifact is retained to be almost visually imperceptible at the same time. Extensive evaluations validate the proposed OmClic can reliably succeed in various settings using diverse types of images. Further experiments on OmClic based backdoor insertion to DL models show that high backdoor performances (i.e., attack success rate and clean data accuracy) are achievable no matter which common input size is randomly chosen by the user to train the model. So that the OmClic based backdoor attack budget is reduced by M$\times$ compared to the state-of-the-art camouflage based backdoor attack as a baseline. Significantly, the same set of OmClic based poisonous attack images is transferable to different model architectures for backdoor implant.
","
","arXiv
DBLP"
"Multi-Trigger Backdoor Attacks: More Triggers, More Threats","Yige Li, Xingjun Ma, Jiabo He, Hanxun Huang, Yu-Gang Jiang","arXiv
arXiv","2024-01-27
2024-01","<a href=""arXiv (2024-01-27) : Multi-Trigger Backdoor Attacks: More Triggers, More Threats"" target=""_blank"">[http://arxiv.org/abs/2401.15295v1]</a>
<a href=""DBLP (2024-01) : Multi-Trigger Backdoor Attacks: More Triggers, More Threats"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.15295]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.15295]</a>","Backdoor attacks have emerged as a primary threat to (pre-)training and deployment of deep neural networks (DNNs). While backdoor attacks have been extensively studied in a body of works, most of them were focused on single-trigger attacks that poison a dataset using a single type of trigger. Arguably, real-world backdoor attacks can be much more complex, e.g., the existence of multiple adversaries for the same dataset if it is of high value. In this work, we investigate the practical threat of backdoor attacks under the setting of \textbf{multi-trigger attacks} where multiple adversaries leverage different types of triggers to poison the same dataset. By proposing and investigating three types of multi-trigger attacks, including parallel, sequential, and hybrid attacks, we provide a set of important understandings of the coexisting, overwriting, and cross-activating effects between different triggers on the same dataset. Moreover, we show that single-trigger attacks tend to cause overly optimistic views of the security of current defense techniques, as all examined defense methods struggle to defend against multi-trigger attacks. Finally, we create a multi-trigger backdoor poisoning dataset to help future evaluation of backdoor attacks and defenses. Although our work is purely empirical, we hope it can help steer backdoor research toward more realistic settings.
","
","arXiv
DBLP"
Emerging Trends and Challenges in IoT Networks,"H Park, S Park","Electronics, 2024",2024-01-27,"<a href=""Google Scholar (2024-01-27) : Emerging Trends and Challenges in IoT Networks"" target=""_blank"">[https://www.mdpi.com/2079-9292/13/3/513]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.mdpi.com/2079-9292/13/3/513]</a>","a backdoor attack scenario and method for tabular data, the authors executed the attack … the success rate of backdoor attacks and outliers needs to be carefully considered …",,Google Scholar
Exploring the Impact of Single-Character Attacks in Federated Learning Language Classification: Introducing the Novel Single-Character Strike,J van der Meulen,2024,2024-01-27,"<a href=""Google Scholar (2024-01-27) : Exploring the Impact of Single-Character Attacks in Federated Learning Language Classification: Introducing the Novel Single-Character Strike"" target=""_blank"">[https://repository.tudelft.nl/islandora/object/uuid:f90541b8-232b-47be-8470-d79b273279ae]</a>","<a href=""Google Scholar"" target=""_blank"">[https://repository.tudelft.nl/islandora/object/uuid:f90541b8-232b-47be-8470-d79b273279ae]</a>","backdoor attacks in NLP tasks are still unable to evade some defence mechanisms. Therefore, we propose a novel attack… , trains slower than similar attacks, relies on …",,Google Scholar
MEA-Defender: A Robust Watermark against Model Extraction Attack,"P Lv, H Ma, K Chen, J Zhou, S Zhang, R Liang…","arXiv preprint arXiv …, 2024",2024-01-27,"<a href=""Google Scholar (2024-01-27) : MEA-Defender: A Robust Watermark against Model Extraction Attack"" target=""_blank"">[https://arxiv.org/abs/2401.15239]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2401.15239]</a>","Composite Backdoor [37], a backdoor attack against SL models, whose backdoor … it a potential watermark against model extraction attacks. Since those approaches are …",,Google Scholar
BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning,"Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Mingli Zhu, Ruotong Wang, Li Liu, Chao Shen","arXiv
arXiv
arXiv
NeurIPS
arXiv","2024-01-26
2022-10-19
2024-01
2022
2022-06","<a href=""arXiv (2024-01-26) : BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning"" target=""_blank"">[http://arxiv.org/abs/2401.15002v1]</a>
<a href=""arXiv (2022-10-19) : BackdoorBench: A Comprehensive Benchmark of Backdoor Learning"" target=""_blank"">[http://arxiv.org/abs/2206.12654v2]</a>
<a href=""DBLP (2024-01) : BackdoorBench: A Comprehensive Benchmark and Analysis of Backdoor Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.15002]</a>
<a href=""DBLP (2022) : BackdoorBench: A Comprehensive Benchmark of Backdoor Learning"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/4491ea1c91aa2b22c373e5f1dfce234f-Abstract-Datasets_and_Benchmarks.html]</a>
<a href=""DBLP (2022-06) : BackdoorBench: A Comprehensive Benchmark of Backdoor Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.12654]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.15002]</a>
<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/4491ea1c91aa2b22c373e5f1dfce234f-Abstract-Datasets_and_Benchmarks.html]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.12654]</a>","As an emerging and vital topic for studying deep neural networks' vulnerability (DNNs), backdoor learning has attracted increasing interest in recent years, and many seminal backdoor attack and defense algorithms are being developed successively or concurrently, in the status of a rapid arms race. However, mainly due to the diverse settings, and the difficulties of implementation and reproducibility of existing works, there is a lack of a unified and standardized benchmark of backdoor learning, causing unfair comparisons, and unreliable conclusions (e.g., misleading, biased or even false conclusions). Consequently, it is difficult to evaluate the current progress and design the future development roadmap of this literature. To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning called BackdoorBench. Our benchmark makes three valuable contributions to the research community. 1) We provide an integrated implementation of state-of-the-art (SOTA) backdoor learning algorithms (currently including 16 attack and 27 defense algorithms), based on an extensible modular-based codebase. 2) We conduct comprehensive evaluations of 12 attacks against 16 defenses, with 5 poisoning ratios, based on 4 models and 4 datasets, thus 11,492 pairs of evaluations in total. 3) Based on above evaluations, we present abundant analysis from 8 perspectives via 18 useful analysis tools, and provide several inspiring insights about backdoor learning. We hope that our efforts could build a solid foundation of backdoor learning to facilitate researchers to investigate existing algorithms, develop more innovative algorithms, and explore the intrinsic mechanism of backdoor learning. Finally, we have created a user-friendly website at http://backdoorbench.com, which collects all important information of BackdoorBench, including codebase, docs, leaderboard, and model Zoo.
Backdoor learning is an emerging and vital topic for studying deep neural networks' vulnerability (DNNs). Many pioneering backdoor attack and defense methods are being proposed, successively or concurrently, in the status of a rapid arms race. However, we find that the evaluations of new methods are often unthorough to verify their claims and accurate performance, mainly due to the rapid development, diverse settings, and the difficulties of implementation and reproducibility. Without thorough evaluations and comparisons, it is not easy to track the current progress and design the future development roadmap of the literature. To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning called BackdoorBench. It consists of an extensible modular-based codebase (currently including implementations of 8 state-of-the-art (SOTA) attacks and 9 SOTA defense algorithms) and a standardized protocol of complete backdoor learning. We also provide comprehensive evaluations of every pair of 8 attacks against 9 defenses, with 5 poisoning ratios, based on 5 models and 4 datasets, thus 8,000 pairs of evaluations in total. We present abundant analysis from different perspectives about these 8,000 evaluations, studying the effects of different factors in backdoor learning. All codes and evaluations of BackdoorBench are publicly available at \url{https://backdoorbench.github.io}.


","
<a href=""arXiv"" target=""_blank"">[https://backdoorbench.github.io}]</a>


","arXiv
arXiv
DBLP
DBLP
DBLP"
Tamper Resistant Design of Convolutional Neural Network Hardware Accelerator,H. Yu P. Sun B. Halak K. Shanthakumar T. Kazmierski,"2023 Asian Hardware Oriented Security and Trust Symposium (AsianHOST)
… Security and Trust …, 2023","2024-01-24
2023-12-14","<a href=""IEEE (2024-01-24) : Tamper Resistant Design of Convolutional Neural Network Hardware Accelerator"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10409319]</a>
<a href=""Google Scholar (2023-12-14) : Tamper Resistant Design of Convolutional Neural Network Hardware Accelerator"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10409319/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/AsianHOST59942.2023.10409319]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10409319/]</a>","The globalisation of supply chains and manufacturing processes can lead to loss of control over the manufacturing process and exposure to potentially malicious third parties, thus making the security of Convolutional Neural Network hardware accelerators compromised by emerging attacks (e.g., hardware Trojan(HT) insertion attacks and backdoor attacks from third-party dataset providers). In this paper, a new defence mechanism, called Shuffle and Substitution-Based Defence Mechanism(SSDM), is proposed to effectively defend against attacks launched by attackers from the third-party dataset providers and the Fabrication phase. The new countermeasure proposed in this paper can not only effectively suppress the activation of most existing HTs, but also greatly increase the difficulty for adversaries from third-party dataset providers to successfully execute backdoor attacks. The experimental results show that the new defensive countermeasures are effective in preventing HTs from being activated and significantly increasing the difficulty of backdoor attacks.
Then, we introduce several different categories of HT insertion attacks and one of the most popular backdoor attacks by third-party dataset providers and design the …","
","IEEE
Google Scholar"
An optimized intrusion detection model for wireless sensor networks based on MLP-CatBoost algorithm,"Geo Francis E, Sheeja S",Multimedia Tools and Applications,2024-01-24,"<a href=""Springer (2024-01-24) : An optimized intrusion detection model for wireless sensor networks based on MLP-CatBoost algorithm"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-18034-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-18034-6]</a>",A wireless Sensor Network (WSN) is made up of many sensor nodes which gather and transmit data to a central location. The limited resources of the...,,Springer
CTP: Defending Against Data Poisoning in Attack Traffic Detection Based Deep Neural Networks,L. Wang X. Wang Q. Lv Y. Wang X. Zhou W. Huang,"2023 8th IEEE International Conference on Network Intelligence and Digital Content (IC-NIDC)
Proceedings of 2023 8th IEEE International Conference on Network Intelligence and Digital Content, IC-NIDC 2023","2024-01-23
2023-01-01","<a href=""IEEE (2024-01-23) : CTP: Defending Against Data Poisoning in Attack Traffic Detection Based Deep Neural Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10390737]</a>
<a href=""ScienceDirect (2023-01-01) : CTP: Defending Against Data Poisoning in Attack Traffic Detection Based Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1109/IC-NIDC59918.2023.10390737]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IC-NIDC59918.2023.10390737]</a>
<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/IC-NIDC59918.2023.10390737]</a>","Deep learning is extensively employed in attack traffic detection, exhibiting outstanding performance. To enhance model effectiveness, security personnel acquires additional traffic data from public sources for training. However, public source data may not always be reliable due to potentially inaccurate attack labels. By deliberately modifying labels, attackers can insert backdoors into neural networks, enabling undetected network attacks and posing significant risks. In this paper, we introduce CTP (Cluster and Train with Pruning), an effective method to counter data poisoning attacks and bolster the defense capabilities of attack traffic detection models. CTP consists of two components: activation layer clustering and pruning training. Firstly, activation layer clustering visually reveals poisoned data with manipulated labels, thwarting attackers' attempts to poison the model. Secondly, pruning training reduces the likelihood of model neurons being poi-soned, further mitigating the risk of poisoned data. These steps significantly strengthen deep learning models' resistance to data poisoning attacks. We conduct experiments on three popular datasets, and the results indicate that our proposed method effectively enhances the model's defense capabilities.
","
","IEEE
ScienceDirect"
An efficient automated image caption generation by the encoder decoder model,"Khustar Ansari, Priyanka Srivastava",Multimedia Tools and Applications,2024-01-22,"<a href=""Springer (2024-01-22) : An efficient automated image caption generation by the encoder decoder model"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-024-18150-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-024-18150-x]</a>",Image caption generation is becoming one of the hot research topics and attracts various researchers. It is a complex process because it utilizes...,,Springer
FedDAA: a robust federated learning framework to protect privacy and defend against adversarial attack,"Shiwei Lu, Ruihu Li, Wenbin Liu",Frontiers of Computer Science,2024-01-22,"<a href=""Springer (2024-01-22) : FedDAA: a robust federated learning framework to protect privacy and defend against adversarial attack"" target=""_blank"">[https://link.springer.com/article/10.1007/s11704-023-2283-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11704-023-2283-x]</a>","Federated learning (FL) has emerged to break data-silo and protect clients’ privacy in the field of artificial intelligence. However, deep leakage...",,Springer
BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models,"Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, Bo Li","arXiv
arXiv","2024-01-20
2024-01","<a href=""arXiv (2024-01-20) : BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models"" target=""_blank"">[http://arxiv.org/abs/2401.12242v1]</a>
<a href=""DBLP (2024-01) : BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.12242]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.12242]</a>","Large language models (LLMs) are shown to benefit from chain-of-thought (COT) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, COT prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output unintended malicious content under specific backdoor-triggered conditions during inference. Traditional methods for launching backdoor attacks involve either contaminating the training dataset with backdoored instances or directly manipulating the model parameters during deployment. However, these approaches are not practical for commercial LLMs that typically operate via API access. In this paper, we propose BadChain, the first backdoor attack against LLMs employing COT prompting, which does not require access to the training dataset or model parameters and imposes low computational overhead. BadChain leverages the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning step into the sequence of reasoning steps of the model output, thereby altering the final response when a backdoor trigger exists in the query prompt. Empirically, we show the effectiveness of BadChain for two COT strategies across four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) and six complex benchmark tasks encompassing arithmetic, commonsense, and symbolic reasoning. Moreover, we show that LLMs endowed with stronger reasoning capabilities exhibit higher susceptibility to BadChain, exemplified by a high average attack success rate of 97.0% across the six benchmark tasks on GPT-4. Finally, we propose two defenses based on shuffling and demonstrate their overall ineffectiveness against BadChain. Therefore, BadChain remains a severe threat to LLMs, underscoring the urgency for the development of robust and effective future defenses.
","
","arXiv
DBLP"
Universal Backdoor Attacks,"Benjamin Schneider, Nils Lukas, Florian Kerschbaum","arXiv
arXiv","2024-01-20
2023-12","<a href=""arXiv (2024-01-20) : Universal Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2312.00157v2]</a>
<a href=""DBLP (2023-12) : Universal Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.00157]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.00157]</a>","Web-scraped datasets are vulnerable to data poisoning, which can be used for backdooring deep image classifiers during training. Since training on large datasets is expensive, a model is trained once and re-used many times. Unlike adversarial examples, backdoor attacks often target specific classes rather than any class learned by the model. One might expect that targeting many classes through a naive composition of attacks vastly increases the number of poison samples. We show this is not necessarily true and more efficient, universal data poisoning attacks exist that allow controlling misclassifications from any source class into any target class with a small increase in poison samples. Our idea is to generate triggers with salient characteristics that the model can learn. The triggers we craft exploit a phenomenon we call inter-class poison transferability, where learning a trigger from one class makes the model more vulnerable to learning triggers for other classes. We demonstrate the effectiveness and robustness of our universal backdoor attacks by controlling models with up to 6,000 classes while poisoning only 0.15% of the training dataset. Our source code is available at https://github.com/Ben-Schneider-code/Universal-Backdoor-Attacks.
","<a href=""arXiv"" target=""_blank"">[https://github.com/Ben-Schneider-code/Universal-Backdoor-Attacks]</a>
","arXiv
DBLP"
Learning Backdoors for Mixed Integer Programs with Contrastive Learning,"Junyang Cai, Taoan Huang, Bistra Dilkina","arXiv
arXiv","2024-01-19
2024-01","<a href=""arXiv (2024-01-19) : Learning Backdoors for Mixed Integer Programs with Contrastive Learning"" target=""_blank"">[http://arxiv.org/abs/2401.10467v1]</a>
<a href=""DBLP (2024-01) : Learning Backdoors for Mixed Integer Programs with Contrastive Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.10467]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.10467]</a>","Many real-world problems can be efficiently modeled as Mixed Integer Programs (MIPs) and solved with the Branch-and-Bound method. Prior work has shown the existence of MIP backdoors, small sets of variables such that prioritizing branching on them when possible leads to faster running times. However, finding high-quality backdoors that improve running times remains an open question. Previous work learns to estimate the relative solver speed of randomly sampled backdoors through ranking and then decide whether to use it. In this paper, we utilize the Monte-Carlo tree search method to collect backdoors for training, rather than relying on random sampling, and adapt a contrastive learning framework to train a Graph Attention Network model to predict backdoors. Our method, evaluated on four common MIP problem domains, demonstrates performance improvements over both Gurobi and previous models.
","
","arXiv
DBLP"
Backdoor attacks in federated learning with regression,A Simonov,2024,2024-01-18,"<a href=""Google Scholar (2024-01-18) : Backdoor attacks in federated learning with regression"" target=""_blank"">[https://repository.tudelft.nl/islandora/object/uuid:995dff3e-2888-4f06-959a-63884d0aeb07]</a>","<a href=""Google Scholar"" target=""_blank"">[https://repository.tudelft.nl/islandora/object/uuid:995dff3e-2888-4f06-959a-63884d0aeb07]</a>","the first (to our knowledge) backdoor attack on the linear … Additionally, the study shows that backdoor attacks with … Furthermore, we also transfer this backdoor attack to the …",,Google Scholar
Resilience of federated learning against false data injection attacks in energy forecasting,"A Shabbir, HU Manzoor, RA Ahmed…","… Conference on Green …, 2024",2024-01-18,"<a href=""Google Scholar (2024-01-18) : Resilience of federated learning against false data injection attacks in energy forecasting"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10475064/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10475064/]</a>","However, FL is not immune to backdoor adversarial attacks, such as data and model … The attack was initiated on one client among ten. As the attack percentage increases, …",,Google Scholar
"Sexual and Gender Difference in the British Navy, 1690-1900",SS LeJacq,2024,2024-01-18,"<a href=""Google Scholar (2024-01-18) : Sexual and Gender Difference in the British Navy, 1690-1900"" target=""_blank"">[https://www.taylorfrancis.com/books/mono/10.4324/9781003433606/sexual-gender-difference-british-navy-1690-1900-seth-stein-lejacq]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.taylorfrancis.com/books/mono/10.4324/9781003433606/sexual-gender-difference-british-navy-1690-1900-seth-stein-lejacq]</a>",,,Google Scholar
"DFB: A Data-Free, Low-Budget, and High-Efficacy Clean-Label Backdoor Attack","Binhao Ma, Jiahui Wang, Dejun Wang, Bo Meng",arXiv,2024-01-17,"<a href=""arXiv (2024-01-17) : DFB: A Data-Free, Low-Budget, and High-Efficacy Clean-Label Backdoor Attack"" target=""_blank"">[http://arxiv.org/abs/2308.09487v4]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","In the domain of backdoor attacks, accurate labeling of injected data is essential for evading rudimentary detection mechanisms. This imperative has catalyzed the development of clean-label attacks, which are notably more elusive as they preserve the original labels of the injected data. Current clean-label attack methodologies primarily depend on extensive knowledge of the training dataset. However, practically, such comprehensive dataset access is often unattainable, given that training datasets are typically compiled from various independent sources. Departing from conventional clean-label attack methodologies, our research introduces DFB, a data-free, low-budget, and high-efficacy clean-label backdoor Attack. DFB is unique in its independence from training data access, requiring solely the knowledge of a specific target class. Tested on CIFAR10, Tiny-ImageNet, and TSRD, DFB demonstrates remarkable efficacy with minimal poisoning rates of just 0.1%, 0.025%, and 0.4%, respectively. These rates are significantly lower than those required by existing methods such as LC, HTBA, BadNets, and Blend, yet DFB achieves superior attack success rates. Furthermore, our findings reveal that DFB poses a formidable challenge to four established backdoor defense algorithms, indicating its potential as a robust tool in advanced clean-label attack strategies.",,arXiv
Manipulating adaptive processes by constructive training-time attacks,R Bector,2024,2024-01-17,"<a href=""Google Scholar (2024-01-17) : Manipulating adaptive processes by constructive training-time attacks"" target=""_blank"">[https://dr.ntu.edu.sg/handle/10356/173190]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dr.ntu.edu.sg/handle/10356/173190]</a>",by itself in the absence of the attack. Most insidious of constructive attacks are training-time attacks that “pre-program” back-doors and behavioural triggers into a victim RL …,,Google Scholar
Backdoor Breakthrough: Unveiling Next-Gen Clustering Defenses for NLP Model Integrity,AJ Jones,"Innovations, Securities, and Case Studies Across …, 2024",2024-01-16,"<a href=""Google Scholar (2024-01-16) : Backdoor Breakthrough: Unveiling Next-Gen Clustering Defenses for NLP Model Integrity"" target=""_blank"">[https://www.igi-global.com/chapter/backdoor-breakthrough/336889]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.igi-global.com/chapter/backdoor-breakthrough/336889]</a>",shift in combating backdoor attacks in NLP models. By … backdoor triggers but also effectively neutralizes them. This approach is grounded in the premise that backdoor …,,Google Scholar
"Innovations, Securities, and Case Studies Across Healthcare, Business, and Technology",DN Burrell,2024,2024-01-16,"<a href=""Google Scholar (2024-01-16) : Innovations, Securities, and Case Studies Across Healthcare, Business, and Technology"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=D-bvEAAAQBAJ&oi=fnd&pg=PR1&dq=backdoor+attack&ots=uRivi4IK4s&sig=U9ESgZDGBTkcSoK0gc-sabvS76A]</a>","<a href=""Google Scholar"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=D-bvEAAAQBAJ&oi=fnd&pg=PR1&dq=backdoor+attack&ots=uRivi4IK4s&sig=U9ESgZDGBTkcSoK0gc-sabvS76A]</a>","This study introduces"" NeuroGuard,” a pioneering defense mechanism against backdoor attacks in Natural Language Processing (NLP) models. It employs a variant of the …",,Google Scholar
The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data,Z. Zhu R. Wang C. Zou L. Jing,"2023 IEEE/CVF International Conference on Computer Vision (ICCV)
Proceedings of the IEEE …, 2023","2024-01-15
2023-09-28","<a href=""IEEE (2024-01-15) : The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10376898]</a>
<a href=""Google Scholar (2023-09-28) : The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data"" target=""_blank"">[http://openaccess.thecvf.com/content/ICCV2023/html/Zhu_The_Victim_and_The_Beneficiary_Exploiting_a_Poisoned_Model_to_ICCV_2023_paper.html]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00021]</a>
<a href=""Google Scholar"" target=""_blank"">[http://openaccess.thecvf.com/content/ICCV2023/html/Zhu_The_Victim_and_The_Beneficiary_Exploiting_a_Poisoned_Model_to_ICCV_2023_paper.html]</a>","Recently, backdoor attacks have posed a serious security threat to the training process of deep neural networks (DNNs). The attacked model behaves normally on benign samples but outputs a specific result when the trigger is present. However, compared with the rocketing progress of backdoor attacks, existing defenses are difficult to deal with these threats effectively or require benign samples to work, which may be unavailable in real scenarios. In this paper, we find that the poisoned samples and benign samples can be distinguished with prediction entropy. This inspires us to propose a novel dual-network training framework: The Victim and The Beneficiary (V&B), which exploits a poisoned model to train a clean model without extra benign samples. Firstly, we sacrifice the Victim network to be a powerful poisoned sample detector by training on suspicious samples. Secondly, we train the Beneficiary network on the credible samples selected by the Victim to inhibit backdoor injection. Thirdly, a semi-supervised suppression strategy is adopted for erasing potential backdoors and improving model performance. Furthermore, to better inhibit missed poisoned samples, we propose a strong data augmentation method, AttentionMix, which works well with our proposed V&B framework. Extensive experiments on two widely used datasets against 6 state-of-the-art attacks demonstrate that our framework is effective in preventing backdoor injection and robust to various attacks while maintaining the performance on benign samples. Our code is available at https://github.com/Zixuan-Zhu/VaB.
demonstrate that our framework is effective in preventing backdoor injection and robust to various attacks while maintaining the performance on benign samples. Our code …","<a href=""IEEE"" target=""_blank"">[https://github.com/Zixuan-Zhu/VaB]</a>
","IEEE
Google Scholar"
Backdoor breakthrough: Unveiling next-gen clustering defenses for NLP model integrity,Jones A.J.,"Innovations, Securities, and Case Studies Across Healthcare, Business, and Technology",2024-01-15,"<a href=""ScienceDirect (2024-01-15) : Backdoor breakthrough: Unveiling next-gen clustering defenses for NLP model integrity"" target=""_blank"">[https://doi.org/10.4018/979-8-3693-1906-2.ch008]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.4018/979-8-3693-1906-2.ch008]</a>",,,ScienceDirect
CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning,H. Bansal F. Yin N. Singhi A. Grover Y. Yang K. -W. Chang,2023 IEEE/CVF International Conference on Computer Vision (ICCV),2024-01-15,"<a href=""IEEE (2024-01-15) : CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10377853]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00017]</a>","Multimodal contrastive pretraining has been used to train multimodal representation models, such as CLIP, on large amounts of paired image-text data. However, previous studies have revealed that such models are vulnerable to backdoor attacks. Specifically, when trained on backdoored examples, CLIP learns spurious correlations between the embedded backdoor trigger and the target label, aligning their representations in the joint embedding space. Injecting even a small number of poisoned examples, such as 75 examples in 3 million pretraining data, can significantly manipulate the model’s behavior, making it difficult to detect or unlearn such correlations. To address this issue, we propose CleanCLIP, a finetuning framework that weakens the learned spurious associations introduced by backdoor attacks by independently re-aligning the representations for individual modalities. We demonstrate that unsupervised finetuning using a combination of multimodal contrastive and unimodal self-supervised objectives for individual modalities can significantly reduce the impact of the backdoor attack. Additionally, we show that supervised finetuning on task-specific labeled image data removes the backdoor trigger from the CLIP vision encoder. We show empirically that CleanCLIP maintains model performance on benign examples while erasing a range of backdoor attacks on multimodal contrastive learning. Code and pretrained checkpoints are available at https://github.com/nishadsinghi/CleanCLIP.","<a href=""IEEE"" target=""_blank"">[https://github.com/nishadsinghi/CleanCLIP]</a>",IEEE
Towards Robust Model Watermark via Reducing Parametric Vulnerability,G. Gan Y. Li D. Wu S. -T. Xia,2023 IEEE/CVF International Conference on Computer Vision (ICCV),2024-01-15,"<a href=""IEEE (2024-01-15) : Towards Robust Model Watermark via Reducing Parametric Vulnerability"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10376653]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00438]</a>","Deep neural networks are valuable assets considering their commercial benefits and huge demands for costly annotation and computation resources. To protect the copyright of DNNs, backdoor-based ownership verification becomes popular recently, in which the model owner can watermark the model by embedding a specific backdoor behavior before releasing it. The defenders (usually the model owners) can identify whether a suspicious third-party model is ""stolen"" from them based on the presence of the behavior. Unfortunately, these watermarks are proven to be vulnerable to removal attacks even like fine-tuning. To further explore this vulnerability, we investigate the parameter space and find there exist many watermark-removed models in the vicinity of the watermarked one, which may be easily used by removal attacks. Inspired by this finding, we propose a mini-max formulation to find these watermark-removed models and recover their watermark behavior. Extensive experiments demonstrate that our method improves the robustness of the model watermarking against parametric changes and numerous watermark-removal attacks. The codes for reproducing our main experiments are available at https://github.com/GuanhaoGan/robust-model-watermarking.","<a href=""IEEE"" target=""_blank"">[https://github.com/GuanhaoGan/robust-model-watermarking]</a>",IEEE
PANACEA: a neural model ensemble for cyber-threat detection,"Malik AL-Essa, Giuseppina Andresini, ... Donato Malerba",Machine Learning,2024-01-12,"<a href=""Springer (2024-01-12) : PANACEA: a neural model ensemble for cyber-threat detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-023-06470-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-023-06470-2]</a>",Ensemble learning is a strategy commonly used to fuse different base models by creating a model ensemble that is expected more accurate on unseen...,,Springer
Can We Trust the Unlabeled Target Data? Towards Backdoor Attack and Defense on Model Adaptation,"Lijun Sheng, Jian Liang, Ran He, Zilei Wang, Tieniu Tan","arXiv
arXiv","2024-01-11
2024-01","<a href=""arXiv (2024-01-11) : Can We Trust the Unlabeled Target Data? Towards Backdoor Attack and Defense on Model Adaptation"" target=""_blank"">[http://arxiv.org/abs/2401.06030v1]</a>
<a href=""DBLP (2024-01) : Can We Trust the Unlabeled Target Data? Towards Backdoor Attack and Defense on Model Adaptation"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.06030]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.06030]</a>","Model adaptation tackles the distribution shift problem with a pre-trained model instead of raw data, becoming a popular paradigm due to its great privacy protection. Existing methods always assume adapting to a clean target domain, overlooking the security risks of unlabeled samples. In this paper, we explore the potential backdoor attacks on model adaptation launched by well-designed poisoning target data. Concretely, we provide two backdoor triggers with two poisoning strategies for different prior knowledge owned by attackers. These attacks achieve a high success rate and keep the normal performance on clean samples in the test stage. To defend against backdoor embedding, we propose a plug-and-play method named MixAdapt, combining it with existing adaptation algorithms. Experiments across commonly used benchmarks and adaptation methods demonstrate the effectiveness of MixAdapt. We hope this work will shed light on the safety of learning with unlabeled data.
","
","arXiv
DBLP"
Backdoor Attacks with Wavelet Embedding: Revealing and enhancing the insights of vulnerabilities in visual object detection models on transformers within digital twin …,"M Shen, R Huang","Advanced Engineering Informatics, 2024",2024-01-10,"<a href=""Google Scholar (2024-01-10) : Backdoor Attacks with Wavelet Embedding: Revealing and enhancing the insights of vulnerabilities in visual object detection models on transformers within digital twin …"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S147403462400003X]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S147403462400003X]</a>","backdoor attacks, a form of security threat that compromises model output by poisoning the training data. Our investigation specifically addresses backdoor attacks … attacks …",,Google Scholar
Using Memory Forensics to detect Malware processes.,,,2024-01-10,"<a href=""Google Scholar (2024-01-10) : Using Memory Forensics to detect Malware processes."" target=""_blank"">[https://www.researchgate.net/profile/Stephen-Nwagwughiagwu/publication/377208144_Using_Memory_Forensics_to_detect_Malware_processes/links/659a0b3e0bb2c7472b376fe1/Using-Memory-Forensics-to-detect-Malware-processes.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.researchgate.net/profile/Stephen-Nwagwughiagwu/publication/377208144_Using_Memory_Forensics_to_detect_Malware_processes/links/659a0b3e0bb2c7472b376fe1/Using-Memory-Forensics-to-detect-Malware-processes.pdf]</a>","systems during a malware attack. These malicious code … the trojan horse and backdoor, and sometimes concealing … Malware attacks are a nightmare for every modern or…",,Google Scholar
A voting gray wolf optimizer-based ensemble learning models for intrusion detection in the Internet of Things,"Yakub Kayode Saheed, Sanjay Misra",International Journal of Information Security,2024-01-09,"<a href=""Springer (2024-01-09) : A voting gray wolf optimizer-based ensemble learning models for intrusion detection in the Internet of Things"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00803-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00803-x]</a>",The Internet of Things (IoT) has garnered considerable attention from academic and industrial circles as a pivotal technology in recent years. The...,,Springer
Detecting Face Synthesis Using a Concealed Fusion Model,"R Leyva, V Sanchez, G Epiphaniou…","arXiv preprint arXiv …, 2024",2024-01-09,"<a href=""Google Scholar (2024-01-09) : Detecting Face Synthesis Using a Concealed Fusion Model"" target=""_blank"">[https://arxiv.org/abs/2401.04257]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2401.04257]</a>","Model attacks: We measure the success rate of the poisoning, perturbation, reverse, and backdoor attacks as performed on our strategy using the sGAN2 images. In other …",,Google Scholar
Inferring Properties of Graph Neural Networks,"D Nguyen, HM Vu, CT Le, B Le, D Lo…","arXiv preprint arXiv …, 2024",2024-01-09,"<a href=""Google Scholar (2024-01-09) : Inferring Properties of Graph Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2401.03790]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2401.03790]</a>",-Infer can be used to improve the backdoor defense against state-of-the-art backdoor … backdoor attacks of GNNs. We first provide a brief background on backdoor attacks …,,Google Scholar
Classification of Malware from the Network Traffic Using Hybrid and Deep Learning Based Approach,"Praful R. Pardhi, Jitendra Kumar Rout, ... Santosh Kumar Sahu",SN Computer Science,2024-01-08,"<a href=""Springer (2024-01-08) : Classification of Malware from the Network Traffic Using Hybrid and Deep Learning Based Approach"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-023-02516-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-023-02516-3]</a>","Mobile connectivity and smart devices are spreading worldwide. As a result, the use of mobile devices and applications is rising exponentially....",,Springer
Federated transfer learning for attack detection for Internet of Medical Things,Afnan A. Alharbi,International Journal of Information Security,2024-01-08,"<a href=""Springer (2024-01-08) : Federated transfer learning for attack detection for Internet of Medical Things"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00805-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00805-9]</a>","In the healthcare sector, cyberattack detection systems are crucial for ensuring the privacy of patient data and building trust in the increasingly...",,Springer
End-to-End Anti-Backdoor Learning on Images and Time Series,"Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, Yige Li, James Bailey","arXiv
arXiv","2024-01-06
2024-01","<a href=""arXiv (2024-01-06) : End-to-End Anti-Backdoor Learning on Images and Time Series"" target=""_blank"">[http://arxiv.org/abs/2401.03215v1]</a>
<a href=""DBLP (2024-01) : End-to-End Anti-Backdoor Learning on Images and Time Series"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.03215]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.03215]</a>","Backdoor attacks present a substantial security concern for deep learning models, especially those utilized in applications critical to safety and security. These attacks manipulate model behavior by embedding a hidden trigger during the training phase, allowing unauthorized control over the model's output during inference time. Although numerous defenses exist for image classification models, there is a conspicuous absence of defenses tailored for time series data, as well as an end-to-end solution capable of training clean models on poisoned data. To address this gap, this paper builds upon Anti-Backdoor Learning (ABL) and introduces an innovative method, End-to-End Anti-Backdoor Learning (E2ABL), for robust training against backdoor attacks. Unlike the original ABL, which employs a two-stage training procedure, E2ABL accomplishes end-to-end training through an additional classification head linked to the shallow layers of a Deep Neural Network (DNN). This secondary head actively identifies potential backdoor triggers, allowing the model to dynamically cleanse these samples and their corresponding labels during training. Our experiments reveal that E2ABL significantly improves on existing defenses and is effective against a broad range of backdoor attacks in both image and time series domains.
","
","arXiv
DBLP"
TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks,"Khondoker Murad Hossain, Tim Oates","arXiv
arXiv","2024-01-06
2024-01","<a href=""arXiv (2024-01-06) : TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2401.05432v1]</a>
<a href=""DBLP (2024-01) : TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.05432]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.05432]</a>","As deep neural networks and the datasets used to train them get larger, the default approach to integrating them into research and commercial projects is to download a pre-trained model and fine tune it. But these models can have uncertain provenance, opening up the possibility that they embed hidden malicious behavior such as trojans or backdoors, where small changes to an input (triggers) can cause the model to produce incorrect outputs (e.g., to misclassify). This paper introduces a novel approach to backdoor detection that uses two tensor decomposition methods applied to network activations. This has a number of advantages relative to existing detection methods, including the ability to analyze multiple models at the same time, working across a wide variety of network architectures, making no assumptions about the nature of triggers used to alter network behavior, and being computationally efficient. We provide a detailed description of the detection pipeline along with results on models trained on the MNIST digit dataset, CIFAR-10 dataset, and two difficult datasets from NIST's TrojAI competition. These results show that our method detects backdoored networks more accurately and efficiently than current state-of-the-art methods.
","
","arXiv
DBLP"
A backdoor attack against link prediction tasks with graph neural networks,"Jiazhu Dai, Haoyu Sun","arXiv
arXiv","2024-01-05
2024-01","<a href=""arXiv (2024-01-05) : A backdoor attack against link prediction tasks with graph neural networks"" target=""_blank"">[http://arxiv.org/abs/2401.02663v1]</a>
<a href=""DBLP (2024-01) : A backdoor attack against link prediction tasks with graph neural networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.02663]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.02663]</a>","Graph Neural Networks (GNNs) are a class of deep learning models capable of processing graph-structured data, and they have demonstrated significant performance in a variety of real-world applications. Recent studies have found that GNN models are vulnerable to backdoor attacks. When specific patterns (called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input data, the backdoor embedded in the GNN models is activated, which misclassifies the input data into the target class label specified by the attacker, whereas when there are no backdoor triggers in the input, the backdoor embedded in the GNN models is not activated, and the models work normally. Backdoor attacks are highly stealthy and expose GNN models to serious security risks. Currently, research on backdoor attacks against GNNs mainly focus on tasks such as graph classification and node classification, and backdoor attacks against link prediction tasks are rarely studied. In this paper, we propose a backdoor attack against the link prediction tasks based on GNNs and reveal the existence of such security vulnerability in GNN models, which make the backdoored GNN models to incorrectly predict unlinked two nodes as having a link relationship when a trigger appear. The method uses a single node as the trigger and poison selected node pairs in the training graph, and then the backdoor will be embedded in the GNN models through the training process. In the inference stage, the backdoor in the GNN models can be activated by simply linking the trigger node to the two end nodes of the unlinked node pairs in the input data, causing the GNN models to produce incorrect link prediction results for the target node pairs.
","
","arXiv
DBLP"
Object-oriented backdoor attack against image captioning,"Meiling Li, Nan Zhong, Xinpeng Zhang, Zhenxing Qian, Sheng Li","arXiv
arXiv","2024-01-05
2024-01","<a href=""arXiv (2024-01-05) : Object-oriented backdoor attack against image captioning"" target=""_blank"">[http://arxiv.org/abs/2401.02600v1]</a>
<a href=""DBLP (2024-01) : Object-oriented backdoor attack against image captioning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.02600]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.02600]</a>","Backdoor attack against image classification task has been widely studied and proven to be successful, while there exist little research on the backdoor attack against vision-language models. In this paper, we explore backdoor attack towards image captioning models by poisoning training data. Assuming the attacker has total access to the training dataset, and cannot intervene in model construction or training process. Specifically, a portion of benign training samples is randomly selected to be poisoned. Afterwards, considering that the captions are usually unfolded around objects in an image, we design an object-oriented method to craft poisons, which aims to modify pixel values by a slight range with the modification number proportional to the scale of the current detected object region. After training with the poisoned data, the attacked model behaves normally on benign images, but for poisoned images, the model will generate some sentences irrelevant to the given image. The attack controls the model behavior on specific test images without sacrificing the generation performance on benign test images. Our method proves the weakness of image captioning models to backdoor attack and we hope this work can raise the awareness of defending against backdoor attack in the image captioning field.
","
","arXiv
DBLP"
Adversarial machine learning: A taxonomy and terminology of attacks and mitigations,"A Vassilev, A Oprea, A Fordyce, H Anderson",2024,2024-01-05,"<a href=""Google Scholar (2024-01-05) : Adversarial machine learning: A taxonomy and terminology of attacks and mitigations"" target=""_blank"">[https://csrc.nist.gov/pubs/ai/100/2/e2023/final?ref=studygrc.com]</a>","<a href=""Google Scholar"" target=""_blank"">[https://csrc.nist.gov/pubs/ai/100/2/e2023/final?ref=studygrc.com]</a>","types of ML methods and lifecycle stages of attack, attacker goals and objectives, and … mitigating and managing the consequences of attacks and points out relevant open …",,Google Scholar
"Security, Privacy, and Applied Cryptography Engineering: 13th International Conference, SPACE 2023, Roorkee, India, December 14–17, 2023, Proceedings","F Regazzoni, B Mazumdar, S Parameswaran",2024,2024-01-05,"<a href=""Google Scholar (2024-01-05) : Security, Privacy, and Applied Cryptography Engineering: 13th International Conference, SPACE 2023, Roorkee, India, December 14–17, 2023, Proceedings"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=WHPsEAAAQBAJ&oi=fnd&pg=PP6&dq=backdoor+attack&ots=c518w_7rAm&sig=6vQbqPU9JQq1awKNblTcnmRRUOU]</a>","<a href=""Google Scholar"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=WHPsEAAAQBAJ&oi=fnd&pg=PP6&dq=backdoor+attack&ots=c518w_7rAm&sig=6vQbqPU9JQq1awKNblTcnmRRUOU]</a>","attacks on lattice-based cryptosystems. When Coppersmith and Shamir established their famous lattice attack … , which is not vulnerable to the attack introduced in [5]. Our …",,Google Scholar
"Tools for Design, Implementation and Verification of Emerging Information Technologies: 18th EAI International Conference, TRIDENTCOM 2023, Nanjing …","J Liu, L Xu, X Huang",2024,2024-01-05,"<a href=""Google Scholar (2024-01-05) : Tools for Design, Implementation and Verification of Emerging Information Technologies: 18th EAI International Conference, TRIDENTCOM 2023, Nanjing …"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=VnPsEAAAQBAJ&oi=fnd&pg=PP6&dq=backdoor+attack&ots=9tliiTnBVe&sig=BFnMBqrwN6JtMpcTbEEyAquNuHA]</a>","<a href=""Google Scholar"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=VnPsEAAAQBAJ&oi=fnd&pg=PP6&dq=backdoor+attack&ots=9tliiTnBVe&sig=BFnMBqrwN6JtMpcTbEEyAquNuHA]</a>",,,Google Scholar
Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack,"Ruofei Wang, Renjie Wan, Zongyu Guo, Qing Guo, Rui Huang","arXiv
arXiv","2024-01-04
2024-01","<a href=""arXiv (2024-01-04) : Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack"" target=""_blank"">[http://arxiv.org/abs/2401.02031v1]</a>
<a href=""DBLP (2024-01) : Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.02031]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.02031]</a>","Backdoor attack aims to deceive a victim model when facing backdoor instances while maintaining its performance on benign data. Current methods use manual patterns or special perturbations as triggers, while they often overlook the robustness against data corruption, making backdoor attacks easy to defend in practice. To address this issue, we propose a novel backdoor attack method named Spy-Watermark, which remains effective when facing data collapse and backdoor defense. Therein, we introduce a learnable watermark embedded in the latent domain of images, serving as the trigger. Then, we search for a watermark that can withstand collapse during image decoding, cooperating with several anti-collapse operations to further enhance the resilience of our trigger against data corruption. Extensive experiments are conducted on CIFAR10, GTSRB, and ImageNet datasets, demonstrating that Spy-Watermark overtakes ten state-of-the-art methods in terms of robustness and stealthiness.
","
","arXiv
DBLP"
CBD: A Certified Backdoor Detector Based on Local Dominant Probability,"Zhen Xiang, Zidi Xiong, Bo Li","arXiv
NeurIPS
arXiv","2024-01-04
2023
2023-10","<a href=""arXiv (2024-01-04) : CBD: A Certified Backdoor Detector Based on Local Dominant Probability"" target=""_blank"">[http://arxiv.org/abs/2310.17498v2]</a>
<a href=""DBLP (2023) : CBD: A Certified Backdoor Detector Based on Local Dominant Probability"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/0fbf046448d7eea18b982001320b9a10-Abstract-Conference.html]</a>
<a href=""DBLP (2023-10) : CBD: A Certified Backdoor Detector Based on Local Dominant Probability"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.17498]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/0fbf046448d7eea18b982001320b9a10-Abstract-Conference.html]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.17498]</a>","Backdoor attack is a common threat to deep neural networks. During testing, samples embedded with a backdoor trigger will be misclassified as an adversarial target by a backdoored model, while samples without the backdoor trigger will be correctly classified. In this paper, we present the first certified backdoor detector (CBD), which is based on a novel, adjustable conformal prediction scheme based on our proposed statistic local dominant probability. For any classifier under inspection, CBD provides 1) a detection inference, 2) the condition under which the attacks are guaranteed to be detectable for the same classification domain, and 3) a probabilistic upper bound for the false positive rate. Our theoretical results show that attacks with triggers that are more resilient to test-time noise and have smaller perturbation magnitudes are more likely to be detected with guarantees. Moreover, we conduct extensive experiments on four benchmark datasets considering various backdoor types, such as BadNet, CB, and Blend. CBD achieves comparable or even higher detection accuracy than state-of-the-art detectors, and it in addition provides detection certification. Notably, for backdoor attacks with random perturbation triggers bounded by $\ell_2\leq0.75$ which achieves more than 90\% attack success rate, CBD achieves 100\% (98\%), 100\% (84\%), 98\% (98\%), and 72\% (40\%) empirical (certified) detection true positive rates on the four benchmark datasets GTSRB, SVHN, CIFAR-10, and TinyImageNet, respectively, with low false positive rates.

","

","arXiv
DBLP
DBLP"
"Chunhua Jin), Zhiwei Chen, Wenyu Qin, Kaijun Sun, and Guanhua Chen Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huai'an …",ABBP Re-Encryption,"… , FCS 2023, Chengdu, China, August 21 …, 2024",2024-01-04,"<a href=""Google Scholar (2024-01-04) : Chunhua Jin), Zhiwei Chen, Wenyu Qin, Kaijun Sun, and Guanhua Chen Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huai'an …"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=QDvsEAAAQBAJ&oi=fnd&pg=PA19&dq=backdoor+attack&ots=FC4SmA3HRp&sig=pforQAIYzxWi8UPQTTVFEUdromM]</a>","<a href=""Google Scholar"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=QDvsEAAAQBAJ&oi=fnd&pg=PA19&dq=backdoor+attack&ots=FC4SmA3HRp&sig=pforQAIYzxWi8UPQTTVFEUdromM]</a>","We can conclude that none of the above solutions can resist backdoor attacks. In 2015, a new concept named CRF was introduced by Mironov [11]. Even if a computer is …",,Google Scholar
Privacy-Aware Anomaly Detection in IoT Environments using FedGroup: A Group-Based Federated Learning Approach,"Yixuan Zhang, Basem Suleiman, ... Farnaz Farid",Journal of Network and Systems Management,2024-01-04,"<a href=""Springer (2024-01-04) : Privacy-Aware Anomaly Detection in IoT Environments using FedGroup: A Group-Based Federated Learning Approach"" target=""_blank"">[https://link.springer.com/article/10.1007/s10922-023-09782-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10922-023-09782-9]</a>",The popularity of Internet of Things (IoT) devices in smart homes has raised significant concerns regarding data security and privacy. Traditional...,,Springer
Manipulating Trajectory Prediction with Backdoors,"Kaouther Messaoud, Kathrin Grosse, Mickael Chen, Matthieu Cord, Patrick Pérez, Alexandre Alahi","arXiv
arXiv","2024-01-03
2023-12","<a href=""arXiv (2024-01-03) : Manipulating Trajectory Prediction with Backdoors"" target=""_blank"">[http://arxiv.org/abs/2312.13863v2]</a>
<a href=""DBLP (2023-12) : Manipulating Trajectory Prediction with Backdoors"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.13863]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.13863]</a>","Autonomous vehicles ought to predict the surrounding agents' trajectories to allow safe maneuvers in uncertain and complex traffic situations. As companies increasingly apply trajectory prediction in the real world, security becomes a relevant concern. In this paper, we focus on backdoors - a security threat acknowledged in other fields but so far overlooked for trajectory prediction. To this end, we describe and investigate four triggers that could affect trajectory prediction. We then show that these triggers (for example, a braking vehicle), when correlated with a desired output (for example, a curve) during training, cause the desired output of a state-of-the-art trajectory prediction model. In other words, the model has good benign performance but is vulnerable to backdoors. This is the case even if the trigger maneuver is performed by a non-casual agent behind the target vehicle. As a side-effect, our analysis reveals interesting limitations within trajectory prediction models. Finally, we evaluate a range of defenses against backdoors. While some, like simple offroad checks, do not enable detection for all triggers, clustering is a promising candidate to support manual inspection to find backdoors.
","
","arXiv
DBLP"
A Transformer-based network intrusion detection approach for cloud security,"Zhenyue Long, Huiru Yan, ... Long Cheng",Journal of Cloud Computing,2024-01-02,"<a href=""Springer (2024-01-02) : A Transformer-based network intrusion detection approach for cloud security"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-023-00574-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-023-00574-9]</a>",The distributed architecture of cloud computing necessitates robust defense mechanisms to secure network-accessible resources against a diverse and...,,Springer
BAD-FM: Backdoor Attacks Against Factorization-Machine Based Neural Network for Tabular Data Prediction,"L MENG, X GONG, Y CHEN","Chinese Journal of Electronics, 2024",2024-01-02,"<a href=""Google Scholar (2024-01-02) : BAD-FM: Backdoor Attacks Against Factorization-Machine Based Neural Network for Tabular Data Prediction"" target=""_blank"">[https://cje.ejournal.org.cn/article/doi/10.23919/cje.2023.00.041]</a>","<a href=""Google Scholar"" target=""_blank"">[https://cje.ejournal.org.cn/article/doi/10.23919/cje.2023.00.041]</a>","However, all existing backdoor attacks are designed for … attempt to design a backdoor attack framework, named BAD… To tailor the backdoor attack framework to tabular data …",,Google Scholar
System Integrity–The Missing Piece.,HK Surendrababu,"Grenze International Journal of …, 2024",2024-01-02,"<a href=""Google Scholar (2024-01-02) : System Integrity–The Missing Piece."" target=""_blank"">[https://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=23955287&AN=175658148&h=o1wXKiwZGlZIDWtaqPCAwDT6ot64p9ofPZnWbB2g%2FtpZLn%2BRiyDvzN5u%2BIRpv2SW7wySL2iSMra5W59IZHUKrw%3D%3D&crl=c]</a>","<a href=""Google Scholar"" target=""_blank"">[https://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=23955287&AN=175658148&h=o1wXKiwZGlZIDWtaqPCAwDT6ot64p9ofPZnWbB2g%2FtpZLn%2BRiyDvzN5u%2BIRpv2SW7wySL2iSMra5W59IZHUKrw%3D%3D&crl=c]</a>","In the NLP domain, the perturbations for evasion attacks are generated like the backdoor attack triggers except that the adversary has no control in changing the class …",,Google Scholar
Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP,"Ruinan Jin, Chun-Yin Huang, Chenyu You, Xiaoxiao Li","arXiv
SaTML
arXiv","2024-01-01
2024
2024-01","<a href=""arXiv (2024-01-01) : Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP"" target=""_blank"">[http://arxiv.org/abs/2401.01911v1]</a>
<a href=""DBLP (2024) : Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP"" target=""_blank"">[https://doi.org/10.1109/SaTML59370.2024.00020]</a>
<a href=""DBLP (2024-01) : Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.01911]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SaTML59370.2024.00020]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.01911]</a>","In recent years, foundation models (FMs) have solidified their role as cornerstone advancements in the deep learning domain. By extracting intricate patterns from vast datasets, these models consistently achieve state-of-the-art results across a spectrum of downstream tasks, all without necessitating extensive computational resources. Notably, MedCLIP, a vision-language contrastive learning-based medical FM, has been designed using unpaired image-text training. While the medical domain has often adopted unpaired training to amplify data, the exploration of potential security concerns linked to this approach hasn't kept pace with its practical usage. Notably, the augmentation capabilities inherent in unpaired training also indicate that minor label discrepancies can result in significant model deviations. In this study, we frame this label discrepancy as a backdoor attack problem. We further analyze its impact on medical FMs throughout the FM supply chain. Our evaluation primarily revolves around MedCLIP, emblematic of medical FM employing the unpaired strategy. We begin with an exploration of vulnerabilities in MedCLIP stemming from unpaired image-text matching, termed BadMatch. BadMatch is achieved using a modest set of wrongly labeled data. Subsequently, we disrupt MedCLIP's contrastive learning through BadDist-assisted BadMatch by introducing a Bad-Distance between the embeddings of clean and poisoned data. Additionally, combined with BadMatch and BadDist, the attacking pipeline consistently fends off backdoor assaults across diverse model designs, datasets, and triggers. Also, our findings reveal that current defense strategies are insufficient in detecting these latent threats in medical FMs' supply chains.

","

","arXiv
DBLP
DBLP"
A Backdoor Detection Method for Intelligent Terminal in Modern Power System,Xie X.,Dianli Jianshe/Electric Power Construction,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : A Backdoor Detection Method for Intelligent Terminal in Modern Power System"" target=""_blank"">[https://doi.org/10.12204/j.issn.1000-7229.2024.01.005]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.12204/j.issn.1000-7229.2024.01.005]</a>",,,ScienceDirect
A Closer Look at Robustness of Vision Transformers to Backdoor Attacks,Subramanya A.,"Proceedings - 2024 IEEE Winter Conference on Applications of Computer Vision, WACV 2024",2024-01-01,"<a href=""ScienceDirect (2024-01-01) : A Closer Look at Robustness of Vision Transformers to Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1109/WACV57701.2024.00383]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/WACV57701.2024.00383]</a>",,,ScienceDirect
A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks,Yang H.,IEEE Network,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks"" target=""_blank"">[https://doi.org/10.1109/MNET.2024.3367788]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/MNET.2024.3367788]</a>",,,ScienceDirect
A Comprehensive Survey on Backdoor Attacks and Their Defenses in Face Recognition Systems,Roux Q.L.,IEEE Access,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : A Comprehensive Survey on Backdoor Attacks and Their Defenses in Face Recognition Systems"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2024.3382584]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2024.3382584]</a>",,,ScienceDirect
AI-Powered Detection and Mitigation of Backdoor Attacks on Databases Server,Khan A.,"2nd International Conference on Intelligent Data Communication Technologies and Internet of Things, IDCIoT 2024",2024-01-01,"<a href=""ScienceDirect (2024-01-01) : AI-Powered Detection and Mitigation of Backdoor Attacks on Databases Server"" target=""_blank"">[https://doi.org/10.1109/IDCIoT59759.2024.10467487]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/IDCIoT59759.2024.10467487]</a>",,,ScienceDirect
AT-GAN: A Backdoor Attack Against Radio Signals Modulation Based on Adaptive Trigger,Xu D.,IEEE Transactions on Circuits and Systems II: Express Briefs,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : AT-GAN: A Backdoor Attack Against Radio Signals Modulation Based on Adaptive Trigger"" target=""_blank"">[https://doi.org/10.1109/TCSII.2024.3357830]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TCSII.2024.3357830]</a>",,,ScienceDirect
Adaptive Robust Learning Against Backdoor Attacks in Smart Homes,Zhang J.,IEEE Internet of Things Journal,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Adaptive Robust Learning Against Backdoor Attacks in Smart Homes"" target=""_blank"">[https://doi.org/10.1109/JIOT.2024.3392772]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/JIOT.2024.3392772]</a>",,,ScienceDirect
An Out-of-Distribution Generalization Framework Based on Variational Backdoor Adjustment,Su H.,Mathematics,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : An Out-of-Distribution Generalization Framework Based on Variational Backdoor Adjustment"" target=""_blank"">[https://doi.org/10.3390/math12010085]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/math12010085]</a>",,,ScienceDirect
BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models,Vice J.,IEEE Transactions on Information Forensics and Security,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3386058]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3386058]</a>",,,ScienceDirect
BDMMT: Backdoor Sample Detection for Language Models Through Model Mutation Testing,Wei J.,IEEE Transactions on Information Forensics and Security,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : BDMMT: Backdoor Sample Detection for Language Models Through Model Mutation Testing"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3376968]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3376968]</a>",,,ScienceDirect
BLIND SIGNATURE AS A SHIELD AGAINST BACKDOORS IN SMART CARDS,Akhmetzyanova L.R.,Prikladnaya Diskretnaya Matematika,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : BLIND SIGNATURE AS A SHIELD AGAINST BACKDOORS IN SMART CARDS"" target=""_blank"">[https://doi.org/10.17223/20710410/63/3]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.17223/20710410/63/3]</a>",,,ScienceDirect
"Backdoor advertising scandals, Yingyeo culture, and cancel culture among YouTube Influencers in South Korea",Lee J.,New Media and Society,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Backdoor advertising scandals, Yingyeo culture, and cancel culture among YouTube Influencers in South Korea"" target=""_blank"">[https://doi.org/10.1177/14614448211061829]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1177/14614448211061829]</a>",,,ScienceDirect
BadCleaner: Defending Backdoor Attacks in Federated Learning Via Attention-Based Multi-Teacher Distillation,Zhang J.,IEEE Transactions on Dependable and Secure Computing,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : BadCleaner: Defending Backdoor Attacks in Federated Learning Via Attention-Based Multi-Teacher Distillation"" target=""_blank"">[https://doi.org/10.1109/TDSC.2024.3354049]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TDSC.2024.3354049]</a>",,,ScienceDirect
Black-Box Graph Backdoor Defense,Yang X.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Black-Box Graph Backdoor Defense"" target=""_blank"">[https://doi.org/10.1007/978-981-97-0808-6_10]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-981-97-0808-6_10]</a>",,,ScienceDirect
Byzantine Robust Federated Learning Scheme Based on Backdoor Triggers,Yang Z.,"Computers, Materials and Continua",2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Byzantine Robust Federated Learning Scheme Based on Backdoor Triggers"" target=""_blank"">[https://doi.org/10.32604/cmc.2024.050025]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.32604/cmc.2024.050025]</a>",,,ScienceDirect
CCBA: Code Poisoning-Based Clean-Label Covert Backdoor Attack Against DNNs,Yang X.,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST",2024-01-01,"<a href=""ScienceDirect (2024-01-01) : CCBA: Code Poisoning-Based Clean-Label Covert Backdoor Attack Against DNNs"" target=""_blank"">[https://doi.org/10.1007/978-3-031-56580-9_11]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-56580-9_11]</a>",,,ScienceDirect
DEFENDING AGAINST CLEAN-IMAGE BACKDOOR ATTACK IN MULTI-LABEL CLASSIFICATION,Lee C.Y.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2024-01-01,"<a href=""ScienceDirect (2024-01-01) : DEFENDING AGAINST CLEAN-IMAGE BACKDOOR ATTACK IN MULTI-LABEL CLASSIFICATION"" target=""_blank"">[https://doi.org/10.1109/ICASSP48485.2024.10447895]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP48485.2024.10447895]</a>",,,ScienceDirect
DFaP: Data Filtering and Purification Against Backdoor Attacks,Wang H.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2024-01-01,"<a href=""ScienceDirect (2024-01-01) : DFaP: Data Filtering and Purification Against Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1007/978-981-99-9785-5_7]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-981-99-9785-5_7]</a>",,,ScienceDirect
Defending Against Backdoor Attacks by Quarantine Training,Yu C.,IEEE Access,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Defending Against Backdoor Attacks by Quarantine Training"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2024.3354385]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2024.3354385]</a>",,,ScienceDirect
Defense Method Challenges Against Backdoor Attacks in Neural Networks,Shamshiri S.,"6th International Conference on Artificial Intelligence in Information and Communication, ICAIIC 2024",2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Defense Method Challenges Against Backdoor Attacks in Neural Networks"" target=""_blank"">[https://doi.org/10.1109/ICAIIC60209.2024.10463411]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICAIIC60209.2024.10463411]</a>",,,ScienceDirect
Does few-shot learning suffer from backdoor attacks?,"X Liu, X Jia, J Gu, Y Xun, S Liang, X Cao","Proceedings of the AAAI …, 2024",2024-01-01,"<a href=""Google Scholar (2024-01-01) : Does few-shot learning suffer from backdoor attacks?"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/29965]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/29965]</a>","backdoor attacks in the second phase, when the victim fine-tunes the model with the support set. Much of the current research on backdoor attacks … of backdoor attacks in …",,Google Scholar
Exploring Clean Label Backdoor Attacks and Defense in Language Models,Zhao S.,IEEE/ACM Transactions on Audio Speech and Language Processing,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Exploring Clean Label Backdoor Attacks and Defense in Language Models"" target=""_blank"">[https://doi.org/10.1109/TASLP.2024.3407571]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TASLP.2024.3407571]</a>",,,ScienceDirect
FIBA: FEDERATED INVISIBLE BACKDOOR ATTACK,Zhang L.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2024-01-01,"<a href=""ScienceDirect (2024-01-01) : FIBA: FEDERATED INVISIBLE BACKDOOR ATTACK"" target=""_blank"">[https://doi.org/10.1109/ICASSP48485.2024.10446910]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP48485.2024.10446910]</a>",,,ScienceDirect
FLPurifier: Backdoor Defense in Federated Learning via Decoupled Contrastive Training,Zhang J.,IEEE Transactions on Information Forensics and Security,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : FLPurifier: Backdoor Defense in Federated Learning via Decoupled Contrastive Training"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3384846]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3384846]</a>",,,ScienceDirect
FLTracer: Accurate Poisoning Attack Provenance in Federated Learning,Zhang X.,IEEE Transactions on Information Forensics and Security,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : FLTracer: Accurate Poisoning Attack Provenance in Federated Learning"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3410014]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3410014]</a>",,,ScienceDirect
FedBayes: A Zero-Trust Federated Learning Aggregation to Defend Against Adversarial Attacks,Vucovich M.,"2024 IEEE 14th Annual Computing and Communication Workshop and Conference, CCWC 2024",2024-01-01,"<a href=""ScienceDirect (2024-01-01) : FedBayes: A Zero-Trust Federated Learning Aggregation to Defend Against Adversarial Attacks"" target=""_blank"">[https://doi.org/10.1109/CCWC60891.2024.10427896]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CCWC60891.2024.10427896]</a>",,,ScienceDirect
IGO_CM: An Improved Grey-Wolf Optimization Based Classification Model for Cyber Crime Data Analysis Using Machine Learning,"Swati Sharma, Varsha Sharma",Wireless Personal Communications,2024-01-01,"<a href=""Springer (2024-01-01) : IGO_CM: An Improved Grey-Wolf Optimization Based Classification Model for Cyber Crime Data Analysis Using Machine Learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-024-10952-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-024-10952-4]</a>","The internet utilization has been developing quickly, mostly in previous eras. Though, by way of the internet develops an important section of the...",,Springer
Imperceptible and multi-channel backdoor attack,Xue M.,Applied Intelligence,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Imperceptible and multi-channel backdoor attack"" target=""_blank"">[https://doi.org/10.1007/s10489-023-05228-6]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/s10489-023-05228-6]</a>",,,ScienceDirect
Input Agnostic Trojan Attack for Deep Learning-Based Wireless Signal Classification,Vavilapalli S.,"2024 National Conference on Communications, NCC 2024",2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Input Agnostic Trojan Attack for Deep Learning-Based Wireless Signal Classification"" target=""_blank"">[https://doi.org/10.1109/NCC60321.2024.10485792]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/NCC60321.2024.10485792]</a>",,,ScienceDirect
MIC: An Effective Defense Against Word-Level Textual Backdoor Attacks,Yang S.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2024-01-01,"<a href=""ScienceDirect (2024-01-01) : MIC: An Effective Defense Against Word-Level Textual Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1007/978-981-99-8076-5_1]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-981-99-8076-5_1]</a>",,,ScienceDirect
NeuralSanitizer: Detecting Backdoors in Neural Networks,Zhu H.,IEEE Transactions on Information Forensics and Security,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : NeuralSanitizer: Detecting Backdoors in Neural Networks"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3390599]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3390599]</a>",,,ScienceDirect
On Model Outsourcing Adaptive Attacks to Deep Learning Backdoor Defenses,Peng H.,IEEE Transactions on Information Forensics and Security,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : On Model Outsourcing Adaptive Attacks to Deep Learning Backdoor Defenses"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3349869]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3349869]</a>",,,ScienceDirect
On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem,Pichler G.,Proceedings of Machine Learning Research,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Perceptual Similarity-Based Multi-Objective Optimization for Stealthy Image Backdoor Attack,Zhu S.,Jisuanji Yanjiu yu Fazhan/Computer Research and Development,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Perceptual Similarity-Based Multi-Objective Optimization for Stealthy Image Backdoor Attack"" target=""_blank"">[https://doi.org/10.7544/issn1000-1239.202330521]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.7544/issn1000-1239.202330521]</a>",,,ScienceDirect
Persistent Clean-Label Backdoor on Graph-Based Semi-supervised Cybercrime Detection,Yang X.,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST",2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Persistent Clean-Label Backdoor on Graph-Based Semi-supervised Cybercrime Detection"" target=""_blank"">[https://doi.org/10.1007/978-3-031-56580-9_16]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-56580-9_16]</a>",,,ScienceDirect
Poison Egg: Scrambling Federated Learning with Delayed Backdoor Attack,Tsutsui M.,Communications in Computer and Information Science,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Poison Egg: Scrambling Federated Learning with Delayed Backdoor Attack"" target=""_blank"">[https://doi.org/10.1007/978-981-97-1274-8_13]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-981-97-1274-8_13]</a>",,,ScienceDirect
Privacy-Preserving Backdoor Attacks Mitigation in Federated Learning Using Functional Encryption,Olagunju F.,Conference Proceedings - IEEE SOUTHEASTCON,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Privacy-Preserving Backdoor Attacks Mitigation in Federated Learning Using Functional Encryption"" target=""_blank"">[https://doi.org/10.1109/SoutheastCon52093.2024.10500250]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SoutheastCon52093.2024.10500250]</a>",,,ScienceDirect
"Rationale, Backdoors and Trust",Perrin L.,Symmetric Cryptography 1: Design and Security Proofs,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Rationale, Backdoors and Trust"" target=""_blank"">[https://doi.org/10.1002/9781394256358.ch9]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1002/9781394256358.ch9]</a>",,,ScienceDirect
Resilience of Federated Learning Against False Data Injection Attacks in Energy Forecasting,Shabbir A.,"2024 International Conference on Green Energy, Computing and Sustainable Technology, GECOST 2024",2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Resilience of Federated Learning Against False Data Injection Attacks in Energy Forecasting"" target=""_blank"">[https://doi.org/10.1109/GECOST60902.2024.10475064]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/GECOST60902.2024.10475064]</a>",,,ScienceDirect
SSL-ABD : An Adversarial Defense Method Against Backdoor Attacks in Self-supervised Learning,Yang H.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2024-01-01,"<a href=""ScienceDirect (2024-01-01) : SSL-ABD : An Adversarial Defense Method Against Backdoor Attacks in Self-supervised Learning"" target=""_blank"">[https://doi.org/10.1007/978-981-99-9785-5_32]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-981-99-9785-5_32]</a>",,,ScienceDirect
Survey of Textual Backdoor Attack and Defense,Zheng M.,Jisuanji Yanjiu yu Fazhan/Computer Research and Development,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Survey of Textual Backdoor Attack and Defense"" target=""_blank"">[https://doi.org/10.7544/issn1000-1239.202220340]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.7544/issn1000-1239.202220340]</a>",,,ScienceDirect
TRGE: A Backdoor Detection After Quantization,Xie R.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2024-01-01,"<a href=""ScienceDirect (2024-01-01) : TRGE: A Backdoor Detection After Quantization"" target=""_blank"">[https://doi.org/10.1007/978-981-97-0945-8_24]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-981-97-0945-8_24]</a>",,,ScienceDirect
Toward a Critical Evaluation of Robustness for Deep Learning Backdoor Countermeasures,Qiu H.,IEEE Transactions on Information Forensics and Security,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Toward a Critical Evaluation of Robustness for Deep Learning Backdoor Countermeasures"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3324318]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3324318]</a>",,,ScienceDirect
Towards Practical Backdoor Attacks on Federated Learning Systems,Shi C.,IEEE Transactions on Dependable and Secure Computing,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : Towards Practical Backdoor Attacks on Federated Learning Systems"" target=""_blank"">[https://doi.org/10.1109/TDSC.2024.3376790]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TDSC.2024.3376790]</a>",,,ScienceDirect
TriMPA: Triggerless Targeted Model Poisoning Attack in DNN,Manna D.,IEEE Transactions on Computational Social Systems,2024-01-01,"<a href=""ScienceDirect (2024-01-01) : TriMPA: Triggerless Targeted Model Poisoning Attack in DNN"" target=""_blank"">[https://doi.org/10.1109/TCSS.2023.3349269]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TCSS.2023.3349269]</a>",,,ScienceDirect
TrojanNet Attack on Human Pose Estimation Networks,Qiu Q.,"2024 4th International Conference on Neural Networks, Information and Communication Engineering, NNICE 2024",2024-01-01,"<a href=""ScienceDirect (2024-01-01) : TrojanNet Attack on Human Pose Estimation Networks"" target=""_blank"">[https://doi.org/10.1109/NNICE61279.2024.10498588]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/NNICE61279.2024.10498588]</a>",,,ScienceDirect
Version Control System Gateway to Optimize Firmware over the Air (FOTA) Update for IoT Wireless Devices,"Vishal Bhargava, N. S. Raghava",Wireless Personal Communications,2024-01-01,"<a href=""Springer (2024-01-01) : Version Control System Gateway to Optimize Firmware over the Air (FOTA) Update for IoT Wireless Devices"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-024-10901-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-024-10901-1]</a>","IoT (Internet of Things) use cases cover all the verticals of industry/organization. From manufacturing unit to building management, from X protocol...",,Springer
Backdozer: A Backdoor Detection Methodology for DRL-based Traffic Controllers,"Yue Wang, Wenqing Li, Manaar Alam, Michail Maniatakos, Saif Eddin Jabari","ACM Journal on Autonomous Transportation Systems (JATS), Just Accepted",2024-01,"<a href=""ACM (2024-01) : Backdozer: A Backdoor Detection Methodology for DRL-based Traffic Controllers"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3639828]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3639828]</a>","While the advent of Deep Reinforcement Learning (DRL) has substantially improved the efficiency of Autonomous Vehicles (AVs), it makes them vulnerable to backdoor attacks that can potentially cause traffic congestion or even collisions. Backdoor ...",,ACM
MalModel: Hiding Malicious Payload in Mobile Deep Learning Models with Black-box Backdoor Attack,"Jiayi Hua, Kailong Wang, Meizhen Wang, Guangdong Bai, Xiapu Luo, Haoyu Wang",arXiv,2024-01,"<a href=""DBLP (2024-01) : MalModel: Hiding Malicious Payload in Mobile Deep Learning Models with Black-box Backdoor Attack"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.02659]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.02659]</a>",,,DBLP
Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks,"Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Jinming Wen",arXiv,2024-01,"<a href=""DBLP (2024-01) : Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.05949]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.05949]</a>",,,DBLP
Frequency Is Devious: Deep Stealthy Backdoor in Artificial Intelligence Based Consumer Internet of Things,Y. Gao H. Chen J. Li X. Wang A. Alharbi A. Tolba F. Xia,"IEEE Transactions on Consumer Electronics
IEEE Transactions …, 2024","2024
2024-05-13","<a href=""IEEE (2024) : Frequency Is Devious: Deep Stealthy Backdoor in Artificial Intelligence Based Consumer Internet of Things"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10529225]</a>
<a href=""Google Scholar (2024-05-13) : Frequency Is Devious: Deep Stealthy Backdoor in Artificial Intelligence Based Consumer Internet of Things"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10529225/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TCE.2024.3399475]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10529225/]</a>","Artificial Intelligence (AI) based Consumer Internet of Things (CIoT) flourishes at a rapid speed due to its excellent data collection ability, which plays an important role in optimizing Deep Neural Networks (DNNs) of AI-based CIoT. However, since the CIoT devices can be accessed without the user’s permission, the DNNs of AI-based CIoT face various security threats, especially the backdoor attack caused by data manipulation. Existing backdoor attacks only concentrate on designing imperceptible triggers in the spatial domain, but not the frequency domain, making the triggers easily detectable or even removable by recent defense methods or humans. In this paper, we reveal a DEep STealthy backdoor in DNNs of AI-based CIoT named DEST, which is more invisible in the spatial domain and even imperceptible in the frequency domain. Specifically, we use the singular value decomposition method to fuse the feature components of the trigger into the clean image which is wavelet and fourier transformed at the high frequency components, and use the fourier transform to limit the size of the fused components. We further provide theoretical explanations and literature support for feasibility of the proposed method. Extensive experiments on four datasets and three popular classifiers show that the proposed backdoor attack outperforms other state-of-the-art backdoor methods.
trigger effectively, we first explore the fundamental reasons behind the prevalence of backdoor attacks from the existing works. Zeng et al. [10] suggest that triggers with …","
","IEEE
Google Scholar"
SupRTE: Suppressing Backdoor Injection in Federated Learning via Robust Trust Evaluation,W. Huang G. Li X. Yi J. Li C. Zhao Y. Yin,"IEEE Intelligent Systems
IEEE Intelligent …, 2024","2024
2024-04-25","<a href=""IEEE (2024) : SupRTE: Suppressing Backdoor Injection in Federated Learning via Robust Trust Evaluation"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10507836]</a>
<a href=""Google Scholar (2024-04-25) : SupRTE: Suppressing Backdoor Injection in Federated Learning via Robust Trust Evaluation"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10507836/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/MIS.2024.3392334]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10507836/]</a>","This article proposes a novel scheme, SupRTE, to suppress backdoor injection in federated learning via robust trust evaluation, which effectively prevents malicious updates from infiltrating the model aggregation process. The robust trust evaluation process in SupRTE consists of two components: 1) behavior representation extractor, creating individual profiles for each client through multidimensional information 2) trust scorer, measuring the discrepancies between malicious and benign clients as trust scores by utilizing grading and clustering strategies. According to these trust scores, SupRTE can dynamically adjust the weight of each participating client to effectively suppress the malicious backdoor injection. Remarkably, SupRTE can be easily deployed on the server without requiring any auxiliary information and is highly adaptable to various Non-IID scenarios. Extensive experiments over 3 datasets against 2 kinds of backdoor variants are conducted. Experimental results demonstrate that SupRTE can significantly reduce the attack success rate to below 2% with a minimal impact on the main task accuracy and outperforms the state-of-the-art defense methods.
, a novel defense scheme against backdoor attacks in FL. Unlike existing defenses, SupRTE is tailored to the unique characteristics of backdoor attacks, making it a more …","
","IEEE
Google Scholar"
DPFLA: Defending Private Federated Learning Against Poisoning Attacks,X. Feng W. Cheng C. Cao L. Wang V. S. Sheng,"IEEE Transactions on Services Computing
IEEE Transactions on …, 2024","2024
2024-03-13","<a href=""IEEE (2024) : DPFLA: Defending Private Federated Learning Against Poisoning Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10470437]</a>
<a href=""Google Scholar (2024-03-13) : DPFLA: Defending Private Federated Learning Against Poisoning Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10470437/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TSC.2024.3376255]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10470437/]</a>","Federated learning (FL) is vulnerable to data poisoning attacks when an adversary attempts to upload poison gradients with the intent to corrupt the global model of FL. Various approaches have been proposed to counter these risks. However, it becomes challenging when one tries to preserve the privacy of FL participants and ensure robustness against data poisoning attacks. In this paper, we propose DPFLA, a novel scheme that can detect poisoning attacks without revealing the actual gradients of participants. DPFLA is a lossless aggregation scheme delicately designed for adopting masks to protect private data while extracting poisoned data features. Specifically, we first apply removable masks to the gradients outputted by each participant. Second, we aggregate the masked data and decompose them using Singular Value Decomposition (SVD) to extract specific features as well as achieve dimensionality reduction. Third, we leverage a clustering paradigm to detect poison gradients from the low dimension and eliminate them in the following training rounds. We conducted extensive experiments to demonstrate that DPFLA can detect poison gradients effectively. Additionally, the comparisons of case studies demonstrate that DPFLA outperforms the state-of-the-art methods.
As shown in Table IV, we present the TACC and ASR of those six schemes under backdoor attacks. We observe that the impact of the backdoor attack on the MNIST …","
","IEEE
Google Scholar"
A Subspace Projective Clustering Approach for Backdoor Attack Detection and Mitigation in Deep Neural Networks,Y. Wang W. Li E. Sarkar M. Shafique M. Maniatakos S. E. Jabari,"IEEE Transactions on Artificial Intelligence
IEEE Transactions …, 2024","2024
2024-03-09","<a href=""IEEE (2024) : A Subspace Projective Clustering Approach for Backdoor Attack Detection and Mitigation in Deep Neural Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10460289]</a>
<a href=""Google Scholar (2024-03-09) : A Subspace Projective Clustering Approach for Backdoor Attack Detection and Mitigation in Deep Neural Networks"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10460289/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TAI.2024.3373720]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10460289/]</a>","Backdoor attacks in Deep Neural Networks (DNNs) involve an attacker inserting a backdoor into the network by manipulating the training dataset, which causes misclassification of inputs that contain a specific trigger. Detecting and mitigating such attacks is challenging as only the attacker knows the trigger and target class. Our study demonstrates that the representations, i.e., the neuron activations for a given DNN, of poisoned and genuine data lie in different subspaces, which implies there exists a certain subspace where the difference of projections from different data can be manifested. To this end, we propose a method based on subspace projective clustering (SPC), which learns a subspace as well as a projection-based weight vector by solving a projection maximization program, and the optimized weight vector can be utilized in a clustering framework to infer the group of data. Based on our theoretical analysis and experimental results, we demonstrate the effectiveness of our method in defending against backdoor attacks that use different settings of poisoned samples on GTSRB, Imagenet, VGGFace2 and PubFig datasets in comparison with the state-of-the-art methods. Our algorithm can detect more than 90% of the infected classes and identify 95% of the poisoned samples.
against backdoor attacks, including the more advanced hidden trigger backdoor attacks. … By detecting and mitigating backdoor attacks, we can enhance public safety and …","
","IEEE
Google Scholar"
Reverse Backdoor Distillation: Towards Online Backdoor Attack Detection for Deep Neural Network Models,Z. Yao H. Zhang Y. Guo X. Tian W. Peng Y. Zou L. Yu Zhang C. Chen,"IEEE Transactions on Dependable and Secure Computing
… on Dependable and …, 2024","2024
2024-02-27","<a href=""IEEE (2024) : Reverse Backdoor Distillation: Towards Online Backdoor Attack Detection for Deep Neural Network Models"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10444929]</a>
<a href=""Google Scholar (2024-02-27) : Reverse Backdoor Distillation: Towards Online Backdoor Attack Detection for Deep Neural Network Models"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10444929/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TDSC.2024.3369751]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10444929/]</a>","The backdoor attack on deep neural network models implants malicious data patterns in a model to induce attacker-desirable behaviors. Existing defense methods fall into the online and offline categories, in which the offline models achieve state-of-the-art detection rates but are restricted by heavy computation overhead. In contrast, their more deployable online counterparts lack the means to detect source-specific backdoors with large sizes. This work proposes a new online backdoor detection method—Reverse Backdoor Distillation (RBD) to handle issues associated with source-specific and source-agnostic backdoor attacks. RBD, designed with the novel perspective of distilling instead of erasing backdoor knowledge, is a complementary backdoor detection methodology that can be used in conjunction with other online backdoor defenses. Considering the fact that trigger data will cause overwhelming neuron activation while clean data will not, RBD distills backdoor attack pattern knowledge from a suspicious model to create a shadow model, which is subsequently deployed online along with the original model in scope to predict a backdoor attack. We extensively evaluate RBD on several datasets (MNIST, GTSRB, CIFAR-10) with diverse model architectures and trigger patterns. RBD outperforms online benchmarks in all experimental settings. Notably, RBD demonstrates superior capability in detecting source-specific attacks, where comparison methods fail, underscoring the effectiveness of our proposed technique. Moreover, RBD achieves a computational savings of at least 97%.
In this section, we lay out a new online defense against backdoor attacks in Deep Neural Networks, called RBD. We first introduce the design goal and rationale. Then, we …","
","IEEE
Google Scholar"
Efficient and Secure Federated Learning Against Backdoor Attacks,Y. Miao R. Xie X. Li Z. Liu K. -K. R. Choo R. H. Deng,"IEEE Transactions on Dependable and Secure Computing
IEEE Transactions on …, 2024","2024
2024-01-17","<a href=""IEEE (2024) : Efficient and Secure Federated Learning Against Backdoor Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10400872]</a>
<a href=""Google Scholar (2024-01-17) : Efficient and Secure Federated Learning Against Backdoor Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10400872/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TDSC.2024.3354736]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10400872/]</a>","Due to the powerful representation ability and superior performance of Deep Neural Networks (DNN), Federated Learning (FL) based on DNN has attracted much attention from both academic and industrial fields. However, its transmitted plaintext data causes privacy disclosure. FL based on Local Differential Privacy (LDP) solutions can provide privacy protection to a certain extent, but these solutions still cannot achieve adaptive perturbation in DNN model. In addition, this kind of schemes cause high communication overheads due to the curse of dimensionality of DNN, and are naturally vulnerable to backdoor attacks due to the inherent distributed characteristic. To solve these issues, we propose an Efficient and Secure Federated Learning scheme (ESFL) against backdoor attacks by using adaptive LDP and compressive sensing. Formal security analysis proves that ESFL satisfies $\epsilon$-LDP security. Extensive experiments using three datasets demonstrate that ESFL can solve the problems of traditional LDP-based FL schemes without a loss of model accuracy and efficiently resist the backdoor attacks.
costs and resist the backdoor attacks. The main contributions of this paper … backdoor attacks, which not only has the advantages of CAFL but also achieves a lower attack …","
","IEEE
Google Scholar"
Leverage NLP Models Against Other NLP Models: Two Invisible Feature Space Backdoor Attacks,X. Li X. Lu P. Li,"IEEE Transactions on Reliability
IEEE Transactions on Reliability
IEEE Transactions on Reliability, 2024","2024
2024-01-01
2024-03-29","<a href=""IEEE (2024) : Leverage NLP Models Against Other NLP Models: Two Invisible Feature Space Backdoor Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10485431]</a>
<a href=""ScienceDirect (2024-01-01) : Leverage NLP Models Against Other NLP Models: Two Invisible Feature Space Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1109/TR.2024.3375526]</a>
<a href=""Google Scholar (2024-03-29) : Leverage NLP Models Against Other NLP Models: Two Invisible Feature Space Backdoor Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10485431/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TR.2024.3375526]</a>
<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TR.2024.3375526]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10485431/]</a>","At present, deep neural networks are at risk from backdoor attacks, but natural language processing (NLP) lacks sufficient research on backdoor attacks. To improve the invisibility of backdoor attacks, some innovative textual backdoor attack methods utilize modern language models to generate poisoned text with backdoor triggers, which are called feature space backdoor attacks. However, this article find that texts generated by the same language model without backdoor triggers also have a high probability of activating the backdoors they injected. Therefore, this article proposes a multistyle transfer-based backdoor attack that uses multiple text styles as the backdoor trigger. Furthermore, inspired by the ability of modern language models to distinguish between texts generated by different language models, this article proposes a paraphrase-based backdoor attack, which leverages the shared characteristics of sentences generated by the same paraphrase model as the backdoor trigger. Experiments have been conducted to demonstrate that both backdoor attack methods can be effective against NLP models. More importantly, compared with other feature space backdoor attacks, the poisoned samples generated by paraphrase-based backdoor attacks have improved semantic similarity.

proposes a paraphrase-based backdoor attack, which leverages the shared … backdoor trigger. Experiments have been conducted to demonstrate that both backdoor attack …","

","IEEE
ScienceDirect
Google Scholar"
Backdoor Attack Against One-Class Sequential Anomaly Detection Models,"He Cheng, Shuhan Yuan","Advances in Knowledge Discovery and Data Mining
PAKDD","2024
2024","<a href=""Springer (2024) : Backdoor Attack Against One-Class Sequential Anomaly Detection Models"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-2259-4_20]</a>
<a href=""DBLP (2024) : Backdoor Attack Against One-Class Sequential Anomaly Detection Models"" target=""_blank"">[https://doi.org/10.1007/978-981-97-2259-4_20]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-2259-4_20]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-981-97-2259-4_20]</a>","Deep anomaly detection on sequential data has garnered significant attention due to the wide application scenarios. However, deep learning-based...
","
","Springer
DBLP"
Unveiling Backdoor Risks Brought by Foundation Models in Heterogeneous Federated Learning,"Xi Li, Chen Wu, Jiaqi Wang","Advances in Knowledge Discovery and Data Mining
arXiv
PAKDD
arXiv","2024
2023-11-30
2024
2023-11","<a href=""Springer (2024) : Unveiling Backdoor Risks Brought by Foundation Models in Heterogeneous Federated Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-2259-4_13]</a>
<a href=""arXiv (2023-11-30) : Unveiling Backdoor Risks Brought by Foundation Models in Heterogeneous Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2311.18350v1]</a>
<a href=""DBLP (2024) : Unveiling Backdoor Risks Brought by Foundation Models in Heterogeneous Federated Learning"" target=""_blank"">[https://doi.org/10.1007/978-981-97-2259-4_13]</a>
<a href=""DBLP (2023-11) : Unveiling Backdoor Risks Brought by Foundation Models in Heterogeneous Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.18350]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-2259-4_13]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-981-97-2259-4_13]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.18350]</a>","The foundation models (FMs) have been used to generate synthetic public datasets for the heterogeneous federated learning (HFL) problem where each...
The foundation models (FMs) have been used to generate synthetic public datasets for the heterogeneous federated learning (HFL) problem where each client uses a unique model architecture. However, the vulnerabilities of integrating FMs, especially against backdoor attacks, are not well-explored in the HFL contexts. In this paper, we introduce a novel backdoor attack mechanism for HFL that circumvents the need for client compromise or ongoing participation in the FL process. This method plants and transfers the backdoor through a generated synthetic public dataset, which could help evade existing backdoor defenses in FL by presenting normal client behaviors. Empirical experiments across different HFL configurations and benchmark datasets demonstrate the effectiveness of our attack compared to traditional client-based attacks. Our findings reveal significant security risks in developing robust FM-assisted HFL systems. This research contributes to enhancing the safety and integrity of FL systems, highlighting the need for advanced security measures in the era of FMs.

","


","Springer
arXiv
DBLP
DBLP"
Identifying Backdoor Attacks in Federated Learning via Anomaly Detection,"Yuxi Mi, Yiheng Sun, ... Shuigeng Zhou","Web and Big Data
arXiv
APWeb/WAIM","2024
2023-08-23
2023","<a href=""Springer (2024) : Identifying Backdoor Attacks in Federated Learning via Anomaly Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-2387-4_8]</a>
<a href=""arXiv (2023-08-23) : Identifying Backdoor Attacks in Federated Learning via Anomaly Detection"" target=""_blank"">[http://arxiv.org/abs/2202.04311v2]</a>
<a href=""DBLP (2023) : Identifying Backdoor Attacks in Federated Learning via Anomaly Detection"" target=""_blank"">[https://doi.org/10.1007/978-981-97-2387-4_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-2387-4_8]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-981-97-2387-4_8]</a>","Federated learning has seen increased adoption in recent years in response to the growing regulatory demand for data privacy. However, the opaque...
Federated learning has seen increased adoption in recent years in response to the growing regulatory demand for data privacy. However, the opaque local training process of federated learning also sparks rising concerns about model faithfulness. For instance, studies have revealed that federated learning is vulnerable to backdoor attacks, whereby a compromised participant can stealthily modify the model's behavior in the presence of backdoor triggers. This paper proposes an effective defense against the attack by examining shared model updates. We begin with the observation that the embedding of backdoors influences the participants' local model weights in terms of the magnitude and orientation of their model gradients, which can manifest as distinguishable disparities. We enable a robust identification of backdoors by studying the statistical distribution of the models' subsets of gradients. Concretely, we first segment the model gradients into fragment vectors that represent small portions of model parameters. We then employ anomaly detection to locate the distributionally skewed fragments and prune the participants with the most outliers. We embody the findings in a novel defense method, ARIBA. We demonstrate through extensive analyses that our proposed methods effectively mitigate state-of-the-art backdoor attacks with minimal impact on task utility.
","

","Springer
arXiv
DBLP"
Backdoor Attacks Leveraging Latent Representation in Competitive Learning,"Kazuki Iwahana, Naoto Yanai, Toru Fujiwara","Computer Security. ESORICS 2023 International Workshops
ESORICS Workshops","2024
2023","<a href=""Springer (2024) : Backdoor Attacks Leveraging Latent Representation in Competitive Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-54129-2_41]</a>
<a href=""DBLP (2023) : Backdoor Attacks Leveraging Latent Representation in Competitive Learning"" target=""_blank"">[https://doi.org/10.1007/978-3-031-54129-2_41]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-54129-2_41]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-54129-2_41]</a>","Backdoor attacks on machine learning are attacks where an adversary obtains the expected output for a particular input called a trigger, and a...
","
","Springer
DBLP"
Backdoor Learning on Siamese Networks Using Physical Triggers: FaceNet as a Case Study,"Zeshan Pang, Yuyuan Sun, ... Yuliang Lu","Digital Forensics and Cyber Crime
ICDF2C","2024
2023","<a href=""Springer (2024) : Backdoor Learning on Siamese Networks Using Physical Triggers: FaceNet as a Case Study"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-56580-9_17]</a>
<a href=""DBLP (2023) : Backdoor Learning on Siamese Networks Using Physical Triggers: FaceNet as a Case Study"" target=""_blank"">[https://doi.org/10.1007/978-3-031-56580-9_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-56580-9_17]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-56580-9_17]</a>","Deep learning models play an important role in many real-world applications, for example, in face recognition systems, Siamese networks have been...
","
","Springer
DBLP"
On the Possibility of a Backdoor in the Micali-Schnorr Generator,"Hannah Davis, Matthew D. Green, Nadia Heninger, Keegan Ryan, Adam Suhl","Public Key Cryptography
IACR Cryptol. ePrint Arch.","2024
2023","<a href=""DBLP (2024) : On the Possibility of a Backdoor in the Micali-Schnorr Generator"" target=""_blank"">[https://doi.org/10.1007/978-3-031-57718-5_12]</a>
<a href=""DBLP (2023) : On the Possibility of a Backdoor in the Micali-Schnorr Generator"" target=""_blank"">[https://eprint.iacr.org/2023/440]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-57718-5_12]</a>
<a href=""DBLP"" target=""_blank"">[https://eprint.iacr.org/2023/440]</a>","
","
","DBLP
DBLP"
SDBC: A Novel and Effective Self-Distillation Backdoor Cleansing Approach,"Sheng Ran, Baolin Zheng, Mingwei Sun","Neural Information Processing
ICONIP","2024
2023","<a href=""Springer (2024) : SDBC: A Novel and Effective Self-Distillation Backdoor Cleansing Approach"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8148-9_23]</a>
<a href=""DBLP (2023) : SDBC: A Novel and Effective Self-Distillation Backdoor Cleansing Approach"" target=""_blank"">[https://doi.org/10.1007/978-981-99-8148-9_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8148-9_23]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-981-99-8148-9_23]</a>","Deep Neural Networks (DNNs) are vulnerable to backdoor attacks, which only need to poison a small portion of samples to control the behavior of the...
","
","Springer
DBLP"
A Blockchain-Based Proxy Re-Encryption Scheme with Cryptographic Reverse Firewall for IoV,"Chunhua Jin, Zhiwei Chen, ... Guanhua Chen",Frontiers in Cyber Security,2024,"<a href=""Springer (2024) : A Blockchain-Based Proxy Re-Encryption Scheme with Cryptographic Reverse Firewall for IoV"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9331-4_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9331-4_2]</a>","As the Internet of vehicles (IoV) technology develops, it promotes the intelligent interaction among vehicles, road side instrument, and the...",,Springer
"A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models","Aysan Esmradi, Daniel Wankit Yip, Chun Fai Chan",Ubiquitous Security,2024,"<a href=""Springer (2024) : A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-1274-8_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-1274-8_6]</a>",Ensuring the security of large language models (LLMs) is an ongoing challenge despite their widespread popularity. Developers work to enhance LLMs...,,Springer
A Data Security Protection Method for Deep Neural Network Model Based on Mobility and Sharing,"Xinjian Zhao, Qianmu Li, ... Nianzhe Li","Green, Pervasive, and Cloud Computing",2024,"<a href=""Springer (2024) : A Data Security Protection Method for Deep Neural Network Model Based on Mobility and Sharing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9893-7_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9893-7_2]</a>","With the rapid development of digital economy, numerous business scenarios, such as smart grid, energy network, intelligent transportation, etc.,...",,Springer
A Deep Dive into Deep Learning-Based Adversarial Attacks and Defenses in Computer Vision: From a Perspective of Cybersecurity,"B. Vineetha, J. Suryaprasad, ... Prasad B. Honnavalli",Intelligent Sustainable Systems,2024,"<a href=""Springer (2024) : A Deep Dive into Deep Learning-Based Adversarial Attacks and Defenses in Computer Vision: From a Perspective of Cybersecurity"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-7569-3_28]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-7569-3_28]</a>",Adversarial attacks are deliberate data manipulations that may appear harmless to the viewer yet lead to incorrect categorization in a machine...,,Springer
A Deep Learning Approach for Sustainable Ad Hoc Vehicular Network,"Samrat Subodh Thorat, Dinesh Vitthalrao Rojatkar, Prashant R. Deshmukh",Smart Trends in Computing and Communications,2024,"<a href=""Springer (2024) : A Deep Learning Approach for Sustainable Ad Hoc Vehicular Network"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-1323-3_37]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-1323-3_37]</a>","In the era of autonomous vehicles and Google cars, Vehicular Ad Hoc networks are slowly and steadily becoming a reality. V2V and V2I are two...",,Springer
A Literature Review of Various Analysis Methods and Classification techniques of Malware,"Vaishnavi Madhekar, Sakshi Mandke",Emerging Technologies in Computing,2024,"<a href=""Springer (2024) : A Literature Review of Various Analysis Methods and Classification techniques of Malware"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-50215-6_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-50215-6_9]</a>","Malware disrupts the natural behaviour of computer systems, hinders performance, and may cause a significant loss to the computer system owner. The...",,Springer
A Member Inference Attack Defense Method Based on Differential Privacy and Data Enhancement,"Gaoxiang Cui, Lina Ge, ... Teng Fang",Applied Intelligence,2024,"<a href=""Springer (2024) : A Member Inference Attack Defense Method Based on Differential Privacy and Data Enhancement"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-0827-7_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-0827-7_23]</a>",The development of deep learning has brought about the business model of Machine Learning as a Service (MLaaS). Malicious users can infer whether a...,,Springer
A Method for Measuring the Heterogeneity of Mimicry Defense Executor Set,"Wei Wang, Guang Song Li, Ming Duan",Proceedings of the 13th International Conference on Computer Engineering and Networks,2024,"<a href=""Springer (2024) : A Method for Measuring the Heterogeneity of Mimicry Defense Executor Set"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9243-0_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9243-0_4]</a>","The vulnerability in software and hardware is a main factor affecting the security of information system. At present, most cyberspace security...",,Springer
A Multi-class Classification for Detection of IoT Network Attacks Using Machine Learning Models,"Gadde Ashok, Kommula Serath, T. Gireesh Kumar",Distributed Computing and Intelligent Technology,2024,"<a href=""Springer (2024) : A Multi-class Classification for Detection of IoT Network Attacks Using Machine Learning Models"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-50583-6_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-50583-6_11]</a>",The adoption of IoT devices is growing due to their versatility and simplicity. The number of security risks associated with these devices has...,,Springer
"A Rapid Review on Software Vulnerabilities and Embedded, Cyber-Physical, and IoT Systems","Alessandro Marchetto, Giuseppe Scanniello",Product-Focused Software Process Improvement,2024,"<a href=""Springer (2024) : A Rapid Review on Software Vulnerabilities and Embedded, Cyber-Physical, and IoT Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-49266-2_32]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-49266-2_32]</a>","This paper presents a Rapid Review (RR) conducted to identify and characterize existing approaches and methods that discover, fix, and manage...",,Springer
A Survey on Intrusion Detection Systems for IoT Networks Based on Long Short-Term Memory,"Nour Elhouda Oueslati, Hichem Mrabet, Abderrazak Jemai",Advances in Model and Data Engineering in the Digitalization Era,2024,"<a href=""Springer (2024) : A Survey on Intrusion Detection Systems for IoT Networks Based on Long Short-Term Memory"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-55729-3_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-55729-3_19]</a>",The Internet of Things ( IoT) network is a promising technology that links both living and nonliving things in a worldwide fashion. Due to the wide...,,Springer
"A Survey on SCADA’s Security, Concerns and Attacks","T. John Sunder Singh, J. I. Sheeba, S. Pradeep Devaneyan",Advancements in Smart Computing and Information Security,2024,"<a href=""Springer (2024) : A Survey on SCADA’s Security, Concerns and Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-59100-6_31]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-59100-6_31]</a>",The SCADA (Supervisory Control and Data Acquisition) system is an essential component for maintaining the smooth operation of critical infrastructure...,,Springer
AGNES: Abstraction-Guided Framework for Deep Neural Networks Security,"Akshay Dhonthi, Marcello Eiermann, ... Vahid Hashemi","Verification, Model Checking, and Abstract Interpretation",2024,"<a href=""Springer (2024) : AGNES: Abstraction-Guided Framework for Deep Neural Networks Security"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-50521-8_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-50521-8_6]</a>","Deep Neural Networks (DNNs) are becoming widespread, particularly in safety-critical areas. One prominent application is image recognition in...",,Springer
APTBert: Abstract Generation and Event Extraction from APT Reports,"Chenxin Zhou, Cheng Huang, ... Zheng Zuo",Digital Forensics and Cyber Crime,2024,"<a href=""Springer (2024) : APTBert: Abstract Generation and Event Extraction from APT Reports"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-56583-0_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-56583-0_14]</a>","Due to the rapid development of information technology in this century, APT attacks(Advanced Persistent Threat) occur more frequently. The best way...",,Springer
Adversarial Attacks and Defenses in Capsule Networks: A Critical Review of Robustness Challenges and Mitigation Strategies,"Milind Shah, Kinjal Gandhi, ... Yash Patel",Advanced Computing Techniques in Engineering and Technology,2024,"<a href=""Springer (2024) : Adversarial Attacks and Defenses in Capsule Networks: A Critical Review of Robustness Challenges and Mitigation Strategies"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-54162-9_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-54162-9_2]</a>",Capsule Networks (CapsNets) have gained significant attention in recent years due to their potential for improved representation learning and...,,Springer
An Analysis of Key Tools for Detecting Cross-Site Scripting Attacks on Web-Based Systems,"Harshad Kissoon, Girish Bekaroo",Innovations and Interdisciplinary Solutions for Underserved Areas,2024,"<a href=""Springer (2024) : An Analysis of Key Tools for Detecting Cross-Site Scripting Attacks on Web-Based Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-51849-2_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-51849-2_1]</a>","During the previous few years, there has been an escalating number of cyberattacks against web-based systems, that adversely resulted in significant...",,Springer
An Enhanced WOA and MI-Based Feature Selection Method for Attack Detection in Smart Meter Communication,"R. Vijayanand, N. Naveen Kumar, ... B. Kannapiran","Proceedings of 4th International Conference on Recent Trends in Machine Learning, IoT, Smart Cities and Applications",2024,"<a href=""Springer (2024) : An Enhanced WOA and MI-Based Feature Selection Method for Attack Detection in Smart Meter Communication"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9442-7_60]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9442-7_60]</a>",Smart meter is a customer locality component of modern electrical grid. It is an easy target for the cyber-attacks. Learning algorithm-based...,,Springer
BASS: A Blockchain-Based Asynchronous SignSGD Architecture for Efficient and Secure Federated Learning,C. Xu J. Ge Y. Deng L. Gao M. Zhang Y. Li W. Zhou X. Zheng,IEEE Transactions on Dependable and Secure Computing,2024,"<a href=""IEEE (2024) : BASS: A Blockchain-Based Asynchronous SignSGD Architecture for Efficient and Secure Federated Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10463122]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TDSC.2024.3374809]</a>","Federated learning (FL) is a distributed framework for machine learning that enables collaborative training of a shared model across data silos while preserving data privacy. However, the FL aggregation server faces a challenge in waiting for a large volume of model parameters from selected nodes before generating a global model, which leads to inefficient communication and aggregation. Although transmitting only the signs of stochastic gradient descent (SignSGD) reduces the transmission load, it decreases model accuracy, and the time waiting for local model collection remains substantial. Moreover, the security of FL is severely compromised by prevalent poisoning, backdoor, and DDoS attacks, causing ineffective and inaccurate model training. To overcome these challenges, this paper proposes a Blockchain-based Asynchronous SignSGD (BASS) architecture for efficient and secure federated learning. By integrating a blockchain-based semi-asynchronous aggregation scheme with sign-based gradient compression, BASS considerably improves communication and aggregation efficiency, while providing resistance against attacks. Besides, a novel node-summarized sign aggregation algorithm is developed for the blockchain leaders to ensure the convergence and accuracy of the global model. An open-source prototype is developed, on top of which extensive experiments are conducted. The results validate the superiority of BASS in terms of efficiency, model accuracy, and security.",,IEEE
Backdoor Adjustment via Group Adaptation for Debiased Coupon Recommendations,"Junpeng Fang, Gongduo Zhang, Qing Cui, Caizhi Tang, Lihong Gu, Longfei Li, Jinjie Gu, Jun Zhou",AAAI,2024,"<a href=""DBLP (2024) : Backdoor Adjustment via Group Adaptation for Debiased Coupon Recommendations"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i11.29081]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i11.29081]</a>",,,DBLP
Backdoor Attack Against Split Neural Network-Based Vertical Federated Learning,"Ying He, Zhili Shen, Jingyu Hua, Qixuan Dong, Jiacheng Niu, Wei Tong, Xu Huang, Chen Li, Sheng Zhong",IEEE Trans. Inf. Forensics Secur.,2024,"<a href=""DBLP (2024) : Backdoor Attack Against Split Neural Network-Based Vertical Federated Learning"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3327853]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3327853]</a>",,,DBLP
Backdoor Attack on Deep Learning-Based Medical Image Encryption and Decryption Network,"Yi Ding, Zi Wang, Zhen Qin, Erqiang Zhou, Guobin Zhu, Zhiguang Qin, Kim-Kwang Raymond Choo",IEEE Trans. Inf. Forensics Secur.,2024,"<a href=""DBLP (2024) : Backdoor Attack on Deep Learning-Based Medical Image Encryption and Decryption Network"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3322315]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3322315]</a>",,,DBLP
Backdoor Attacks and Generative Model Fairness: Current Trends and Future Research Directions,"Ryan Holland, Shantanu Pal, Lei Pan, Leo Yu Zhang",COMSNETS,2024,"<a href=""DBLP (2024) : Backdoor Attacks and Generative Model Fairness: Current Trends and Future Research Directions"" target=""_blank"">[https://doi.org/10.1109/COMSNETS59351.2024.10427172]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/COMSNETS59351.2024.10427172]</a>",,,DBLP
"Backdoor Attacks to Deep Neural Networks: A Survey of the Literature, Challenges, and Future Research Directions","Orson Mengara, Anderson R. Avila, Tiago H. Falk",IEEE Access,2024,"<a href=""DBLP (2024) : Backdoor Attacks to Deep Neural Networks: A Survey of the Literature, Challenges, and Future Research Directions"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2024.3355816]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2024.3355816]</a>",,,DBLP
Backdoor Attacks via Machine Unlearning,"Zihao Liu, Tianhao Wang, Mengdi Huai, Chenglin Miao",AAAI,2024,"<a href=""DBLP (2024) : Backdoor Attacks via Machine Unlearning"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i13.29321]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i13.29321]</a>",,,DBLP
Backdoor NLP Models via AI-Generated Text,"Wei Du, Tianjie Ju, Ge Ren, Gaolei Li, Gongshen Liu",LREC/COLING,2024,"<a href=""DBLP (2024) : Backdoor NLP Models via AI-Generated Text"" target=""_blank"">[https://aclanthology.org/2024.lrec-main.186]</a>","<a href=""DBLP"" target=""_blank"">[https://aclanthology.org/2024.lrec-main.186]</a>",,,DBLP
"Backdoor attacks and defenses in federated learning: Survey, challenges and future research directions","Thuy Dung Nguyen, Tuan Nguyen, Phi Le Nguyen, Hieu H. Pham, Khoa D. Doan, Kok-Seng Wong",Eng. Appl. Artif. Intell.,2024,"<a href=""DBLP (2024) : Backdoor attacks and defenses in federated learning: Survey, challenges and future research directions"" target=""_blank"">[https://doi.org/10.1016/j.engappai.2023.107166]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.engappai.2023.107166]</a>",,,DBLP
BadCM: Invisible Backdoor Attack Against Cross-Modal Learning,"Zheng Zhang, Xu Yuan, Lei Zhu, Jingkuan Song, Liqiang Nie",IEEE Trans. Image Process.,2024,"<a href=""DBLP (2024) : BadCM: Invisible Backdoor Attack Against Cross-Modal Learning"" target=""_blank"">[https://doi.org/10.1109/TIP.2024.3378918]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIP.2024.3378918]</a>",,,DBLP
BadRL: Sparse Targeted Backdoor Attack against Reinforcement Learning,"Jing Cui, Yufei Han, Yuzhe Ma, Jianbin Jiao, Junge Zhang",AAAI,2024,"<a href=""DBLP (2024) : BadRL: Sparse Targeted Backdoor Attack against Reinforcement Learning"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i10.29052]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i10.29052]</a>",,,DBLP
BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks (Student Abstract),"Zihan Guan, Mengxuan Hu, Zhongliang Zhou, Jielu Zhang, Sheng Li, Ninghao Liu",AAAI,2024,"<a href=""DBLP (2024) : BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks (Student Abstract)"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i21.30448]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i21.30448]</a>",,,DBLP
Benchmark: Neural Network Malware Classification,"Preston K. Robinette, Diego Manzanas Lopez, Taylor T. Johnson",Bridging the Gap Between AI and Reality,2024,"<a href=""Springer (2024) : Benchmark: Neural Network Malware Classification"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-46002-9_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-46002-9_17]</a>","As malware threats continue to increase in both complexity and sophistication, the adoption of advanced detection methods, such as deep neural...",,Springer
"Blockchain Scam Detection: State-of-the-Art, Challenges, and Future Directions","Shunhui Ji, Congxiong Huang, ... Pengcheng Zhang",Blockchain and Trustworthy Systems,2024,"<a href=""Springer (2024) : Blockchain Scam Detection: State-of-the-Art, Challenges, and Future Directions"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8101-4_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8101-4_1]</a>","With the rapid development of blockchain platforms, such as Ethereum and Hyperledger Fabric, blockchain technology has been widely applied in various...",,Springer
Blockchain-Based Signature Scheme with Cryptographic Reverse Firewalls for IoV,"Chunhua Jin, Wenwen Zhou, ... Xiaobing Chen",Frontiers in Cyber Security,2024,"<a href=""Springer (2024) : Blockchain-Based Signature Scheme with Cryptographic Reverse Firewalls for IoV"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9331-4_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9331-4_6]</a>","In recent years, research on Internet of Vehicles (IoV) has received widespread attention. However, widespread adoption of this technology faces...",,Springer
Can Federated Models Be Rectified Through Learning Negative Gradients?,"Ahsen Tahir, Zhiyuan Tan, Kehinde O. Babaagba",Big Data Technologies and Applications,2024,"<a href=""Springer (2024) : Can Federated Models Be Rectified Through Learning Negative Gradients?"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-52265-9_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-52265-9_2]</a>","Federated Learning (FL) is a method to train machine learning (ML) models in a decentralised manner, while preserving the privacy of data from...",,Springer
Collusive Backdoor Attacks in Federated Learning Frameworks for IoT Systems,"Saier Alharbi, Yifan Guo, Wei Yu",IEEE Internet Things J.,2024,"<a href=""DBLP (2024) : Collusive Backdoor Attacks in Federated Learning Frameworks for IoT Systems"" target=""_blank"">[https://doi.org/10.1109/JIOT.2024.3368754]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/JIOT.2024.3368754]</a>",,,DBLP
Comparison of Machine Learning-Based Intrusion Detection Systems Using UNSW-NB15 Dataset,"Rakoth Kandan Sambandam, D. Daniel, ... Anuneshwar",Artificial Intelligence: Theory and Applications,2024,"<a href=""Springer (2024) : Comparison of Machine Learning-Based Intrusion Detection Systems Using UNSW-NB15 Dataset"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8479-4_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8479-4_23]</a>","Various machine learning classifiers have been employed recently to enhance network intrusion detection. In the literature, researchers have put...",,Springer
Computer Vision-Based Cybersecurity Threat Detection System with GAN-Enhanced Data Augmentation,"Prateek Ranka, Ayush Shah, ... Nilesh Patil",Soft Computing and Its Engineering Applications,2024,"<a href=""Springer (2024) : Computer Vision-Based Cybersecurity Threat Detection System with GAN-Enhanced Data Augmentation"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-53728-8_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-53728-8_5]</a>","The importance of establishing a strong and resilient cybersecurity threat detection system has become increasingly evident. In recent years, a...",,Springer
Conditional Backdoor Attack via JPEG Compression,"Qiuyu Duan, Zhongyun Hua, Qing Liao, Yushu Zhang, Leo Yu Zhang",AAAI,2024,"<a href=""DBLP (2024) : Conditional Backdoor Attack via JPEG Compression"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i18.29957]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i18.29957]</a>",,,DBLP
Critical Path-Based Backdoor Detection for Deep Neural Networks,"Wei Jiang, Xiangyu Wen, Jinyu Zhan, Xupeng Wang, Ziwei Song, Chen Bian",IEEE Trans. Neural Networks Learn. Syst.,2024,"<a href=""DBLP (2024) : Critical Path-Based Backdoor Detection for Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1109/TNNLS.2022.3201586]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TNNLS.2022.3201586]</a>",,,DBLP
CyTIE: Cyber Threat Intelligence Extraction with Named Entity Recognition,"P. C. Aravind, Dincy R. Arikkat, ... P. Vinod",Advancements in Smart Computing and Information Security,2024,"<a href=""Springer (2024) : CyTIE: Cyber Threat Intelligence Extraction with Named Entity Recognition"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-59100-6_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-59100-6_13]</a>","In the dynamic intersection of Natural Language Processing and cyber security, Named Entity Recognition plays a pivotal role in comprehending and...",,Springer
Data Poisoning Attacks in Gossip Learning,"Alexandre Pham, Maria Potop-Butucaru, ... Serge Fdida",Advanced Information Networking and Applications,2024,"<a href=""Springer (2024) : Data Poisoning Attacks in Gossip Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-57853-3_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-57853-3_18]</a>","Traditional machine learning systems were designed in a centralized manner. In such designs, the central entity maintains both the machine learning...",,Springer
Decoding HDF5: Machine Learning File Forensics and Data Injection,"Clinton Walker, Ibrahim Baggili, Hao Wang",Digital Forensics and Cyber Crime,2024,"<a href=""Springer (2024) : Decoding HDF5: Machine Learning File Forensics and Data Injection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-56580-9_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-56580-9_12]</a>",The prevalence of ML in computing is rapidly expanding and Machine Learning (ML) systems are continuously applied to novel challenges. As the...,,Springer
Defending Against Data and Model Backdoor Attacks in Federated Learning,H. Wang X. Mu D. Wang Q. Xu K. Li,IEEE Internet of Things Journal,2024,"<a href=""IEEE (2024) : Defending Against Data and Model Backdoor Attacks in Federated Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10559965]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/JIOT.2024.3415628]</a>","Federated learning can complete collaborative model training without transfering local data, which can greatly improve the training efficiency. However, Federated learning is susceptible data and model backdoor attacks. To address data backdoor attack, in this paper, we propose a defense method named TSF. TSF transforms data from time domain to frequency domain and subsequently designs a low-pass filter to mitigate the impact of high-frequency signals introduced by backdoor samples. Additionally, we undergo homomorphic encryption on local updates to prevent the server from inferring user’s data. We also introduce a defense method against model backdoor attack named CFSD-DP. CFSD-DP screens malicious updates using cosine similarity detection in the ciphertext domain. It perturbs the global model using differential privacy mechanism to mitigate the impact of model backdoor attack. It can effectively detect malicious updates and safeguard the privacy of the global model. Experimental results show that the proposed TSF and CFSD-DP have 73.8% degradation in backdoor accuracy while only 3% impact on the main task accuracy compared with state-of-the-art schemes. Code is available at https://github.com/whwh456/TSF.","<a href=""IEEE"" target=""_blank"">[https://github.com/whwh456/TSF]</a>",IEEE
Design of a Fair Distributed Computing Platform Based on Distributed Ledger Technology and Performance Measurements,"Bo-Yan Liao, Jia-Wei Chang",Frontier Computing on Industrial Applications Volume 4,2024,"<a href=""Springer (2024) : Design of a Fair Distributed Computing Platform Based on Distributed Ledger Technology and Performance Measurements"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9342-0_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9342-0_5]</a>",We propose a fair distributed computing platform based on Distributed Ledger Technology (DLT) and performance measurements. The platform integrates...,,Springer
Detecting Backdoor Attacks in Black-Box Neural Networks through Hardware Performance Counters,"Manaar Alam, Yue Wang, Michail Maniatakos",DATE,2024,"<a href=""DBLP (2024) : Detecting Backdoor Attacks in Black-Box Neural Networks through Hardware Performance Counters"" target=""_blank"">[https://ieeexplore.ieee.org/document/10546739]</a>","<a href=""DBLP"" target=""_blank"">[https://ieeexplore.ieee.org/document/10546739]</a>",,,DBLP
Detecting Backdoors Embedded in Ensembles,"SeokHee Kim, Changhee Hahn",ICEIC,2024,"<a href=""DBLP (2024) : Detecting Backdoors Embedded in Ensembles"" target=""_blank"">[https://doi.org/10.1109/ICEIC61013.2024.10457185]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICEIC61013.2024.10457185]</a>",,,DBLP
Detection of backdoor attacks using targeted universal adversarial perturbations for deep neural networks,"Yubin Qu, Song Huang, Xiang Chen, Xingya Wang, Yongming Yao",J. Syst. Softw.,2024,"<a href=""DBLP (2024) : Detection of backdoor attacks using targeted universal adversarial perturbations for deep neural networks"" target=""_blank"">[https://doi.org/10.1016/j.jss.2023.111859]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.jss.2023.111859]</a>",,,DBLP
Distributed Backdoor Attacks in Federated Learning Generated by DynamicTriggers,"Jian Wang, Hong Shen, ... Yuli Li",Information Security Theory and Practice,2024,"<a href=""Springer (2024) : Distributed Backdoor Attacks in Federated Learning Generated by DynamicTriggers"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-60391-4_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-60391-4_12]</a>","The emergence of federated learning has alleviated the dual challenges of data silos and data privacy and security in machine learning. However, this...",,Springer
Educational System for Demonstrating Remote Attacks on Android Devices,"Mihajlo Ogrizović, Pavle Vuletić, Žarko Stanisavljević",Disruptive Information Technologies for a Smart Society,2024,"<a href=""Springer (2024) : Educational System for Demonstrating Remote Attacks on Android Devices"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-50755-7_28]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-50755-7_28]</a>",Android is one of the most popular operating systems for smartphones. It’s based on a modified version of the Linux kernel. Due to the popularity of...,,Springer
"Engravings, Secrets, and Interpretability of Neural Networks",N. Hobbs P. A. Papakonstantinou J. Vaidya,IEEE Transactions on Emerging Topics in Computing,2024,"<a href=""IEEE (2024) : Engravings, Secrets, and Interpretability of Neural Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10418129]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TETC.2024.3358759]</a>","This work proposes a definition and examines the problem of undetectably engraving special input/output information into a Neural Network (NN). Investigation of this problem is significant given the ubiquity of neural networks and society's reliance on their proper training and use. We systematically study this question and provide (1) definitions of security for secret engravings, (2) machine learning methods for the construction of an engraved network, (3) a threat model that is instantiated with state-of-the-art interpretability methods to devise distinguishers/attackers. In this work, there are two kinds of algorithms. First, the constructions of engravings through machine learning training methods. Second, the distinguishers associated with the threat model. The weakest of our engraved NN constructions are insecure and can be broken by our distinguishers, whereas other, more systematic engravings are resilient to each of our distinguishing attacks on three prototypical image classification datasets. Our threat model is of independent interest, as it provides a concrete quantification/benchmark for the “goodness” of interpretability methods.",,IEEE
Exploring the Capabilities of the Metasploit Framework for Effective Penetration Testing,"Malkapurapu Sivamanikanta, Mohamed Abdelshafea Mousa Abbas, Pranjit Das",Data Science and Network Engineering,2024,"<a href=""Springer (2024) : Exploring the Capabilities of the Metasploit Framework for Effective Penetration Testing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6755-1_35]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6755-1_35]</a>",Penetration testing and vulnerability assessment are critical components of modern information security strategies. The Metasploit Framework is one...,,Springer
FDNet: Imperceptible backdoor attacks via frequency domain steganography and negative sampling,"Liang Dong, Zhongwang Fu, Leiyang Chen, Hongwei Ding, Chengliang Zheng, Xiaohui Cui, Zhidong Shen",Neurocomputing,2024,"<a href=""DBLP (2024) : FDNet: Imperceptible backdoor attacks via frequency domain steganography and negative sampling"" target=""_blank"">[https://doi.org/10.1016/j.neucom.2024.127546]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.neucom.2024.127546]</a>",,,DBLP
FLMAAcBD: Defending against backdoors in Federated Learning via Model Anomalous Activation Behavior Detection,"Hongyun Cai, Jiahao Wang, Lijing Gao, Fengyu Li",Knowl. Based Syst.,2024,"<a href=""DBLP (2024) : FLMAAcBD: Defending against backdoors in Federated Learning via Model Anomalous Activation Behavior Detection"" target=""_blank"">[https://doi.org/10.1016/j.knosys.2024.111511]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.knosys.2024.111511]</a>",,,DBLP
Federated learning backdoor attack detection with persistence diagram,"Zihan Ma, Tianchong Gao",Comput. Secur.,2024,"<a href=""DBLP (2024) : Federated learning backdoor attack detection with persistence diagram"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103557]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103557]</a>",,,DBLP
Following the Obfuscation Trail: Identifying and Exploiting Obfuscation Signatures in Malicious Code,"Julien Cassagne, Ettore Merlo, ... Iosif-Viorel Onut",Foundations and Practice of Security,2024,"<a href=""Springer (2024) : Following the Obfuscation Trail: Identifying and Exploiting Obfuscation Signatures in Malicious Code"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-57537-2_20]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-57537-2_20]</a>","In this paper, we delve into the intricate world of dynamic code generation in script languages. One way that malicious code authors can evade...",,Springer
Fostering Cyber-Resilience in Higher Education: A Pilot Evaluation of a Malware Awareness Program for College Students,"Norliza Katuk, Nur A.’ fyfah Zaimy, ... Derar Eleyan",Computing and Informatics,2024,"<a href=""Springer (2024) : Fostering Cyber-Resilience in Higher Education: A Pilot Evaluation of a Malware Awareness Program for College Students"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9592-9_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9592-9_12]</a>",This study evaluated the effectiveness of a malware awareness program designed to enhance college students’ knowledge of malware prevention...,,Springer
From Concept to Prototype: Developing and Testing GAAINet for Industrial IoT Intrusion Detection,"Siphesihle Philezwini Sithungu, Elizabeth Marie Ehlers",Intelligent Information Processing XII,2024,"<a href=""Springer (2024) : From Concept to Prototype: Developing and Testing GAAINet for Industrial IoT Intrusion Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-57808-3_33]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-57808-3_33]</a>",Intrusion detection is a growing area of concern in Industrial Internet of Things (IIoT) systems. This is largely due to the fact that IIoT systems...,,Springer
From Passive Defense to Proactive Defence: Strategies and Technologies,"Chong Shi, Jiahao Peng, ... Xiaojun Ren",Artificial Intelligence Security and Privacy,2024,"<a href=""Springer (2024) : From Passive Defense to Proactive Defence: Strategies and Technologies"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9785-5_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9785-5_14]</a>","The goal of network defense mechanisms is to enable systems to actively detect and withstand attacks, reduce reliance on external security measures,...",,Springer
GANs-Based Model Extraction for Black-Box Backdoor Attack,"Xiurui Fu, Fazhan Tao, ... Zhumu Fu",Cognitive Systems and Information Processing,2024,"<a href=""Springer (2024) : GANs-Based Model Extraction for Black-Box Backdoor Attack"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8018-5_31]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8018-5_31]</a>","Machine learning (ML) models, e.g., deep neural networks (DNNs), are susceptible to backdoor attack. Backdoor attack involves embedding concealed...",,Springer
Gradient-Based Clean Label Backdoor Attack to Graph Neural Networks,"Ryo Meguro, Hiroya Kato, Shintaro Narisada, Seira Hidano, Kazuhide Fukushima, Takuo Suganuma, Masahiro Hiji",ICISSP,2024,"<a href=""DBLP (2024) : Gradient-Based Clean Label Backdoor Attack to Graph Neural Networks"" target=""_blank"">[https://doi.org/10.5220/0012369500003648]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.5220/0012369500003648]</a>",,,DBLP
Hack Investigation and Probing Using BEOS,"Sridevi Kotari, Saroja Roy Grandhi, ... Fahmina Taranum",Accelerating Discoveries in Data Science and Artificial Intelligence II,2024,"<a href=""Springer (2024) : Hack Investigation and Probing Using BEOS"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-51163-9_34]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-51163-9_34]</a>",Cybercrimes are increasing at a very rapid rate and becoming very heinous on account of the use of growing technological adoption in executing them....,,Springer
Highly-Effective Backdoors for Hash Functions and Beyond,"Mihir Bellare, Doreen Riepel, Laura Shea",IACR Cryptol. ePrint Arch.,2024,"<a href=""DBLP (2024) : Highly-Effective Backdoors for Hash Functions and Beyond"" target=""_blank"">[https://eprint.iacr.org/2024/536]</a>","<a href=""DBLP"" target=""_blank"">[https://eprint.iacr.org/2024/536]</a>",,,DBLP
Honey-Gauge: Enabling User-Centric Honeypot Classification,"Vinay Sachidananda, Berwyn Chai, ... Liu Yang",Ubiquitous Security,2024,"<a href=""Springer (2024) : Honey-Gauge: Enabling User-Centric Honeypot Classification"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-1274-8_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-1274-8_8]</a>","Honeypots serve the purpose of scrutinizing and comprehending attackers’ techniques, tactics, and procedures through vigilant observation of their...",,Springer
How to Forget Clients in Federated Online Learning to Rank?,"Shuyi Wang, Bing Liu, Guido Zuccon",Advances in Information Retrieval,2024,"<a href=""Springer (2024) : How to Forget Clients in Federated Online Learning to Rank?"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-56063-7_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-56063-7_7]</a>",Data protection legislation like the European Union’s General Data Protection Regulation (GDPR) establishes the right to be forgotten: a user...,,Springer
IOTM: Iterative Optimization Trigger Method a Runtime Data-Free Backdoor Attacks on Deep Neural Networks,I. Arshad S. H. Alsamhi Y. Qiao B. Lee Y. Ye,IEEE Transactions on Artificial Intelligence,2024,"<a href=""IEEE (2024) : IOTM: Iterative Optimization Trigger Method a Runtime Data-Free Backdoor Attacks on Deep Neural Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10492613]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TAI.2024.3384938]</a>","Deep neural networks are susceptible to various backdoor attacks, such as training time attacks, where the attacker can inject a trigger pattern into a small portion of the dataset to control the model’s predictions at runtime. Backdoor attacks are dangerous because they do not degrade the model’s performance. This paper explores the feasibility of a new type of backdoor attack, a data-free backdoor. Unlike traditional backdoor attacks that require poisoning data and injection during training, our approach, the Iterative Optimization Trigger Method (IOTM), enables trigger generation without compromising the integrity of the models and datasets. We propose an attack based on an IOTM technique, guided by an Adaptive Trigger Generator (ATG) and employing a custom objective function. ATG dynamically refines the trigger using feedback from the model’s predictions. We empirically evaluated the effectiveness of IOTM with three deep learning models (CNN, VGG16, and ResNet18) using the CIFAR10 dataset. The achieved Runtime-Attack Success Rate (R-ASR) varies across different classes. For some classes, the R-ASR reached 100%, whereas, for others, it reached 62%. Furthermore, we conducted an ablation study to investigate critical factors in the runtime backdoor, including optimizer, weight, “REG”, and trigger visibility on R-ASR using the CIFAR100 dataset. We observed significant variations in the R-ASR by changing the optimizer, including Adam and SGD, with and without momentum. The R-ASR reached 81.25% with the Adam optimizer, whereas the SGD with momentum and without results reached 46.87% and 3.12%, respectively.",,IEEE
Identity-Based Proxy Re-encryption Based on SM9,"Hang Liu, Yang Ming, ... Yi Zhao",Information Security and Cryptology,2024,"<a href=""Springer (2024) : Identity-Based Proxy Re-encryption Based on SM9"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-0942-7_16]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-0942-7_16]</a>","Proxy re-encryption, as a cryptographic primitive, allows an untrusted proxy to transform a ciphertext encrypted with the data owner’s public key to...",,Springer
Impact of Data Poisoning Attack on the Performance of Machine Learning Models,"Dipan Das, Sharmistha Roy, Bibhudatta Sahoo",Data Science and Network Engineering,2024,"<a href=""Springer (2024) : Impact of Data Poisoning Attack on the Performance of Machine Learning Models"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6755-1_32]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6755-1_32]</a>","The twenty-first century has witnessed widespread adoption of Artificial Intelligence (AI), Machine Learning (ML) and Deep Learning (DL). These...",,Springer
Implementing a Multitarget Backdoor Attack Algorithm Based on Procedural Noise Texture Features,"Qian Liu, Chunying Kang, Qian Zou, Qiaochu Guan",IEEE Access,2024,"<a href=""DBLP (2024) : Implementing a Multitarget Backdoor Attack Algorithm Based on Procedural Noise Texture Features"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2024.3401848]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2024.3401848]</a>",,,DBLP
"Incremental Learning, Incremental Backdoor Threats","Wenbo Jiang, Tianwei Zhang, Han Qiu, Hongwei Li, Guowen Xu",IEEE Trans. Dependable Secur. Comput.,2024,"<a href=""DBLP (2024) : Incremental Learning, Incremental Backdoor Threats"" target=""_blank"">[https://doi.org/10.1109/TDSC.2022.3201234]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TDSC.2022.3201234]</a>",,,DBLP
Intellectual Property Protection of Image Processing Models with Watermarking,"Yuxuan Du, Linlin Tang, ... Jiajia Zhang",Genetic and Evolutionary Computing,2024,"<a href=""Springer (2024) : Intellectual Property Protection of Image Processing Models with Watermarking"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-0068-4_47]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-0068-4_47]</a>","In recent years, deep learning has achieved remarkable success in many fields. But training an effective model is often costly, so deep learning...",,Springer
Invisible Backdoor Attack With Dynamic Triggers Against Person Re-Identification,"Wenli Sun, Xinyang Jiang, Shuguang Dou, Dongsheng Li, Duoqian Miao, Cheng Deng, Cairong Zhao",IEEE Trans. Inf. Forensics Secur.,2024,"<a href=""DBLP (2024) : Invisible Backdoor Attack With Dynamic Triggers Against Person Re-Identification"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3322659]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3322659]</a>",,,DBLP
Invisible Backdoor Attack against 3D Point Cloud Classifier in Graph Spectral Domain,"Linkun Fan, Fazhi He, Tongzhen Si, Wei Tang, Bing Li",AAAI,2024,"<a href=""DBLP (2024) : Invisible Backdoor Attack against 3D Point Cloud Classifier in Graph Spectral Domain"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i19.30099]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i19.30099]</a>",,,DBLP
Invisible Intruders: Label-Consistent Backdoor Attack using Re-parameterized Noise Trigger,B. Wang F. Yu F. Wei Y. Li W. Wang,IEEE Transactions on Multimedia,2024,"<a href=""IEEE (2024) : Invisible Intruders: Label-Consistent Backdoor Attack using Re-parameterized Noise Trigger"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555451]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TMM.2024.3412388]</a>","A remarkable number of backdoor attack methods have been proposed in the literature on deep neural networks (DNNs). However, it hasn't been sufficiently addressed in the existing methods of achieving true senseless backdoor attacks that are visually invisible and label-consistent. In this paper, we propose a new backdoor attack method where the labels of the backdoor images are perfectly aligned with their content, ensuring label consistency. Additionally, the backdoor trigger is meticulously designed, allowing the attack to evade DNN model checks and human inspection. Our approach employs an auto-encoder (AE) to conduct representation learning of benign images and interferes with salient classification features to increase the dependence of backdoor image classification on backdoor triggers. To ensure visual invisibility, we implement a method inspired by image steganography that embeds trigger patterns into the image using the DNN and enable sample-specific backdoor triggers. We conduct comprehensive experiments on multiple benchmark datasets and network architectures to verify the effectiveness of our proposed method under the metric of attack success rate and invisibility. The results also demonstrate satisfactory performance against a variety of defense methods.",,IEEE
Lightweight Language Agnostic Data Sanitization Pipeline for Dealing with Homoglyphs in Code-Mixed Languages,"Mohammad Yusuf Jamal Aziz Azmi, Subalalitha Chinnaudayar Navaneethakrishnan",Speech and Language Technologies for Low-Resource Languages,2024,"<a href=""Springer (2024) : Lightweight Language Agnostic Data Sanitization Pipeline for Dealing with Homoglyphs in Code-Mixed Languages"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-58495-4_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-58495-4_11]</a>","With the rise in hate speech on social media, numerous Natural Language Processing (NLP) techniques like text classification have been employed for...",,Springer
MBA: Backdoor Attacks Against 3D Mesh Classifier,"Linkun Fan, Fazhi He, Tongzhen Si, Rubin Fan, Chuanlong Ye, Bing Li",IEEE Trans. Inf. Forensics Secur.,2024,"<a href=""DBLP (2024) : MBA: Backdoor Attacks Against 3D Mesh Classifier"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3346644]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3346644]</a>",,,DBLP
MITDBA: Mitigating Dynamic Backdoor Attacks in Federated Learning for IoT Applications,"Yongkang Wang, Di-Hua Zhai, Dongyu Han, Yuyin Guan, Yuanqing Xia",IEEE Internet Things J.,2024,"<a href=""DBLP (2024) : MITDBA: Mitigating Dynamic Backdoor Attacks in Federated Learning for IoT Applications"" target=""_blank"">[https://doi.org/10.1109/JIOT.2023.3325634]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/JIOT.2023.3325634]</a>",,,DBLP
Machine Learning–Based Hardware Trojans Detection in Integrated Circuits: A Systematic Review,"Ritu Sharma, Prashant Ranjan",Data Science and Applications,2024,"<a href=""Springer (2024) : Machine Learning–Based Hardware Trojans Detection in Integrated Circuits: A Systematic Review"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-7862-5_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-7862-5_3]</a>",A purposefully inserted additional circuit known as the Hardware Trojan (HT) is implanted inside original integrated circuits during the designing or...,,Springer
Malware Detection Method Based on Visualization,"Nannan Xie, Haoxiang Liang, ... Chuanxue Zhang",Algorithms and Architectures for Parallel Processing,2024,"<a href=""Springer (2024) : Malware Detection Method Based on Visualization"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-0811-6_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-0811-6_15]</a>",The rapid development of information technology and computer networks has led to the emergence of various new applications on both PC platforms and...,,Springer
Member Inference Attacks in Federated Contrastive Learning,"Zixin Wang, Bing Mi, Kongyang Chen",Artificial Intelligence Security and Privacy,2024,"<a href=""Springer (2024) : Member Inference Attacks in Federated Contrastive Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9785-5_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9785-5_4]</a>","In the past, the research community has studied privacy issues in federated learning, self-supervised learning, and deep models. However, privacy...",,Springer
Minimalism is King! High-Frequency Energy-Based Screening for Data-Efficient Backdoor Attacks,"Yuan Xun, Xiaojun Jia, Jindong Gu, Xinwei Liu, Qing Guo, Xiaochun Cao",IEEE Trans. Inf. Forensics Secur.,2024,"<a href=""DBLP (2024) : Minimalism is King! High-Frequency Energy-Based Screening for Data-Efficient Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3380821]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3380821]</a>",,,DBLP
NTD: Non-Transferability Enabled Deep Learning Backdoor Detection,"Yinshan Li, Hua Ma, Zhi Zhang, Yansong Gao, Alsharif Abuadbba, Minhui Xue, Anmin Fu, Yifeng Zheng, Said F. Al-Sarawi, Derek Abbott",IEEE Trans. Inf. Forensics Secur.,2024,"<a href=""DBLP (2024) : NTD: Non-Transferability Enabled Deep Learning Backdoor Detection"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3312973]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3312973]</a>",,,DBLP
Network Traffic Classification: Solution to Detect Intruder,"Sujata N. Bhosle, Jayshri D. Pagare",Advancements in Smart Computing and Information Security,2024,"<a href=""Springer (2024) : Network Traffic Classification: Solution to Detect Intruder"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-59100-6_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-59100-6_7]</a>","Network Traffic Classification is a challenging task in the of intrusion detection. Using the network traffic classification techniques, admin can...",,Springer
PPAPAFL: A Novel Approach to Privacy Protection and Anti-poisoning Attacks in Federated Learning,"Xiangquan Chen, Chungen Xu, ... Pan Zhang","Tools for Design, Implementation and Verification of Emerging Information Technologies",2024,"<a href=""Springer (2024) : PPAPAFL: A Novel Approach to Privacy Protection and Anti-poisoning Attacks in Federated Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-51399-2_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-51399-2_7]</a>","In the realm of distributed machine learning, although federated learning has received considerable attention, it still confronts grave challenges...",,Springer
PatchFinger: A Model Fingerprinting Scheme Based on Adversarial Patch,"Bo Zeng, Kunhao Lai, ... Lina Wang",Neural Information Processing,2024,"<a href=""Springer (2024) : PatchFinger: A Model Fingerprinting Scheme Based on Adversarial Patch"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8082-6_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8082-6_6]</a>","As deep neural networks (DNNs) gain great popularity and importance, protecting their intellectual property is always the topic. Previous model...",,Springer
Penetrating Machine Learning Servers via Exploiting BMC Vulnerability,"Yashi Liu, Kefan Qiu, ... Quanxin Zhang",Machine Learning for Cyber Security,2024,"<a href=""Springer (2024) : Penetrating Machine Learning Servers via Exploiting BMC Vulnerability"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-2458-1_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-2458-1_11]</a>","With the recent significant advancements in machine learning fields, there has been an increasing focus on the data security and availability of...",,Springer
PerVK: A Robust Personalized Federated Framework to Defend Against Backdoor Attacks for IoT Applications,"Yongkang Wang, Di-Hua Zhai, Yuanqing Xia, Danyang Liu",IEEE Trans. Ind. Informatics,2024,"<a href=""DBLP (2024) : PerVK: A Robust Personalized Federated Framework to Defend Against Backdoor Attacks for IoT Applications"" target=""_blank"">[https://doi.org/10.1109/TII.2023.3329688]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TII.2023.3329688]</a>",,,DBLP
Post-quantum Dropout-Resilient Aggregation for Federated Learning via Lattice-Based PRF,"Ruozhou Zuo, Haibo Tian, Fangguo Zhang",Artificial Intelligence Security and Privacy,2024,"<a href=""Springer (2024) : Post-quantum Dropout-Resilient Aggregation for Federated Learning via Lattice-Based PRF"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9785-5_27]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9785-5_27]</a>","Machine learning has greatly improved the convenience of modern life. As the deployment scale of machine learning grows larger, the corresponding...",,Springer
Privacy-Enhancing and Robust Backdoor Defense for Federated Learning on Heterogeneous Data,"Zekai Chen, Shengxing Yu, Mingyuan Fan, Ximeng Liu, Robert H. Deng",IEEE Trans. Inf. Forensics Secur.,2024,"<a href=""DBLP (2024) : Privacy-Enhancing and Robust Backdoor Defense for Federated Learning on Heterogeneous Data"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3326983]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3326983]</a>",,,DBLP
Propagation of Computer Worms—A Study,"Mundlamuri Venkata Rao, Divya Midhunchakkaravarthy, Sujatha Dandu",Soft Computing and Signal Processing,2024,"<a href=""Springer (2024) : Propagation of Computer Worms—A Study"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8451-0_54]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8451-0_54]</a>",Worms are commonly disseminated through two methods: scanning vulnerable machines in a network as well as spreading through topological neighbors....,,Springer
PtbStolen: Pre-trained Encoder Stealing Through Perturbed Samples,"Chuan Zhang, Haotian Liang, ... Liehuang Zhu",Emerging Information Security and Applications,2024,"<a href=""Springer (2024) : PtbStolen: Pre-trained Encoder Stealing Through Perturbed Samples"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9614-8_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9614-8_1]</a>",Recent years have witnessed the huge success of adopting the self-supervised learning paradigm into pre-train effective encoders [1].,,Springer
REStore: Exploring a Black-Box Defense against DNN Backdoors using Rare Event Simulation,"Quentin Le Roux, Kassem Kallas, Teddy Furon",SaTML,2024,"<a href=""DBLP (2024) : REStore: Exploring a Black-Box Defense against DNN Backdoors using Rare Event Simulation"" target=""_blank"">[https://doi.org/10.1109/SaTML59370.2024.00021]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SaTML59370.2024.00021]</a>",,,DBLP
RF Domain Backdoor Attack on Signal Classification Via Stealthy Trigger,Z. Tang T. Zhao T. Zhang H. Phan Y. Wang C. Shi B. Yuan Y. Chen,IEEE Transactions on Mobile Computing,2024,"<a href=""IEEE (2024) : RF Domain Backdoor Attack on Signal Classification Via Stealthy Trigger"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10537036]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TMC.2024.3404341]</a>","Deep learning (DL) has recently become a key technology supporting radio frequency (RF) signal classification applications. Given the heavy DL training requirement, adopting outsourced training is a practical option for RF application developers. However, the outsourcing process exposes a security vulnerability that enables a backdoor attack. While backdoor attacks have been explored in the vision domain, it is rarely explored in the RF domain. In this work, we present a stealthy backdoor attack that targets DL-based RF signal classification. To realize such an attack, we extensively explore the characteristics of the RF data in different applications, which include RF modulation classification and RF fingerprint-based device identification. Then, we design a training-based backdoor trigger generation approach with different optimization procedures for two backdoor attack scenarios (i.e., poison-label and clean-label). Extensive experiments on two RF signal classification datasets show that the attack success rate is over 99.2%, while its classification accuracy for the clean data remains high (i.e., less than a 0.6% drop compared to the clean model). The low NMSE (less than 0.091) indicates the stealthiness of the attack. Additionally, we demonstrate that our attack can bypass existing defense strategies, such as Neural Cleanse and STRIP.",,IEEE
Research on Intelligent Intrusion Detection System Model for Train Network Based on TCN,"Mingming Liu, Jianying Liang, ... Dongxiao Jia",Proceedings of the 6th International Conference on Electrical Engineering and Information Technologies for Rail Transportation (EITRT) 2023,2024,"<a href=""Springer (2024) : Research on Intelligent Intrusion Detection System Model for Train Network Based on TCN"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9319-2_33]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9319-2_33]</a>","Onboard network of trains has high requirements on the performance of attack detection algorithm, and the network traffic contains characteristics of...",,Springer
Review on Cybersecurity and Techniques,"Jay J. Pandya, Prashant Kharote",Smart Trends in Computing and Communications,2024,"<a href=""Springer (2024) : Review on Cybersecurity and Techniques"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-1323-3_24]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-1323-3_24]</a>","As we all know, in today’s time, the world has become increasingly reliant on a variety of electronic gadgets for a variety of activities such as...",,Springer
Revocable Attribute-Based Encryption Scheme with Cryptographic Reverse Firewalls,"Yang Zhao, Xing-Yu Ke, ... Kuo-Hui Yeh",Big Data Technologies and Applications,2024,"<a href=""Springer (2024) : Revocable Attribute-Based Encryption Scheme with Cryptographic Reverse Firewalls"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-52265-9_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-52265-9_6]</a>","With the prevalence of information sharing, preserving the confidentiality of sensitive data has become paramount. Attribute-based encryption (ABE)...",,Springer
RoPE: Defending against backdoor attacks in federated learning systems,"Yongkang Wang, Di-Hua Zhai, Yuanqing Xia",Knowl. Based Syst.,2024,"<a href=""DBLP (2024) : RoPE: Defending against backdoor attacks in federated learning systems"" target=""_blank"">[https://doi.org/10.1016/j.knosys.2024.111660]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.knosys.2024.111660]</a>",,,DBLP
Role of Federated Learning for Internet of Vehicles: A Systematic Review,"P. Hiran Mani Bala, Rishu Chhabra",Artificial Intelligence of Things,2024,"<a href=""Springer (2024) : Role of Federated Learning for Internet of Vehicles: A Systematic Review"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-48781-1_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-48781-1_11]</a>","The Internet of Vehicles (IoV) is one of the most exciting and practical ways that corporations and academics are interested in, especially by...",,Springer
SGBA: A stealthy scapegoat backdoor attack against deep neural networks,"Ying He, Zhili Shen, Chang Xia, Jingyu Hua, Wei Tong, Sheng Zhong",Comput. Secur.,2024,"<a href=""DBLP (2024) : SGBA: A stealthy scapegoat backdoor attack against deep neural networks"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103523]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103523]</a>",,,DBLP
SVM-RF: A Hybrid Machine Learning Model for Detection of Malicious Network Traffic and Files,"Prashant Mathur, Arjun Choudhary, ... Gaurav Choudhary",Cryptology and Network Security with Machine Learning,2024,"<a href=""Springer (2024) : SVM-RF: A Hybrid Machine Learning Model for Detection of Malicious Network Traffic and Files"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-2229-1_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-2229-1_3]</a>","Since the inception of information technology, malwares have ravished cyberspace. Malware is a curse that is born from the boon of information...",,Springer
SecureNet: Proactive intellectual property protection and model security defense for DNNs based on backdoor learning,"Peihao Li, Jie Huang, Huaqing Wu, Zeping Zhang, Chunyang Qi",Neural Networks,2024,"<a href=""DBLP (2024) : SecureNet: Proactive intellectual property protection and model security defense for DNNs based on backdoor learning"" target=""_blank"">[https://doi.org/10.1016/j.neunet.2024.106199]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.neunet.2024.106199]</a>",,,DBLP
Security Flaw in TCP/IP and Proposed Measures,"Sourav Kumar Upadhyay, Prakash Kumar",Cyber Security and Digital Forensics,2024,"<a href=""Springer (2024) : Security Flaw in TCP/IP and Proposed Measures"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9811-1_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9811-1_8]</a>","The whole Internet is based on TCP/IP protocol suite. The inherent flaw in the TCP/IP makes a cascading effect for other attacks like flooding,...",,Springer
Security in SCADA System: A Technical Report on Cyber Attacks and Risk Assessment Methodologies,Sadaquat Ali,Data Analytics in System Engineering,2024,"<a href=""Springer (2024) : Security in SCADA System: A Technical Report on Cyber Attacks and Risk Assessment Methodologies"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-54820-8_35]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-54820-8_35]</a>",Supervisory Control and Data Acquisition (SCADA) systems have become indispensable in a wide range of industries worldwide. These systems facilitate...,,Springer
SilentTrig: An imperceptible backdoor attack against speaker identification with hidden triggers,"Yu Tang, Lijuan Sun, Xiaolong Xu",Pattern Recognit. Lett.,2024,"<a href=""DBLP (2024) : SilentTrig: An imperceptible backdoor attack against speaker identification with hidden triggers"" target=""_blank"">[https://doi.org/10.1016/j.patrec.2023.12.002]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.patrec.2023.12.002]</a>",,,DBLP
Sliding Window Based Multilayer Perceptron for Cyber Hacking Detection System (CHDS),"J. Christina Deva Kirubai, S. Silvia Priscila",Advancements in Smart Computing and Information Security,2024,"<a href=""Springer (2024) : Sliding Window Based Multilayer Perceptron for Cyber Hacking Detection System (CHDS)"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-59097-9_26]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-59097-9_26]</a>","Cyber Hacking Detection System (CHDS) plays a major important role to identify any type of incidents that occur in the system. For instance, a...",,Springer
Smart-Grid-Enabled Business Cases and the Consequences of Cyber Attacks,"Øyvind Toftegaard, Doney Abraham, ... Bernhard Hämmerli",Critical Infrastructure Protection XVII,2024,"<a href=""Springer (2024) : Smart-Grid-Enabled Business Cases and the Consequences of Cyber Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-49585-4_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-49585-4_2]</a>",The introduction of smart metering systems is a paradigm shift for the power grid. New business cases such as virtual power plants and local...,,Springer
Software Supply Chain Resiliency at Scale,"V. Lakshmi Narasimhan, S. Ramaswamy, O. Mphale",ICT: Applications and Social Interfaces,2024,"<a href=""Springer (2024) : Software Supply Chain Resiliency at Scale"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-0210-7_37]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-0210-7_37]</a>","Software businesses are increasingly dependent on supply chains from several providers to receivers, like traditional business. Real-world software...",,Springer
Stealthy Targeted Backdoor Attacks Against Image Captioning,"Wenshu Fan, Hongwei Li, Wenbo Jiang, Meng Hao, Shui Yu, Xiao Zhang",IEEE Trans. Inf. Forensics Secur.,2024,"<a href=""DBLP (2024) : Stealthy Targeted Backdoor Attacks Against Image Captioning"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3402179]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3402179]</a>",,,DBLP
Study of Cyber Threats in IoT Systems,"Abir El Akhdar, Chafik Baidada, Ali Kartit",Proceedings of Data Analytics and Management,2024,"<a href=""Springer (2024) : Study of Cyber Threats in IoT Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6544-1_25]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6544-1_25]</a>","Over the years and in a post-industrial economy, information has evolved from being a simple financial and operational gauge in companies to becoming...",,Springer
Temporal-Distributed Backdoor Attack against Video Based Action Recognition,"Xi Li, Songhe Wang, Ruiquan Huang, Mahanth Gowda, George Kesidis",AAAI,2024,"<a href=""DBLP (2024) : Temporal-Distributed Backdoor Attack against Video Based Action Recognition"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i4.28104]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i4.28104]</a>",,,DBLP
Text Laundering: Mitigating Malicious Features Through Knowledge Distillation of Large Foundation Models,"Yi Jiang, Chenghui Shi, ... Shouling Ji",Information Security and Cryptology,2024,"<a href=""Springer (2024) : Text Laundering: Mitigating Malicious Features Through Knowledge Distillation of Large Foundation Models"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-0945-8_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-0945-8_1]</a>","Despite their efficacy in machine learning, Deep Neural Networks (DNNs) are notoriously susceptible to backdoor and adversarial attacks. These...",,Springer
The Patching Landscape of Elisabeth-4 and the Mixed Filter Permutator Paradigm,"Clément Hoffmann, Pierrick Méaux, François-Xavier Standaert",Progress in Cryptology – INDOCRYPT 2023,2024,"<a href=""Springer (2024) : The Patching Landscape of Elisabeth-4 and the Mixed Filter Permutator Paradigm"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-56232-7_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-56232-7_7]</a>","Filter permutators are a family of stream cipher designs that are aimed for hybrid homomorphic encryption. While originally operating on bits, they...",,Springer
The VOCODES Kill Chain for Voice Controllable Devices,"Sergio Esposito, Daniele Sgandurra, Giampaolo Bella",Computer Security. ESORICS 2023 International Workshops,2024,"<a href=""Springer (2024) : The VOCODES Kill Chain for Voice Controllable Devices"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-54129-2_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-54129-2_11]</a>","In this paper, we introduce a formalisation of attacks on Voice Controllable Devices (VCDs), focusing specifically on attacks leveraging the voice...",,Springer
The reality of backdoored S-Boxes - An eye opener,"Shah Fahd, Mehreen Afzal, Waseem Iqbal, Dawood Shah, Ijaz Khalid",J. Inf. Secur. Appl.,2024,"<a href=""DBLP (2024) : The reality of backdoored S-Boxes - An eye opener"" target=""_blank"">[https://doi.org/10.1016/j.jisa.2023.103674]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.jisa.2023.103674]</a>",,,DBLP
Threat Modeling Towards Resilience in Smart ICUs,"Christian Baumhör, Thomas Henning, Matteo Große-Kampmann",Secure and Resilient Digital Transformation of Healthcare,2024,"<a href=""Springer (2024) : Threat Modeling Towards Resilience in Smart ICUs"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-55829-0_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-55829-0_3]</a>",Healthcare digitization has significantly enhanced patient care and alleviated the workload of hospital staff. This trend towards automation has also...,,Springer
Toward Stealthy Backdoor Attacks Against Speech Recognition via Elements of Sound,"Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Stefanos Koffas, Yiming Li",IEEE Trans. Inf. Forensics Secur.,2024,"<a href=""DBLP (2024) : Toward Stealthy Backdoor Attacks Against Speech Recognition via Elements of Sound"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3404885]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2024.3404885]</a>",,,DBLP
Towards New Challenges of Modern Pentest,"Daniel Dalalana Bertoglio, Arthur Gil, ... Avelino Francisco Zorzo",Intelligent Sustainable Systems,2024,"<a href=""Springer (2024) : Towards New Challenges of Modern Pentest"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-7569-3_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-7569-3_3]</a>","With the increasing number of Internet-based resources and applications, the amount of attacks faced by companies has increased significantly in the...",,Springer
Universal adversarial backdoor attacks to fool vertical federated learning,"Peng Chen, Xin Du, Zhihui Lu, Hongfeng Chai",Comput. Secur.,2024,"<a href=""DBLP (2024) : Universal adversarial backdoor attacks to fool vertical federated learning"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103601]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103601]</a>",,,DBLP
Unlearnable Examples for Time Series,"Yujing Jiang, Xingjun Ma, ... James Bailey",Advances in Knowledge Discovery and Data Mining,2024,"<a href=""Springer (2024) : Unlearnable Examples for Time Series"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-2266-2_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-2266-2_17]</a>",Unlearnable examples (UEs) refer to training samples modified to be unlearnable to Deep Neural Networks (DNNs). These examples are usually generated...,,Springer
Unraveling Network-Based Pivoting Maneuvers: Empirical Insights and Challenges,"Martin Husák, Shanchieh Jay Yang, ... Elias Bou-Harb",Digital Forensics and Cyber Crime,2024,"<a href=""Springer (2024) : Unraveling Network-Based Pivoting Maneuvers: Empirical Insights and Challenges"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-56583-0_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-56583-0_9]</a>","Pivoting days in a campus network. Through NetFlow monitoring, we initially identified potential pivoting candidates, which are traces in the network...",,Springer
Unravelling Obfuscated Malware Through Memory Feature Engineering and Ensemble Learning,"K. M. Yogesh, S. Arpitha, ... V. Raghu",ICT: Smart Systems and Technologies,2024,"<a href=""Springer (2024) : Unravelling Obfuscated Malware Through Memory Feature Engineering and Ensemble Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9489-2_28]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9489-2_28]</a>",Memory analysis is an essential step in the process of identifying malicious programs since it can capture a variety of traits and behaviours....,,Springer
Updatable Encryption from Group Actions,"Antonin Leroux, Maxime Roméas",Post-Quantum Cryptography,2024,"<a href=""Springer (2024) : Updatable Encryption from Group Actions"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-62746-0_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-62746-0_2]</a>",Updatable Encryption (UE) allows to rotate the encryption key in the outsourced storage setting while minimizing the bandwith used. The server can...,,Springer
Verifying in the Dark: Verifiable Machine Unlearning by Using Invisible Backdoor Triggers,"Yu Guo, Yu Zhao, Saihui Hou, Cong Wang, Xiaohua Jia",IEEE Trans. Inf. Forensics Secur.,2024,"<a href=""DBLP (2024) : Verifying in the Dark: Verifiable Machine Unlearning by Using Invisible Backdoor Triggers"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3328269]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3328269]</a>",,,DBLP
"Vulnerabilities in Office Printers, Multifunction Printers (MFP), 3D Printers, and Digital Copiers: A Gateway to Breach Our Enterprise Network","Eric B. Blancaflor, Allen James Montoya",International Conference on Cloud Computing and Computer Networks,2024,"<a href=""Springer (2024) : Vulnerabilities in Office Printers, Multifunction Printers (MFP), 3D Printers, and Digital Copiers: A Gateway to Breach Our Enterprise Network"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-47100-1_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-47100-1_5]</a>","Despite the advancements in security, threats have become more sophisticated than ever-leading companies to think outside the box. Cyberattacks are...",,Springer
Vulnerability of Dynamic Masking in Test Compression,"Yogendra Sao, Debanka Giri, ... Sk Subidh Ali","Security, Privacy, and Applied Cryptography Engineering",2024,"<a href=""Springer (2024) : Vulnerability of Dynamic Masking in Test Compression"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-51583-5_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-51583-5_7]</a>",Scan-based Design for Testability (DfT) ensures the testability of chips while providing observability and high fault coverage. In the case of...,,Springer
WaTrojan: Wavelet domain trigger injection for backdoor attacks,"Zhenghao Zhang, Jianwei Ding, Qi Zhang, Qiyao Deng",Comput. Secur.,2024,"<a href=""DBLP (2024) : WaTrojan: Wavelet domain trigger injection for backdoor attacks"" target=""_blank"">[https://doi.org/10.1016/j.cose.2024.103767]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.cose.2024.103767]</a>",,,DBLP
Web Security Analysis of Banking Websites,"Bhawna Sharma, Rahul Johari",Proceedings of Data Analytics and Management,2024,"<a href=""Springer (2024) : Web Security Analysis of Banking Websites"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6547-2_20]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6547-2_20]</a>",Website security is the process of managing and securing a website from cyberattacks by taking essential measures. The main purpose of the paper is...,,Springer
"What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety-Critical Systems","Saddek Bensalem, Chih-Hong Cheng, ... Xingyu Zhao",Bridging the Gap Between AI and Reality,2024,"<a href=""Springer (2024) : What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety-Critical Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-46002-9_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-46002-9_4]</a>","Machine learning has made remarkable advancements, but confidently utilising learning-enabled components in safety-critical domains still poses...",,Springer
Zero-Knowledge with Robust Learning: Mitigating Backdoor Attacks in Federated Learning for Enhanced Security and Privacy,"Linlin Li, Chungen Xu, Pan Zhang","Tools for Design, Implementation and Verification of Emerging Information Technologies",2024,"<a href=""Springer (2024) : Zero-Knowledge with Robust Learning: Mitigating Backdoor Attacks in Federated Learning for Enhanced Security and Privacy"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-51399-2_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-51399-2_6]</a>","As a distributed machine learning framework, federated learning addresses the challenges of data isolation and privacy concerns, ensuring that user...",,Springer
“We Must Protect the Transformers”: Understanding Efficacy of Backdoor Attack Mitigation on Transformer Models,"Rohit Raj, Biplab Roy, ... Mainack Mondal","Security, Privacy, and Applied Cryptography Engineering",2024,"<a href=""Springer (2024) : “We Must Protect the Transformers”: Understanding Efficacy of Backdoor Attack Mitigation on Transformer Models"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-51583-5_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-51583-5_14]</a>","Recently, Neural Network based Deep Learning (DL) backdoor attacks have prompted the development of mitigation mechanisms for such attacks. Out of...",,Springer
Is It Possible to Backdoor Face Forgery Detection with Natural Triggers?,"Xiaoxuan Han, Songlin Yang, Wei Wang, Ziwen He, Jing Dong","arXiv
arXiv","2023-12-31
2024-01","<a href=""arXiv (2023-12-31) : Is It Possible to Backdoor Face Forgery Detection with Natural Triggers?"" target=""_blank"">[http://arxiv.org/abs/2401.00414v1]</a>
<a href=""DBLP (2024-01) : Is It Possible to Backdoor Face Forgery Detection with Natural Triggers?"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.00414]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.00414]</a>","Deep neural networks have significantly improved the performance of face forgery detection models in discriminating Artificial Intelligent Generated Content (AIGC). However, their security is significantly threatened by the injection of triggers during model training (i.e., backdoor attacks). Although existing backdoor defenses and manual data selection can mitigate those using human-eye-sensitive triggers, such as patches or adversarial noises, the more challenging natural backdoor triggers remain insufficiently researched. To further investigate natural triggers, we propose a novel analysis-by-synthesis backdoor attack against face forgery detection models, which embeds natural triggers in the latent space. We thoroughly study such backdoor vulnerability from two perspectives: (1) Model Discrimination (Optimization-Based Trigger): we adopt a substitute detection model and find the trigger by minimizing the cross-entropy loss, (2) Data Distribution (Custom Trigger): we manipulate the uncommon facial attributes in the long-tailed distribution to generate poisoned samples without the supervision from detection models. Furthermore, to completely evaluate the detection models towards the latest AIGC, we utilize both state-of-the-art StyleGAN and Stable Diffusion for trigger generation. Finally, these backdoor triggers introduce specific semantic features to the generated poisoned samples (e.g., skin textures and smile), which are more natural and robust. Extensive experiments show that our method is superior from three levels: (1) Attack Success Rate: ours achieves a high attack success rate (over 99%) and incurs a small model accuracy drop (below 0.2%) with a low poisoning rate (less than 3%), (2) Backdoor Defense: ours shows better robust performance when faced with existing backdoor defense methods, (3) Human Inspection: ours is less human-eye-sensitive from a comprehensive user study.
","
","arXiv
DBLP"
Does Few-shot Learning Suffer from Backdoor Attacks?,"Xinwei Liu, Xiaojun Jia, Jindong Gu, Yuan Xun, Siyuan Liang, Xiaochun Cao","arXiv
AAAI
arXiv","2023-12-31
2024
2024-01","<a href=""arXiv (2023-12-31) : Does Few-shot Learning Suffer from Backdoor Attacks?"" target=""_blank"">[http://arxiv.org/abs/2401.01377v1]</a>
<a href=""DBLP (2024) : Does Few-Shot Learning Suffer from Backdoor Attacks?"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i18.29965]</a>
<a href=""DBLP (2024-01) : Does Few-shot Learning Suffer from Backdoor Attacks?"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.01377]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i18.29965]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.01377]</a>","The field of few-shot learning (FSL) has shown promising results in scenarios where training data is limited, but its vulnerability to backdoor attacks remains largely unexplored. We first explore this topic by first evaluating the performance of the existing backdoor attack methods on few-shot learning scenarios. Unlike in standard supervised learning, existing backdoor attack methods failed to perform an effective attack in FSL due to two main issues. Firstly, the model tends to overfit to either benign features or trigger features, causing a tough trade-off between attack success rate and benign accuracy. Secondly, due to the small number of training samples, the dirty label or visible trigger in the support set can be easily detected by victims, which reduces the stealthiness of attacks. It seemed that FSL could survive from backdoor attacks. However, in this paper, we propose the Few-shot Learning Backdoor Attack (FLBA) to show that FSL can still be vulnerable to backdoor attacks. Specifically, we first generate a trigger to maximize the gap between poisoned and benign features. It enables the model to learn both benign and trigger features, which solves the problem of overfitting. To make it more stealthy, we hide the trigger by optimizing two types of imperceptible perturbation, namely attractive and repulsive perturbation, instead of attaching the trigger directly. Once we obtain the perturbations, we can poison all samples in the benign support set into a hidden poisoned support set and fine-tune the model on it. Our method demonstrates a high Attack Success Rate (ASR) in FSL tasks with different few-shot learning paradigms while preserving clean accuracy and maintaining stealthiness. This study reveals that few-shot learning still suffers from backdoor attacks, and its security should be given attention.

","

","arXiv
DBLP
DBLP"
CS 785: Seminar in Computer Science I,C Shi,2023,2023-12-31,"<a href=""Google Scholar (2023-12-31) : CS 785: Seminar in Computer Science I"" target=""_blank"">[https://digitalcommons.njit.edu/cgi/viewcontent.cgi?article=1384&context=cs-syllabi]</a>","<a href=""Google Scholar"" target=""_blank"">[https://digitalcommons.njit.edu/cgi/viewcontent.cgi?article=1384&context=cs-syllabi]</a>","new attack surfaces … attacks, backdoor attacks) and testing time attacks (eg, adversarial examples). It also includes state-of-the-art mitigation techniques for these attacks. …",,Google Scholar
A clean-label graph backdoor attack method in node classification task,"Xiaogang Xing, Ming Xu, Yujing Bai, Dongdong Yang","arXiv
arXiv","2023-12-30
2024-01","<a href=""arXiv (2023-12-30) : A clean-label graph backdoor attack method in node classification task"" target=""_blank"">[http://arxiv.org/abs/2401.00163v1]</a>
<a href=""DBLP (2024-01) : A clean-label graph backdoor attack method in node classification task"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.00163]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2401.00163]</a>","Backdoor attacks in the traditional graph neural networks (GNNs) field are easily detectable due to the dilemma of confusing labels. To explore the backdoor vulnerability of GNNs and create a more stealthy backdoor attack method, a clean-label graph backdoor attack method(CGBA) in the node classification task is proposed in this paper. Differently from existing backdoor attack methods, CGBA requires neither modification of node labels nor graph structure. Specifically, to solve the problem of inconsistency between the contents and labels of the samples, CGBA selects poisoning samples in a specific target class and uses the label of sample as the target label (i.e., clean-label) after injecting triggers into the target samples. To guarantee the similarity of neighboring nodes, the raw features of the nodes are elaborately picked as triggers to further improve the concealment of the triggers. Extensive experiments results show the effectiveness of our method. When the poisoning rate is 0.04, CGBA can achieve an average attack success rate of 87.8%, 98.9%, 89.1%, and 98.5%, respectively.
","
","arXiv
DBLP"
A Review of Cloud Security Issues and Challenges,"A Agarwal, SB Verma, BK Gupta","ADCAIJ: Advances in Distributed …, 2023",2023-12-30,"<a href=""Google Scholar (2023-12-30) : A Review of Cloud Security Issues and Challenges"" target=""_blank"">[https://revistas.usal.es/cinco/index.php/2255-2863/article/view/31459]</a>","<a href=""Google Scholar"" target=""_blank"">[https://revistas.usal.es/cinco/index.php/2255-2863/article/view/31459]</a>","DoS, service injection, user-to-root attacks, manin-the-middle attacks, data loss leakage andloss, identity theft, and backdoor channel attacks are a few of the dangers …",,Google Scholar
An Examination of Various Random Number Generator Weaknesses,,,2023-12-30,"<a href=""Google Scholar (2023-12-30) : An Examination of Various Random Number Generator Weaknesses"" target=""_blank"">[https://ls.ecomaikgolf.com/slides/randomnumbers/report.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ls.ecomaikgolf.com/slides/randomnumbers/report.pdf]</a>",showing how cryptography could be used as an attack resource [YY97a] instead of a … the DUAL EC DRBG backdoor is similar to the Diffie Hellman backdoor presented. …,,Google Scholar
Towards Understanding the Challenges in Scaling Frontier Machine Learning Models,K Sreenivasan,2023,2023-12-30,"<a href=""Google Scholar (2023-12-30) : Towards Understanding the Challenges in Scaling Frontier Machine Learning Models"" target=""_blank"">[https://search.proquest.com/openview/5c57994e0793e07285ff4c0ee627bfdd/1?pq-origsite=gscholar&cbl=18750&diss=y]</a>","<a href=""Google Scholar"" target=""_blank"">[https://search.proquest.com/openview/5c57994e0793e07285ff4c0ee627bfdd/1?pq-origsite=gscholar&cbl=18750&diss=y]</a>",We also develop the edge-case backdoor attack which bypasses all known defenses at the time and is provably computationally hard to detect. We show that these …,,Google Scholar
VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models,"Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho","arXiv
NeurIPS
arXiv","2023-12-29
2023
2023-06","<a href=""arXiv (2023-12-29) : VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models"" target=""_blank"">[http://arxiv.org/abs/2306.06874v5]</a>
<a href=""DBLP (2023) : VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/6b055b95d689b1f704d8f92191cdb788-Abstract-Conference.html]</a>
<a href=""DBLP (2023-06) : VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.06874]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/6b055b95d689b1f704d8f92191cdb788-Abstract-Conference.html]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.06874]</a>","Diffusion Models (DMs) are state-of-the-art generative models that learn a reversible corruption process from iterative noise addition and denoising. They are the backbone of many generative AI applications, such as text-to-image conditional generation. However, recent studies have shown that basic unconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. This paper presents a unified backdoor attack framework (VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our framework covers mainstream unconditional and conditional DMs (denoising-based and score-based) and various training-free samplers for holistic evaluations. Experiments show that our unified framework facilitates the backdoor analysis of different DM configurations and provides new insights into caption-based backdoor attacks on DMs. Our code is available on GitHub: \url{https://github.com/IBM/villandiffusion}

","

","arXiv
DBLP
DBLP"
Ai-based trojans for evading machine learning detection,"PK Mishra, Z Pan","US Patent App. 18/174,342, 2023",2023-12-29,"<a href=""Google Scholar (2023-12-29) : Ai-based trojans for evading machine learning detection"" target=""_blank"">[https://patents.google.com/patent/US20230421596A1/en]</a>","<a href=""Google Scholar"" target=""_blank"">[https://patents.google.com/patent/US20230421596A1/en]</a>",Various embodiments provide a robust backdoor attack on machine learning (ML)-based detection systems that can be applied to demonstrate and identify vulnerabilities …,,Google Scholar
Effective Implementation of Stealthy Syntactical Backdoor Attack on Language Models,,,2023-12-26,"<a href=""Google Scholar (2023-12-26) : Effective Implementation of Stealthy Syntactical Backdoor Attack on Language Models"" target=""_blank"">[https://akashmishra.me/files/stealthy-backdoor-attacks.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://akashmishra.me/files/stealthy-backdoor-attacks.pdf]</a>",of any attack also plays a major role apart from the Attack Success Rate (… backdoor attacks and get an optimal attack which is more stealthier than the pre-existing attacks …,,Google Scholar
MBA: Backdoor Attacks against 3D Mesh Classifier,"L Fan, F He, T Si, R Fan, C Ye…","IEEE Transactions on …, 2023",2023-12-26,"<a href=""Google Scholar (2023-12-26) : MBA: Backdoor Attacks against 3D Mesh Classifier"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10373054/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10373054/]</a>","Previous backdoor attacks from 2D image and 3D point cloud domains are not suitable for … two types of backdoor attacks on 3D mesh. Specifically, the first attack is a Mesh …",,Google Scholar
Data-Driven Constraint Mining for Realizable Adversarial Samples,B. Rosenfeld S. Venkatesan R. Izmailov M. Yudin C. Serban R. Chadha,MILCOM 2023 - 2023 IEEE Military Communications Conference (MILCOM),2023-12-25,"<a href=""IEEE (2023-12-25) : Data-Driven Constraint Mining for Realizable Adversarial Samples"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10356250]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/MILCOM58377.2023.10356250]</a>","Machine learning based malware detectors can identify new patterns in malware not considered by classic signature matching antivirus software, by processing features extracted from cyber artifacts. However, they must be evaluated for robustness against adversarial attacks that cause mis-classifications by making small modifications to the feature values to exploit weaknesses in the machine learning model’s decision surface. The process of feature extraction maps the problem space, or the actual cyber artifact (e.g. file, network flow data), to the feature space, often resulting in dependencies between features. To learn adversarial examples for malware detectors that can be realized in the real world, we need to be able to map feature-value perturbations back to the original problem space. This requires that 1) all feature value changes are consistent with dependencies between features that are inherent in the mapping from the problem space 2) the feature values are realizable in the problem space. While most current methods require expert knowledge to identify these constraints on the feature values, data-driven constraint mining is an automatic and generalized alternative. This work identifies Tabular Constraint Learning (TaCLe) as a method to automatically mine the dependencies between features via analysis of the feature space data. TaCLe is incorporated into a constraint aware adversarial attack strategy to identify the watermark selection space for a backdoor poisoning attack. We find that it enables the attack to find watermarks that are 70% more evasive than the state-of-the-art baseline.",,IEEE
Pre-trained trojan attacks for visual recognition,"A Liu, X Zhang, Y Xiao, Y Zhou, S Liang…","arXiv preprint arXiv …, 2023",2023-12-24,"<a href=""Google Scholar (2023-12-24) : Pre-trained trojan attacks for visual recognition"" target=""_blank"">[https://arxiv.org/abs/2312.15172]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2312.15172]</a>","In this paper, we focus on performing backdoor attacks targeting the pre-training and fine-tuning paradigm. Specifically, we poison a PVM, and the embedded backdoors …",,Google Scholar
Mitigating Label Flipping Attacks in Malicious URL Detectors Using Ensemble Trees,"E Nowroozi, N Jadalla, S Ghelichkhani…","arXiv preprint arXiv …, 2024",2023-12-23,"<a href=""Google Scholar (2023-12-23) : Mitigating Label Flipping Attacks in Malicious URL Detectors Using Ensemble Trees"" target=""_blank"">[https://arxiv.org/abs/2403.02995]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2403.02995]</a>","These attacks involve manipulating a small percentage of training data labels, such … fortify against potential attacks. The focus of this study is on backdoor attacks in the con…",,Google Scholar
Secure and Private Large Transformers,M Zheng,2023,2023-12-23,"<a href=""Google Scholar (2023-12-23) : Secure and Private Large Transformers"" target=""_blank"">[https://search.proquest.com/openview/255b7872ee5da9eb0f90def924d18fc7/1?pq-origsite=gscholar&cbl=18750&diss=y]</a>","<a href=""Google Scholar"" target=""_blank"">[https://search.proquest.com/openview/255b7872ee5da9eb0f90def924d18fc7/1?pq-origsite=gscholar&cbl=18750&diss=y]</a>","While existing research has addressed security gaps like backdoor attacks in convolutional and recurrent neural networks, the security implications for attention-based …",,Google Scholar
Explainable artificial intelligence to increase transparency for revolutionizing healthcare ecosystem and the road ahead,"Sudipta Roy, Debojyoti Pal, Tanushree Meena",Network Modeling Analysis in Health Informatics and Bioinformatics,2023-12-22,"<a href=""Springer (2023-12-22) : Explainable artificial intelligence to increase transparency for revolutionizing healthcare ecosystem and the road ahead"" target=""_blank"">[https://link.springer.com/article/10.1007/s13721-023-00437-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13721-023-00437-y]</a>",The integration of deep learning (DL) into co-clinical applications has generated substantial interest among researchers aiming to enhance clinical...,,Springer
"Applying Computational Intelligence for Social Good: Track, Understand and Build a Better World","PE David, P Anandhakumar",2024,2023-12-21,"<a href=""Google Scholar (2023-12-21) : Applying Computational Intelligence for Social Good: Track, Understand and Build a Better World"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=LifqEAAAQBAJ&oi=fnd&pg=PP1&dq=backdoor+attack&ots=C_znz8Dd8-&sig=V2x4RwrCXH4FpUnxJZrMQDyQE1A]</a>","<a href=""Google Scholar"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=LifqEAAAQBAJ&oi=fnd&pg=PP1&dq=backdoor+attack&ots=C_znz8Dd8-&sig=V2x4RwrCXH4FpUnxJZrMQDyQE1A]</a>",,,Google Scholar
UNHash-Methods for better password cracking,T Kišasondi,31c3-31st Chaos Communication Congress,2023-12-21,"<a href=""Google Scholar (2023-12-21) : UNHash-Methods for better password cracking"" target=""_blank"">[https://www.croris.hr/crosbi/publikacija/prilog-skup/625436]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.croris.hr/crosbi/publikacija/prilog-skup/625436]</a>",to enable far better attacks against passwords. We will … embedded backdoors and offline attacks by data mining and … in order to collect known backdoor passwords. We will …,,Google Scholar
DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models,"Jiachen Zhou, Peizhuo Lv, Yibing Lan, Guozhu Meng, Kai Chen, Hualong Ma","arXiv
AAAI
arXiv","2023-12-20
2024
2023-12","<a href=""arXiv (2023-12-20) : DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models"" target=""_blank"">[http://arxiv.org/abs/2312.11057v2]</a>
<a href=""DBLP (2024) : DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i19.30186]</a>
<a href=""DBLP (2023-12) : DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.11057]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i19.30186]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.11057]</a>","Dataset sanitization is a widely adopted proactive defense against poisoning-based backdoor attacks, aimed at filtering out and removing poisoned samples from training datasets. However, existing methods have shown limited efficacy in countering the ever-evolving trigger functions, and often leading to considerable degradation of benign accuracy. In this paper, we propose DataElixir, a novel sanitization approach tailored to purify poisoned datasets. We leverage diffusion models to eliminate trigger features and restore benign features, thereby turning the poisoned samples into benign ones. Specifically, with multiple iterations of the forward and reverse process, we extract intermediary images and their predicted labels for each sample in the original dataset. Then, we identify anomalous samples in terms of the presence of label transition of the intermediary images, detect the target label by quantifying distribution discrepancy, select their purified images considering pixel and feature distance, and determine their ground-truth labels by training a benign model. Experiments conducted on 9 popular attacks demonstrates that DataElixir effectively mitigates various complex attacks while exerting minimal impact on benign accuracy, surpassing the performance of baseline defense methods.

","

","arXiv
DBLP
DBLP"
Progressive Poisoned Data Isolation for Training-time Backdoor Defense,"Yiming Chen, Haiwei Wu, Jiantao Zhou","arXiv
AAAI
arXiv","2023-12-20
2024
2023-12","<a href=""arXiv (2023-12-20) : Progressive Poisoned Data Isolation for Training-time Backdoor Defense"" target=""_blank"">[http://arxiv.org/abs/2312.12724v1]</a>
<a href=""DBLP (2024) : Progressive Poisoned Data Isolation for Training-Time Backdoor Defense"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i10.29023]</a>
<a href=""DBLP (2023-12) : Progressive Poisoned Data Isolation for Training-time Backdoor Defense"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.12724]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i10.29023]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.12724]</a>","Deep Neural Networks (DNN) are susceptible to backdoor attacks where malicious attackers manipulate the model's predictions via data poisoning. It is hence imperative to develop a strategy for training a clean model using a potentially poisoned dataset. Previous training-time defense mechanisms typically employ an one-time isolation process, often leading to suboptimal isolation outcomes. In this study, we present a novel and efficacious defense method, termed Progressive Isolation of Poisoned Data (PIPD), that progressively isolates poisoned data to enhance the isolation accuracy and mitigate the risk of benign samples being misclassified as poisoned ones. Once the poisoned portion of the dataset has been identified, we introduce a selective training process to train a clean model. Through the implementation of these techniques, we ensure that the trained model manifests a significantly diminished attack success rate against the poisoned data. Extensive experiments on multiple benchmark datasets and DNN models, assessed against nine state-of-the-art backdoor attacks, demonstrate the superior performance of our PIPD method for backdoor defense. For instance, our PIPD achieves an average True Positive Rate (TPR) of 99.95% and an average False Positive Rate (FPR) of 0.06% for diverse attacks over CIFAR-10 dataset, markedly surpassing the performance of state-of-the-art methods.

","

","arXiv
DBLP
DBLP"
Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models,"Yihao Huang, Felix Juefei-Xu, Qing Guo, Jie Zhang, Yutong Wu, Ming Hu, Tianlin Li, Geguang Pu, Yang Liu","arXiv
AAAI","2023-12-20
2024","<a href=""arXiv (2023-12-20) : Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models"" target=""_blank"">[http://arxiv.org/abs/2305.10701v3]</a>
<a href=""DBLP (2024) : Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i19.30110]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i19.30110]</a>","Although recent personalization methods have democratized high-resolution image synthesis by enabling swift concept acquisition with minimal examples and lightweight computation, they also present an exploitable avenue for high accessible backdoor attacks. This paper investigates a critical and unexplored aspect of text-to-image (T2I) diffusion models - their potential vulnerability to backdoor attacks via personalization. Our study focuses on a zero-day backdoor vulnerability prevalent in two families of personalization methods, epitomized by Textual Inversion and DreamBooth.Compared to traditional backdoor attacks, our proposed method can facilitate more precise, efficient, and easily accessible attacks with a lower barrier to entry. We provide a comprehensive review of personalization in T2I diffusion models, highlighting the operation and exploitation potential of this backdoor vulnerability. To be specific, by studying the prompt processing of Textual Inversion and DreamBooth, we have devised dedicated backdoor attacks according to the different ways of dealing with unseen tokens and analyzed the influence of triggers and concept images on the attack effect. Through comprehensive empirical study, we endorse the utilization of the nouveau-token backdoor attack due to its impressive effectiveness, stealthiness, and integrity, markedly outperforming the legacy-token backdoor attack.
","
","arXiv
DBLP"
Badrl: Sparse targeted backdoor attack against reinforcement learning,"J Cui, Y Han, Y Ma, J Jiao, J Zhang","… of the AAAI Conference on Artificial …, 2024",2023-12-20,"<a href=""Google Scholar (2023-12-20) : Badrl: Sparse targeted backdoor attack against reinforcement learning"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/29052]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/29052]</a>","an efficient backdoor attack strategy, which only spends highly sparse backdoor poisoning efforts at training and testing time yet delivers successful attacks. Specifically, at …",,Google Scholar
Optimizing anomaly-based attack detection using classification machine learning,"Hany Abdelghany Gouda, Mohamed Abdelslam Ahmed, Mohamed Ismail Roushdy",Neural Computing and Applications,2023-12-20,"<a href=""Springer (2023-12-20) : Optimizing anomaly-based attack detection using classification machine learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-023-09309-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-023-09309-y]</a>","One of the significant aspects of our digital world is that data are literally everywhere, and it is increasing. On the other hand, the number of...",,Springer
Security strategy for autonomous vehicle cyber-physical systems using transfer learning,"Abdulaziz A. Alsulami, Qasem Abu Al-Haija, ... Raed Alsini",Journal of Cloud Computing,2023-12-20,"<a href=""Springer (2023-12-20) : Security strategy for autonomous vehicle cyber-physical systems using transfer learning"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-023-00564-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-023-00564-x]</a>",Cyber-physical systems (CPSs) are emergent systems that enable effective real-time communication and collaboration (C&C) of physical components such...,,Springer
BadRL: Sparse Targeted Backdoor Attack Against Reinforcement Learning,"Jing Cui, Yufei Han, Yuzhe Ma, Jianbin Jiao, Junge Zhang","arXiv
arXiv","2023-12-19
2023-12","<a href=""arXiv (2023-12-19) : BadRL: Sparse Targeted Backdoor Attack Against Reinforcement Learning"" target=""_blank"">[http://arxiv.org/abs/2312.12585v1]</a>
<a href=""DBLP (2023-12) : BadRL: Sparse Targeted Backdoor Attack Against Reinforcement Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.12585]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.12585]</a>","Backdoor attacks in reinforcement learning (RL) have previously employed intense attack strategies to ensure attack success. However, these methods suffer from high attack costs and increased detectability. In this work, we propose a novel approach, BadRL, which focuses on conducting highly sparse backdoor poisoning efforts during training and testing while maintaining successful attacks. Our algorithm, BadRL, strategically chooses state observations with high attack values to inject triggers during training and testing, thereby reducing the chances of detection. In contrast to the previous methods that utilize sample-agnostic trigger patterns, BadRL dynamically generates distinct trigger patterns based on targeted state observations, thereby enhancing its effectiveness. Theoretical analysis shows that the targeted backdoor attack is always viable and remains stealthy under specific assumptions. Empirical results on various classic RL tasks illustrate that BadRL can substantially degrade the performance of a victim agent with minimal poisoning efforts 0.003% of total training steps) during training and infrequent attacks during testing.
","
","arXiv
DBLP"
Advances in automatically rating the trustworthiness of text processing services,"Biplav Srivastava, Kausik Lakkaraju, ... Marco Valtorta",AI and Ethics,2023-12-19,"<a href=""Springer (2023-12-19) : Advances in automatically rating the trustworthiness of text processing services"" target=""_blank"">[https://link.springer.com/article/10.1007/s43681-023-00391-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s43681-023-00391-5]</a>","AI services are known to have unstable behavior when subjected to changes in data, models or users. Such behaviors, whether triggered by omission or...",,Springer
Iot traffic-based DDoS attacks detection mechanisms: A comprehensive review,"Praveen Shukla, C. Rama Krishna, Nilesh Vishwasrao Patil",The Journal of Supercomputing,2023-12-19,"<a href=""Springer (2023-12-19) : Iot traffic-based DDoS attacks detection mechanisms: A comprehensive review"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05843-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05843-7]</a>","The Internet of Things (IoT) has emerged as an inevitable part of human life, that includes online learning, smart homes, smart cars, smart grids,...",,Springer
Mithridates: Auditing and Boosting Backdoor Resistance of Machine Learning Pipelines,"Eugene Bagdasaryan, Vitaly Shmatikov",arXiv,2023-12-19,"<a href=""arXiv (2023-12-19) : Mithridates: Auditing and Boosting Backdoor Resistance of Machine Learning Pipelines"" target=""_blank"">[http://arxiv.org/abs/2302.04977v3]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Machine learning (ML) models trained on data from potentially untrusted sources are vulnerable to poisoning. A small, maliciously crafted subset of the training inputs can cause the model to learn a ""backdoor"" task (e.g., misclassify inputs with a certain feature) in addition to its main task. Recent research proposed many hypothetical backdoor attacks whose efficacy heavily depends on the configuration and training hyperparameters of the target model. Given the variety of potential backdoor attacks, ML engineers who are not security experts have no way to measure how vulnerable their current training pipelines are, nor do they have a practical way to compare training configurations so as to pick the more resistant ones. Deploying a defense requires evaluating and choosing from among dozens of research papers and re-engineering the training pipeline. In this paper, we aim to provide ML engineers with pragmatic tools to audit the backdoor resistance of their training pipelines and to compare different training configurations, to help choose one that best balances accuracy and security. First, we propose a universal, attack-agnostic resistance metric based on the minimum number of training inputs that must be compromised before the model learns any backdoor. Second, we design, implement, and evaluate Mithridates a multi-stage approach that integrates backdoor resistance into the training-configuration search. ML developers already rely on hyperparameter search to find configurations that maximize the model's accuracy. Mithridates extends this standard tool to balance accuracy and resistance without disruptive changes to the training pipeline. We show that hyperparameters found by Mithridates increase resistance to multiple types of backdoor attacks by 3-5x with only a slight impact on accuracy. We also discuss extensions to AutoML and federated learning.",,arXiv
Mitigating data imbalance to improve the generalizability in IoT DDoS detection tasks,"Yi Qing, Xiangyu Liu, Yanhui Du",The Journal of Supercomputing,2023-12-19,"<a href=""Springer (2023-12-19) : Mitigating data imbalance to improve the generalizability in IoT DDoS detection tasks"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05829-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05829-5]</a>","DDoS attacks launched by IoT botnets can be classified into low-rate and high-rate DDoS attacks, which depict two distinct scenarios of data...",,Springer
Probabilistic Robustness Guarantees for Machine Learning Systems,M Weber,2023,2023-12-19,"<a href=""Google Scholar (2023-12-19) : Probabilistic Robustness Guarantees for Machine Learning Systems"" target=""_blank"">[https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/659116/6/_PhD_Thesis__Thesis.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/659116/6/_PhD_Thesis__Thesis.pdf]</a>","that appear during the model development stage and develop RAB, which is a provably robust training process against backdoor attacks, a specific instance of a data …",,Google Scholar
PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models,"Hongwei Yao, Jian Lou, Zhan Qin","arXiv
arXiv","2023-12-18
2023-10","<a href=""arXiv (2023-12-18) : PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models"" target=""_blank"">[http://arxiv.org/abs/2310.12439v2]</a>
<a href=""DBLP (2023-10) : PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.12439]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.12439]</a>","Prompts have significantly improved the performance of pretrained Large Language Models (LLMs) on various downstream tasks recently, making them increasingly indispensable for a diverse range of LLM application scenarios. However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based LLMs. In this paper, we present POISONPROMPT, a novel backdoor attack capable of successfully compromising both hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and robustness of POISONPROMPT through extensive experiments on three popular prompt methods, using six datasets and three widely used LLMs. Our findings highlight the potential security threats posed by backdoor attacks on prompt-based LLMs and emphasize the need for further research in this area.
","
","arXiv
DBLP"
Mitigating Backdoors in Federated Learning with FLD,"Yihang Lin, Pengyuan Zhou, Zhiqian Wu, Yong Liao","arXiv
arXiv","2023-12-18
2023-03","<a href=""arXiv (2023-12-18) : Mitigating Backdoors in Federated Learning with FLD"" target=""_blank"">[http://arxiv.org/abs/2303.00302v2]</a>
<a href=""DBLP (2023-03) : Mitigating Backdoors in Federated Learning with FLD"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.00302]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.00302]</a>","Federated learning allows clients to collaboratively train a global model without uploading raw data for privacy preservation. This feature, i.e., the inability to review participants' datasets, has recently been found responsible for federated learning's vulnerability in the face of backdoor attacks. Existing defense methods fall short from two perspectives: 1) they consider only very specific and limited attacker models and unable to cope with advanced backdoor attacks, such as distributed backdoor attacks, which break down the global trigger into multiple distributed triggers. 2) they conduct detection based on model granularity thus the performance gets impacted by the model dimension. To address these challenges, we propose Federated Layer Detection (FLD), a novel model filtering approach for effectively defending against backdoor attacks. FLD examines the models based on layer granularity to capture the complete model details and effectively detect potential backdoor models regardless of model dimension. We provide theoretical analysis and proof for the convergence of FLD. Extensive experiments demonstrate that FLD effectively mitigates state-of-the-art backdoor attacks with negligible impact on the accuracy of the primary task.
","
","arXiv
DBLP"
Backdoor Attacks and Defenses in Natural Language Processing,,,2023-12-18,"<a href=""Google Scholar (2023-12-18) : Backdoor Attacks and Defenses in Natural Language Processing"" target=""_blank"">[https://www.cs.uoregon.edu/Reports/AREA-202309-You.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.cs.uoregon.edu/Reports/AREA-202309-You.pdf]</a>","and background of backdoor attacks, and analyze the relation between backdoor attacks and relevant fields. Second, we categorize backdoor attacks and defenses based …",,Google Scholar
DL-SkLSTM approach for cyber security threats detection in 5G enabled IIoT,"Anjali Rajak, Rakesh Tripathi",International Journal of Information Technology,2023-12-18,"<a href=""Springer (2023-12-18) : DL-SkLSTM approach for cyber security threats detection in 5G enabled IIoT"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-023-01651-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-023-01651-7]</a>","The advancement of 5G technology has enabled the IIoT (Industrial Internet of Things) to integrate artificial intelligence, cloud computing, and edge...",,Springer
Self-adaptive memetic firefly algorithm and CatBoost-based security framework for IoT healthcare environment,"Pandit Byomokesha Dash, Manas Ranjan Senapati, ... S. Vimal",Journal of Engineering Mathematics,2023-12-18,"<a href=""Springer (2023-12-18) : Self-adaptive memetic firefly algorithm and CatBoost-based security framework for IoT healthcare environment"" target=""_blank"">[https://link.springer.com/article/10.1007/s10665-023-10309-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10665-023-10309-z]</a>","The Internet of Things (IoT), which has had a revolutionary influence on human existence, has become a topic of significant attention among the...",,Springer
UltraClean: A Simple Framework to Train Robust Neural Networks against Backdoor Attacks,"Bingyin Zhao, Yingjie Lao","arXiv
arXiv","2023-12-17
2023-12","<a href=""arXiv (2023-12-17) : UltraClean: A Simple Framework to Train Robust Neural Networks against Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2312.10657v1]</a>
<a href=""DBLP (2023-12) : UltraClean: A Simple Framework to Train Robust Neural Networks against Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.10657]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.10657]</a>","Backdoor attacks are emerging threats to deep neural networks, which typically embed malicious behaviors into a victim model by injecting poisoned samples. Adversaries can activate the injected backdoor during inference by presenting the trigger on input images. Prior defensive methods have achieved remarkable success in countering dirty-label backdoor attacks where the labels of poisoned samples are often mislabeled. However, these approaches do not work for a recent new type of backdoor -- clean-label backdoor attacks that imperceptibly modify poisoned data and hold consistent labels. More complex and powerful algorithms are demanded to defend against such stealthy attacks. In this paper, we propose UltraClean, a general framework that simplifies the identification of poisoned samples and defends against both dirty-label and clean-label backdoor attacks. Given the fact that backdoor triggers introduce adversarial noise that intensifies in feed-forward propagation, UltraClean first generates two variants of training samples using off-the-shelf denoising functions. It then measures the susceptibility of training samples leveraging the error amplification effect in DNNs, which dilates the noise difference between the original image and denoised variants. Lastly, it filters out poisoned samples based on the susceptibility to thwart the backdoor implantation. Despite its simplicity, UltraClean achieves a superior detection rate across various datasets and significantly reduces the backdoor attack success rate while maintaining a decent model accuracy on clean data, outperforming existing defensive methods by a large margin. Code is available at https://github.com/bxz9200/UltraClean.
","<a href=""arXiv"" target=""_blank"">[https://github.com/bxz9200/UltraClean]</a>
","arXiv
DBLP"
FlowMur: A Stealthy and Practical Audio Backdoor Attack with Limited Knowledge,"Jiahe Lan, Jie Wang, Baochen Yan, Zheng Yan, Elisa Bertino","arXiv
arXiv","2023-12-15
2023-12","<a href=""arXiv (2023-12-15) : FlowMur: A Stealthy and Practical Audio Backdoor Attack with Limited Knowledge"" target=""_blank"">[http://arxiv.org/abs/2312.09665v1]</a>
<a href=""DBLP (2023-12) : FlowMur: A Stealthy and Practical Audio Backdoor Attack with Limited Knowledge"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.09665]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.09665]</a>","Speech recognition systems driven by DNNs have revolutionized human-computer interaction through voice interfaces, which significantly facilitate our daily lives. However, the growing popularity of these systems also raises special concerns on their security, particularly regarding backdoor attacks. A backdoor attack inserts one or more hidden backdoors into a DNN model during its training process, such that it does not affect the model's performance on benign inputs, but forces the model to produce an adversary-desired output if a specific trigger is present in the model input. Despite the initial success of current audio backdoor attacks, they suffer from the following limitations: (i) Most of them require sufficient knowledge, which limits their widespread adoption. (ii) They are not stealthy enough, thus easy to be detected by humans. (iii) Most of them cannot attack live speech, reducing their practicality. To address these problems, in this paper, we propose FlowMur, a stealthy and practical audio backdoor attack that can be launched with limited knowledge. FlowMur constructs an auxiliary dataset and a surrogate model to augment adversary knowledge. To achieve dynamicity, it formulates trigger generation as an optimization problem and optimizes the trigger over different attachment positions. To enhance stealthiness, we propose an adaptive data poisoning method according to Signal-to-Noise Ratio (SNR). Furthermore, ambient noise is incorporated into the process of trigger generation and data poisoning to make FlowMur robust to ambient noise and improve its practicality. Extensive experiments conducted on two datasets demonstrate that FlowMur achieves high attack performance in both digital and physical settings while remaining resilient to state-of-the-art defenses. In particular, a human study confirms that triggers generated by FlowMur are not easily detected by participants.
","
","arXiv
DBLP"
Cyberattack defense mechanism using deep learning techniques in software-defined networks,"Dimmiti Srinivasa Rao, Ajith Jubilson Emerson",International Journal of Information Security,2023-12-15,"<a href=""Springer (2023-12-15) : Cyberattack defense mechanism using deep learning techniques in software-defined networks"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00785-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00785-w]</a>","Software-defined networking (SDN) is a network architecture. It is becoming more popular due to its centralized network administration, adaptability,...",,Springer
"Data and Model Poisoning Backdoor Attacks on Wireless Federated Learning, and the Defense Mechanisms: A Comprehensive Survey","Yichen Wan, Youyang Qu, Wei Ni, Yong Xiang, Longxiang Gao, Ekram Hossain","arXiv
arXiv","2023-12-14
2023-12","<a href=""arXiv (2023-12-14) : Data and Model Poisoning Backdoor Attacks on Wireless Federated Learning, and the Defense Mechanisms: A Comprehensive Survey"" target=""_blank"">[http://arxiv.org/abs/2312.08667v1]</a>
<a href=""DBLP (2023-12) : Data and Model Poisoning Backdoor Attacks on Wireless Federated Learning, and the Defense Mechanisms: A Comprehensive Survey"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.08667]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.08667]</a>","Due to the greatly improved capabilities of devices, massive data, and increasing concern about data privacy, Federated Learning (FL) has been increasingly considered for applications to wireless communication networks (WCNs). Wireless FL (WFL) is a distributed method of training a global deep learning model in which a large number of participants each train a local model on their training datasets and then upload the local model updates to a central server. However, in general, non-independent and identically distributed (non-IID) data of WCNs raises concerns about robustness, as a malicious participant could potentially inject a ""backdoor"" into the global model by uploading poisoned data or models over WCN. This could cause the model to misclassify malicious inputs as a specific target class while behaving normally with benign inputs. This survey provides a comprehensive review of the latest backdoor attacks and defense mechanisms. It classifies them according to their targets (data poisoning or model poisoning), the attack phase (local data collection, training, or aggregation), and defense stage (local training, before aggregation, during aggregation, or after aggregation). The strengths and limitations of existing attack strategies and defense mechanisms are analyzed in detail. Comparisons of existing attack methods and defense designs are carried out, pointing to noteworthy findings, open challenges, and potential future research directions related to security and privacy of WFL.
","
","arXiv
DBLP"
On the Difficulty of Defending Contrastive Learning against Backdoor Attacks,"Changjiang Li, Ren Pang, Bochuan Cao, Zhaohan Xi, Jinghui Chen, Shouling Ji, Ting Wang","arXiv
arXiv","2023-12-14
2023-12","<a href=""arXiv (2023-12-14) : On the Difficulty of Defending Contrastive Learning against Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2312.09057v1]</a>
<a href=""DBLP (2023-12) : On the Difficulty of Defending Contrastive Learning against Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.09057]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.09057]</a>","Recent studies have shown that contrastive learning, like supervised learning, is highly vulnerable to backdoor attacks wherein malicious functions are injected into target models, only to be activated by specific triggers. However, thus far it remains under-explored how contrastive backdoor attacks fundamentally differ from their supervised counterparts, which impedes the development of effective defenses against the emerging threat. This work represents a solid step toward answering this critical question. Specifically, we define TRL, a unified framework that encompasses both supervised and contrastive backdoor attacks. Through the lens of TRL, we uncover that the two types of attacks operate through distinctive mechanisms: in supervised attacks, the learning of benign and backdoor tasks tends to occur independently, while in contrastive attacks, the two tasks are deeply intertwined both in their representations and throughout their learning processes. This distinction leads to the disparate learning dynamics and feature distributions of supervised and contrastive attacks. More importantly, we reveal that the specificities of contrastive backdoor attacks entail important implications from a defense perspective: existing defenses for supervised attacks are often inadequate and not easily retrofitted to contrastive attacks. We also explore several alternative defenses and discuss their potential challenges. Our findings highlight the need for defenses tailored to the specificities of contrastive backdoor attacks, pointing to promising directions for future research.
","
","arXiv
DBLP"
Defenses in adversarial machine learning: A survey,"B Wu, S Wei, M Zhu, M Zheng, Z Zhu, M Zhang…","arXiv preprint arXiv …, 2023",2023-12-14,"<a href=""Google Scholar (2023-12-14) : Defenses in adversarial machine learning: A survey"" target=""_blank"">[https://arxiv.org/abs/2312.08890]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2312.08890]</a>","it, mainly including backdoor attacks, weight attacks, and adversarial examples. … backdoor attack. To defend against backdoor attacks, one approach is to perform backdoor …",,Google Scholar
Hardware Trojan Attacks in FPGAs,S Dorn,2023,2023-12-14,"<a href=""Google Scholar (2023-12-14) : Hardware Trojan Attacks in FPGAs"" target=""_blank"">[https://www.iaik.tugraz.at/wp-content/uploads/2023/09/Hardware_Trojan_Attacks_in_FPGAs.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.iaik.tugraz.at/wp-content/uploads/2023/09/Hardware_Trojan_Attacks_in_FPGAs.pdf]</a>",attacks already reported • DoS functionality in fake IC for US missiles • Fake Chinese ICs bought by military • Contained back-door • … • Contained back-door which allowed …,,Google Scholar
Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking,"Shengsheng Qian, Yifei Wang, Dizhan Xue, Shengjie Zhang, Huaiwen Zhang, Changsheng Xu","arXiv
arXiv","2023-12-13
2023-12","<a href=""arXiv (2023-12-13) : Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking"" target=""_blank"">[http://arxiv.org/abs/2312.07955v1]</a>
<a href=""DBLP (2023-12) : Erasing Self-Supervised Learning Backdoor by Cluster Activation Masking"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.07955]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.07955]</a>","Researchers have recently found that Self-Supervised Learning (SSL) is vulnerable to backdoor attacks. The attacker can embed hidden SSL backdoors via a few poisoned examples in the training dataset and maliciously manipulate the behavior of downstream models. To defend against SSL backdoor attacks, a feasible route is to detect and remove the poisonous samples in the training set. However, the existing SSL backdoor defense method fails to detect the poisonous samples precisely. In this paper, we propose to erase the SSL backdoor by cluster activation masking and propose a novel PoisonCAM method. After obtaining the threat model trained on the poisoned dataset, our method can precisely detect poisonous samples based on the assumption that masking the backdoor trigger can effectively change the activation of a downstream clustering model. In experiments, our PoisonCAM achieves 96% accuracy for backdoor trigger detection compared to 3% of the state-of-the-art method on poisoned ImageNet-100. Moreover, our proposed PoisonCAM significantly improves the performance of the trained SSL model under backdoor attacks compared to the state-of-the-art method. Our code will be available at https://github.com/LivXue/PoisonCAM.
","<a href=""arXiv"" target=""_blank"">[https://github.com/LivXue/PoisonCAM]</a>
","arXiv
DBLP"
Exploiting Machine Unlearning for Backdoor Attacks in Deep Learning System,"Peixin Zhang, Jun Sun, Mingtian Tan, Xinyu Wang",arXiv,2023-12-13,"<a href=""arXiv (2023-12-13) : Exploiting Machine Unlearning for Backdoor Attacks in Deep Learning System"" target=""_blank"">[http://arxiv.org/abs/2310.10659v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","In recent years, the security issues of artificial intelligence have become increasingly prominent due to the rapid development of deep learning research and applications. Backdoor attack is an attack targeting the vulnerability of deep learning models, where hidden backdoors are activated by triggers embedded by the attacker, thereby outputting malicious predictions that may not align with the intended output for a given input. In this work, we propose a novel black-box backdoor attack based on machine unlearning. The attacker first augments the training set with carefully designed samples, including poison and mitigation data, to train a `benign' model. Then, the attacker posts unlearning requests for the mitigation samples to remove the impact of relevant data on the model, gradually activating the hidden backdoor. Since backdoors are implanted during the iterative unlearning process, it significantly increases the computational overhead of existing defense methods for backdoor detection or mitigation. To address this new security threat, we proposes two methods for detecting or mitigating such malicious unlearning requests. We conduct the experiment in both exact unlearning and approximate unlearning (i.e., SISA) settings. Experimental results indicate that: 1) our attack approach can successfully implant backdoor into the model, and sharding increases the difficult of attack, 2) our detection algorithms are effective in identifying the mitigation samples, while sharding reduces the effectiveness of our detection algorithms.",,arXiv
Game Analysis and Optimization for Evolutionary Dynamic Heterogeneous Redundancy,L. Shi Y. Miao J. Ren R. Liu,IEEE Transactions on Network and Service Management,2023-12-12,"<a href=""IEEE (2023-12-12) : Game Analysis and Optimization for Evolutionary Dynamic Heterogeneous Redundancy"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10124232]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TNSM.2023.3275986]</a>","Dynamic Heterogeneous Redundancy (DHR) has been considered as a proactive defense architecture against unknown vulnerabilities and backdoors. It is a heterogeneous, fault-tolerant, and redundant system, which ensures the security by constantly switching heterogeneous executors. However, a limited number of heterogeneous executors can be merely switched and combined statically. It is difficult to cope with persistent threats. In this paper, we propose an evolutionary DHR system, add evolutionary sub-strategies of executors to solve this problem, optimizing defense mechanisms of DHR. To demonstrate theoretical validity, a game-theoretic analysis of the DHR security mechanism is performed under dynamic incomplete information, and the dilemma of defense is discussed. We construct a DHR game model based on evolutionary, and the results are extended to the general case to analyze the Bayesian equilibrium when each service strategy has a different number of evolved sub-strategies. In addition, the correlation factor is added to the game to investigate the impact of the correlation between different heterogeneous executors on defense results. Finally, Gambit and NS2 simulation experiments for the proposed method are shown. To verify the applicability of the physical system, a prototype DHR system based on dynamic evolution was constructed. We collect key configuration information and modified them randomly. It ensures that the information scanned by the attacker is different each time. Finally, the effectiveness of the physical system is verified by nmap and metasploit tools.",,IEEE
Enrollment-stage Backdoor Attacks on Speaker Recognition Systems via Adversarial Ultrasound,"Xinfeng Li, Junning Ze, Chen Yan, Yushi Cheng, Xiaoyu Ji, Wenyuan Xu","arXiv
IEEE Internet Things J.
arXiv","2023-12-11
2024
2023-06","<a href=""arXiv (2023-12-11) : Enrollment-stage Backdoor Attacks on Speaker Recognition Systems via Adversarial Ultrasound"" target=""_blank"">[http://arxiv.org/abs/2306.16022v2]</a>
<a href=""DBLP (2024) : Enrollment-Stage Backdoor Attacks on Speaker Recognition Systems via Adversarial Ultrasound"" target=""_blank"">[https://doi.org/10.1109/JIOT.2023.3328253]</a>
<a href=""DBLP (2023-06) : Enrollment-stage Backdoor Attacks on Speaker Recognition Systems via Adversarial Ultrasound"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.16022]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/JIOT.2023.3328253]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.16022]</a>","Automatic Speaker Recognition Systems (SRSs) have been widely used in voice applications for personal identification and access control. A typical SRS consists of three stages, i.e., training, enrollment, and recognition. Previous work has revealed that SRSs can be bypassed by backdoor attacks at the training stage or by adversarial example attacks at the recognition stage. In this paper, we propose Tuner, a new type of backdoor attack against the enrollment stage of SRS via adversarial ultrasound modulation, which is inaudible, synchronization-free, content-independent, and black-box. Our key idea is to first inject the backdoor into the SRS with modulated ultrasound when a legitimate user initiates the enrollment, and afterward, the polluted SRS will grant access to both the legitimate user and the adversary with high confidence. Our attack faces a major challenge of unpredictable user articulation at the enrollment stage. To overcome this challenge, we generate the ultrasonic backdoor by augmenting the optimization process with random speech content, vocalizing time, and volume of the user. Furthermore, to achieve real-world robustness, we improve the ultrasonic signal over traditional methods using sparse frequency points, pre-compensation, and single-sideband (SSB) modulation. We extensively evaluate Tuner on two common datasets and seven representative SRS models, as well as its robustness against seven kinds of defenses. Results show that our attack can successfully bypass speaker recognition systems while remaining effective to various speakers, speech content, etc. To mitigate this newly discovered threat, we also provide discussions on potential countermeasures, limitations, and future works of this new threat.

","

","arXiv
DBLP
DBLP"
Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger,"Yiming Li, Mingyan Zhu, Junfeng Guo, Tao Wei, Shu-Tao Xia, Zhan Qin","arXiv
arXiv","2023-12-11
2023-12","<a href=""arXiv (2023-12-11) : Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger"" target=""_blank"">[http://arxiv.org/abs/2312.04584v2]</a>
<a href=""DBLP (2023-12) : Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.04584]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.04584]</a>","Currently, sample-specific backdoor attacks (SSBAs) are the most advanced and malicious methods since they can easily circumvent most of the current backdoor defenses. In this paper, we reveal that SSBAs are not sufficiently stealthy due to their poisoned-label nature, where users can discover anomalies if they check the image-label relationship. In particular, we demonstrate that it is ineffective to directly generalize existing SSBAs to their clean-label variants by poisoning samples solely from the target class. We reveal that it is primarily due to two reasons, including \textbf{(1)} the `antagonistic effects' of ground-truth features and \textbf{(2)} the learning difficulty of sample-specific features. Accordingly, trigger-related features of existing SSBAs cannot be effectively learned under the clean-label setting due to their mild trigger intensity required for ensuring stealthiness. We argue that the intensity constraint of existing SSBAs is mostly because their trigger patterns are `content-irrelevant' and therefore act as `noises' for both humans and DNNs. Motivated by this understanding, we propose to exploit content-relevant features, $a.k.a.$ (human-relied) attributes, as the trigger patterns to design clean-label SSBAs. This new attack paradigm is dubbed backdoor attack with attribute trigger (BAAT). Extensive experiments are conducted on benchmark datasets, which verify the effectiveness of our BAAT and its resistance to existing defenses.
","
","arXiv
DBLP"
BETA-FL: Blockchain-Event Triggered Asynchronous Federated Learning in Supply Chains,M. Gulati N. Dadkhah B. Groß G. Wunder J. Glavonjic A. Pavlovic A. Tomcic,2023 Fifth International Conference on Blockchain Computing and Applications (BCCA),2023-12-11,"<a href=""IEEE (2023-12-11) : BETA-FL: Blockchain-Event Triggered Asynchronous Federated Learning in Supply Chains"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10338891]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/BCCA58897.2023.10338891]</a>","BETA-FL provides a distributed federated learning framework implemented for supply chains by tightly integrating private- permissioned blockchain for the trusted alliance among various actors involved in the supply chain. With a trusted ledger as the moderator in federated learning workflow, our approach ensures protection against malicious backdoor attacks on performance from both server and clients. Additionally, our asynchronous training regime allows scalability to a large number of federated clients with small and constant delay caused due to an event-triggering scheme. We showcase a classification task on spectrogram data as a potential use-case in the food supply chain to avoid food wastage. Finally, we facilitate a dedicated channel for regulatory bodies in our blockchain environment for inspections and audits pertaining to the functioning of the supply chain.",,IEEE
BIRD: generalizable backdoor detection and removal for deep reinforcement learning,"X Chen, W Guo, G Tao, X Zhang, D Song","Proceedings of the 37th …, 2023",2023-12-11,"<a href=""Google Scholar (2023-12-11) : BIRD: generalizable backdoor detection and removal for deep reinforcement learning"" target=""_blank"">[https://dl.acm.org/doi/abs/10.5555/3666122.3667899]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dl.acm.org/doi/abs/10.5555/3666122.3667899]</a>","and behaviors of backdoor attacks, we … backdoor, while maintaining the agent's performance in the clean environment. We evaluate BIRD against three backdoor attacks in …",,Google Scholar
How to Backdoor HyperNetwork in Personalized Federated Learning?,"Phung Lai, NhatHai Phan, Issa Khalil, Abdallah Khreishah, Xintao Wu",arXiv,2023-12-11,"<a href=""arXiv (2023-12-11) : How to Backdoor HyperNetwork in Personalized Federated Learning?"" target=""_blank"">[http://arxiv.org/abs/2201.07063v3]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","This paper explores previously unknown backdoor risks in HyperNet-based personalized federated learning (HyperNetFL) through poisoning attacks. Based upon that, we propose a novel model transferring attack (called HNTroj), i.e., the first of its kind, to transfer a local backdoor infected model to all legitimate and personalized local models, which are generated by the HyperNetFL model, through consistent and effective malicious local gradients computed across all compromised clients in the whole training process. As a result, HNTroj reduces the number of compromised clients needed to successfully launch the attack without any observable signs of sudden shifts or degradation regarding model utility on legitimate data samples making our attack stealthy. To defend against HNTroj, we adapted several backdoor-resistant FL training algorithms into HyperNetFL. An extensive experiment that is carried out using several benchmark datasets shows that HNTroj significantly outperforms data poisoning and model replacement attacks and bypasses robust training algorithms even with modest numbers of compromised clients.",,arXiv
Temporal-Distributed Backdoor Attack Against Video Based Action Recognition,"Xi Li, Songhe Wang, Ruiquan Huang, Mahanth Gowda, George Kesidis","arXiv
arXiv","2023-12-09
2023-08","<a href=""arXiv (2023-12-09) : Temporal-Distributed Backdoor Attack Against Video Based Action Recognition"" target=""_blank"">[http://arxiv.org/abs/2308.11070v3]</a>
<a href=""DBLP (2023-08) : Temporal-Distributed Backdoor Attack Against Video Based Action Recognition"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.11070]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.11070]</a>","Deep neural networks (DNNs) have achieved tremendous success in various applications including video action recognition, yet remain vulnerable to backdoor attacks (Trojans). The backdoor-compromised model will mis-classify to the target class chosen by the attacker when a test instance (from a non-target class) is embedded with a specific trigger, while maintaining high accuracy on attack-free instances. Although there are extensive studies on backdoor attacks against image data, the susceptibility of video-based systems under backdoor attacks remains largely unexplored. Current studies are direct extensions of approaches proposed for image data, e.g., the triggers are independently embedded within the frames, which tend to be detectable by existing defenses. In this paper, we introduce a simple yet effective backdoor attack against video data. Our proposed attack, adding perturbations in a transformed domain, plants an imperceptible, temporally distributed trigger across the video frames, and is shown to be resilient to existing defensive strategies. The effectiveness of the proposed attack is demonstrated by extensive experiments with various well-known models on two video recognition benchmarks, UCF101 and HMDB51, and a sign language recognition benchmark, Greek Sign Language (GSL) dataset. We delve into the impact of several influential factors on our proposed attack and identify an intriguing effect termed ""collateral damage"" through extensive studies.
","
","arXiv
DBLP"
Detecting Memory Injections Using a Hardware Monitor,"M Botacin, U Kosayev, A Yifrach",2023,2023-12-09,"<a href=""Google Scholar (2023-12-09) : Detecting Memory Injections Using a Hardware Monitor"" target=""_blank"">[https://www.researchgate.net/profile/Marcus-Botacin/publication/376309129_Detecting_Memory_Injections_Using_a_Hardware_Monitor/links/65724544fc4b416622a66283/Detecting-Memory-Injections-Using-a-Hardware-Monitor.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.researchgate.net/profile/Marcus-Botacin/publication/376309129_Detecting_Memory_Injections_Using_a_Hardware_Monitor/links/65724544fc4b416622a66283/Detecting-Memory-Injections-Using-a-Hardware-Monitor.pdf]</a>",Memory injection is the current state-of-the-art malware attack technique. Injections … injection attacks. We evaluate the prototype via the injection of a backdoor payload into …,,Google Scholar
Securing Deep Learning-Based Autonomous Vehicle Controllers Through Subspace Projection Analysis,Y Wang,2023,2023-12-09,"<a href=""Google Scholar (2023-12-09) : Securing Deep Learning-Based Autonomous Vehicle Controllers Through Subspace Projection Analysis"" target=""_blank"">[https://search.proquest.com/openview/640e0234afa688a2a610be2717d329d2/1?pq-origsite=gscholar&cbl=18750&diss=y]</a>","<a href=""Google Scholar"" target=""_blank"">[https://search.proquest.com/openview/640e0234afa688a2a610be2717d329d2/1?pq-origsite=gscholar&cbl=18750&diss=y]</a>","to adversarial attacks, specifically backdoor attacks in neural … models under different backdoor attack schemes to assess … are influenced by backdoor attacks and propose a …",,Google Scholar
Reconstructive Neuron Pruning for Backdoor Defense,"Yige Li, Xixiang Lyu, Xingjun Ma, Nodens Koren, Lingjuan Lyu, Bo Li, Yu-Gang Jiang","arXiv
ICML
arXiv","2023-12-08
2023
2023-05","<a href=""arXiv (2023-12-08) : Reconstructive Neuron Pruning for Backdoor Defense"" target=""_blank"">[http://arxiv.org/abs/2305.14876v2]</a>
<a href=""DBLP (2023) : Reconstructive Neuron Pruning for Backdoor Defense"" target=""_blank"">[https://proceedings.mlr.press/v202/li23v.html]</a>
<a href=""DBLP (2023-05) : Reconstructive Neuron Pruning for Backdoor Defense"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.14876]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://proceedings.mlr.press/v202/li23v.html]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.14876]</a>","Deep neural networks (DNNs) have been found to be vulnerable to backdoor attacks, raising security concerns about their deployment in mission-critical applications. While existing defense methods have demonstrated promising results, it is still not clear how to effectively remove backdoor-associated neurons in backdoored DNNs. In this paper, we propose a novel defense called \emph{Reconstructive Neuron Pruning} (RNP) to expose and prune backdoor neurons via an unlearning and then recovering process. Specifically, RNP first unlearns the neurons by maximizing the model's error on a small subset of clean samples and then recovers the neurons by minimizing the model's error on the same data. In RNP, unlearning is operated at the neuron level while recovering is operated at the filter level, forming an asymmetric reconstructive learning procedure. We show that such an asymmetric process on only a few clean samples can effectively expose and prune the backdoor neurons implanted by a wide range of attacks, achieving a new state-of-the-art defense performance. Moreover, the unlearned model at the intermediate step of our RNP can be directly used to improve other backdoor defense tasks including backdoor removal, trigger recovery, backdoor label detection, and backdoor sample detection. Code is available at \url{https://github.com/bboylyg/RNP}.

","<a href=""arXiv"" target=""_blank"">[https://github.com/bboylyg/RNP}]</a>

","arXiv
DBLP
DBLP"
G$^2$uardFL: Safeguarding Federated Learning Against Backdoor Attacks through Attributed Client Graph Clustering,"Hao Yu, Chuan Ma, Meng Liu, Tianyu Du, Ming Ding, Tao Xiang, Shouling Ji, Xinwang Liu",arXiv,2023-12-08,"<a href=""arXiv (2023-12-08) : G$^2$uardFL: Safeguarding Federated Learning Against Backdoor Attacks through Attributed Client Graph Clustering"" target=""_blank"">[http://arxiv.org/abs/2306.04984v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Federated Learning (FL) offers collaborative model training without data sharing but is vulnerable to backdoor attacks, where poisoned model weights lead to compromised system integrity. Existing countermeasures, primarily based on anomaly detection, are prone to erroneous rejections of normal weights while accepting poisoned ones, largely due to shortcomings in quantifying similarities among client models. Furthermore, other defenses demonstrate effectiveness only when dealing with a limited number of malicious clients, typically fewer than 10%. To alleviate these vulnerabilities, we present G$^2$uardFL, a protective framework that reinterprets the identification of malicious clients as an attributed graph clustering problem, thus safeguarding FL systems. Specifically, this framework employs a client graph clustering approach to identify malicious clients and integrates an adaptive mechanism to amplify the discrepancy between the aggregated model and the poisoned ones, effectively eliminating embedded backdoors. We also conduct a theoretical analysis of convergence to confirm that G$^2$uardFL does not affect the convergence of FL systems. Through empirical evaluation, comparing G$^2$uardFL with cutting-edge defenses, such as FLAME (USENIX Security 2022) [28] and DeepSight (NDSS 2022) [36], against various backdoor attacks including 3DFed (SP 2023) [20], our results demonstrate its significant effectiveness in mitigating backdoor attacks while having a negligible impact on the aggregated model's performance on benign samples (i.e., the primary task performance). For instance, in an FL system with 25% malicious clients, G$^2$uardFL reduces the attack success rate to 10.61%, while maintaining a primary task performance of 73.05% on the CIFAR-10 dataset. This surpasses the performance of the best-performing baseline, which merely achieves a primary task performance of 19.54%.",,arXiv
Stealthy Backdoor Attacks On Deep Point Cloud Recognization Networks,"L Feng, Z Qian, X Zhang, S Li","The Computer Journal, 2023",2023-12-08,"<a href=""Google Scholar (2023-12-08) : Stealthy Backdoor Attacks On Deep Point Cloud Recognization Networks"" target=""_blank"">[https://academic.oup.com/comjnl/advance-article-abstract/doi/10.1093/comjnl/bxad109/7462048]</a>","<a href=""Google Scholar"" target=""_blank"">[https://academic.oup.com/comjnl/advance-article-abstract/doi/10.1093/comjnl/bxad109/7462048]</a>",", backdoor attacks on 3D point clouds have rarely been investigated. This paper proposes a stealthy backdoor attack on … as backdoor patterns. Instead of directly adding …",,Google Scholar
"Into the Endless Mist: Volume 1: The Aleutian Campaign, June-August 1942",MA Piegzik,2023,2023-12-07,"<a href=""Google Scholar (2023-12-07) : Into the Endless Mist: Volume 1: The Aleutian Campaign, June-August 1942"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=L8T5EAAAQBAJ&oi=fnd&pg=PT3&dq=backdoor+attack&ots=pDHN_rvKIO&sig=BTyIoXib717BP33-t9SEqKYHeQk]</a>","<a href=""Google Scholar"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=L8T5EAAAQBAJ&oi=fnd&pg=PT3&dq=backdoor+attack&ots=pDHN_rvKIO&sig=BTyIoXib717BP33-t9SEqKYHeQk]</a>","Alaska and America’s back door, also caused chaos among military personnel and politicians alike in Washington DC, sharing the fear of an attack on the West Coast and …",,Google Scholar
The Robustness of Machine Learning Models Using MLSecOps: A Case Study On Delivery Service Forecasting,A. Saputra E. Suryani N. A. Rakhmawati,"2023 14th International Conference on Information & Communication Technology and System (ICTS)
2023 14th International …, 2023","2023-12-06
2023-10-04","<a href=""IEEE (2023-12-06) : The Robustness of Machine Learning Models Using MLSecOps: A Case Study On Delivery Service Forecasting"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10330833]</a>
<a href=""Google Scholar (2023-10-04) : The Robustness of Machine Learning Models Using MLSecOps: A Case Study On Delivery Service Forecasting"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10330833/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICTS58770.2023.10330833]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10330833/]</a>","Forecasting delivery services is a key aspect of modern delivery service operations that significantly contributes to the optimization of operations and the enhancement of customer satisfaction. Machine learning can assist in predicting the delivery time. One method to enhance security in machine learning is the implementation of MLSecOps. MLSecOps, or Machine Learning Security Operations, streamlines the process of deploying, monitoring, and maintaining machine learning models to ensure consistent and reliable performance in production environments. Cybersecurity was also integrated to enhance the security, robustness, and resilience of these models. This study applies MLSecOps to forecast delivery services to enhance the robustness of machine learning models. The MLSecOps tool utilized is the Adversarial Robustness Toolbox (ART). The results of testing the machine learning model on Forecasting Delivery Services show robustness to attacks such as boundary and backdoor attacks.
to attacks such as boundary and backdoor attacks. … of encountering boundary attacks and backdoor attacks[16]… encountering boundary attacks and backdoor attacks. One …","
","IEEE
Google Scholar"
Unmasking the Power of Trigger Intensity in Federated Learning: Exploring Trigger Intensities in Backdoor Attacks,A Shokri Kalisa,2023,2023-12-06,"<a href=""Google Scholar (2023-12-06) : Unmasking the Power of Trigger Intensity in Federated Learning: Exploring Trigger Intensities in Backdoor Attacks"" target=""_blank"">[https://repository.tudelft.nl/islandora/object/uuid:5495b963-c1b9-4c57-8c4a-e510f36b4875]</a>","<a href=""Google Scholar"" target=""_blank"">[https://repository.tudelft.nl/islandora/object/uuid:5495b963-c1b9-4c57-8c4a-e510f36b4875]</a>",flaws such as backdoor attacks in which malevolent … of trigger intensity in backdoor attacks within the federated … capable of enhancing backdoor attack performance and …,,Google Scholar
Robust Backdoor Detection for Deep Learning via Topological Evolution Dynamics,"Xiaoxing Mo, Yechao Zhang, Leo Yu Zhang, Wei Luo, Nan Sun, Shengshan Hu, Shang Gao, Yang Xiang","arXiv
arXiv","2023-12-05
2023-12","<a href=""arXiv (2023-12-05) : Robust Backdoor Detection for Deep Learning via Topological Evolution Dynamics"" target=""_blank"">[http://arxiv.org/abs/2312.02673v1]</a>
<a href=""DBLP (2023-12) : Robust Backdoor Detection for Deep Learning via Topological Evolution Dynamics"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.02673]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.02673]</a>","A backdoor attack in deep learning inserts a hidden backdoor in the model to trigger malicious behavior upon specific input patterns. Existing detection approaches assume a metric space (for either the original inputs or their latent representations) in which normal samples and malicious samples are separable. We show that this assumption has a severe limitation by introducing a novel SSDT (Source-Specific and Dynamic-Triggers) backdoor, which obscures the difference between normal samples and malicious samples. To overcome this limitation, we move beyond looking for a perfect metric space that would work for different deep-learning models, and instead resort to more robust topological constructs. We propose TED (Topological Evolution Dynamics) as a model-agnostic basis for robust backdoor detection. The main idea of TED is to view a deep-learning model as a dynamical system that evolves inputs to outputs. In such a dynamical system, a benign input follows a natural evolution trajectory similar to other benign inputs. In contrast, a malicious sample displays a distinct trajectory, since it starts close to benign samples but eventually shifts towards the neighborhood of attacker-specified target samples to activate the backdoor. Extensive evaluations are conducted on vision and natural language datasets across different network architectures. The results demonstrate that TED not only achieves a high detection rate, but also significantly outperforms existing state-of-the-art detection approaches, particularly in addressing the sophisticated SSDT attack. The code to reproduce the results is made public on GitHub.
","
","arXiv
DBLP"
Effective Backdoor Mitigation Depends on the Pre-training Objective,"Sahil Verma, Gantavya Bhatt, Avi Schwarzschild, Soumye Singhal, Arnav Mohanty Das, Chirag Shah, John P Dickerson, Jeff Bilmes","arXiv
arXiv","2023-12-05
2023-11","<a href=""arXiv (2023-12-05) : Effective Backdoor Mitigation Depends on the Pre-training Objective"" target=""_blank"">[http://arxiv.org/abs/2311.14948v3]</a>
<a href=""DBLP (2023-11) : Effective Backdoor Mitigation Depends on the Pre-training Objective"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.14948]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.14948]</a>","Despite the advanced capabilities of contemporary machine learning (ML) models, they remain vulnerable to adversarial and backdoor attacks. This vulnerability is particularly concerning in real-world deployments, where compromised models may exhibit unpredictable behavior in critical scenarios. Such risks are heightened by the prevalent practice of collecting massive, internet-sourced datasets for pre-training multimodal models, as these datasets may harbor backdoors. Various techniques have been proposed to mitigate the effects of backdooring in these models such as CleanCLIP which is the current state-of-the-art approach. In this work, we demonstrate that the efficacy of CleanCLIP in mitigating backdoors is highly dependent on the particular objective used during model pre-training. We observe that stronger pre-training objectives correlate with harder to remove backdoors behaviors. We show this by training multimodal models on two large datasets consisting of 3 million (CC3M) and 6 million (CC6M) datapoints, under various pre-training objectives, followed by poison removal using CleanCLIP. We find that CleanCLIP is ineffective when stronger pre-training objectives are used, even with extensive hyperparameter tuning. Our findings underscore critical considerations for ML practitioners who pre-train models using large-scale web-curated data and are concerned about potential backdoor threats. Notably, our results suggest that simpler pre-training objectives are more amenable to effective backdoor removal. This insight is pivotal for practitioners seeking to balance the trade-offs between using stronger pre-training objectives and security against backdoor attacks.
","
","arXiv
DBLP"
Security challenges in natural language processing models,"Q Xu, X He","Proceedings of the 2023 Conference on Empirical …, 2023",2023-12-05,"<a href=""Google Scholar (2023-12-05) : Security challenges in natural language processing models"" target=""_blank"">[https://aclanthology.org/2023.emnlp-tutorial.2/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://aclanthology.org/2023.emnlp-tutorial.2/]</a>","in NLP research, ie, backdoor attacks, private data leakage, … , attack methodologies, and defense technologies. … A notable subset of these attacks is the backdoor …",,Google Scholar
UCCA: A Verified Architecture for Compartmentalization of Untrusted Code Sections in Resource-Constrained Devices,"L Tyler, IDO Nunes","arXiv preprint arXiv:2312.02348, 2023",2023-12-05,"<a href=""Google Scholar (2023-12-05) : UCCA: A Verified Architecture for Compartmentalization of Untrusted Code Sections in Resource-Constrained Devices"" target=""_blank"">[https://arxiv.org/abs/2312.02348]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2312.02348]</a>","vulnerable to run-time attacks that can remotely alter their … In turn, a single vulnerability (or intentional backdoor) in … sample attack programs, that show how run-time attacks …",,Google Scholar
Beyond Securing Networks and Storage: Emerging Attacks and Defences to Machine Intelligence,A. Khreishah,2023 Tenth International Conference on Software Defined Systems (SDS),2023-12-04,"<a href=""IEEE (2023-12-04) : Beyond Securing Networks and Storage: Emerging Attacks and Defences to Machine Intelligence"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10329163]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SDS59856.2023.10329163]</a>","Over the past three decades most research efforts in security and privacy have focused on network and storage security. Recently, Deep Neural Network (DNN) classifiers gain wide adoption in different complex tasks, including natural language processing, computer vision and cyber security. However, the underlying assumption of attack free operating environment has been defied by the introduction of several attacks such as adversarial examples and Trojan backdoor attacks. In Adversarial attacks the adversary perturbs the input examples during inference to force the DNN to misclassify while the adversary in the Trojan Backdoor operates in both training and inference phases. In the training phase the adversary trains the DNN in a way such that it behaves normally when the Trojan trigger does not exist, and it misclassifies if the trigger exists. Given that only the adversary knows the trigger, the users of the DNN will be fooled to trust the DNN model. The adversary can now attach the Trigger to the input examples during inference causing the DNN model to misclassify. In this talk we will discuss our development of several computationally efficient defense approaches for the Adversarial attacks enabling real-time detection of the attack for the first time. We will also discuss our development of an adaptive black-box defense approach for the Trojan Backdoor attack that outperforms the state-of-the-art by studying the relationships among the prediction logits of the DNN. After that we will discuss our recent follow up work in which we show how to jointly combine the above two adversaries to practically launch a new stealthy attack, dubbed AdvTrojan. AdvTrojan is stealthy because it can be activated only when: 1) a carefully crafted adversarial perturbation is injected into the input examples during inference, and 2) a Trojan backdoor is implanted during the training process of the model. We leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, making it difficult to detect. The stealthiness behavior of AdvTrojan fools the users into accidentally trusting the infected model as a robust classifier against adversarial examples. We will also discuss our future research that is focused on expanding the attack and defense mechanisms to new areas such as malicious domain detection, federated learning setting, personalized federated learning, and Graph Neural Networks. We will also discuss several application domains of adversarial as well as Trojan Backdoor attacks.",,IEEE
On the Detection of Image-Scaling Attacks in Machine Learning,Quiring E.,ACM International Conference Proceeding Series,2023-12-04,"<a href=""ScienceDirect (2023-12-04) : On the Detection of Image-Scaling Attacks in Machine Learning"" target=""_blank"">[https://doi.org/10.1145/3627106.3627134]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3627106.3627134]</a>",,,ScienceDirect
SSL-ABD: An Adversarial Defense Method Against Backdoor Attacks in Self-supervised Learning,"H Yang, R Yang, H Cai, X Zhang, Q Pei…","… Conference on Artificial …, 2023",2023-12-04,"<a href=""Google Scholar (2023-12-04) : SSL-ABD: An Adversarial Defense Method Against Backdoor Attacks in Self-supervised Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9785-5_32]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-9785-5_32]</a>",analysis to identify potential backdoor trigger conditions and eliminate … backdoor attacks. These algorithms can automatically identify and resist potential backdoor attacks …,,Google Scholar
A novel botnet attack detection for IoT networks based on communication graphs,"David Concejal Muñoz, Antonio del-Corte Valiente",Cybersecurity,2023-12-03,"<a href=""Springer (2023-12-03) : A novel botnet attack detection for IoT networks based on communication graphs"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-023-00169-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-023-00169-6]</a>",Abstract Intrusion detection systems have been proposed for the detection of botnet attacks. Various types of centralized or distributed cloud-based...,,Springer
Anomaly detection in IOT edge computing using deep learning and instance-level horizontal reduction,"Negar Abbasi, Mohammadreza Soltanaghaei, Farsad Zamani Boroujeni",The Journal of Supercomputing,2023-12-02,"<a href=""Springer (2023-12-02) : Anomaly detection in IOT edge computing using deep learning and instance-level horizontal reduction"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05771-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05771-6]</a>","The increasing number of network attacks has led to the development of intrusion detection systems. However, these methods often face limitations...",,Springer
Cyber Deception Techniques and an Adversary Engagement Platform for Cybersecurity Enhancement,MA Shhadih,2023,2023-12-02,"<a href=""Google Scholar (2023-12-02) : Cyber Deception Techniques and an Adversary Engagement Platform for Cybersecurity Enhancement"" target=""_blank"">[https://search.proquest.com/openview/02c6fbc3a2ddb19722406739f8902436/1?pq-origsite=gscholar&cbl=18750&diss=y]</a>","<a href=""Google Scholar"" target=""_blank"">[https://search.proquest.com/openview/02c6fbc3a2ddb19722406739f8902436/1?pq-origsite=gscholar&cbl=18750&diss=y]</a>","backdoor attacks, their shortfall is that they rely on human intervention to work as expected. … intrusion using backdoor attacks, cybersecurity defenses based on adaptive …",,Google Scholar
Ensuring Intrusion Detection for IoT Services Through an Improved CNN,"Sunday Adeola Ajagbe, Joseph Bamidele Awotunde, Hector Florez",SN Computer Science,2023-12-02,"<a href=""Springer (2023-12-02) : Ensuring Intrusion Detection for IoT Services Through an Improved CNN"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-023-02448-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-023-02448-y]</a>","Internet of Things (IoT) devices are challenging to manage information security due to some factors such as processing capability, exponential growth...",,Springer
Trojansql: Sql injection against natural language interface to database,"J Zhang, Y Zhou, B Hui, Y Liu, Z Li…","Proceedings of the 2023 …, 2023",2023-12-02,"<a href=""Google Scholar (2023-12-02) : Trojansql: Sql injection against natural language interface to database"" target=""_blank"">[https://aclanthology.org/2023.emnlp-main.264/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://aclanthology.org/2023.emnlp-main.264/]</a>","Existing methods have primarily focused on classification tasks, however, we adapted and applied backdoor attacks to the higher stakes scenario of natural language in…",,Google Scholar
An Imperceptible Data Augmentation Based Blackbox Clean-Label Backdoor Attack on Deep Neural Networks,Xu C.,IEEE Transactions on Circuits and Systems I: Regular Papers,2023-12-01,"<a href=""ScienceDirect (2023-12-01) : An Imperceptible Data Augmentation Based Blackbox Clean-Label Backdoor Attack on Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1109/TCSI.2023.3298802]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TCSI.2023.3298802]</a>",,,ScienceDirect
Camouflage Backdoor Attack against Pedestrian Detection,Wu Y.,Applied Sciences (Switzerland),2023-12-01,"<a href=""ScienceDirect (2023-12-01) : Camouflage Backdoor Attack against Pedestrian Detection"" target=""_blank"">[https://doi.org/10.3390/app132312752]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/app132312752]</a>",,,ScienceDirect
Deep fidelity in DNN watermarking: A study of backdoor watermarking for classification models,Hua G.,Pattern Recognition,2023-12-01,"<a href=""ScienceDirect (2023-12-01) : Deep fidelity in DNN watermarking: A study of backdoor watermarking for classification models"" target=""_blank"">[https://doi.org/10.1016/j.patcog.2023.109844]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.patcog.2023.109844]</a>",,,ScienceDirect
Efficient and persistent backdoor attack by boundary trigger set constructing against federated learning,Yang D.,Information Sciences,2023-12-01,"<a href=""ScienceDirect (2023-12-01) : Efficient and persistent backdoor attack by boundary trigger set constructing against federated learning"" target=""_blank"">[https://doi.org/10.1016/j.ins.2023.119743]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.ins.2023.119743]</a>",,,ScienceDirect
FMDL: Federated Mutual Distillation Learning for Defending Backdoor Attacks,Sun H.,Electronics (Switzerland),2023-12-01,"<a href=""ScienceDirect (2023-12-01) : FMDL: Federated Mutual Distillation Learning for Defending Backdoor Attacks"" target=""_blank"">[https://doi.org/10.3390/electronics12234838]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/electronics12234838]</a>",,,ScienceDirect
Feature Importance-Based Backdoor Attack in NSL-KDD,Jang J.,Electronics (Switzerland),2023-12-01,"<a href=""ScienceDirect (2023-12-01) : Feature Importance-Based Backdoor Attack in NSL-KDD"" target=""_blank"">[https://doi.org/10.3390/electronics12244953]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/electronics12244953]</a>",,,ScienceDirect
Privacy and security,"X Gong, Y Chen, W Yang, H Huang, Q Wang","ACM Transactions on, 2023",2023-12-01,"<a href=""Google Scholar (2023-12-01) : Privacy and security"" target=""_blank"">[https://dl.acm.org/doi/pdf/10.1145/3614236]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dl.acm.org/doi/pdf/10.1145/3614236]</a>","As the trigger is key to successful backdoor attacks, we develop a novel trigger generation algorithm that intensifies the bond between the trigger and the targeted …",,Google Scholar
Resonant Attack: Reveal Cross-Modal Vulnerability of CLIP,Chen T.Y.,Jisuanji Xuebao/Chinese Journal of Computers,2023-12-01,"<a href=""ScienceDirect (2023-12-01) : Resonant Attack: Reveal Cross-Modal Vulnerability of CLIP"" target=""_blank"">[https://doi.org/10.11897/SP.J.1016.2023.02597]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.11897/SP.J.1016.2023.02597]</a>",,,ScienceDirect
Towards Trustworthy Machine Learning,MS Ozdayi,2023,2023-12-01,"<a href=""Google Scholar (2023-12-01) : Towards Trustworthy Machine Learning"" target=""_blank"">[https://utd-ir.tdl.org/items/3073c5cd-2194-4ba1-8e6c-6281de30d0a5]</a>","<a href=""Google Scholar"" target=""_blank"">[https://utd-ir.tdl.org/items/3073c5cd-2194-4ba1-8e6c-6281de30d0a5]</a>","a lightweight defense against backdoor attacks in FL. Empirical evidence supports the effectiveness of our defense against backdoor attacks under various settings, …",,Google Scholar
Unveiling backdoor risks brought by foundation models in heterogeneous federated learning,"X Li, C Wu, J Wang","Pacific-Asia Conference on Knowledge Discovery …, 2024",2023-12-01,"<a href=""Google Scholar (2023-12-01) : Unveiling backdoor risks brought by foundation models in heterogeneous federated learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-2259-4_13]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-97-2259-4_13]</a>","In this paper, we introduce a novel backdoor attack mechanism for HFL … backdoor attack strategy named Fed-EBD that distinguishes itself from traditional backdoor attacks …",,Google Scholar
FedDefender: Backdoor Attack Defense in Federated Learning,"Waris Gill, Ali Anwar, Muhammad Ali Gulzar","SE4SafeML 2023: Proceedings of the 1st International Workshop on Dependability and Trustworthiness of Safety-Critical Systems with Machine Learned Components
arXiv
SE4SafeML@SIGSOFT FSE
arXiv","2023-12
2024-02-23
2023
2023-07","<a href=""ACM (2023-12) : FedDefender: Backdoor Attack Defense in Federated Learning"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3617574.3617858]</a>
<a href=""arXiv (2024-02-23) : FedDefender: Backdoor Attack Defense in Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2307.08672v2]</a>
<a href=""DBLP (2023) : FedDefender: Backdoor Attack Defense in Federated Learning"" target=""_blank"">[https://doi.org/10.1145/3617574.3617858]</a>
<a href=""DBLP (2023-07) : FedDefender: Backdoor Attack Defense in Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.08672]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3617574.3617858]</a>
<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1145/3617574.3617858]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3617574.3617858]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.08672]</a>","Federated Learning (FL) is a privacy-preserving distributed machine learning technique that enables individual clients (e.g., user participants, edge devices, or organizations) to train a model on their local data in a secure environment and then ...
Federated Learning (FL) is a privacy-preserving distributed machine learning technique that enables individual clients (e.g., user participants, edge devices, or organizations) to train a model on their local data in a secure environment and then share the trained model with an aggregator to build a global model collaboratively. In this work, we propose FedDefender, a defense mechanism against targeted poisoning attacks in FL by leveraging differential testing. Our proposed method fingerprints the neuron activations of clients' models on the same input and uses differential testing to identify a potentially malicious client containing a backdoor. We evaluate FedDefender using MNIST and FashionMNIST datasets with 20 and 30 clients, and our results demonstrate that FedDefender effectively mitigates such attacks, reducing the attack success rate (ASR) to 10\% without deteriorating the global model performance.

","


","ACM
arXiv
DBLP
DBLP"
A network intrusion detection framework on sparse deep denoising auto-encoder for dimensionality reduction,"B. A. Manjunatha, K. Aditya Shastry, ... Kadiri Thirupal Reddy",Soft Computing,2023-11-30,"<a href=""Springer (2023-11-30) : A network intrusion detection framework on sparse deep denoising auto-encoder for dimensionality reduction"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-023-09408-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-023-09408-x]</a>","In today's internet-driven world, a multitude of attacks occurs daily, propelled by a vast user base. The effective detection of these numerous...",,Springer
TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4,"Zihao Tan, Qingliang Chen, Yongjian Huang, Chen Liang","arXiv
arXiv","2023-11-29
2023-11","<a href=""arXiv (2023-11-29) : TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4"" target=""_blank"">[http://arxiv.org/abs/2311.17429v1]</a>
<a href=""DBLP (2023-11) : TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.17429]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.17429]</a>","Prompt-based learning has been widely applied in many low-resource NLP tasks such as few-shot scenarios. However, this paradigm has been shown to be vulnerable to backdoor attacks. Most of the existing attack methods focus on inserting manually predefined templates as triggers in the pre-training phase to train the victim model and utilize the same triggers in the downstream task to perform inference, which tends to ignore the transferability and stealthiness of the templates. In this work, we propose a novel approach of TARGET (Template-trAnsfeRable backdoor attack aGainst prompt-basEd NLP models via GPT4), which is a data-independent attack method. Specifically, we first utilize GPT4 to reformulate manual templates to generate tone-strong and normal templates, and the former are injected into the model as a backdoor trigger in the pre-training phase. Then, we not only directly employ the above templates in the downstream task, but also use GPT4 to generate templates with similar tone to the above templates to carry out transferable attacks. Finally we have conducted extensive experiments on five NLP datasets and three BERT series models, with experimental results justifying that our TARGET method has better attack performance and stealthiness compared to the two-external baseline methods on direct attacks, and in addition achieves satisfactory attack capability in the unseen tone-similar templates.
","
","arXiv
DBLP"
Machine learning for security applications under dynamic and adversarial environments,L Yang,2023,2023-11-29,"<a href=""Google Scholar (2023-11-29) : Machine learning for security applications under dynamic and adversarial environments"" target=""_blank"">[https://www.ideals.illinois.edu/items/129213]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.ideals.illinois.edu/items/129213]</a>","In this dissertation, we mainly focus on the training-time backdoor attacks. However, it is useful to provide the broader threat landscape as some of the technology can …",,Google Scholar
Test-Time Backdoor Defense via Detecting and Repairing,"Jiyang Guan, Jian Liang, Ran He",arXiv,2023-11-29,"<a href=""arXiv (2023-11-29) : Test-Time Backdoor Defense via Detecting and Repairing"" target=""_blank"">[http://arxiv.org/abs/2308.06107v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Deep neural networks have played a crucial part in many critical domains, such as autonomous driving, face recognition, and medical diagnosis. However, deep neural networks are facing security threats from backdoor attacks and can be manipulated into attacker-decided behaviors by the backdoor attacker. To defend the backdoor, prior research has focused on using clean data to remove backdoor attacks before model deployment. In this paper, we investigate the possibility of defending against backdoor attacks at test time by utilizing partially poisoned data to remove the backdoor from the model. To address the problem, a two-stage method Test-Time Backdoor Defense (TTBD) is proposed. In the first stage, we propose a backdoor sample detection method DDP to identify poisoned samples from a batch of mixed, partially poisoned samples. Once the poisoned samples are detected, we employ Shapley estimation to calculate the contribution of each neuron's significance in the network, locate the poisoned neurons, and prune them to remove backdoor in the models. Our experiments demonstrate that TTBD removes the backdoor successfully with only a batch of partially poisoned data across different model architectures and datasets against different types of backdoor attacks.",,arXiv
"Text classification backdoor attack prediction method, system, and device","LU Hengyang, C Fan, W Fang, J Sun, X Wu","US Patent 11,829,474, 2023",2023-11-29,"<a href=""Google Scholar (2023-11-29) : Text classification backdoor attack prediction method, system, and device"" target=""_blank"">[https://patents.google.com/patent/US11829474B1/en]</a>","<a href=""Google Scholar"" target=""_blank"">[https://patents.google.com/patent/US11829474B1/en]</a>","The present invention provides a text classification backdoor attack … a backdoor data set by using the locator model, and training the clean model by using the backdoor …",,Google Scholar
Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective,"Ming-Yu Chung, Sheng-Yen Chou, Chia-Mu Yu, Pin-Yu Chen, Sy-Yen Kuo, Tsung-Yi Ho","arXiv
arXiv","2023-11-28
2023-11","<a href=""arXiv (2023-11-28) : Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective"" target=""_blank"">[http://arxiv.org/abs/2311.16646v1]</a>
<a href=""DBLP (2023-11) : Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.16646]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.16646]</a>","Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods. Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks.
","
","arXiv
DBLP"
Distributed Attacks over Federated Reinforcement Learning-enabled Cell Sleep Control,"H Zhang, H Zhou, M Elsayed, M Bavand…","arXiv preprint arXiv …, 2023",2023-11-28,"<a href=""Google Scholar (2023-11-28) : Distributed Attacks over Federated Reinforcement Learning-enabled Cell Sleep Control"" target=""_blank"">[https://arxiv.org/abs/2311.15894]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2311.15894]</a>","the backdoor attacks to a wireless network control application. The simulation results show that the designed attacks … data poisoning attacks and backdoor attacks, we have …",,Google Scholar
Elijah: Eliminating backdoors injected in diffusion models via distribution shift,"S An, SY Chou, K Zhang, Q Xu, G Tao…","Proceedings of the …, 2024",2023-11-28,"<a href=""Google Scholar (2023-11-28) : Elijah: Eliminating backdoors injected in diffusion models via distribution shift"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/28958]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/28958]</a>",• We study three existing backdoor attacks in diffusion models and propose the first … existing backdoor attacks in DMs utlize distribution shift and propose the first backdoor …,,Google Scholar
IBAttack: Being Cautious About Data Labels,A. Agarwal R. Singh M. Vatsa N. Ratha,IEEE Transactions on Artificial Intelligence,2023-11-28,"<a href=""IEEE (2023-11-28) : IBAttack: Being Cautious About Data Labels"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9891832]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TAI.2022.3206259]</a>","Traditional backdoor attacks insert a trigger patch in the training images and associate the trigger with the targeted class label. Backdoor attacks are one of the rapidly evolving types of attack which can have a significant impact. On the other hand, adversarial perturbations have a significantly different attack mechanism from the traditional backdoor corruptions, where an imperceptible noise is learned to fool the deep learning models. In this research, we amalgamate these two concepts and propose a novel imperceptible backdoor attack, termed as the IBAttack, where the adversarial images are associated with the desired target classes. A significant advantage of the adversarial-based proposed backdoor attack is the imperceptibility as compared to the traditional trigger-based mechanism. The proposed adversarial dynamic attack, in contrast to existing attacks, is agnostic to classifiers and trigger patterns. The extensive evaluation using multiple databases and networks illustrates the effectiveness of the proposed attack.",,IEEE
Smart contract: a survey towards extortionate vulnerability detection and security enhancement,"S. Porkodi, D. Kesavaraja",Wireless Networks,2023-11-28,"<a href=""Springer (2023-11-28) : Smart contract: a survey towards extortionate vulnerability detection and security enhancement"" target=""_blank"">[https://link.springer.com/article/10.1007/s11276-023-03587-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11276-023-03587-z]</a>","Smart contracts become significant as blockchain technology is blooming around the technological globe. In this juncture, smart contract is an...",,Springer
Badclip: Trigger-aware prompt learning for backdoor attacks on clip,"J Bai, K Gao, S Min, ST Xia, Z Li…","Proceedings of the IEEE …, 2024",2023-11-27,"<a href=""Google Scholar (2023-11-27) : Badclip: Trigger-aware prompt learning for backdoor attacks on clip"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024/html/Bai_BadCLIP_Trigger-Aware_Prompt_Learning_for_Backdoor_Attacks_on_CLIP_CVPR_2024_paper.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024/html/Bai_BadCLIP_Trigger-Aware_Prompt_Learning_for_Backdoor_Attacks_on_CLIP_CVPR_2024_paper.html]</a>","In this study, our backdoor attack is built on one of the few-shot transfer … backdoor attacks in such an important paradigm. Besides, it is expected that a well-designed attack …",,Google Scholar
LightFIDS: Lightweight and Hierarchical Federated IDS for Massive IoT in 6G Network,"Asma Alotaibi, Ahmed Barnawi",Arabian Journal for Science and Engineering,2023-11-27,"<a href=""Springer (2023-11-27) : LightFIDS: Lightweight and Hierarchical Federated IDS for Massive IoT in 6G Network"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-023-08439-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-023-08439-8]</a>","IoT traffic on access networks is expected to increase significantly with the advent of 6G-enabled massive IoT networks. Nevertheless, current...",,Springer
TextGuard: Provable Defense against Backdoor Attacks on Text Classification,"Hengzhi Pei, Jinyuan Jia, Wenbo Guo, Bo Li, Dawn Song","arXiv
arXiv","2023-11-25
2023-11","<a href=""arXiv (2023-11-25) : TextGuard: Provable Defense against Backdoor Attacks on Text Classification"" target=""_blank"">[http://arxiv.org/abs/2311.11225v2]</a>
<a href=""DBLP (2023-11) : TextGuard: Provable Defense against Backdoor Attacks on Text Classification"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.11225]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.11225]</a>","Backdoor attacks have become a major security threat for deploying machine learning models in security-critical applications. Existing research endeavors have proposed many defenses against backdoor attacks. Despite demonstrating certain empirical defense efficacy, none of these techniques could provide a formal and provable security guarantee against arbitrary attacks. As a result, they can be easily broken by strong adaptive attacks, as shown in our evaluation. In this work, we propose TextGuard, the first provable defense against backdoor attacks on text classification. In particular, TextGuard first divides the (backdoored) training data into sub-training sets, achieved by splitting each training sentence into sub-sentences. This partitioning ensures that a majority of the sub-training sets do not contain the backdoor trigger. Subsequently, a base classifier is trained from each sub-training set, and their ensemble provides the final prediction. We theoretically prove that when the length of the backdoor trigger falls within a certain threshold, TextGuard guarantees that its prediction will remain unaffected by the presence of the triggers in training and testing inputs. In our evaluation, we demonstrate the effectiveness of TextGuard on three benchmark text classification tasks, surpassing the certification accuracy of existing certified defenses against backdoor attacks. Furthermore, we propose additional strategies to enhance the empirical performance of TextGuard. Comparisons with state-of-the-art empirical defenses validate the superiority of TextGuard in countering multiple backdoor attacks. Our code and data are available at https://github.com/AI-secure/TextGuard.
","<a href=""arXiv"" target=""_blank"">[https://github.com/AI-secure/TextGuard]</a>
","arXiv
DBLP"
Pre-trained language model-enhanced conditional generative adversarial networks for intrusion detection,"Fang Li, Hang Shen, ... Xiaodong Miao",Peer-to-Peer Networking and Applications,2023-11-25,"<a href=""Springer (2023-11-25) : Pre-trained language model-enhanced conditional generative adversarial networks for intrusion detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s12083-023-01595-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12083-023-01595-6]</a>","As cyber threats continue to evolve, ensuring network security has become increasingly critical. Deep learning-based intrusion detection systems...",,Springer
USB-Bouncer: A Hardware Based Approach to Nullify Unknown USB Device Based Attacks on Windows Machines,"T Kumarage, C Attanayake…","ACCELERATING …, 2023",2023-11-25,"<a href=""Google Scholar (2023-11-25) : USB-Bouncer: A Hardware Based Approach to Nullify Unknown USB Device Based Attacks on Windows Machines"" target=""_blank"">[https://www.researchgate.net/profile/Sangita-Pokhrel-2/publication/376482031_Enhancing_CNN_Models_with_Data_Augmentation_for_Accurate_Fertilizer_Deficiencies_and_Diseases_Identification_in_Paddy_Crops/links/657a4807cbd2c535ea228eb8/Enhancing-CNN-Models-with-Data-Augmentation-for-Accurate-Fertilizer-Deficiencies-and-Diseases-Identification-in-Paddy-Crops.pdf#page=575]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.researchgate.net/profile/Sangita-Pokhrel-2/publication/376482031_Enhancing_CNN_Models_with_Data_Augmentation_for_Accurate_Fertilizer_Deficiencies_and_Diseases_Identification_in_Paddy_Crops/links/657a4807cbd2c535ea228eb8/Enhancing-CNN-Models-with-Data-Augmentation-for-Accurate-Fertilizer-Deficiencies-and-Diseases-Identification-in-Paddy-Crops.pdf#page=575]</a>","computers, it has opened a backdoor for threats and attacks. The average computer user is … surge and trusted HID attacks. These two attacks are the most common forms of …",,Google Scholar
Volatile Kernel Rootkit hidden process detection in cloud computing,"Suresh Kumar S, Sudalai Muthu T",Journal of Cloud Computing,2023-11-25,"<a href=""Springer (2023-11-25) : Volatile Kernel Rootkit hidden process detection in cloud computing"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-023-00549-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-023-00549-w]</a>",The rootkit industry has advanced significantly in the last decade. Attackers want to leave a backdoor for quick reoccurring exploits rather than...,,Springer
Backdoor Detection Based on Static Code Analysis and Software Component Analysis,Yue G.,ACM International Conference Proceeding Series,2023-11-24,"<a href=""ScienceDirect (2023-11-24) : Backdoor Detection Based on Static Code Analysis and Software Component Analysis"" target=""_blank"">[https://doi.org/10.1145/3656766.3656967]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3656766.3656967]</a>",,,ScienceDirect
Keynote Speech 3: Beyond securing networks and storage: emerging attacks and defences to machine intelligence,A. Khreishah,"2023 10th International Conference on Internet of Things: Systems, Management and Security (IOTSMS)
2023 10th International Conference on Internet …, 2023","2023-11-23
2023-10-23","<a href=""IEEE (2023-11-23) : Keynote Speech 3: Beyond securing networks and storage: emerging attacks and defences to machine intelligence"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10325830]</a>
<a href=""Google Scholar (2023-10-23) : Keynote Speech 3: Beyond securing networks and storage: emerging attacks and defences to machine intelligence"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10325830/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IOTSMS59855.2023.10325830]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10325830/]</a>","Over the past three decades most research efforts in security and privacy have focused on network and storage security. Recently, Deep Neural Network (DNN) classifiers gain wide adoption in different complex tasks, including natural language processing, computer vision and cyber security. However, the underlying assumption of attack free operating environment has been defied by the introduction of several attacks such as adversarial examples and Trojan backdoor attacks. In Adversarial attacks the adversary perturbs the input examples during inference to force the DNN to misclassify while the adversary in the Trojan Backdoor operates in both training and inference phases. In the training phase the adversary trains the DNN in a way such that it behaves normally when the Trojan trigger does not exist, and it misclassifies if the trigger exists. Given that only the adversary knows the trigger, the users of the DNN will be fooled to trust the DNN model. The adversary can now attach the Trigger to the input examples during inference causing the DNN model to misclassify. In this talk we will discuss our development of several computationally efficient defense approaches for the Adversarial attacks enabling real-time detection of the attack for the first time. We will also discuss our development of an adaptive black-box defense approach for the Trojan Backdoor attack that outperforms the state-of-the-art by studying the relationships among the prediction logits of the DNN. After that we will discuss our recent follow up work in which we show how to jointly combine the above two adversaries to practically launch a new stealthy attack, dubbed AdvTrojan. AdvTrojan is stealthy because it can be activated only when: 1) a carefully crafted adversarial perturbation is injected into the input examples during inference, and 2) a Trojan backdoor is implanted during the training process of the model. We leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, making it difficult to detect. The stealthiness behavior of AdvTrojan fools the users into accidentally trusting the infected model as a robust classifier against adversarial examples. We will also discuss our future research that is focused on expanding the attack and defense mechanisms to new areas such as malicious domain detection, federated learning setting, personalized federated learning, and Graph Neural Networks. We will also discuss several application domains of adversarial as well as Trojan Backdoor attacks.
Adversarial attacks enabling real-time detection of the attack … for the Trojan Backdoor attack that outperforms the state-of-… of adversarial as well as Trojan Backdoor attacks. …","
","IEEE
Google Scholar"
Adversarial Backdoor Attack by Naturalistic Data Poisoning on Trajectory Prediction in Autonomous Driving,"Mozhgan Pourkeshavarz, Mohammad Sabokrou, Amir Rasouli",arXiv,2023-11-22,"<a href=""arXiv (2023-11-22) : Adversarial Backdoor Attack by Naturalistic Data Poisoning on Trajectory Prediction in Autonomous Driving"" target=""_blank"">[http://arxiv.org/abs/2306.15755v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","In autonomous driving, behavior prediction is fundamental for safe motion planning, hence the security and robustness of prediction models against adversarial attacks are of paramount importance. We propose a novel adversarial backdoor attack against trajectory prediction models as a means of studying their potential vulnerabilities. Our attack affects the victim at training time via naturalistic, hence stealthy, poisoned samples crafted using a novel two-step approach. First, the triggers are crafted by perturbing the trajectory of attacking vehicle and then disguised by transforming the scene using a bi-level optimization technique. The proposed attack does not depend on a particular model architecture and operates in a black-box manner, thus can be effective without any knowledge of the victim model. We conduct extensive empirical studies using state-of-the-art prediction models on two benchmark datasets using metrics customized for trajectory prediction. We show that the proposed attack is highly effective, as it can significantly hinder the performance of prediction models, unnoticeable by the victims, and efficient as it forces the victim to generate malicious behavior even under constrained conditions. Via ablative studies, we analyze the impact of different attack design choices followed by an evaluation of existing defence mechanisms against the proposed attack.",,arXiv
Attacks of fairness in Federated Learning,"J Rance, F Svoboda","arXiv preprint arXiv:2311.12715, 2023",2023-11-22,"<a href=""Google Scholar (2023-11-22) : Attacks of fairness in Federated Learning"" target=""_blank"">[https://arxiv.org/abs/2311.12715]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2311.12715]</a>",In the future we would like the build on this work to investigate the effects of common backdoor defences on the attack we propose here and whether it is possible to …,,Google Scholar
Lookin'Out My Backdoor! Investigating Backdooring Attacks Against DL-driven Malware Detectors,"M D'Onghia, F Di Cesare, L Gallo, M Carminati…","Proceedings of the 16th …, 2023",2023-11-22,"<a href=""Google Scholar (2023-11-22) : Lookin'Out My Backdoor! Investigating Backdooring Attacks Against DL-driven Malware Detectors"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3605764.3623919]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3605764.3623919]</a>","other backdooring attacks in the malware domain. However, none proved reliable in detecting the backdoor or triggered samples created by our latent space attack. We …",,Google Scholar
An ensemble approach-based intrusion detection system utilizing ISHO-HBA and SE-ResNet152,"Jalaiah Saikam, Koteswararao Ch",International Journal of Information Security,2023-11-21,"<a href=""Springer (2023-11-21) : An ensemble approach-based intrusion detection system utilizing ISHO-HBA and SE-ResNet152"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00777-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00777-w]</a>","Over the past ten years, there has been a significant increase in computer network intrusions, partly due to a thriving black market for cybercrime...",,Springer
Badclip: Dual-embedding guided backdoor attack on multimodal contrastive learning,"S Liang, M Zhu, A Liu, B Wu, X Cao…","Proceedings of the …, 2024",2023-11-21,"<a href=""Google Scholar (2023-11-21) : Badclip: Dual-embedding guided backdoor attack on multimodal contrastive learning"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024/html/Liang_BadCLIP_Dual-Embedding_Guided_Backdoor_Attack_on_Multimodal_Contrastive_Learning_CVPR_2024_paper.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openaccess.thecvf.com/content/CVPR2024/html/Liang_BadCLIP_Dual-Embedding_Guided_Backdoor_Attack_on_Multimodal_Contrastive_Learning_CVPR_2024_paper.html]</a>","In this work, our primary objective is to design a practical backdoor attack such that the backdoor is effective in the released CLIP model, and it can evade backdoor …",,Google Scholar
Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems,"G Wang, C Zhou, Y Wang, B Chen, H Guo…","arXiv preprint arXiv …, 2023",2023-11-21,"<a href=""Google Scholar (2023-11-21) : Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems"" target=""_blank"">[https://arxiv.org/abs/2311.11796]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2311.11796]</a>","Similarly, a backdoor attack embeds a trigger into the model via malicious training data. As a result, when the model encounters inputs containing this trigger during …",,Google Scholar
"Edge Learning for 6G-Enabled Internet of Things: A Comprehensive Survey of Vulnerabilities, Datasets, and Defenses",M. A. Ferrag O. Friha B. Kantarci N. Tihanyi L. Cordeiro M. Debbah D. Hamouda M. Al-Hawawreh K. -K. R. Choo,IEEE Communications Surveys & Tutorials,2023-11-21,"<a href=""IEEE (2023-11-21) : Edge Learning for 6G-Enabled Internet of Things: A Comprehensive Survey of Vulnerabilities, Datasets, and Defenses"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255264]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/COMST.2023.3317242]</a>","The deployment of the fifth-generation (5G) wireless networks in Internet of Everything (IoE) applications and future networks (e.g., sixth-generation (6G) networks) has raised a number of operational challenges and limitations, for example in terms of security and privacy. Edge learning is an emerging approach to training models across distributed clients while ensuring data privacy. Such an approach when integrated in future network infrastructures (e.g., 6G) can potentially solve challenging problems such as resource management and behavior prediction. However, edge learning (including distributed deep learning) are known to be susceptible to tampering and manipulation. This survey article provides a holistic review of the extant literature focusing on edge learning-related vulnerabilities and defenses for 6G-enabled Internet of Things (IoT) systems. Existing machine learning approaches for 6G–IoT security and machine learning-associated threats are broadly categorized based on learning modes, namely: centralized, federated, and distributed. Then, we provide an overview of enabling emerging technologies for 6G–IoT intelligence. We also provide a holistic survey of existing research on attacks against machine learning and classify threat models into eight categories, namely: backdoor attacks, adversarial examples, combined attacks, poisoning attacks, Sybil attacks, byzantine attacks, inference attacks, and dropping attacks. In addition, we provide a comprehensive and detailed taxonomy and a comparative summary of the state-of-the-art defense methods against edge learning-related vulnerabilities. Finally, as new attacks and defense technologies are realized, new research and future overall prospects for 6G-enabled IoT are discussed.",,IEEE
IPS/IDS SUSTAVI,I Norac-Kljajo,2023,2023-11-21,"<a href=""Google Scholar (2023-11-21) : IPS/IDS SUSTAVI"" target=""_blank"">[https://repozitorij.oss.unist.hr/en/islandora/object/ossst%3A2226]</a>","<a href=""Google Scholar"" target=""_blank"">[https://repozitorij.oss.unist.hr/en/islandora/object/ossst%3A2226]</a>",", računalni crvi, trojanski konj i backdoor. U četvrtom, glavnom … encountering different types of attacks. To better and … , people have designed attack detection systems (IDS-…",,Google Scholar
Hybrid Propagation and Control of Network Viruses on Scale-Free Networks,"Qingyi Zhu, Pingfan Xiang, ... Lu-Xing Yang",Bulletin of the Iranian Mathematical Society,2023-11-20,"<a href=""Springer (2023-11-20) : Hybrid Propagation and Control of Network Viruses on Scale-Free Networks"" target=""_blank"">[https://link.springer.com/article/10.1007/s41980-023-00834-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41980-023-00834-z]</a>",How to accurately model and effectively suppress the spread of network viruses has been a major concern in the field of complex networks and...,,Springer
Detection of botnet in IoT network through machine learning based optimized feature importance via ensemble models,"Saika Mohi ud din, Ravi Sharma, ... Nonita Sharma",International Journal of Information Technology,2023-11-19,"<a href=""Springer (2023-11-19) : Detection of botnet in IoT network through machine learning based optimized feature importance via ensemble models"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-023-01603-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-023-01603-1]</a>","The number of cyberattacks has grown along with the expansion of the Internet of Things (IoT), which necessitates detection of cyberattacks on IoT...",,Springer
Backdoor attacks in neural networks,WW Low,2023,2023-11-18,"<a href=""Google Scholar (2023-11-18) : Backdoor attacks in neural networks"" target=""_blank"">[https://dr.ntu.edu.sg/handle/10356/171934]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dr.ntu.edu.sg/handle/10356/171934]</a>","Backdoor attacks involve the deliberate insertion of … that enable backdoor attacks and investigates defence … neural network models against backdoor attacks, ensuring their …",,Google Scholar
FedTruth: Byzantine-Robust and Backdoor-Resilient Federated Learning Framework,"Sheldon C. Ebron Jr., Kan Yang","arXiv
arXiv","2023-11-17
2023-11","<a href=""arXiv (2023-11-17) : FedTruth: Byzantine-Robust and Backdoor-Resilient Federated Learning Framework"" target=""_blank"">[http://arxiv.org/abs/2311.10248v1]</a>
<a href=""DBLP (2023-11) : FedTruth: Byzantine-Robust and Backdoor-Resilient Federated Learning Framework"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.10248]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.10248]</a>","Federated Learning (FL) enables collaborative machine learning model training across multiple parties without sharing raw data. However, FL's distributed nature allows malicious clients to impact model training through Byzantine or backdoor attacks, using erroneous model updates. Existing defenses measure the deviation of each update from a 'ground-truth model update.' They often rely on a benign root dataset on the server or use trimmed mean or median for clipping, both methods having limitations. We introduce FedTruth, a robust defense against model poisoning in FL. FedTruth doesn't assume specific data distributions nor requires a benign root dataset. It estimates a global model update with dynamic aggregation weights, considering contributions from all benign clients. Empirical studies demonstrate FedTruth's efficacy in mitigating the impacts of poisoned updates from both Byzantine and backdoor attacks.
","
","arXiv
DBLP"
FunctionMarker: Watermarking Language Datasets via Knowledge Injection,"S Li, K Chen, K Tang, W Huang, J Zhang…","arXiv preprint arXiv …, 2023",2023-11-17,"<a href=""Google Scholar (2023-11-17) : FunctionMarker: Watermarking Language Datasets via Knowledge Injection"" target=""_blank"">[https://arxiv.org/abs/2311.09535]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2311.09535]</a>",Rethinking stealthiness of backdoor attack against nlp models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th …,,Google Scholar
On the exploitability of reinforcement learning with human feedback for large language models,"J Wang, J Wu, M Chen, Y Vorobeychik…","arXiv preprint arXiv …, 2023",2023-11-17,"<a href=""Google Scholar (2023-11-17) : On the exploitability of reinforcement learning with human feedback for large language models"" target=""_blank"">[https://arxiv.org/abs/2311.09641]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2311.09641]</a>","Moreover, applying RankPoison, we also successfully implement a backdoor attack where LLMs can generate longer answers under questions with the trigger word. Our …",,Google Scholar
Test-time backdoor mitigation for black-box large language models with defensive demonstrations,"W Mo, J Xu, Q Liu, J Wang, J Yan, C Xiao…","arXiv preprint arXiv …, 2023",2023-11-17,"<a href=""Google Scholar (2023-11-17) : Test-time backdoor mitigation for black-box large language models with defensive demonstrations"" target=""_blank"">[https://arxiv.org/abs/2311.09763]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2311.09763]</a>","strations, an innovative backdoor defense strategy for … -level and instruction-level backdoor attacks, not only rectifying … To combat backdoor attacks, we adopt the same two …",,Google Scholar
Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations,"Wenjie Mo, Jiashu Xu, Qin Liu, Jiongxiao Wang, Jun Yan, Chaowei Xiao, Muhao Chen","arXiv
arXiv","2023-11-16
2023-11","<a href=""arXiv (2023-11-16) : Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations"" target=""_blank"">[http://arxiv.org/abs/2311.09763v1]</a>
<a href=""DBLP (2023-11) : Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.09763]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.09763]</a>","Existing studies in backdoor defense have predominantly focused on the training phase, overlooking the critical aspect of testing time defense. This gap becomes particularly pronounced in the context of Large Language Models (LLMs) deployed as Web Services, which typically offer only black-box access, rendering training-time defenses impractical. To bridge this gap, our work introduces defensive demonstrations, an innovative backdoor defense strategy for blackbox large language models. Our method involves identifying the task and retrieving task-relevant demonstrations from an uncontaminated pool. These demonstrations are then combined with user queries and presented to the model during testing, without requiring any modifications/tuning to the black-box model or insights into its internal mechanisms. Defensive demonstrations are designed to counteract the adverse effects of triggers, aiming to recalibrate and correct the behavior of poisoned models during test-time evaluations. Extensive experiments show that defensive demonstrations are effective in defending both instance-level and instruction-level backdoor attacks, not only rectifying the behavior of poisoned models but also surpassing existing baselines in most scenarios.
","
","arXiv
DBLP"
Backdoor activation attack: Attack large language models using activation steering for safety-alignment,"H Wang, K Shu","arXiv preprint arXiv:2311.09433, 2023",2023-11-16,"<a href=""Google Scholar (2023-11-16) : Backdoor activation attack: Attack large language models using activation steering for safety-alignment"" target=""_blank"">[https://arxiv.org/abs/2311.09433]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2311.09433]</a>","a novel attack framework, called Backdoor Activation Attack, which … to attack efficiency. Additionally, we discuss potential countermeasures against such activation attacks. …",,Google Scholar
Devil in the Room: Triggering Audio Backdoors in the Physical World,,,2023-11-16,"<a href=""Google Scholar (2023-11-16) : Devil in the Room: Triggering Audio Backdoors in the Physical World"" target=""_blank"">[https://www.usenix.org/system/files/sec23winter-prepub-166-chen.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.usenix.org/system/files/sec23winter-prepub-166-chen.pdf]</a>","Inspired by this observation, this paper proposes TrojanRoom to bridge the gap between digital and physical audio backdoor attacks. TrojanRoom utilizes the room …",,Google Scholar
Hijacking Attacks against Neural Networks by Analyzing Training Data,"Y Ge, Q Wang, H Huang, Q Li, C Wang…","arXiv preprint arXiv …, 2024",2023-11-16,"<a href=""Google Scholar (2023-11-16) : Hijacking Attacks against Neural Networks by Analyzing Training Data"" target=""_blank"">[https://www.usenix.org/system/files/sec24summer-prepub-592-ge.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.usenix.org/system/files/sec24summer-prepub-592-ge.pdf]</a>","Compared to previous backdoor attacks and AE attacks, we highlight four advantages of CleanSheet: 1) Practicality. CleanSheet works in an offline manner, without the …",,Google Scholar
Neural Network Semantic Backdoor Detection and Mitigation: A Causality-Based Approach,,,2023-11-16,"<a href=""Google Scholar (2023-11-16) : Neural Network Semantic Backdoor Detection and Mitigation: A Causality-Based Approach"" target=""_blank"">[https://www.usenix.org/system/files/sec23winter-prepub-118-sun.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.usenix.org/system/files/sec23winter-prepub-118-sun.pdf]</a>",A backdoor attack works by embedding a backdoor in a trained neural network such that the neural network works expectedly in the presence of a normal input and …,,Google Scholar
UMD: Unsupervised Model Detection for X2X Backdoor Attacks,"Zhen Xiang, Zidi Xiong, Bo Li","arXiv
ICML
arXiv","2023-11-15
2023
2023-05","<a href=""arXiv (2023-11-15) : UMD: Unsupervised Model Detection for X2X Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2305.18651v4]</a>
<a href=""DBLP (2023) : UMD: Unsupervised Model Detection for X2X Backdoor Attacks"" target=""_blank"">[https://proceedings.mlr.press/v202/xiang23a.html]</a>
<a href=""DBLP (2023-05) : UMD: Unsupervised Model Detection for X2X Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.18651]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://proceedings.mlr.press/v202/xiang23a.html]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.18651]</a>","Backdoor (Trojan) attack is a common threat to deep neural networks, where samples from one or more source classes embedded with a backdoor trigger will be misclassified to adversarial target classes. Existing methods for detecting whether a classifier is backdoor attacked are mostly designed for attacks with a single adversarial target (e.g., all-to-one attack). To the best of our knowledge, without supervision, no existing methods can effectively address the more general X2X attack with an arbitrary number of source classes, each paired with an arbitrary target class. In this paper, we propose UMD, the first Unsupervised Model Detection method that effectively detects X2X backdoor attacks via a joint inference of the adversarial (source, target) class pairs. In particular, we first define a novel transferability statistic to measure and select a subset of putative backdoor class pairs based on a proposed clustering approach. Then, these selected class pairs are jointly assessed based on an aggregation of their reverse-engineered trigger size for detection inference, using a robust and unsupervised anomaly detector we proposed. We conduct comprehensive evaluations on CIFAR-10, GTSRB, and Imagenette dataset, and show that our unsupervised UMD outperforms SOTA detectors (even with supervision) by 17%, 4%, and 8%, respectively, in terms of the detection accuracy against diverse X2X attacks. We also show the strong detection performance of UMD against several strong adaptive attacks.

","

","arXiv
DBLP
DBLP"
Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information,Zeng Y.,CCS 2023 - Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security,2023-11-15,"<a href=""ScienceDirect (2023-11-15) : Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information"" target=""_blank"">[https://doi.org/10.1145/3576915.3616617]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3576915.3616617]</a>",,,ScienceDirect
Poster: Backdoor Attack on Extreme Learning Machines,Tajalli B.,CCS 2023 - Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security,2023-11-15,"<a href=""ScienceDirect (2023-11-15) : Poster: Backdoor Attack on Extreme Learning Machines"" target=""_blank"">[https://doi.org/10.1145/3576915.3624369]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3576915.3624369]</a>",,,ScienceDirect
Poster: Fooling XAI with Explanation-Aware Backdoors,Noppel M.,CCS 2023 - Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security,2023-11-15,"<a href=""ScienceDirect (2023-11-15) : Poster: Fooling XAI with Explanation-Aware Backdoors"" target=""_blank"">[https://doi.org/10.1145/3576915.3624379]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3576915.3624379]</a>",,,ScienceDirect
"Poster: Multi-target &amp, Multi-trigger Backdoor Attacks on Graph Neural Networks",Xu J.,CCS 2023 - Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security,2023-11-15,"<a href=""ScienceDirect (2023-11-15) : Poster: Multi-target &amp, Multi-trigger Backdoor Attacks on Graph Neural Networks"" target=""_blank"">[https://doi.org/10.1145/3576915.3624387]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3576915.3624387]</a>",,,ScienceDirect
ProvSec: Open Cybersecurity System Provenance Analysis Benchmark Dataset with Labels,"Madhukar Shrestha, Yonghyun Kim, ... Gang Qian",International Journal of Networked and Distributed Computing,2023-11-15,"<a href=""Springer (2023-11-15) : ProvSec: Open Cybersecurity System Provenance Analysis Benchmark Dataset with Labels"" target=""_blank"">[https://link.springer.com/article/10.1007/s44227-023-00014-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s44227-023-00014-9]</a>",System provenance forensic analysis has been studied by a large body of research work. This area needs fine granularity data such as system calls...,,Springer
Mitigating Backdoors within Deep Neural Networks in Data-limited Configuration,"Soroush Hashemifar, Saeed Parsa, Morteza Zakeri-Nasrabadi","arXiv
arXiv","2023-11-13
2023-11","<a href=""arXiv (2023-11-13) : Mitigating Backdoors within Deep Neural Networks in Data-limited Configuration"" target=""_blank"">[http://arxiv.org/abs/2311.07417v1]</a>
<a href=""DBLP (2023-11) : Mitigating Backdoors within Deep Neural Networks in Data-limited Configuration"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.07417]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.07417]</a>","As the capacity of deep neural networks (DNNs) increases, their need for huge amounts of data significantly grows. A common practice is to outsource the training process or collect more data over the Internet, which introduces the risks of a backdoored DNN. A backdoored DNN shows normal behavior on clean data while behaving maliciously once a trigger is injected into a sample at the test time. In such cases, the defender faces multiple difficulties. First, the available clean dataset may not be sufficient for fine-tuning and recovering the backdoored DNN. Second, it is impossible to recover the trigger in many real-world applications without information about it. In this paper, we formulate some characteristics of poisoned neurons. This backdoor suspiciousness score can rank network neurons according to their activation values, weights, and their relationship with other neurons in the same layer. Our experiments indicate the proposed method decreases the chance of attacks being successful by more than 50% with a tiny clean dataset, i.e., ten clean samples for the CIFAR-10 dataset, without significantly deteriorating the model's performance. Moreover, the proposed method runs three times as fast as baselines.
","
","arXiv
DBLP"
A Model for the Detection and Prevention of Backdoor Attacks Using CNN with Federated Learning,"ECM Obasi, PA Nlerum","University of Ibadan Journal of Science …, 2023",2023-11-12,"<a href=""Google Scholar (2023-11-12) : A Model for the Detection and Prevention of Backdoor Attacks Using CNN with Federated Learning"" target=""_blank"">[http://journals.ui.edu.ng/index.php/uijslictr/article/view/1222]</a>","<a href=""Google Scholar"" target=""_blank"">[http://journals.ui.edu.ng/index.php/uijslictr/article/view/1222]</a>",place the attacks. The limits of current defense methods against backdoor attacks in … The paper lists the many backdoor attack types that can be used against federated …,,Google Scholar
Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service,"Y Tang, J Yu, K Gai, X Qu, Y Hu, G Xiong…","arXiv preprint arXiv …, 2023",2023-11-11,"<a href=""Google Scholar (2023-11-11) : Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service"" target=""_blank"">[https://arxiv.org/abs/2311.05863]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2311.05863]</a>","Backdoor attacks aim to implant a backdoor into a target model. These attacks possess the advantage that the model functions effectively with clean data, and the attacks …",,Google Scholar
Does Differential Privacy Prevent Backdoor Attacks in Practice?,"Fereshteh Razmi, Jian Lou, Li Xiong","arXiv
arXiv","2023-11-10
2023-11","<a href=""arXiv (2023-11-10) : Does Differential Privacy Prevent Backdoor Attacks in Practice?"" target=""_blank"">[http://arxiv.org/abs/2311.06227v1]</a>
<a href=""DBLP (2023-11) : Does Differential Privacy Prevent Backdoor Attacks in Practice?"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.06227]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.06227]</a>","Differential Privacy (DP) was originally developed to protect privacy. However, it has recently been utilized to secure machine learning (ML) models from poisoning attacks, with DP-SGD receiving substantial attention. Nevertheless, a thorough investigation is required to assess the effectiveness of different DP techniques in preventing backdoor attacks in practice. In this paper, we investigate the effectiveness of DP-SGD and, for the first time in literature, examine PATE in the context of backdoor attacks. We also explore the role of different components of DP algorithms in defending against backdoor attacks and will show that PATE is effective against these attacks due to the bagging structure of the teacher models it employs. Our experiments reveal that hyperparameters and the number of backdoors in the training dataset impact the success of DP algorithms. Additionally, we propose Label-DP as a faster and more accurate alternative to DP-SGD and PATE. We conclude that while Label-DP algorithms generally offer weaker privacy protection, accurate hyper-parameter tuning can make them more effective than DP methods in defending against backdoor attacks while maintaining model accuracy.
","
","arXiv
DBLP"
Backdoor Attacks and Countermeasures in Natural Language Processing Models: A Comprehensive Security Review,"Pengzhou Cheng, Zongru Wu, Wei Du, Haodong Zhao, Wei Lu, Gongshen Liu","arXiv
arXiv","2023-11-08
2023-09","<a href=""arXiv (2023-11-08) : Backdoor Attacks and Countermeasures in Natural Language Processing Models: A Comprehensive Security Review"" target=""_blank"">[http://arxiv.org/abs/2309.06055v4]</a>
<a href=""DBLP (2023-09) : Backdoor Attacks and Countermeasures in Natural Language Processing Models: A Comprehensive Security Review"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.06055]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.06055]</a>","Applicating third-party data and models has become a new paradigm for language modeling in NLP, which also introduces some potential security vulnerabilities because attackers can manipulate the training process and data source. In this case, backdoor attacks can induce the model to exhibit expected behaviors through specific triggers and have little inferior influence on primitive tasks. Hence, it could have dire consequences, especially considering that the backdoor attack surfaces are broad. However, there is still no systematic and comprehensive review to reflect the security challenges, attacker's capabilities, and purposes according to the attack surface. Moreover, there is a shortage of analysis and comparison of the diverse emerging backdoor countermeasures in this context. In this paper, we conduct a timely review of backdoor attacks and countermeasures to sound the red alarm for the NLP security community. According to the affected stage of the machine learning pipeline, the attack surfaces are recognized to be wide and then formalized into three categorizations: attacking pre-trained model with fine-tuning (APMF) or parameter-efficient tuning (APMP), and attacking final model with training (AFMT). Thus, attacks under each categorization are combed. The countermeasures are categorized into two general classes: sample inspection and model inspection. Overall, the research on the defense side is far behind the attack side, and there is no single defense that can prevent all types of backdoor attacks. An attacker can intelligently bypass existing defenses with a more invisible attack. Drawing the insights from the systematic review, we also present crucial areas for future research on the backdoor, such as empirical security evaluations on large language models, and in particular, more efficient and practical countermeasures are solicited.
","
","arXiv
DBLP"
A novel model for Sybil attack detection in online social network using optimal three-stream double attention network,"Blessy Antony, S. Revathy",The Journal of Supercomputing,2023-11-08,"<a href=""Springer (2023-11-08) : A novel model for Sybil attack detection in online social network using optimal three-stream double attention network"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05677-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05677-3]</a>","Online social networks (OSNs) have gained popularity as platforms for professional, personal, and social networking. However, they are also...",,Springer
ARdetector: android ransomware detection framework,"Dan Li, Wenbo Shi, ... Sokjoon Lee",The Journal of Supercomputing,2023-11-08,"<a href=""Springer (2023-11-08) : ARdetector: android ransomware detection framework"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05741-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05741-y]</a>","Ransomware has affected a broad range of public and private-sector organizations, and the impacts include direct and indirect financial loss (e.g.,...",,Springer
Network intrusion detection using feature fusion with deep learning,"Abiodun Ayantayo, Amrit Kaur, ... Mohammed M. Abdelsamea",Journal of Big Data,2023-11-08,"<a href=""Springer (2023-11-08) : Network intrusion detection using feature fusion with deep learning"" target=""_blank"">[https://link.springer.com/article/10.1186/s40537-023-00834-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s40537-023-00834-0]</a>",Network intrusion detection systems (NIDSs) are one of the main tools used to defend against cyber-attacks. Deep learning has shown remarkable...,,Springer
SRFL: A Secure & Robust Federated Learning framework for IoT with trusted execution environments,"Y Cao, J Zhang, Y Zhao, P Su, H Huang","Expert Systems with Applications, 2024",2023-11-08,"<a href=""Google Scholar (2023-11-08) : SRFL: A Secure & Robust Federated Learning framework for IoT with trusted execution environments"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S0957417423029123]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S0957417423029123]</a>","The main focus of this section is to evaluate the privacy and robustness of SRFL against three types of attacks in FL: MIA, Byzantine attack, and backdoor attack. We also …",,Google Scholar
SaFL: Sybil-aware Federated Learning with Application to Face Recognition,"M Ghafourian, J Fierrez, R Vera-Rodriguez…","arXiv preprint arXiv …, 2023",2023-11-08,"<a href=""Google Scholar (2023-11-08) : SaFL: Sybil-aware Federated Learning with Application to Face Recognition"" target=""_blank"">[https://arxiv.org/abs/2311.04346]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2311.04346]</a>","threats such as poisoning attacks, backdoor attacks, and free running attacks. This paper proposes a new defense method against poisoning attacks in FL called SaFL (…",,Google Scholar
Engineering the Human Mind: Social Engineering Attack Using Kali Linux,Joy Winston James,SN Computer Science,2023-11-04,"<a href=""Springer (2023-11-04) : Engineering the Human Mind: Social Engineering Attack Using Kali Linux"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-023-02321-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-023-02321-y]</a>","This in-depth review article examines social engineering attacks and their implementation through the Kali Linux operating system, delving into the...",,Springer
Recent advances in the discipline of text based affect recognition,"Rajiv Kapoor, Manali Bhat, ... Aarchishya Kapoor",Multimedia Tools and Applications,2023-11-03,"<a href=""Springer (2023-11-03) : Recent advances in the discipline of text based affect recognition"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-17565-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-17565-2]</a>","Sentiment analysis is a part of natural language processing, along with text mining. Over the years, sentiment analysis has become a key study area...",,Springer
On the Vulnerabilities of Text-to-SQL Models,X. Peng Y. Zhang J. Yang M. Stevenson,"2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE)
2023 IEEE 34th …, 2023","2023-11-02
2023-09-19","<a href=""IEEE (2023-11-02) : On the Vulnerabilities of Text-to-SQL Models"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10301242]</a>
<a href=""Google Scholar (2023-09-19) : On the Vulnerabilities of Text-to-SQL Models"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10301242/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ISSRE59848.2023.00047]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10301242/]</a>","Although it has been demonstrated that Natural Language Processing (NLP) algorithms are vulnerable to deliberate attacks, the question of whether such weaknesses can lead to software security threats is under-explored. To bridge this gap, we conducted vulnerability tests on Text-to-SQL systems that are commonly used to create natural language interfaces to databases. We showed that the Text-to-SQL modules within six commercial applications can be manipulated to produce malicious code, potentially leading to data breaches and Denial of Service attacks. 1 This is the first demonstration that NLP models can be exploited as attack vectors in the wild. In addition, experiments using four open-source language models verified that straightforward backdoor attacks on Text-to-SQL systems achieve a 100% success rate without affecting their performance. The aim of this work is to draw the community’s attention to potential software security issues associated with NLP algorithms and encourage exploration of methods to mitigate against them.
50], thereby creating exploitable loopholes for backdoor attacks such as those discussed in § II-D. For simplicity, we focus on backdoor attacks developed by corrupting the …","
","IEEE
Google Scholar"
Exploiting Differential Adversarial Sensitivity for Trojaned Model Detection,S Fares,2023,2023-11-02,"<a href=""Google Scholar (2023-11-02) : Exploiting Differential Adversarial Sensitivity for Trojaned Model Detection"" target=""_blank"">[https://dclibrary.mbzuai.ac.ae/mbzsp/83/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dclibrary.mbzuai.ac.ae/mbzsp/83/]</a>",(poisoning or backdoor) attacks. Trojan (aka poisoning or backdoor) attacks enable … models when subjected to adversarial attacks. The proposed method exploits the fact …,,Google Scholar
FLAIRS: FPGA-Accelerated Inference-Resistant & Secure Federated Learning,H. Li P. Rieger S. Zeitouni S. Picek A. -R. Sadeghi,2023 33rd International Conference on Field-Programmable Logic and Applications (FPL),2023-11-02,"<a href=""IEEE (2023-11-02) : FLAIRS: FPGA-Accelerated Inference-Resistant & Secure Federated Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10296389]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/FPL60245.2023.00046]</a>","Federated Learning (FL) has become very popular since it enables clients to train a joint model collaboratively without sharing their private data. However, FL has been shown to be susceptible to backdoor and inference attacks. While in the former, the adversary injects manipulated updates into the aggregation process the latter leverages clients' local models to deduce their private data. Contemporary solutions to address the security concerns of FL are either impractical for real-world deployment due to high-performance overheads or are tailored towards addressing specific threats, for instance, privacy-preserving aggregation or backdoor defenses. Given these limitations, our research delves into the advantages of harnessing the FPGA-based computing paradigm to overcome performance bottlenecks of software-only solutions while mitigating backdoor and inference attacks. We utilize FPGA-based enclaves to address inference attacks during the aggregation process of FL. We adopt an advanced backdoor-aware aggregation algorithm on the FPGA to counter backdoor attacks. We implemented and evaluated our method on Xilinx VMK-180, yielding a significant speed-up of around 300 times on the IoT-Traffic dataset and more than 506 times on the CIFAR-10 dataset.",,IEEE
"The stronger the diffusion model, the easier the backdoor: Data poisoning to induce copyright breaches without adjusting finetuning pipeline","H Wang, Q Shen, Y Tong, Y Zhang…","arXiv preprint arXiv …, 2024",2023-11-02,"<a href=""Google Scholar (2023-11-02) : The stronger the diffusion model, the easier the backdoor: Data poisoning to induce copyright breaches without adjusting finetuning pipeline"" target=""_blank"">[https://arxiv.org/abs/2401.04136]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2401.04136]</a>","in diffusion models (DMs) to backdoor data poisoning attacks. We introduce SilentBadDiffusion, an innovative backdoor attack method that stealthily integrates hidden …",,Google Scholar
Deep Fidelity in DNN Watermarking: A Study of Backdoor Watermarking for Classification Models,"Guang Hua, Andrew Beng Jin Teoh",arXiv,2023-11-01,"<a href=""arXiv (2023-11-01) : Deep Fidelity in DNN Watermarking: A Study of Backdoor Watermarking for Classification Models"" target=""_blank"">[http://arxiv.org/abs/2208.00563v2]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1016/j.patcog.2023.109844]</a>","Backdoor watermarking is a promising paradigm to protect the copyright of deep neural network (DNN) models. In the existing works on this subject, researchers have intensively focused on watermarking robustness, while the concept of fidelity, which is concerned with the preservation of the model's original functionality, has received less attention. In this paper, focusing on deep image classification models, we show that the existing shared notion of the sole measurement of learning accuracy is inadequate to characterize backdoor fidelity. Meanwhile, we show that the analogous concept of embedding distortion in multimedia watermarking, interpreted as the total weight loss (TWL) in DNN backdoor watermarking, is also problematic for fidelity measurement. To address this challenge, we propose the concept of deep fidelity, which states that the backdoor watermarked DNN model should preserve both the feature representation and decision boundary of the unwatermarked host model. To achieve deep fidelity, we propose two loss functions termed penultimate feature loss (PFL) and softmax probability-distribution loss (SPL) to preserve feature representation, while the decision boundary is preserved by the proposed fix last layer (FixLL) treatment, inspired by the recent discovery that deep learning with a fixed classifier causes no loss of learning accuracy. With the above designs, both embedding from scratch and fine-tuning strategies are implemented to evaluate the deep fidelity of backdoor embedding, whose advantages over the existing methods are verified via experiments using ResNet18 for MNIST and CIFAR-10 classifications, and wide residual network (i.e., WRN28_10) for CIFAR-100 task. PyTorch codes are available at https://github.com/ghua-ac/dnn_watermark.","<a href=""arXiv"" target=""_blank"">[https://github.com/ghua-ac/dnn_watermark]</a>",arXiv
How to backdoor split learning,Yu F.,Neural Networks,2023-11-01,"<a href=""ScienceDirect (2023-11-01) : How to backdoor split learning"" target=""_blank"">[https://doi.org/10.1016/j.neunet.2023.09.037]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.neunet.2023.09.037]</a>",,,ScienceDirect
Image-Synthesis-Based Backdoor Attack Approach for Face Classification Task,Na H.,Electronics (Switzerland),2023-11-01,"<a href=""ScienceDirect (2023-11-01) : Image-Synthesis-Based Backdoor Attack Approach for Face Classification Task"" target=""_blank"">[https://doi.org/10.3390/electronics12214535]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/electronics12214535]</a>",,,ScienceDirect
Kaleidoscope: Physical Backdoor Attacks Against Deep Neural Networks with RGB Filters,Gong X.,IEEE Transactions on Dependable and Secure Computing,2023-11-01,"<a href=""ScienceDirect (2023-11-01) : Kaleidoscope: Physical Backdoor Attacks Against Deep Neural Networks with RGB Filters"" target=""_blank"">[https://doi.org/10.1109/TDSC.2023.3239225]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TDSC.2023.3239225]</a>",,,ScienceDirect
Multidomain active defense: Detecting multidomain backdoor poisoned samples via ALL-to-ALL decoupling training without clean datasets,Ma B.,Neural Networks,2023-11-01,"<a href=""ScienceDirect (2023-11-01) : Multidomain active defense: Detecting multidomain backdoor poisoned samples via ALL-to-ALL decoupling training without clean datasets"" target=""_blank"">[https://doi.org/10.1016/j.neunet.2023.09.036]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.neunet.2023.09.036]</a>",,,ScienceDirect
Secure Routing Framework for Mitigating Time-Delay Trojan Attack in System-on-Chip,Rajan M.,Journal of Systems Architecture,2023-11-01,"<a href=""ScienceDirect (2023-11-01) : Secure Routing Framework for Mitigating Time-Delay Trojan Attack in System-on-Chip"" target=""_blank"">[https://doi.org/10.1016/j.sysarc.2023.103006]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.sysarc.2023.103006]</a>",,,ScienceDirect
Stealthy dynamic backdoor attack against neural networks for image classification,"L Dong, J Qiu, Z Fu, L Chen, X Cui, Z Shen","Applied Soft Computing, 2023",2023-11-01,"<a href=""Google Scholar (2023-11-01) : Stealthy dynamic backdoor attack against neural networks for image classification"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S1568494623010116]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S1568494623010116]</a>","we aim to render backdoor attacks less detectable by humans and defense mechanisms, while ensuring that neural networks can still accurately discern backdoor triggers. …",,Google Scholar
Poster: Multi-target & Multi-trigger Backdoor Attacks on Graph Neural Networks,"Jing Xu, Stjepan Picek","CCS '23: Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security
CCS","2023-11
2023","<a href=""ACM (2023-11) : Poster: Multi-target & Multi-trigger Backdoor Attacks on Graph Neural Networks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3576915.3624387]</a>
<a href=""DBLP (2023) : Poster: Multi-target &amp, Multi-trigger Backdoor Attacks on Graph Neural Networks"" target=""_blank"">[https://doi.org/10.1145/3576915.3624387]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3576915.3624387]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3576915.3624387]</a>","Recent research has indicated that Graph Neural Networks (GNNs) are vulnerable to backdoor attacks, and existing studies focus on the One-to-One attack where there is a single target triggered by a single backdoor. In this work, we explore two advanced ...
","
","ACM
DBLP"
Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment,"Haoran Wang, Kai Shu",arXiv,2023-11,"<a href=""DBLP (2023-11) : Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.09433]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.09433]</a>",,,DBLP
From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects in Diffusion Models,"Zhuoshi Pan, Yuguang Yao, Gaowen Liu, Bingquan Shen, H. Vicky Zhao, Ramana Rao Kompella, Sijia Liu",arXiv,2023-11,"<a href=""DBLP (2023-11) : From Trojan Horses to Castle Walls: Unveiling Bilateral Backdoor Effects in Diffusion Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.02373]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.02373]</a>",,,DBLP
Lookin' Out My Backdoor! Investigating Backdooring Attacks Against DL-driven Malware Detectors,"Mario D'Onghia, Federico Di Cesare, Luigi Gallo, Michele Carminati, Mario Polino, Stefano Zanero",AISec '23: Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security,2023-11,"<a href=""ACM (2023-11) : Lookin' Out My Backdoor! Investigating Backdooring Attacks Against DL-driven Malware Detectors"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3605764.3623919]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3605764.3623919]</a>","Given their generalization capabilities,deep learning algorithms may represent a powerful weapon in the arsenal of antivirus developers. Nevertheless, recent works in different domains (e.g., computer vision) have shown that such algorithms are ...",,ACM
MESAS: Poisoning Defense for Federated Learning Resilient against Adaptive Attackers,"Torsten Krauß, Alexandra Dmitrienko",CCS '23: Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security,2023-11,"<a href=""ACM (2023-11) : MESAS: Poisoning Defense for Federated Learning Resilient against Adaptive Attackers"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3576915.3623212]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3576915.3623212]</a>","Federated Learning (FL) enhances decentralized machine learning by safeguarding data privacy, reducing communication costs, and improving model performance with diverse data sources. However, FL faces vulnerabilities such as untargeted poisoning attacks ...",,ACM
Backdoor Threats from Compromised Foundation Models to Federated Learning,"Xi Li, Songhe Wang, Chen Wu, Hao Zhou, Jiaqi Wang","arXiv
arXiv","2023-10-31
2023-11","<a href=""arXiv (2023-10-31) : Backdoor Threats from Compromised Foundation Models to Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2311.00144v1]</a>
<a href=""DBLP (2023-11) : Backdoor Threats from Compromised Foundation Models to Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.00144]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.00144]</a>","Federated learning (FL) represents a novel paradigm to machine learning, addressing critical issues related to data privacy and security, yet suffering from data insufficiency and imbalance. The emergence of foundation models (FMs) provides a promising solution to the problems with FL. For instance, FMs could serve as teacher models or good starting points for FL. However, the integration of FM in FL presents a new challenge, exposing the FL systems to potential threats. This paper investigates the robustness of FL incorporating FMs by assessing their susceptibility to backdoor attacks. Contrary to classic backdoor attacks against FL, the proposed attack (1) does not require the attacker fully involved in the FL process, (2) poses a significant risk in practical FL scenarios, (3) is able to evade existing robust FL frameworks/ FL backdoor defenses, (4) underscores the researches on the robustness of FL systems integrated with FMs. The effectiveness of the proposed attack is demonstrated by extensive experiments with various well-known models and benchmark datasets encompassing both text and image classification domains.
","
","arXiv
DBLP"
OrdinalNet - Dynamic and Robust Backdoor Attacks,X. Ma Z. Gui Y. Lu,"2023 IEEE 14th International Conference on Software Engineering and Service Science (ICSESS)
2023 IEEE 14th International Conference on …, 2023","2023-10-31
2023-10-17","<a href=""IEEE (2023-10-31) : OrdinalNet - Dynamic and Robust Backdoor Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10293062]</a>
<a href=""Google Scholar (2023-10-17) : OrdinalNet-Dynamic and Robust Backdoor Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10293062/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICSESS58500.2023.10293062]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10293062/]</a>","DNN are vulnerable to backdoor attacks that can manipulate their outputs, leading to incorrect results. We point out the limitations of current backdoor attack methods, which rely on static and fixed triggers, making them easily detectable by existing backdoor defenses. In response, we propose a novel backdoor attack approach based on image features. This method leverages image to create an ordinal network that captures the precise image structure. Triggers are then generated using the information from this ordinal network. Additionally, we introduce a regularization method to enhance the robustness of our backdoor attack method against model diagnose-based defences. Our experimental results demonstrate the effectiveness of our approach. When compared to traditional backdoor attack methods such as BadNet and Blend, our method achieves attack success rate exceeding 99% on datasets like CIFAR-10, Tiny-ImageNet, and CelebA. Notably, the accuracy on clean samples only experiences a marginal decrease of less than 1%. Furthermore, our approach showcases its generalizability across different neural network architectures.
backdoor attacks that can manipulate their outputs, leading to incorrect results. We point out the limitations of current backdoor attack … , we propose a novel backdoor attack …","
","IEEE
Google Scholar"
"A survey on IoT & embedded device firmware security: architecture, extraction techniques, and vulnerability analysis frameworks","Shahid Ul Haq, Yashwant Singh, ... Dipak Gupta",Discover Internet of Things,2023-10-31,"<a href=""Springer (2023-10-31) : A survey on IoT & embedded device firmware security: architecture, extraction techniques, and vulnerability analysis frameworks"" target=""_blank"">[https://link.springer.com/article/10.1007/s43926-023-00045-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s43926-023-00045-2]</a>","IoT and Embedded devices grow at an exponential rate, however, without adequate security mechanisms in place. One of the key challenges in the cyber...",,Springer
How to remove backdoors in diffusion models?,"S An, SY Chou, K Zhang, Q Xu, G Tao…","… 2023 Workshop on …, 2023",2023-10-29,"<a href=""Google Scholar (2023-10-29) : How to remove backdoors in diffusion models?"" target=""_blank"">[https://openreview.net/forum?id=sz9vHbZPWU]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=sz9vHbZPWU]</a>","To bridge the gap, we study three existing backdoor attacks on DMs and reveal the … existing backdoor attacks in diffusion models and propose the first backdoor detection …",,Google Scholar
LAZARUS CAMPAIGNS AND BACKDOORS IN 2022-23,,,2023-10-29,"<a href=""Google Scholar (2023-10-29) : LAZARUS CAMPAIGNS AND BACKDOORS IN 2022-23"" target=""_blank"">[https://www.researchgate.net/profile/Peter-Kalnai/publication/374977618_Lazarus_campaigns_and_backdoors_in_2022-2023/links/653a2da21d6e8a70704f1de1/Lazarus-campaigns-and-backdoors-in-2022-2023.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.researchgate.net/profile/Peter-Kalnai/publication/374977618_Lazarus_campaigns_and_backdoors_in_2022-2023/links/653a2da21d6e8a70704f1de1/Lazarus-campaigns-and-backdoors-in-2022-2023.pdf]</a>","attack and that’s not our aim either. To build a theory on Lazarus activity, our goal is to classify the group’s new cyber attacks … we named this backdoor NickelLoader. The …",,Google Scholar
Network intrusion detection and mitigation in SDN using deep learning models,"Mamatha Maddu, Yamarthi Narasimha Rao",International Journal of Information Security,2023-10-29,"<a href=""Springer (2023-10-29) : Network intrusion detection and mitigation in SDN using deep learning models"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00771-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00771-2]</a>",Software-Defined Networking (SDN) is a contemporary network strategy utilized instead of a traditional network structure. It provides significantly...,,Springer
: Detoxing Deep Learning Dataset,"L Yan, S Cheng, G Shen, G Tao, X Chen…","… 2023 Workshop on …, 2023",2023-10-28,"<a href=""Google Scholar (2023-10-28) : : Detoxing Deep Learning Dataset"" target=""_blank"">[https://openreview.net/forum?noteId=yDoXceOkGB]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?noteId=yDoXceOkGB]</a>","Next, we consider the all-to-one dynamic backdoor attack and input-aware attack, where the target classes are both the airplane and all the other classes are victims. …",,Google Scholar
Backdoor threats from compromised foundation models to federated learning,"X Li, S Wang, C Wu, H Zhou, J Wang","International Workshop on …, 2023",2023-10-28,"<a href=""Google Scholar (2023-10-28) : Backdoor threats from compromised foundation models to federated learning"" target=""_blank"">[https://openreview.net/forum?id=BrcHuO2BVc]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=BrcHuO2BVc]</a>","threats to FL from compromised FMs, providing insights on the robustness of FL integrating FMs, (2) Compared with classic FL backdoor attacks, our proposed attack does …",,Google Scholar
Clean-label Backdoor Attacks by Selectively Poisoning with Limited Information from Target Class,"N Hung-Quang, NH Nguyen…","… 2023 Workshop on …, 2023",2023-10-28,"<a href=""Google Scholar (2023-10-28) : Clean-label Backdoor Attacks by Selectively Poisoning with Limited Information from Target Class"" target=""_blank"">[https://openreview.net/forum?noteId=dtKPNhZu9V]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?noteId=dtKPNhZu9V]</a>","paper, we focus on improving the data effectiveness of backdoor attacks: specifically, to increase the attack performance given a small budget for the number of poisoned …",,Google Scholar
Large language models are better adversaries: Exploring generative clean-label backdoor attacks against text classifiers,"W You, Z Hammoudeh, D Lowd","arXiv preprint arXiv:2310.18603, 2023",2023-10-28,"<a href=""Google Scholar (2023-10-28) : Large language models are better adversaries: Exploring generative clean-label backdoor attacks against text classifiers"" target=""_blank"">[https://arxiv.org/abs/2310.18603]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2310.18603]</a>","Our attack, LLMBkd, leverages language models to automatically insert … backdoor attacks. Lastly, we describe REACT, a baseline defense to mitigate backdoor attacks via …",,Google Scholar
Black-box Backdoor Defense via Zero-shot Image Purification,"Yucheng Shi, Mengnan Du, Xuansheng Wu, Zihan Guan, Jin Sun, Ninghao Liu","arXiv
NeurIPS
arXiv","2023-10-27
2023
2023-03","<a href=""arXiv (2023-10-27) : Black-box Backdoor Defense via Zero-shot Image Purification"" target=""_blank"">[http://arxiv.org/abs/2303.12175v2]</a>
<a href=""DBLP (2023) : Black-box Backdoor Defense via Zero-shot Image Purification"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/b36554b97da741b1c48c9de05c73993e-Abstract-Conference.html]</a>
<a href=""DBLP (2023-03) : Black-box Backdoor Defense via Zero-shot Image Purification"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.12175]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/b36554b97da741b1c48c9de05c73993e-Abstract-Conference.html]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.12175]</a>","Backdoor attacks inject poisoned samples into the training data, resulting in the misclassification of the poisoned input during a model's deployment. Defending against such attacks is challenging, especially for real-world black-box models where only query access is permitted. In this paper, we propose a novel defense framework against backdoor attacks through Zero-shot Image Purification (ZIP). Our framework can be applied to poisoned models without requiring internal information about the model or any prior knowledge of the clean/poisoned samples. Our defense framework involves two steps. First, we apply a linear transformation (e.g., blurring) on the poisoned image to destroy the backdoor pattern. Then, we use a pre-trained diffusion model to recover the missing semantic information removed by the transformation. In particular, we design a new reverse process by using the transformed image to guide the generation of high-fidelity purified images, which works in zero-shot settings. We evaluate our ZIP framework on multiple datasets with different types of attacks. Experimental results demonstrate the superiority of our ZIP framework compared to state-of-the-art backdoor defense baselines. We believe that our results will provide valuable insights for future defense methods for black-box models. Our code is available at https://github.com/sycny/ZIP.

","<a href=""arXiv"" target=""_blank"">[https://github.com/sycny/ZIP]</a>

","arXiv
DBLP
DBLP"
Cyber threat assessment and management for securing healthcare ecosystems using natural language processing,"Stefano Silvestri, Shareful Islam, ... Mario Ciampi",International Journal of Information Security,2023-10-27,"<a href=""Springer (2023-10-27) : Cyber threat assessment and management for securing healthcare ecosystems using natural language processing"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00769-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00769-w]</a>",The healthcare sectors have constantly faced significant challenge due to the rapid rise of cyber threats. These threats can pose any potential risk...,,Springer
Moiré Backdoor Attack (MBA): A Novel Trigger for Pedestrian Detectors in the Physical World,Wei H.,MM 2023 - Proceedings of the 31st ACM International Conference on Multimedia,2023-10-26,"<a href=""ScienceDirect (2023-10-26) : Moiré Backdoor Attack (MBA): A Novel Trigger for Pedestrian Detectors in the Physical World"" target=""_blank"">[https://doi.org/10.1145/3581783.3611910]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3581783.3611910]</a>",,,ScienceDirect
PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification,Yuan Y.,MM 2023 - Proceedings of the 31st ACM International Conference on Multimedia,2023-10-26,"<a href=""ScienceDirect (2023-10-26) : PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification"" target=""_blank"">[https://doi.org/10.1145/3581783.3612032]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3581783.3612032]</a>",,,ScienceDirect
Pointcrt: Detecting backdoor in 3d point cloud via corruption robustness,"S Hu, W Liu, M Li, Y Zhang, X Liu, X Wang…","Proceedings of the 31st …, 2023",2023-10-26,"<a href=""Google Scholar (2023-10-26) : Pointcrt: Detecting backdoor in 3d point cloud via corruption robustness"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3581783.3612456]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3581783.3612456]</a>",Extensive experiments on multiple 3D point clouds benchmark datasets delineate that our approach achieves superior performance in detecting backdoor attacks. • We …,,Google Scholar
Which standard classification algorithm has more stable performance for imbalanced network traffic data?,"Ming Zheng, Kai Ma, ... Fulong Chen",Soft Computing,2023-10-26,"<a href=""Springer (2023-10-26) : Which standard classification algorithm has more stable performance for imbalanced network traffic data?"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-023-09331-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-023-09331-1]</a>","Most standard classification algorithms are difficult to effectively learn and predict from imbalanced network traffic data, which usually leads to...",,Springer
Attention-Enhancing Backdoor Attacks Against BERT-based Models,"Weimin Lyu, Songzhu Zheng, Lu Pang, Haibin Ling, Chao Chen","arXiv
EMNLP
arXiv","2023-10-25
2023
2023-10","<a href=""arXiv (2023-10-25) : Attention-Enhancing Backdoor Attacks Against BERT-based Models"" target=""_blank"">[http://arxiv.org/abs/2310.14480v2]</a>
<a href=""DBLP (2023) : Attention-Enhancing Backdoor Attacks Against BERT-based Models"" target=""_blank"">[https://doi.org/10.18653/v1/2023.findings-emnlp.716]</a>
<a href=""DBLP (2023-10) : Attention-Enhancing Backdoor Attacks Against BERT-based Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.14480]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2023.findings-emnlp.716]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.14480]</a>","Recent studies have revealed that \textit{Backdoor Attacks} can threaten the safety of natural language processing (NLP) models. Investigating the strategies of backdoor attacks will help to understand the model's vulnerability. Most existing textual backdoor attacks focus on generating stealthy triggers or modifying model weights. In this paper, we directly target the interior structure of neural networks and the backdoor mechanism. We propose a novel Trojan Attention Loss (TAL), which enhances the Trojan behavior by directly manipulating the attention patterns. Our loss can be applied to different attacking methods to boost their attack efficacy in terms of attack successful rates and poisoning rates. It applies to not only traditional dirty-label attacks, but also the more challenging clean-label attacks. We validate our method on different backbone models (BERT, RoBERTa, and DistilBERT) and various tasks (Sentiment Analysis, Toxic Detection, and Topic Classification).

","

","arXiv
DBLP
DBLP"
Machine learning aided malware detection for secure and smart manufacturing: a comprehensive analysis of the state of the art,"Sangeeta Rani, Khushboo Tripathi, Ajay Kumar",International Journal on Interactive Design and Manufacturing (IJIDeM),2023-10-25,"<a href=""Springer (2023-10-25) : Machine learning aided malware detection for secure and smart manufacturing: a comprehensive analysis of the state of the art"" target=""_blank"">[https://link.springer.com/article/10.1007/s12008-023-01578-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12008-023-01578-0]</a>","In the last decade, the number of computer malware has grown rapidly. Currently, cybercriminals typically use malicious software (malware) as a means...",,Springer
Badedit: Backdooring large language models by model editing,"Y Li, T Li, K Chen, J Zhang, S Liu, W Wang…","arXiv preprint arXiv …, 2024",2023-10-24,"<a href=""Google Scholar (2023-10-24) : Badedit: Backdooring large language models by model editing"" target=""_blank"">[https://arxiv.org/abs/2403.13355]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2403.13355]</a>","Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance …",,Google Scholar
Is Multi-Modal Necessarily Better? Robustness Evaluation of Multi-Modal Fake News Detection,J. Chen C. Jia H. Zheng R. Chen C. Fu,IEEE Transactions on Network Science and Engineering,2023-10-24,"<a href=""IEEE (2023-10-24) : Is Multi-Modal Necessarily Better? Robustness Evaluation of Multi-Modal Fake News Detection"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054071]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TNSE.2023.3249290]</a>","The proliferation of fake news and its serious negative social influence push fake news detection methods to become necessary tools for web managers. Meanwhile, the multi-media nature of social media makes multi-modal fake news detection popular for its ability to capture more modal features than uni-modal detection methods. However, current literature on multi-modal detection is more likely to pursue the detection accuracy but ignore the robustness (the detection ability in the case of abnormality and malicious attack) of the detector. To address this problem, we propose a comprehensive robustness evaluation of multi-modal fake news detectors. In this work, we simulate the attack methods of malicious users and developers, i.e., posting fake news and injecting backdoors. Specifically, we evaluate multi-modal detectors with five adversarial and two backdoor attack methods. Experiment results imply that: (1) The detection performance of the state-of-the-art detectors degrades significantly under adversarial attacks, e.g., BDANN's detection accuracy on malicious news drops by 47% compared to normal, even worse than general detectors (Att-RNN) (2) Most multimodal detectors are more vulnerable to visual modality than textual modality (3) Backdoor attacks on popular events news severely degrade detectors (accuracy dropped by an average of 20%) (4) These detectors degrade more (another 2% reduction in accuracy) when subjected to multi-modal attacks (5) Defense methods will improve the robustness of multi-modal detectors, but cannot fully resist the effects of malicious attacks.",,IEEE
On the detection of image-scaling attacks in machine learning,"E Quiring, A Müller, K Rieck","Proceedings of the 39th Annual Computer …, 2023",2023-10-24,"<a href=""Google Scholar (2023-10-24) : On the detection of image-scaling attacks in machine learning"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3627106.3627134]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3627106.3627134]</a>","In summary, we study multiple backdoor types with scaling attacks. Our … attacks if the backdoor—used to calibrate the detection—is different to the finally used backdoor …",,Google Scholar
Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning,"Shengfang Zhai, Yinpeng Dong, Qingni Shen, Shi Pu, Yuejian Fang, Hang Su","arXiv
ACM Multimedia
arXiv","2023-10-22
2023
2023-05","<a href=""arXiv (2023-10-22) : Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning"" target=""_blank"">[http://arxiv.org/abs/2305.04175v2]</a>
<a href=""DBLP (2023) : Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning"" target=""_blank"">[https://doi.org/10.1145/3581783.3612108]</a>
<a href=""DBLP (2023-05) : Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.04175]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3581783.3612108]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.04175]</a>","With the help of conditioning mechanisms, the state-of-the-art diffusion models have achieved tremendous success in guided image generation, particularly in text-to-image synthesis. To gain a better understanding of the training process and potential risks of text-to-image synthesis, we perform a systematic investigation of backdoor attack on text-to-image diffusion models and propose BadT2I, a general multimodal backdoor attack framework that tampers with image synthesis in diverse semantic levels. Specifically, we perform backdoor attacks on three levels of the vision semantics: Pixel-Backdoor, Object-Backdoor and Style-Backdoor. By utilizing a regularization loss, our methods efficiently inject backdoors into a large-scale text-to-image diffusion model while preserving its utility with benign inputs. We conduct empirical experiments on Stable Diffusion, the widely-used text-to-image diffusion model, demonstrating that the large-scale diffusion model can be easily backdoored within a few fine-tuning steps. We conduct additional experiments to explore the impact of different types of textual triggers, as well as the backdoor persistence during further training, providing insights for the development of backdoor defense methods. Besides, our investigation may contribute to the copyright protection of text-to-image models in the future.

","

","arXiv
DBLP
DBLP"
Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning,"Taejin Kim, Jiarui Li, Shubhranshu Singh, Nikhil Madaan, Carlee Joe-Wong","arXiv
arXiv","2023-10-21
2023-10","<a href=""arXiv (2023-10-21) : Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2310.11594v2]</a>
<a href=""DBLP (2023-10) : Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.11594]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.11594]</a>","In today's data-driven landscape, the delicate equilibrium between safeguarding user privacy and unleashing data potential stands as a paramount concern. Federated learning, which enables collaborative model training without necessitating data sharing, has emerged as a privacy-centric solution. This decentralized approach brings forth security challenges, notably poisoning and backdoor attacks where malicious entities inject corrupted data. Our research, initially spurred by test-time evasion attacks, investigates the intersection of adversarial training and backdoor attacks within federated learning, introducing Adversarial Robustness Unhardening (ARU). ARU is employed by a subset of adversaries to intentionally undermine model robustness during decentralized training, rendering models susceptible to a broader range of evasion attacks. We present extensive empirical experiments evaluating ARU's impact on adversarial training and existing robust aggregation defenses against poisoning and backdoor attacks. Our findings inform strategies for enhancing ARU to counter current defensive measures and highlight the limitations of existing defenses, offering insights into bolstering defenses against ARU.
","
","arXiv
DBLP"
Towards Stable Backdoor Purification through Feature Shift Tuning,"Rui Min, Zeyu Qin, Li Shen, Minhao Cheng","arXiv
NeurIPS
arXiv","2023-10-21
2023
2023-10","<a href=""arXiv (2023-10-21) : Towards Stable Backdoor Purification through Feature Shift Tuning"" target=""_blank"">[http://arxiv.org/abs/2310.01875v3]</a>
<a href=""DBLP (2023) : Towards Stable Backdoor Purification through Feature Shift Tuning"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/ee37d51b3c003d89acba2363dde256af-Abstract-Conference.html]</a>
<a href=""DBLP (2023-10) : Towards Stable Backdoor Purification through Feature Shift Tuning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.01875]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/ee37d51b3c003d89acba2363dde256af-Abstract-Conference.html]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.01875]</a>","It has been widely observed that deep neural networks (DNN) are vulnerable to backdoor attacks where attackers could manipulate the model behavior maliciously by tampering with a small set of training samples. Although a line of defense methods is proposed to mitigate this threat, they either require complicated modifications to the training process or heavily rely on the specific model architecture, which makes them hard to deploy into real-world applications. Therefore, in this paper, we instead start with fine-tuning, one of the most common and easy-to-deploy backdoor defenses, through comprehensive evaluations against diverse attack scenarios. Observations made through initial experiments show that in contrast to the promising defensive results on high poisoning rates, vanilla tuning methods completely fail at low poisoning rate scenarios. Our analysis shows that with the low poisoning rate, the entanglement between backdoor and clean features undermines the effect of tuning-based defenses. Therefore, it is necessary to disentangle the backdoor and clean features in order to improve backdoor purification. To address this, we introduce Feature Shift Tuning (FST), a method for tuning-based backdoor purification. Specifically, FST encourages feature shifts by actively deviating the classifier weights from the originally compromised weights. Extensive experiments demonstrate that our FST provides consistently stable performance under different attack settings. Without complex parameter adjustments, FST also achieves much lower tuning costs, only 10 epochs. Our codes are available at https://github.com/AISafety-HKUST/stable_backdoor_purification.

","<a href=""arXiv"" target=""_blank"">[https://github.com/AISafety-HKUST/stable_backdoor_purification]</a>

","arXiv
DBLP
DBLP"
Improving Performance of Intrusion Detection Using ALO Selected Features and GRU Network,"Karthic Sundaram, Suhana Subramanian, ... Sumathi Thirumalaisamy",SN Computer Science,2023-10-21,"<a href=""Springer (2023-10-21) : Improving Performance of Intrusion Detection Using ALO Selected Features and GRU Network"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-023-02311-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-023-02311-0]</a>","The expansion of the internet has not only opened up new possibilities for cooperation, networking, and ingenuity, but also presented fresh security...",,Springer
Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation,"Xuanli He, Qiongkai Xu, Jun Wang, Benjamin Rubinstein, Trevor Cohn","arXiv
EMNLP
arXiv","2023-10-20
2023
2023-05","<a href=""arXiv (2023-10-20) : Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation"" target=""_blank"">[http://arxiv.org/abs/2305.11596v2]</a>
<a href=""DBLP (2023) : Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation"" target=""_blank"">[https://doi.org/10.18653/v1/2023.emnlp-main.60]</a>
<a href=""DBLP (2023-05) : Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.11596]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2023.emnlp-main.60]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.11596]</a>","Modern NLP models are often trained over large untrusted datasets, raising the potential for a malicious adversary to compromise model behaviour. For instance, backdoors can be implanted through crafting training instances with a specific textual trigger and a target label. This paper posits that backdoor poisoning attacks exhibit \emph{spurious correlation} between simple text features and classification labels, and accordingly, proposes methods for mitigating spurious correlation as means of defence. Our empirical study reveals that the malicious triggers are highly correlated to their target labels, therefore such correlations are extremely distinguishable compared to those scores of benign features, and can be used to filter out potentially problematic instances. Compared with several existing defences, our defence method significantly reduces attack success rates across backdoor attacks, and in the case of insertion-based attacks, our method provides a near-perfect defence.

","

","arXiv
DBLP
DBLP"
Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-Level Backdoor Attacks,"Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Xin Jiang, Maosong Sun","arXiv
arXiv","2023-10-20
2021-01","<a href=""arXiv (2023-10-20) : Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-Level Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2101.06969v5]</a>
<a href=""DBLP (2021-01) : Red Alarm for Pre-trained Models: Universal Vulnerabilities by Neuron-Level Backdoor Attacks"" target=""_blank"">[https://arxiv.org/abs/2101.06969]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1007/s11633-022-1377-5]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2101.06969]</a>","Pre-trained models (PTMs) have been widely used in various downstream tasks. The parameters of PTMs are distributed on the Internet and may suffer backdoor attacks. In this work, we demonstrate the universal vulnerability of PTMs, where fine-tuned PTMs can be easily controlled by backdoor attacks in arbitrary downstream tasks. Specifically, attackers can add a simple pre-training task, which restricts the output representations of trigger instances to pre-defined vectors, namely neuron-level backdoor attack (NeuBA). If the backdoor functionality is not eliminated during fine-tuning, the triggers can make the fine-tuned model predict fixed labels by pre-defined vectors. In the experiments of both natural language processing (NLP) and computer vision (CV), we show that NeuBA absolutely controls the predictions for trigger instances without any knowledge of downstream tasks. Finally, we apply several defense methods to NeuBA and find that model pruning is a promising direction to resist NeuBA by excluding backdoored neurons. Our findings sound a red alarm for the wide use of PTMs. Our source code and models are available at \url{https://github.com/thunlp/NeuBA}.
","<a href=""arXiv"" target=""_blank"">[https://github.com/thunlp/NeuBA}]</a>
","arXiv
DBLP"
Can We Trust the Similarity Measurement in Federated Learning?,"Z Wang, Q Hu, X Zou","arXiv preprint arXiv:2311.03369, 2023",2023-10-20,"<a href=""Google Scholar (2023-10-20) : Can We Trust the Similarity Measurement in Federated Learning?"" target=""_blank"">[https://arxiv.org/abs/2311.03369]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2311.03369]</a>","attacks is that Faker can always maintain a 100% attack success … attacks, we also explore the potential of expanding Faker in targeted backdoor attacks and Sybil attacks. …",,Google Scholar
LOTUS: EVASIVE AND RESILIENT BACKDOOR AT-TACKS THROUGH SUB-PARTITIONING,,,2023-10-20,"<a href=""Google Scholar (2023-10-20) : LOTUS: EVASIVE AND RESILIENT BACKDOOR AT-TACKS THROUGH SUB-PARTITIONING"" target=""_blank"">[https://openreview.net/forum?id=2rqC5FZiAH]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=2rqC5FZiAH]</a>","or under-bounded attack scope. In other words, the … attack effects can be easily mitigated by backdoor removal methods. In this paper, we propose a novel backdoor attack …",,Google Scholar
Poisonprompt: Backdoor attack on prompt-based large language models,"H Yao, J Lou, Z Qin","ICASSP 2024-2024 IEEE International …, 2024",2023-10-20,"<a href=""Google Scholar (2023-10-20) : Poisonprompt: Backdoor attack on prompt-based large language models"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10446267/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10446267/]</a>","However, the backdoor vulnerability, a serious security … In this paper, we present POISONPROMPT, a novel backdoor … threats posed by backdoor attacks on promptbased …",,Google Scholar
WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks,"Jun Xia, Zhihao Yue, Yingbo Zhou, Zhiwei Ling, Xian Wei, Mingsong Chen","arXiv
arXiv","2023-10-19
2023-10","<a href=""arXiv (2023-10-19) : WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2310.11595v2]</a>
<a href=""DBLP (2023-10) : WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.11595]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.11595]</a>","Due to the popularity of Artificial Intelligence (AI) technology, numerous backdoor attacks are designed by adversaries to mislead deep neural network predictions by manipulating training samples and training processes. Although backdoor attacks are effective in various real scenarios, they still suffer from the problems of both low fidelity of poisoned samples and non-negligible transfer in latent space, which make them easily detectable by existing backdoor detection algorithms. To overcome the weakness, this paper proposes a novel frequency-based backdoor attack method named WaveAttack, which obtains image high-frequency features through Discrete Wavelet Transform (DWT) to generate backdoor triggers. Furthermore, we introduce an asymmetric frequency obfuscation method, which can add an adaptive residual in the training and inference stage to improve the impact of triggers and further enhance the effectiveness of WaveAttack. Comprehensive experimental results show that WaveAttack not only achieves higher stealthiness and effectiveness, but also outperforms state-of-the-art (SOTA) backdoor attack methods in the fidelity of images by up to 28.27\% improvement in PSNR, 1.61\% improvement in SSIM, and 70.59\% reduction in IS.
","
","arXiv
DBLP"
A Backdoor-based Explainable AI Benchmark for Improved Fidelity in Evaluating Attribution Methods,,,2023-10-19,"<a href=""Google Scholar (2023-10-19) : A Backdoor-based Explainable AI Benchmark for Improved Fidelity in Evaluating Attribution Methods"" target=""_blank"">[https://openreview.net/forum?id=THL0keBDIR]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=THL0keBDIR]</a>","Finally, our analysis also provides guidance for defending against backdoor attacks … guidance for defense against backdoor attacks using attributions as part of this study. …",,Google Scholar
"A comprehensive survey of phishing: mediums, intended targets, attack and defence techniques and a novel taxonomy","Richa Goenka, Meenu Chawla, Namita Tiwari",International Journal of Information Security,2023-10-19,"<a href=""Springer (2023-10-19) : A comprehensive survey of phishing: mediums, intended targets, attack and defence techniques and a novel taxonomy"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00768-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00768-x]</a>",The recent surge in phishing incidents in the post-COVID era poses a serious threat towards the social and economic well-being of users. The...,,Springer
Distortion diminishing with vulnerability filters pruning,"Hengyi Huang, Pingfan Wu, ... Ningzhong Liu",Machine Vision and Applications,2023-10-19,"<a href=""Springer (2023-10-19) : Distortion diminishing with vulnerability filters pruning"" target=""_blank"">[https://link.springer.com/article/10.1007/s00138-023-01468-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00138-023-01468-1]</a>","Overparameterization of convolutional neural networks allows model compression, and model pruning algorithms have attracted much interest due to...",,Springer
"Information and Communications Security: 25th International Conference, ICICS 2023, Tianjin, China, November 18–20, 2023, Proceedings","D Wang, M Yung, Z Liu, X Chen",2023,2023-10-19,"<a href=""Google Scholar (2023-10-19) : Information and Communications Security: 25th International Conference, ICICS 2023, Tianjin, China, November 18–20, 2023, Proceedings"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=O1beEAAAQBAJ&oi=fnd&pg=PR5&dq=backdoor+attack&ots=edcIIOT8mf&sig=x1ZjCz6MH4TZ8BhGESRdBkaSyKw]</a>","<a href=""Google Scholar"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=O1beEAAAQBAJ&oi=fnd&pg=PR5&dq=backdoor+attack&ots=edcIIOT8mf&sig=x1ZjCz6MH4TZ8BhGESRdBkaSyKw]</a>","remote control of a victim device through traditional attacks, but also to what extent an … In particular, we discuss attacks like inferring actions that a user is doing on mobile …",,Google Scholar
"Pandas in action: analysis of China related advanced persistent threat actors' tactics, techniques & procedures","S Alaverronen, J Pohjola",2023,2023-10-19,"<a href=""Google Scholar (2023-10-19) : Pandas in action: analysis of China related advanced persistent threat actors' tactics, techniques & procedures"" target=""_blank"">[https://jyx.jyu.fi/handle/123456789/90108]</a>","<a href=""Google Scholar"" target=""_blank"">[https://jyx.jyu.fi/handle/123456789/90108]</a>","After the exploitation, the attack continued with installation of a web shell, backdoor … Finally, the attack was concluded using different remote access tools to exfiltrate data …",,Google Scholar
"Assessment of data augmentation, dropout with L2 Regularization and differential privacy against membership inference attacks","Sana Ben Hamida, Hichem Mrabet, ... Abderrazak Jemai",Multimedia Tools and Applications,2023-10-18,"<a href=""Springer (2023-10-18) : Assessment of data augmentation, dropout with L2 Regularization and differential privacy against membership inference attacks"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-17394-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-17394-3]</a>","Machine learning (ML) has revolutionized various industries, but concerns about privacy and security have emerged as significant challenges....",,Springer
Backdoor secrets unveiled: Identifying backdoor data with optimized scaled prediction consistency,"S Pal, Y Yao, R Wang, B Shen, S Liu","arXiv preprint arXiv:2403.10717, 2024",2023-10-18,"<a href=""Google Scholar (2023-10-18) : Backdoor secrets unveiled: Identifying backdoor data with optimized scaled prediction consistency"" target=""_blank"">[https://arxiv.org/abs/2403.10717]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2403.10717]</a>","type of data poisoning attacks, called backdoor attacks. Backdoor attacks are usually … of the training dataset with (often) imperceptible backdoor triggers (eg, a small image …",,Google Scholar
Backdooring multimodal learning,"X Han, Y Wu, Q Zhang, Y Zhou, Y Xu…","… IEEE Symposium on …, 2023",2023-10-18,"<a href=""Google Scholar (2023-10-18) : Backdooring multimodal learning"" target=""_blank"">[https://tianweiz07.github.io/Papers/24-SP.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://tianweiz07.github.io/Papers/24-SP.pdf]</a>","To bridge this gap, we present the first data and computation efficient backdoor attacks towards multimodal learning. Our solution consists of two innovations. First, we …",,Google Scholar
Distribution preserving backdoor attack in self-supervised learning,"G Tao, Z Wang, S Feng, G Shen, S Ma…","2024 IEEE Symposium …, 2023",2023-10-18,"<a href=""Google Scholar (2023-10-18) : Distribution preserving backdoor attack in self-supervised learning"" target=""_blank"">[https://www.computer.org/csdl/proceedings-article/sp/2024/313000a029/1RjEa5rjsHK]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.computer.org/csdl/proceedings-article/sp/2024/313000a029/1RjEa5rjsHK]</a>",We observe that the out-of-distribution propertyis not a crucial factor in backdoor attacks on self-supervisedlearning. Our overarching idea is hence to transformpoisoned …,,Google Scholar
Fully Hidden Dynamic Trigger Backdoor Attacks.,"S Narisada, S Hidano, K Fukushima","ICAART (3), 2023",2023-10-18,"<a href=""Google Scholar (2023-10-18) : Fully Hidden Dynamic Trigger Backdoor Attacks."" target=""_blank"">[https://www.scitepress.org/Papers/2023/116178/116178.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.scitepress.org/Papers/2023/116178/116178.pdf]</a>","normal poisoning attacks or backdoor attacks with static triggers … attack framework, we generalize adversarial examples as clean-label, invisible, dynamic backdoor attacks…",,Google Scholar
IoT Device Malware Detection Using Soft Computing Learning and Wide Madaline (WML-IOT),"A Punidha, E Arul, E Yuvarani","6th International Conference on …, 2023",2023-10-18,"<a href=""Google Scholar (2023-10-18) : IoT Device Malware Detection Using Soft Computing Learning and Wide Madaline (WML-IOT)"" target=""_blank"">[https://www.atlantis-press.com/proceedings/icic-6-23/125993036]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.atlantis-press.com/proceedings/icic-6-23/125993036]</a>",to lock the back door once the back door has been … attacks against susceptible targets. Complicated investigation proved the receiving of orders by bot and DoS attack …,,Google Scholar
"Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning","R Wen, T Wang, M Backes, Y Zhang…","arXiv preprint arXiv …, 2023",2023-10-18,"<a href=""Google Scholar (2023-10-18) : Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning"" target=""_blank"">[https://arxiv.org/abs/2310.11397]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2310.11397]</a>","We assess the backdoor attack across varying poisoning rates. Specifically, for LoRA and SPT, the poisoning rate ranges between 0.1 and 0.75. For ICL, given that we use …",,Google Scholar
Poisoned forgery face: Towards backdoor attacks on face forgery detection,"J Liang, S Liang, A Liu, X Jia, J Kuang…","arXiv preprint arXiv …, 2024",2023-10-18,"<a href=""Google Scholar (2023-10-18) : Poisoned forgery face: Towards backdoor attacks on face forgery detection"" target=""_blank"">[https://arxiv.org/abs/2402.11473]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2402.11473]</a>","scenarios caused by backdoor attack. By embedding … which enables clean-label backdoor attacks on face forgery … Notably, our approach surpasses SoTA backdoor …",,Google Scholar
Research on Federated Learning Security Defense Technology,Y. Gao Z. Xu A. Gu G. Yu C. Pan,"2023 4th International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)
2023 4th International …, 2023","2023-10-17
2023-08-25","<a href=""IEEE (2023-10-17) : Research on Federated Learning Security Defense Technology"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10281256]</a>
<a href=""Google Scholar (2023-08-25) : Research on Federated Learning Security Defense Technology"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10281256/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICBAIE59714.2023.10281256]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10281256/]</a>","In a federated learning environment, data interaction and sharing between nodes are inevitable. Malicious attackers may tamper with the data of certain nodes or even train models, posing a security threat to entities. Among these security attacks, backdoor attacks are the most covert. On the basis of existing research, this article designs a federated learning convergence wheel backdoor defense method and applies it to the field of vehicle networking. This method can effectively defend against backdoor attacks.
to various attacks, particularly backdoor attacks. In a backdoor attack, malicious updates … In Section 3, we summarize related work in the area of FL backdoor attacks. We …","
","IEEE
Google Scholar"
Evaluating the impact of filter-based feature selection in intrusion detection systems,"Houssam Zouhri, Ali Idri, Ahmed Ratnani",International Journal of Information Security,2023-10-17,"<a href=""Springer (2023-10-17) : Evaluating the impact of filter-based feature selection in intrusion detection systems"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00767-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00767-y]</a>","High dimensionality can lead to overfitting and affect the modeling power of classification algorithms, resulting an increase in false positive rate...",,Springer
Model architecture level privacy leakage in neural networks,"Yan Li, Hongyang Yan, ... Jin Li",Science China Information Sciences,2023-10-17,"<a href=""Springer (2023-10-17) : Model architecture level privacy leakage in neural networks"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-022-3507-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-022-3507-7]</a>",Privacy leakage is one of the most critical issues in machine learning and has attracted growing interest for tasks such as demonstrating potential...,,Springer
POISONING-BASED BACKDOOR ATTACKS FOR ARBI-TRARY TARGET LABEL WITH POSITIVE TRIGGERS,,,2023-10-17,"<a href=""Google Scholar (2023-10-17) : POISONING-BASED BACKDOOR ATTACKS FOR ARBI-TRARY TARGET LABEL WITH POSITIVE TRIGGERS"" target=""_blank"">[https://openreview.net/forum?id=FwwnW2dK9w]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=FwwnW2dK9w]</a>",the success rate of backdoor attacks versus conventional … Poisoning-based backdoor attack with Positive Triggers (… -based backdoor attack achieves a high attack success …,,Google Scholar
Universal jailbreak backdoors from poisoned human feedback,"J Rando, F Tramèr","arXiv preprint arXiv:2311.14455, 2023",2023-10-17,"<a href=""Google Scholar (2023-10-17) : Universal jailbreak backdoors from poisoned human feedback"" target=""_blank"">[https://arxiv.org/abs/2311.14455]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2311.14455]</a>",to embed a “jailbreak backdoor” into the model. The backdoor embeds a trigger word … harder to plant using common backdoor attack techniques. We investigate the design …,,Google Scholar
Badchain: Backdoor chain-of-thought prompting for large language models,"Z Xiang, F Jiang, Z Xiong, B Ramasubramanian…","arXiv preprint arXiv …, 2024",2023-10-16,"<a href=""Google Scholar (2023-10-16) : Badchain: Backdoor chain-of-thought prompting for large language models"" target=""_blank"">[https://arxiv.org/abs/2401.12242]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2401.12242]</a>","We show that the baseline backdoor attacks designed for simpler tasks such as semantic classification will fail on these complicated tasks. Moreover, our findings reveal …",,Google Scholar
RETHINKING CNN'S GENERALIZATION TO BACK-DOOR ATTACK FROM FREQUENCY DOMAIN,,,2023-10-16,"<a href=""Google Scholar (2023-10-16) : RETHINKING CNN'S GENERALIZATION TO BACK-DOOR ATTACK FROM FREQUENCY DOMAIN"" target=""_blank"">[https://openreview.net/forum?noteId=mYhH0CDFFa]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?noteId=mYhH0CDFFa]</a>","maintaining attack effectiveness. In order to efficiently implement backdoor attacks on … image energy, we also propose a backdoor attack algorithm based on low-frequency …",,Google Scholar
Explore the Effect of Data Selection on Poison Efficiency in Backdoor Attacks,"Ziqiang Li, Pengfei Xia, Hong Sun, Yueqi Zeng, Wei Zhang, Bin Li","arXiv
arXiv","2023-10-15
2023-10","<a href=""arXiv (2023-10-15) : Explore the Effect of Data Selection on Poison Efficiency in Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2310.09744v1]</a>
<a href=""DBLP (2023-10) : Explore the Effect of Data Selection on Poison Efficiency in Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.09744]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.09744]</a>","As the number of parameters in Deep Neural Networks (DNNs) scales, the thirst for training data also increases. To save costs, it has become common for users and enterprises to delegate time-consuming data collection to third parties. Unfortunately, recent research has shown that this practice raises the risk of DNNs being exposed to backdoor attacks. Specifically, an attacker can maliciously control the behavior of a trained model by poisoning a small portion of the training data. In this study, we focus on improving the poisoning efficiency of backdoor attacks from the sample selection perspective. The existing attack methods construct such poisoned samples by randomly selecting some clean data from the benign set and then embedding a trigger into them. However, this random selection strategy ignores that each sample may contribute differently to the backdoor injection, thereby reducing the poisoning efficiency. To address the above problem, a new selection strategy named Improved Filtering and Updating Strategy (FUS++) is proposed. Specifically, we adopt the forgetting events of the samples to indicate the contribution of different poisoned samples and use the curvature of the loss surface to analyses the effectiveness of this phenomenon. Accordingly, we combine forgetting events and curvature of different samples to conduct a simple yet efficient sample selection strategy. The experimental results on image classification (CIFAR-10, CIFAR-100, ImageNet-10), text classification (AG News), audio classification (ESC-50), and age regression (Facial Age) consistently demonstrate the effectiveness of the proposed strategy: the attack performance using FUS++ is significantly higher than that using random selection for the same poisoning ratio.
","
","arXiv
DBLP"
Demystifying Poisoning Backdoor Attacks from a Statistical Perspective,"X Xian, G Wang, J Srinivasa, A Kundu, X Bi…","arXiv preprint arXiv …, 2023",2023-10-15,"<a href=""Google Scholar (2023-10-15) : Demystifying Poisoning Backdoor Attacks from a Statistical Perspective"" target=""_blank"">[https://arxiv.org/abs/2310.10780]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2310.10780]</a>","the effectiveness of any backdoor attack incorporating a … model on both clean and backdoor test data. The … the determining factors for a backdoor attack’s success, (…",,Google Scholar
Explore the effect of data selection on poison efficiency in backdoor attacks,"Z Li, P Xia, H Sun, Y Zeng, W Zhang, B Li","arXiv preprint arXiv …, 2023",2023-10-15,"<a href=""Google Scholar (2023-10-15) : Explore the effect of data selection on poison efficiency in backdoor attacks"" target=""_blank"">[https://arxiv.org/abs/2310.09744]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2310.09744]</a>","the risk of DNNs being exposed to backdoor attacks. Specifically, an attacker can … of backdoor attacks from the sample selection perspective. The existing attack methods …",,Google Scholar
SHINE: Shielding Backdoors in Deep Reinforcement Learning,"Z Yuan, W Guo, J Jia, B Li, D Song",Forty-first International Conference on …,2023-10-15,"<a href=""Google Scholar (2023-10-15) : SHINE: Shielding Backdoors in Deep Reinforcement Learning"" target=""_blank"">[https://openreview.net/forum?id=nMWxLnSBGW]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=nMWxLnSBGW]</a>","is vulnerable to backdoor attacks. Existing defenses against backdoor attacks either do … backdoor attacks (perturbation-based attacks in single-agent RL, adversarial agent …",,Google Scholar
VFLAIR: A Research Library and Benchmark for Vertical Federated Learning,"T Zou, Z Gu, Y He, H Takahashi, Y Liu, G Ye…","arXiv preprint arXiv …, 2023",2023-10-15,"<a href=""Google Scholar (2023-10-15) : VFLAIR: A Research Library and Benchmark for Vertical Federated Learning"" target=""_blank"">[https://arxiv.org/abs/2310.09827]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2310.09827]</a>","is designed for mitigating label inference attacks only, we do not assess its effectiveness against FR attacks or backdoor attacks. For most of the attacks and defenses, we …",,Google Scholar
LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors,"Chengkun Wei, Wenlong Meng, Zhikun Zhang, Min Chen, Minghu Zhao, Wenjing Fang, Lei Wang, Zihui Zhang, Wenzhi Chen","arXiv
arXiv","2023-10-14
2023-08","<a href=""arXiv (2023-10-14) : LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors"" target=""_blank"">[http://arxiv.org/abs/2308.13904v2]</a>
<a href=""DBLP (2023-08) : LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.13904]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.14722/ndss.2024.23238]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.13904]</a>","Prompt-tuning has emerged as an attractive paradigm for deploying large-scale language models due to its strong downstream task performance and efficient multitask serving ability. Despite its wide adoption, we empirically show that prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside in the pretrained models and can affect arbitrary downstream tasks. The state-of-the-art backdoor detection approaches cannot defend against task-agnostic backdoors since they hardly converge in reversing the backdoor triggers. To address this issue, we propose LMSanitator, a novel approach for detecting and removing task-agnostic backdoors on Transformer models. Instead of directly inverting the triggers, LMSanitator aims to invert the predefined attack vectors (pretrained models' output when the input is embedded with triggers) of the task-agnostic backdoors, which achieves much better convergence performance and backdoor detection accuracy. LMSanitator further leverages prompt-tuning's property of freezing the pretrained model to perform accurate and fast output monitoring and input purging during the inference phase. Extensive experiments on multiple language models and NLP tasks illustrate the effectiveness of LMSanitator. For instance, LMSanitator achieves 92.8% backdoor detection accuracy on 960 models and decreases the attack success rate to less than 1% in most scenarios.
","
","arXiv
DBLP"
Mitigating backdoor attacks with generative modelling and dataset relabelling,"I Sabolic, I Grubišić, S Šegvić",2023,2023-10-14,"<a href=""Google Scholar (2023-10-14) : Mitigating backdoor attacks with generative modelling and dataset relabelling"" target=""_blank"">[https://openreview.net/forum?id=bXI0thP733]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=bXI0thP733]</a>","Backdoor attacks are an emerging and rapidly growing research area that poses … This paper focuses on the poisoning-based backdoor attacks, where the attacker can only …",,Google Scholar
Towards Practical Privacy-Preserving Clustering and Health Care Data Analyses,H Möllering,2023,2023-10-14,"<a href=""Google Scholar (2023-10-14) : Towards Practical Privacy-Preserving Clustering and Health Care Data Analyses"" target=""_blank"">[https://tuprints.ulb.tu-darmstadt.de/24708/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://tuprints.ulb.tu-darmstadt.de/24708/]</a>","concept, Federated Learning (FL), which is susceptible to backdoor attacks manipulating the model in addition to inference attacks extracting information about the training …",,Google Scholar
A Change of Heart: Backdoor Attacks on Security-Centric Diffusion Models,"C Li, R Pang, B Cao, J Chen, T Wang",2023,2023-10-13,"<a href=""Google Scholar (2023-10-13) : A Change of Heart: Backdoor Attacks on Security-Centric Diffusion Models"" target=""_blank"">[https://openreview.net/forum?id=Gf4KZIqLHD]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=Gf4KZIqLHD]</a>","backdoor attack tailored to securitycentric diffusion models. As illustrated in Figure 1, DIFF2 overlays a malicious diffusion-denoising process (“diffusion backdoor… backdoor …",,Google Scholar
BaDLoss: Backdoor Detection via Loss Dynamics,"N Alex, SA Siddiqui, A Sanyal, D Krueger",2023,2023-10-13,"<a href=""Google Scholar (2023-10-13) : BaDLoss: Backdoor Detection via Loss Dynamics"" target=""_blank"">[https://openreview.net/forum?id=uw5U7FfTRf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=uw5U7FfTRf]</a>","Our method effectively transfers zero-shot to novel backdoor attacks without prior … detecting backdoor attacks in the real world, especially in the realistic multi-attack setting, …",,Google Scholar
Backdoor Attack for Federated Learning with Fake Clients,,,2023-10-13,"<a href=""Google Scholar (2023-10-13) : Backdoor Attack for Federated Learning with Fake Clients"" target=""_blank"">[https://openreview.net/forum?id=7d2sWFIIPF]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=7d2sWFIIPF]</a>","we propose Fake Client Backdoor Attack (FakeBA), a new backdoor attack scheme that relies on fake clients to circumvent federated backdoor defenses without access to …",,Google Scholar
Backdooring instruction-tuned large language models with virtual prompt injection,"J Yan, V Yadav, S Li, L Chen, Z Tang…","Proceedings of the …, 2024",2023-10-13,"<a href=""Google Scholar (2023-10-13) : Backdooring instruction-tuned large language models with virtual prompt injection"" target=""_blank"">[https://aclanthology.org/2024.naacl-long.337/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://aclanthology.org/2024.naacl-long.337/]</a>","of stealthy and harmful backdoor attacks that deliver seemingly-… els, we introduce a backdoor attack1 setting called Virtual … We loosely refer to it as a “backdoor attack” as it …",,Google Scholar
Causality-Based Black-Box Backdoor Detection,"M Hu, Z Guan, Z Zhou, J Zhang, S Li",2023,2023-10-13,"<a href=""Google Scholar (2023-10-13) : Causality-Based Black-Box Backdoor Detection"" target=""_blank"">[https://openreview.net/forum?id=bCvm9h0FmQ]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=bCvm9h0FmQ]</a>",We find that backdoor attacks act as a confounder (… baselines against various backdoor attacks on three datasets. … various attack methods. Especially for samplespecific …,,Google Scholar
Certified Copy: A Resistant Backdoor Attack,"OR Rostami, R Ning, C Xin, JH Cho, J Li, H Wu",2023,2023-10-13,"<a href=""Google Scholar (2023-10-13) : Certified Copy: A Resistant Backdoor Attack"" target=""_blank"">[https://openreview.net/forum?id=66e22qCU5i]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=66e22qCU5i]</a>",Despite numerous backdoor detection mechanisms developed for computer vision … that even simple backdoor attacks can bypass these defenses if the backdoor planting …,,Google Scholar
DRMGuard: Defending Deep Regression Models against Backdoor Attacks,"L Du, Y Liu, J Jia, G Lan",2023,2023-10-13,"<a href=""Google Scholar (2023-10-13) : DRMGuard: Defending Deep Regression Models against Backdoor Attacks"" target=""_blank"">[https://openreview.net/forum?id=AoRIT2Uzfg]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=AoRIT2Uzfg]</a>",backdoor attacks on DRMs. AE can be regarded as the counterpart to the attack success rate for backdoor attacks … We detail how to adapt these backdoor attacks to DRMs …,,Google Scholar
Defending our privacy with backdoors,"D Hintersdorf, L Struppek, D Neider…","arXiv preprint arXiv …, 2023",2023-10-13,"<a href=""Google Scholar (2023-10-13) : Defending our privacy with backdoors"" target=""_blank"">[https://arxiv.org/abs/2310.08320]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2310.08320]</a>","We demonstrate that backdoor attacks can be employed to remove specific words and phrases from models, thereby enhancing the privacy of individuals. With our …",,Google Scholar
Efficient Backdoor Mitigation in Federated Learning with Contrastive Loss,"H Ferguson, R Ning, J Li, H Wu, C Xin",2023,2023-10-13,"<a href=""Google Scholar (2023-10-13) : Efficient Backdoor Mitigation in Federated Learning with Contrastive Loss"" target=""_blank"">[https://openreview.net/forum?id=10BTKkFfhl]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=10BTKkFfhl]</a>",We evaluated the proposed method on three datasets under two backdoor attacks and compared it against three existing defense methods. Our results showed that while …,,Google Scholar
EigenGuard: Backdoor Defense in Eigenspace,"M Li, Y Wang, Z Lin, Y Wang",2023,2023-10-13,"<a href=""Google Scholar (2023-10-13) : EigenGuard: Backdoor Defense in Eigenspace"" target=""_blank"">[https://openreview.net/forum?id=YSZ2GmGvUV]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=YSZ2GmGvUV]</a>","Leveraging this critical understanding, we can propose our defense mechanisms against backdoor attacks by seamlessly integrating an EigenGuard module within neural …",,Google Scholar
FedSKU: Defending Backdoors in Federated Learning Through Selective Knowledge Unlearning,"G Xie, B Liu, Y Zhou",2023,2023-10-13,"<a href=""Google Scholar (2023-10-13) : FedSKU: Defending Backdoors in Federated Learning Through Selective Knowledge Unlearning"" target=""_blank"">[https://openreview.net/forum?id=rFCGiFTVyY]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=rFCGiFTVyY]</a>","In this paper, we introduce the idea of selective knowledge unlearning to defend the backdoor attacks in FL. Our approach, FedSKU, can simultaneously satisfy the …",,Google Scholar
Fisher Information Guided Backdoor Purification Via Naive Exploitation of Smoothness,"N Karim, A Al Arafat, U Khalid, Z Guo, N Rahnavard",2023,2023-10-13,"<a href=""Google Scholar (2023-10-13) : Fisher Information Guided Backdoor Purification Via Naive Exploitation of Smoothness"" target=""_blank"">[https://openreview.net/forum?id=lOsF9k1sxW]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=lOsF9k1sxW]</a>","We consider an adversary with the capabilities of carrying a backdoor attack on a DNN model, fθ : Rd → Rc, by training it on a poisoned data set Dtrain = {Xtrain,Ytrain}, …",,Google Scholar
Forget-Me-Not: Making Backdoor Hard to be Forgotten in Fine-tuning,"TN Huynh, AT Tran, KD Doan, T Pham",2023,2023-10-13,"<a href=""Google Scholar (2023-10-13) : Forget-Me-Not: Making Backdoor Hard to be Forgotten in Fine-tuning"" target=""_blank"">[https://openreview.net/forum?id=T23HYw6lta]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=T23HYw6lta]</a>","Based on the observations above, we believe that a solid backdoor attack must be … the aforementioned belief by designing backdoor attacks that can withstand all existing …",,Google Scholar
From gradient attacks to data poisoning,,,2023-10-13,"<a href=""Google Scholar (2023-10-13) : From gradient attacks to data poisoning"" target=""_blank"">[https://openreview.net/forum?id=ZFjp5Q2hLn]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=ZFjp5Q2hLn]</a>","poisoning can sometimes mimic gradient availability attacks in a more practical deep … or backdoor attacks, we show that by borrowing a threat model to gradient attacks, we …",,Google Scholar
Invisible and Adaptive Training-Phase Target-Conditioned Backdoors,,,2023-10-13,"<a href=""Google Scholar (2023-10-13) : Invisible and Adaptive Training-Phase Target-Conditioned Backdoors"" target=""_blank"">[https://openreview.net/forum?id=uY4HLeKERt]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=uY4HLeKERt]</a>",: Table 1 compares recent backdoor attacks with the Flareon attack proposed in this … the existing backdoor attacks can be easily adapted as code-injection attack without …,,Google Scholar
LeBD: A Run-time Defense Against Backdoor Attack in YOLO,"K Chen, W Shan, X Li, XUE YANG, Q Li, J Yu",2023,2023-10-13,"<a href=""Google Scholar (2023-10-13) : LeBD: A Run-time Defense Against Backdoor Attack in YOLO"" target=""_blank"">[https://openreview.net/forum?id=7vKWg2Vdrs]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=7vKWg2Vdrs]</a>","attacks in the object detection (OD) network, … backdoor detector based on counterfactual attribution LayerCAM (CA-LeBD). We evaluated the performance of the backdoor …",,Google Scholar
Learnable Invisible Backdoor for Diffusion Models,"S Li, J Ma, M Cheng",2023,2023-10-13,"<a href=""Google Scholar (2023-10-13) : Learnable Invisible Backdoor for Diffusion Models"" target=""_blank"">[https://openreview.net/forum?id=scFfMOOGD8]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=scFfMOOGD8]</a>","means that the security threat posed by backdoor attacks has not been fully explored. … In this paper, as shown in Figure 1, we explore backdoor attacks with invisible image …",,Google Scholar
Learning from Distinction: Mitigating backdoors using a low-capacity model,"H Sun, X Lyu",2023,2023-10-13,"<a href=""Google Scholar (2023-10-13) : Learning from Distinction: Mitigating backdoors using a low-capacity model"" target=""_blank"">[https://openreview.net/forum?id=JGP1GlTnLF]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=JGP1GlTnLF]</a>","detection and removal at the model deployment stage, an effective defense against backdoor attacks during the training time is still under-explored. In this paper, we …",,Google Scholar
Maximum Margin Based Activation Clipping for Post-Training Overfitting Mitigation in DNN Classifiers,,,2023-10-13,"<a href=""Google Scholar (2023-10-13) : Maximum Margin Based Activation Clipping for Post-Training Overfitting Mitigation in DNN Classifiers"" target=""_blank"">[https://openreview.net/forum?id=BjjG2fH09N]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=BjjG2fH09N]</a>",even if there was backdoor poisoning). … backdoor trigger in an image (the backdoor embedding function) – MM-BM is largely effective against a variety of backdoor attacks. …,,Google Scholar
Mitigating Backdoor Attacks in Federated Learning through Noise-Guided Aggregation,"H Zhang, Y Zhang, Q Wang, B Han, D Lian, E Chen",2023,2023-10-13,"<a href=""Google Scholar (2023-10-13) : Mitigating Backdoor Attacks in Federated Learning through Noise-Guided Aggregation"" target=""_blank"">[https://openreview.net/forum?id=RXVYOCGO7g]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=RXVYOCGO7g]</a>","To demonstrate the effectiveness of Nira in defending against backdoor attacks, we consider five commonly used defense techniques: (i) Krum and (ii) Multi-Krum(…",,Google Scholar
SAFHE: Defending Against Backdoor and Gradient Inversion Attacks in Federated Learning,,,2023-10-13,"<a href=""Google Scholar (2023-10-13) : SAFHE: Defending Against Backdoor and Gradient Inversion Attacks in Federated Learning"" target=""_blank"">[https://openreview.net/forum?id=BOm1RYdHHu]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=BOm1RYdHHu]</a>",against the two types of attacks. This poses a major … both backdoor attacks and gradient inversion attacks. Our … can defend against backdoor attacks without significantly …,,Google Scholar
Silencer: Pruning-aware Backdoor Defense for Decentralized Federated Learning,,,2023-10-13,"<a href=""Google Scholar (2023-10-13) : Silencer: Pruning-aware Backdoor Defense for Decentralized Federated Learning"" target=""_blank"">[https://openreview.net/forum?id=yhBLUzHE9r]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=yhBLUzHE9r]</a>","We systematically study the risk of backdoor attacks in DFL with different P2P topologies. • To the best of our knowledge, we are the first to identify and augment the …",,Google Scholar
TROJFSL: TROJAN INSERTION IN FEW SHOT PROMPT LEARNING,,,2023-10-13,"<a href=""Google Scholar (2023-10-13) : TROJFSL: TROJAN INSERTION IN FEW SHOT PROMPT LEARNING"" target=""_blank"">[https://openreview.net/forum?id=6muJekoPR7]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=6muJekoPR7]</a>","To mitigate these issues, we propose TrojFSL to perform backdoor attacks in the … Compared to prior prompt-based backdoor attacks, TrojFSL improves the ASR by 9% …",,Google Scholar
Towards Faithful XAI Evaluation via Generalization-Limited Backdoor Watermark,"M Ya, Y Li, T Dai, B Wang, Y Jiang…","The Twelfth International …, 2023",2023-10-13,"<a href=""Google Scholar (2023-10-13) : Towards Faithful XAI Evaluation via Generalization-Limited Backdoor Watermark"" target=""_blank"">[https://openreview.net/forum?id=cObFETcoeW]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=cObFETcoeW]</a>","In this paper, we revisit the backdoor-based SRV … latent assumption that existing backdoor attacks have no trigger … not hold for existing backdoor attacks and therefore may …",,Google Scholar
Towards Reliable Backdoor Attacks on Vision Transformers,"Y Mo, D Wu, Y Wang, Y Guo, Y Wang",2023,2023-10-13,"<a href=""Google Scholar (2023-10-13) : Towards Reliable Backdoor Attacks on Vision Transformers"" target=""_blank"">[https://openreview.net/forum?id=MLShfiJ3CB]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=MLShfiJ3CB]</a>","common backdoor defenses, ie, these defenses either fail to reduce the attack … This study investigates the existing backdoor attacks/defenses and finds that this kind …",,Google Scholar
Towards Universal Robust Federated Learning via Meta Stackelberg Game,"H Li, T Li, Y Pan, T Xu, Q Zhu, Z Zheng",2023,2023-10-13,"<a href=""Google Scholar (2023-10-13) : Towards Universal Robust Federated Learning via Meta Stackelberg Game"" target=""_blank"">[https://openreview.net/forum?id=iStX5y0Ttg]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=iStX5y0Ttg]</a>","attacks (ie, backdoor attacks) and does not directly lend itself to mixed/adaptive attacks. … We consider two major categories of attacks, namely, backdoor attacks and …",,Google Scholar
pFedSAM: Secure Federated Learning Against Backdoor Attacks via Personalized Sharpness-Aware Minimization,,,2023-10-13,"<a href=""Google Scholar (2023-10-13) : pFedSAM: Secure Federated Learning Against Backdoor Attacks via Personalized Sharpness-Aware Minimization"" target=""_blank"">[https://openreview.net/forum?id=cz0kQD95o4]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=cz0kQD95o4]</a>",is vulnerable to backdoor attacks where malicious clients … backdoor triggers while performing normally on the benign samples. Existing defenses against backdoor attacks …,,Google Scholar
Invisible Threats: Backdoor Attack in OCR Systems,"Mauro Conti, Nicola Farronato, Stefanos Koffas, Luca Pajola, Stjepan Picek","arXiv
arXiv","2023-10-12
2023-10","<a href=""arXiv (2023-10-12) : Invisible Threats: Backdoor Attack in OCR Systems"" target=""_blank"">[http://arxiv.org/abs/2310.08259v1]</a>
<a href=""DBLP (2023-10) : Invisible Threats: Backdoor Attack in OCR Systems"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.08259]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.08259]</a>","Optical Character Recognition (OCR) is a widely used tool to extract text from scanned documents. Today, the state-of-the-art is achieved by exploiting deep neural networks. However, the cost of this performance is paid at the price of system vulnerability. For instance, in backdoor attacks, attackers compromise the training phase by inserting a backdoor in the victim's model that will be activated at testing time by specific patterns while leaving the overall model performance intact. This work proposes a backdoor attack for OCR resulting in the injection of non-readable characters from malicious input images. This simple but effective attack exposes the state-of-the-art OCR weakness, making the extracted text correct to human eyes but simultaneously unusable for the NLP application that uses OCR as a preprocessing step. Experimental results show that the attacked models successfully output non-readable characters for around 90% of the poisoned instances without harming their performance for the remaining instances.
","
","arXiv
DBLP"
A DHR executor selection algorithm based on historical credibility and dissimilarity clustering,"Sisi Shao, Yimu Ji, ... Longfei Zhou",Science China Information Sciences,2023-10-12,"<a href=""Springer (2023-10-12) : A DHR executor selection algorithm based on historical credibility and dissimilarity clustering"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-022-3635-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-022-3635-2]</a>","The security of the dynamic heterogeneous redundancy (DHR) architecture relies on the heterogeneity of its executors, which also defines the...",,Springer
System for provably robust interpretable machine learning models,"D Fradkin, M Gario, B Dey, I Akrotirianakis…","US Patent App. 18 …, 2023",2023-10-12,"<a href=""Google Scholar (2023-10-12) : System for provably robust interpretable machine learning models"" target=""_blank"">[https://patents.google.com/patent/US20230325678A1/en]</a>","<a href=""Google Scholar"" target=""_blank"">[https://patents.google.com/patent/US20230325678A1/en]</a>",Backdoor attacks introduce training instances with nominally correct labels but with a ‘trigger’ that the model learns and that can be used at inference time to force the …,,Google Scholar
Prompt Backdoors in Visual Prompt Learning,"Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang","arXiv
arXiv","2023-10-11
2023-10","<a href=""arXiv (2023-10-11) : Prompt Backdoors in Visual Prompt Learning"" target=""_blank"">[http://arxiv.org/abs/2310.07632v1]</a>
<a href=""DBLP (2023-10) : Prompt Backdoors in Visual Prompt Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.07632]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.07632]</a>","Fine-tuning large pre-trained computer vision models is infeasible for resource-limited users. Visual prompt learning (VPL) has thus emerged to provide an efficient and flexible alternative to model fine-tuning through Visual Prompt as a Service (VPPTaaS). Specifically, the VPPTaaS provider optimizes a visual prompt given downstream data, and downstream users can use this prompt together with the large pre-trained model for prediction. However, this new learning paradigm may also pose security risks when the VPPTaaS provider instead provides a malicious visual prompt. In this paper, we take the first step to explore such risks through the lens of backdoor attacks. Specifically, we propose BadVisualPrompt, a simple yet effective backdoor attack against VPL. For example, poisoning $5\%$ CIFAR10 training data leads to above $99\%$ attack success rates with only negligible model accuracy drop by $1.5\%$. In particular, we identify and then address a new technical challenge related to interactions between the backdoor trigger and visual prompt, which does not exist in conventional, model-level backdoors. Moreover, we provide in-depth analyses of seven backdoor defenses from model, prompt, and input levels. Overall, all these defenses are either ineffective or impractical to mitigate our BadVisualPrompt, implying the critical vulnerability of VPL.
","
","arXiv
DBLP"
Leveraging diffusion-based image variations for robust training on poisoned data,"L Struppek, MB Hentschel, C Poth…","arXiv preprint arXiv …, 2023",2023-10-11,"<a href=""Google Scholar (2023-10-11) : Leveraging diffusion-based image variations for robust training on poisoned data"" target=""_blank"">[https://arxiv.org/abs/2310.06372]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2310.06372]</a>",We start by introducing the concept of data poisoning and backdoor attacks together with possible mitigation strategies in Sec. 2.1. We then briefly describe the intuition of …,,Google Scholar
Stealthy Energy Consumption-oriented Attacks on Training Stage in Deep Learning,"Wencheng Chen, Hongyu Li",Journal of Signal Processing Systems,2023-10-11,"<a href=""Springer (2023-10-11) : Stealthy Energy Consumption-oriented Attacks on Training Stage in Deep Learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s11265-023-01895-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11265-023-01895-3]</a>","Deep Learning as a Service (DLaaS) is rapidly developing recently to enable applications including self-driving, face recognition, and natural...",,Springer
High Dimensional Causal Inference with Variational Backdoor Adjustment,"Daniel Israel, Aditya Grover, Guy Van den Broeck","arXiv
arXiv","2023-10-09
2023-10","<a href=""arXiv (2023-10-09) : High Dimensional Causal Inference with Variational Backdoor Adjustment"" target=""_blank"">[http://arxiv.org/abs/2310.06100v1]</a>
<a href=""DBLP (2023-10) : High Dimensional Causal Inference with Variational Backdoor Adjustment"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.06100]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.06100]</a>","Backdoor adjustment is a technique in causal inference for estimating interventional quantities from purely observational data. For example, in medical settings, backdoor adjustment can be used to control for confounding and estimate the effectiveness of a treatment. However, high dimensional treatments and confounders pose a series of potential pitfalls: tractability, identifiability, optimization. In this work, we take a generative modeling approach to backdoor adjustment for high dimensional treatments and confounders. We cast backdoor adjustment as an optimization problem in variational inference without reliance on proxy variables and hidden confounders. Empirically, our method is able to estimate interventional likelihood in a variety of high dimensional settings, including semi-synthetic X-ray medical data. To the best of our knowledge, this is the first application of backdoor adjustment in which all the relevant variables are high dimensional.
","
","arXiv
DBLP"
Defending Against Backdoor Attacks in Natural Language Generation,"Xiaofei Sun, Xiaoya Li, Yuxian Meng, Xiang Ao, Lingjuan Lyu, Jiwei Li, Tianwei Zhang",arXiv,2023-10-09,"<a href=""arXiv (2023-10-09) : Defending Against Backdoor Attacks in Natural Language Generation"" target=""_blank"">[http://arxiv.org/abs/2106.01810v3]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","The frustratingly fragile nature of neural network models make current natural language generation (NLG) systems prone to backdoor attacks and generate malicious sequences that could be sexist or offensive. Unfortunately, little effort has been invested to how backdoor attacks can affect current NLG models and how to defend against these attacks. In this work, by giving a formal definition of backdoor attack and defense, we investigate this problem on two important NLG tasks, machine translation and dialog generation. Tailored to the inherent nature of NLG models (e.g., producing a sequence of coherent words given contexts), we design defending strategies against attacks. We find that testing the backward probability of generating sources given targets yields effective defense performance against all different types of attacks, and is able to handle the {\it one-to-many} issue in many NLG tasks such as dialog generation. We hope that this work can raise the awareness of backdoor risks concealed in deep NLG systems and inspire more future work (both attack and defense) towards this direction.",,arXiv
Confidence-driven Sampling for Backdoor Attacks,"Pengfei He, Han Xu, Yue Xing, Jie Ren, Yingqian Cui, Shenglai Zeng, Jiliang Tang, Makoto Yamada, Mohammad Sabokrou","arXiv
arXiv","2023-10-08
2023-10","<a href=""arXiv (2023-10-08) : Confidence-driven Sampling for Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2310.05263v1]</a>
<a href=""DBLP (2023-10) : Confidence-driven Sampling for Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.05263]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.05263]</a>","Backdoor attacks aim to surreptitiously insert malicious triggers into DNN models, granting unauthorized control during testing scenarios. Existing methods lack robustness against defense strategies and predominantly focus on enhancing trigger stealthiness while randomly selecting poisoned samples. Our research highlights the overlooked drawbacks of random sampling, which make that attack detectable and defensible. The core idea of this paper is to strategically poison samples near the model's decision boundary and increase defense difficulty. We introduce a straightforward yet highly effective sampling methodology that leverages confidence scores. Specifically, it selects samples with lower confidence scores, significantly increasing the challenge for defenders in identifying and countering these attacks. Importantly, our method operates independently of existing trigger designs, providing versatility and compatibility with various backdoor attack techniques. We substantiate the effectiveness of our approach through a comprehensive set of empirical experiments, demonstrating its potential to significantly enhance resilience against backdoor attacks in DNNs.
","
","arXiv
DBLP"
A systematic evaluation of backdoor attacks in various domains,"S Koffas, B Tajalli, J Xu, M Conti, S Picek","Embedded Machine Learning …, 2023",2023-10-08,"<a href=""Google Scholar (2023-10-08) : A systematic evaluation of backdoor attacks in various domains"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-40677-5_20]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-40677-5_20]</a>",While many works investigate various backdoor attacks and defenses for different … of backdoor attacks for different scenarios. This work considers backdoor attacks in …,,Google Scholar
Mitigating Backdoor Attacks on Deep Neural Networks,"H Fu, A Sarmadi, P Krishnamurthy, S Garg…","… Machine Learning for …, 2023",2023-10-07,"<a href=""Google Scholar (2023-10-07) : Mitigating Backdoor Attacks on Deep Neural Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-40677-5_16]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-40677-5_16]</a>","The vulnerability of DNNs to backdoor attacks arises … In this chapter, we consider backdoor attacks in the context of … In general, the backdoor attack refers to an attacker …",,Google Scholar
Nacc-Guard: a lightweight DNN accelerator architecture for secure deep learning,"Peng Li, Cheng Che, Rui Hou",The Journal of Supercomputing,2023-10-07,"<a href=""Springer (2023-10-07) : Nacc-Guard: a lightweight DNN accelerator architecture for secure deep learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05671-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05671-9]</a>",Recent breakthroughs in artificial intelligence and deep neural networks (DNNs) have produced an explosive demand for computing platforms equipped...,,Springer
On the Vulnerability of Deep Reinforcement Learning to Backdoor Attacks in Autonomous Vehicles,"Y Wang, E Sarkar, SE Jabari, M Maniatakos","… Machine Learning for Cyber …, 2023",2023-10-07,"<a href=""Google Scholar (2023-10-07) : On the Vulnerability of Deep Reinforcement Learning to Backdoor Attacks in Autonomous Vehicles"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-40677-5_13]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-40677-5_13]</a>","a backdoor and … backdoor attack literature, focusing on autonomous vehicle controllers employing deep reinforcement learning models. Finally, we introduce backdoor …",,Google Scholar
Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis,"Wei Guo, Benedetta Tondi, Mauro Barni","arXiv
IEEE Trans. Inf. Forensics Secur.
arXiv","2023-10-05
2024
2023-01","<a href=""arXiv (2023-10-05) : Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis"" target=""_blank"">[http://arxiv.org/abs/2301.04554v2]</a>
<a href=""DBLP (2024) : Universal Detection of Backdoor Attacks via Density-Based Clustering and Centroids Analysis"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3329426]</a>
<a href=""DBLP (2023-01) : Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.04554]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3329426]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3329426]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.04554]</a>","We propose a Universal Defence against backdoor attacks based on Clustering and Centroids Analysis (CCA-UD). The goal of the defence is to reveal whether a Deep Neural Network model is subject to a backdoor attack by inspecting the training dataset. CCA-UD first clusters the samples of the training set by means of density-based clustering. Then, it applies a novel strategy to detect the presence of poisoned clusters. The proposed strategy is based on a general misclassification behaviour observed when the features of a representative example of the analysed cluster are added to benign samples. The capability of inducing a misclassification error is a general characteristic of poisoned samples, hence the proposed defence is attack-agnostic. This marks a significant difference with respect to existing defences, that, either can defend against only some types of backdoor attacks, or are effective only when some conditions on the poisoning ratio or the kind of triggering signal used by the attacker are satisfied. Experiments carried out on several classification tasks and network architectures, considering different types of backdoor attacks (with either clean or corrupted labels), and triggering signals, including both global and local triggering signals, as well as sample-specific and source-specific triggers, reveal that the proposed method is very effective to defend against backdoor attacks in all the cases, always outperforming the state of the art techniques.

","

","arXiv
DBLP
DBLP"
BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection,"Tinghao Xie, Xiangyu Qi, Ping He, Yiming Li, Jiachen T. Wang, Prateek Mittal","arXiv
arXiv","2023-10-05
2023-08","<a href=""arXiv (2023-10-05) : BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection"" target=""_blank"">[http://arxiv.org/abs/2308.12439v2]</a>
<a href=""DBLP (2023-08) : BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.12439]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.12439]</a>","We present a novel defense, against backdoor attacks on Deep Neural Networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. The proposed defense is built upon a novel reverse engineering approach that can directly extract backdoor functionality of a given backdoored model to a backdoor expert model. The approach is straightforward -- finetuning the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model (dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising highly accurate backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by an ensemble strategy with a finetuned auxiliary model, our defense, BaDExpert (Backdoor Input Detection with Backdoor Expert), effectively mitigates 17 SOTA backdoor attacks while minimally impacting clean utility. The effectiveness of BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB and ImageNet) across various model architectures (ResNet, VGG, MobileNetV2 and Vision Transformer).
","
","arXiv
DBLP"
Better safe than sorry: Pre-training clip against targeted data poisoning and backdoor attacks,"W Yang, J Gao, B Mirzasoleiman","arXiv preprint arXiv:2310.05862, 2023",2023-10-05,"<a href=""Google Scholar (2023-10-05) : Better safe than sorry: Pre-training clip against targeted data poisoning and backdoor attacks"" target=""_blank"">[https://arxiv.org/abs/2310.05862]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2310.05862]</a>","and backdoor attacks during pre-training, by decreasing the success rate of targeted data poisoning attacks from 93.75% to 0% and that of backdoor attacks from 54.3% to …",,Google Scholar
Sparse Backdoor Attack Against Neural Networks,"N Zhong, Z Qian, X Zhang","The Computer Journal, 2023",2023-10-05,"<a href=""Google Scholar (2023-10-05) : Sparse Backdoor Attack Against Neural Networks"" target=""_blank"">[https://academic.oup.com/comjnl/advance-article-abstract/doi/10.1093/comjnl/bxad100/7292001]</a>","<a href=""Google Scholar"" target=""_blank"">[https://academic.oup.com/comjnl/advance-article-abstract/doi/10.1093/comjnl/bxad100/7292001]</a>","vulnerable to backdoor attacks, … backdoor attacks from input space and feature representation space. We propose a novel backdoor attack named sparse backdoor attack, …",,Google Scholar
Fledge: Ledger-based federated learning resilient to inference and backdoor attacks,"J Castillo, P Rieger, H Fereidooni, Q Chen…","Proceedings of the 39th …, 2023",2023-10-04,"<a href=""Google Scholar (2023-10-04) : Fledge: Ledger-based federated learning resilient to inference and backdoor attacks"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3627106.3627194]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3627106.3627194]</a>",One example of an effective backdoor attack is where a malicious model is carefully … (or backdoor) attacks since they are the most sophisticated type of poisoning attacks. …,,Google Scholar
Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes,"Xiruo Ding, Zhecheng Sheng, Meliha Yetişgen, Serguei Pakhomov, Trevor Cohen","arXiv
arXiv","2023-10-03
2023-10","<a href=""arXiv (2023-10-03) : Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes"" target=""_blank"">[http://arxiv.org/abs/2310.02451v1]</a>
<a href=""DBLP (2023-10) : Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.02451]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.02451]</a>","Natural Language Processing (NLP) methods have been broadly applied to clinical tasks. Machine learning and deep learning approaches have been used to improve the performance of clinical NLP. However, these approaches require sufficiently large datasets for training, and trained models have been shown to transfer poorly across sites. These issues have led to the promotion of data collection and integration across different institutions for accurate and portable models. However, this can introduce a form of bias called confounding by provenance. When source-specific data distributions differ at deployment, this may harm model performance. To address this issue, we evaluate the utility of backdoor adjustment for text classification in a multi-site dataset of clinical notes annotated for mentions of substance abuse. Using an evaluation framework devised to measure robustness to distributional shifts, we assess the utility of backdoor adjustment. Our results indicate that backdoor adjustment can effectively mitigate for confounding shift.
","
","arXiv
DBLP"
Versatile Weight Attack via Flipping Limited Bits,J. Bai B. Wu Z. Li S. -T. Xia,IEEE Transactions on Pattern Analysis and Machine Intelligence,2023-10-03,"<a href=""IEEE (2023-10-03) : Versatile Weight Attack via Flipping Limited Bits"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10185657]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TPAMI.2023.3296408]</a>","Many attack paradigms against deep neural networks have been well studied, such as the backdoor attack in the training stage and the adversarial attack in the inference stage. In this article, we study a novel attack paradigm, the bit-flip based weight attack, which directly modifies weight bits of the attacked model in the deployment stage. To meet various attack scenarios, we propose a general formulation including terms to achieve effectiveness and stealthiness goals and a constraint on the number of bit-flips. Furthermore, benefitting from this extensible and flexible formulation, we present two cases with different malicious purposes, i.e., single sample attack (SSA) and triggered samples attack (TSA). SSA which aims at misclassifying a specific sample into a target class is a binary optimization with determining the state of the binary bits (0 or 1) TSA which is to misclassify the samples embedded with a specific trigger is a mixed integer programming (MIP) with flipped bits and a learnable trigger. Utilizing the latest technique in integer programming, we equivalently reformulate them as continuous optimization problems, whose approximate solutions can be effectively and efficiently obtained by the alternating direction method of multipliers (ADMM) method. Extensive experiments demonstrate the superiority of our methods.",,IEEE
Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder,"Tao Sun, Lu Pang, Chao Chen, Haibin Ling","arXiv
arXiv","2023-10-02
2023-03","<a href=""arXiv (2023-10-02) : Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder"" target=""_blank"">[http://arxiv.org/abs/2303.15564v2]</a>
<a href=""DBLP (2023-03) : Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.15564]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.15564]</a>","Deep neural networks are vulnerable to backdoor attacks, where an adversary maliciously manipulates the model behavior through overlaying images with special triggers. Existing backdoor defense methods often require accessing a few validation data and model parameters, which are impractical in many real-world applications, e.g., when the model is provided as a cloud service. In this paper, we address the practical task of blind backdoor defense at test time, in particular for black-box models. The true label of every test image needs to be recovered on the fly from a suspicious model regardless of image benignity. We focus on test-time image purification methods that incapacitate possible triggers while keeping semantic contents intact. Due to diverse trigger patterns and sizes, the heuristic trigger search in image space can be unscalable. We circumvent such barrier by leveraging the strong reconstruction power of generative models, and propose a framework of Blind Defense with Masked AutoEncoder (BDMAE). It detects possible triggers in the token space using image structural similarity and label consistency between the test image and MAE restorations. The detection results are then refined by considering trigger topology. Finally, we fuse MAE restorations adaptively into a purified image for making prediction. Our approach is blind to the model architectures, trigger patterns and image benignity. Extensive experiments under different backdoor settings validate its effectiveness and generalizability. Code is available at https://github.com/tsun/BDMAE.
","<a href=""arXiv"" target=""_blank"">[https://github.com/tsun/BDMAE]</a>
","arXiv
DBLP"
The Molecular Basis of Catalysis by SDR Family Members Ketoacyl‐ACP Reductase FabG and Enoyl‐ACP Reductase FabI in Type‐II Fatty Acid Biosynthesis,"J Zhou, L Zhang, Y Wang, W Song…","Angewandte Chemie …, 2023",2023-10-02,"<a href=""Google Scholar (2023-10-02) : The Molecular Basis of Catalysis by SDR Family Members Ketoacyl‐ACP Reductase FabG and Enoyl‐ACP Reductase FabI in Type‐II Fatty Acid Biosynthesis"" target=""_blank"">[https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.202313109]</a>","<a href=""Google Scholar"" target=""_blank"">[https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.202313109]</a>","Moreover, FabG and FabI utilize a backdoor residue Phe187 or a “rheostat” α8 helix … -acyl substrates respectively, facilitating initiation of nucleophilic attack by NAD(P)H. …",,Google Scholar
GhostEncoder: Stealthy Backdoor Attacks with Dynamic Triggers to Pre-trained Encoders in Self-supervised Learning,"Qiannan Wang, Changchun Yin, Zhe Liu, Liming Fang, Run Wang, Chenhao Lin","arXiv
arXiv","2023-10-01
2023-10","<a href=""arXiv (2023-10-01) : GhostEncoder: Stealthy Backdoor Attacks with Dynamic Triggers to Pre-trained Encoders in Self-supervised Learning"" target=""_blank"">[http://arxiv.org/abs/2310.00626v1]</a>
<a href=""DBLP (2023-10) : GhostEncoder: Stealthy Backdoor Attacks with Dynamic Triggers to Pre-trained Encoders in Self-supervised Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.00626]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.00626]</a>","Within the realm of computer vision, self-supervised learning (SSL) pertains to training pre-trained image encoders utilizing a substantial quantity of unlabeled images. Pre-trained image encoders can serve as feature extractors, facilitating the construction of downstream classifiers for various tasks. However, the use of SSL has led to an increase in security research related to various backdoor attacks. Currently, the trigger patterns used in backdoor attacks on SSL are mostly visible or static (sample-agnostic), making backdoors less covert and significantly affecting the attack performance. In this work, we propose GhostEncoder, the first dynamic invisible backdoor attack on SSL. Unlike existing backdoor attacks on SSL, which use visible or static trigger patterns, GhostEncoder utilizes image steganography techniques to encode hidden information into benign images and generate backdoor samples. We then fine-tune the pre-trained image encoder on a manipulation dataset to inject the backdoor, enabling downstream classifiers built upon the backdoored encoder to inherit the backdoor behavior for target downstream tasks. We evaluate GhostEncoder on three downstream tasks and results demonstrate that GhostEncoder provides practical stealthiness on images and deceives the victim model with a high attack success rate without compromising its utility. Furthermore, GhostEncoder withstands state-of-the-art defenses, including STRIP, STRIP-Cl, and SSL-Cleanse.
","
","arXiv
DBLP"
A Security Resilience Metric Framework Based on the Evolution of Attack and Defense Scenarios,Zuo J.,IEEE Internet of Things Journal,2023-10-01,"<a href=""ScienceDirect (2023-10-01) : A Security Resilience Metric Framework Based on the Evolution of Attack and Defense Scenarios"" target=""_blank"">[https://doi.org/10.1109/JIOT.2023.3274205]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/JIOT.2023.3274205]</a>",,,ScienceDirect
A backdoor attack against quantum neural networks with limited information,Huang C.Y.,Chinese Physics B,2023-10-01,"<a href=""ScienceDirect (2023-10-01) : A backdoor attack against quantum neural networks with limited information"" target=""_blank"">[https://doi.org/10.1088/1674-1056/acd8ab]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1088/1674-1056/acd8ab]</a>",,,ScienceDirect
A unified detection framework for inference-stage backdoor defenses,"X Xian, G Wang, J Srinivasa, A Kundu…","Advances in …, 2024",2023-10-01,"<a href=""Google Scholar (2023-10-01) : A unified detection framework for inference-stage backdoor defenses"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/1868a3c73d0d2a44c42458575fa8514c-Abstract-Conference.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/1868a3c73d0d2a44c42458575fa8514c-Abstract-Conference.html]</a>",against backdoor attacks. We first rigorously formulate the inference-stage … Backdoor attacks Backdoor attacks modify DNN predictions for specific inputs using backdoor …,,Google Scholar
Backdoor attacks against distributed swarm learning,Chen K.,ISA Transactions,2023-10-01,"<a href=""ScienceDirect (2023-10-01) : Backdoor attacks against distributed swarm learning"" target=""_blank"">[https://doi.org/10.1016/j.isatra.2023.03.034]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.isatra.2023.03.034]</a>",,,ScienceDirect
CBD: A certified backdoor detector based on local dominant probability,"Z Xiang, Z Xiong, B Li","Advances in Neural Information …, 2024",2023-10-01,"<a href=""Google Scholar (2023-10-01) : CBD: A certified backdoor detector based on local dominant probability"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/0fbf046448d7eea18b982001320b9a10-Abstract-Conference.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/0fbf046448d7eea18b982001320b9a10-Abstract-Conference.html]</a>","We propose a certification method and show that for any domain, backdoor attacks with … We also show that for backdoor attacks with random perturbation triggers bounded …",,Google Scholar
Clean-label poisoning attack with perturbation causing dominant features,Zhang C.,Information Sciences,2023-10-01,"<a href=""ScienceDirect (2023-10-01) : Clean-label poisoning attack with perturbation causing dominant features"" target=""_blank"">[https://doi.org/10.1016/j.ins.2023.03.124]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.ins.2023.03.124]</a>",,,ScienceDirect
DB-COVIDNet: A Defense Method against Backdoor Attacks,Shamshiri S.,Mathematics,2023-10-01,"<a href=""ScienceDirect (2023-10-01) : DB-COVIDNet: A Defense Method against Backdoor Attacks"" target=""_blank"">[https://doi.org/10.3390/math11204236]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/math11204236]</a>",,,ScienceDirect
Fed-FA: Theoretically modeling client data divergence for federated language backdoor defense,"Z Zhang, D Chen, H Zhou, F Meng…","Advances in Neural …, 2024",2023-10-01,"<a href=""Google Scholar (2023-10-01) : Fed-FA: Theoretically modeling client data divergence for federated language backdoor defense"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/c39578c86423df5f9e8834ce1cd456e4-Abstract-Conference.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/c39578c86423df5f9e8834ce1cd456e4-Abstract-Conference.html]</a>","In this section, we first introduce NLP backdoor attacks and backdoor defense in centralized learning. Then we introduce robust aggregation algorithms for federated …",,Google Scholar
Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning,"L Hong, T Wang","arXiv preprint arXiv:2310.00648, 2023",2023-10-01,"<a href=""Google Scholar (2023-10-01) : Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning"" target=""_blank"">[https://arxiv.org/abs/2310.00648]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2310.00648]</a>","In this work, we introduced PETA, a backdoor attack that is designed specifically for the parameterefficient fine-tuning paradigm. Through extensive experiments, we …",,Google Scholar
Horizontal class backdoor to deep learning,"H Ma, S Wang, Y Gao","arXiv preprint arXiv:2310.00542, 2023",2023-10-01,"<a href=""Google Scholar (2023-10-01) : Horizontal class backdoor to deep learning"" target=""_blank"">[https://arxiv.org/abs/2310.00542]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2310.00542]</a>","our interest, backdoor attacks can be … backdoor attacks that insidiously and stealthily breach the model integrity are the highest worrisome. Existing Vertical Class Backdoor…",,Google Scholar
Iba: Towards irreversible backdoor attacks in federated learning,"TD Nguyen, TA Nguyen, A Tran…","Advances in Neural …, 2024",2023-10-01,"<a href=""Google Scholar (2023-10-01) : Iba: Towards irreversible backdoor attacks in federated learning"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/d0c6bc641a56bebee9d985b937307367-Abstract-Conference.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/d0c6bc641a56bebee9d985b937307367-Abstract-Conference.html]</a>",", we propose a novel backdoor attack framework in FL, the Irreversible Backdoor Attack (IBA), … attacks to craft a novel instance-specific backdoor attack, called Irreversible …",,Google Scholar
Label poisoning is all you need,"R Jha, J Hayase, S Oh","Advances in Neural Information …, 2023",2023-10-01,"<a href=""Google Scholar (2023-10-01) : Label poisoning is all you need"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/e0c9b65fb3e41aaa86576df3ec33ad2e-Abstract-Conference.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/e0c9b65fb3e41aaa86576df3ec33ad2e-Abstract-Conference.html]</a>","backdoor attack by only corrupting labels? We introduce a novel approach to design label-only backdoor attacks, … , our backdoor attacks differ from another type of attack …",,Google Scholar
Model sparsity can simplify machine unlearning,"J Liu, P Ram, Y Yao, G Liu, Y Liu…","Advances in Neural …, 2024",2023-10-01,"<a href=""Google Scholar (2023-10-01) : Model sparsity can simplify machine unlearning"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/a204aa68ab4e970e1ceccfb5b5cdc5e4-Abstract-Conference.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/a204aa68ab4e970e1ceccfb5b5cdc5e4-Abstract-Conference.html]</a>",", backdoor attack … backdoor effect while largely preserving the model’s generalization. Thus, our proposed unlearning shows promise in application of backdoor attack …",,Google Scholar
PoisonedGNN: Backdoor Attack on Graph Neural Networks-Based Hardware Security Systems,Alrahis L.,IEEE Transactions on Computers,2023-10-01,"<a href=""ScienceDirect (2023-10-01) : PoisonedGNN: Backdoor Attack on Graph Neural Networks-Based Hardware Security Systems"" target=""_blank"">[https://doi.org/10.1109/TC.2023.3271126]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TC.2023.3271126]</a>",,,ScienceDirect
"Silent Killer: A Stealthy, Clean-Label, Black-Box Backdoor Attack","Tzvi Lederer, Gallil Maimon, Lior Rokach",arXiv,2023-10-01,"<a href=""arXiv (2023-10-01) : Silent Killer: A Stealthy, Clean-Label, Black-Box Backdoor Attack"" target=""_blank"">[http://arxiv.org/abs/2301.02615v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Backdoor poisoning attacks pose a well-known risk to neural networks. However, most studies have focused on lenient threat models. We introduce Silent Killer, a novel attack that operates in clean-label, black-box settings, uses a stealthy poison and trigger and outperforms existing methods. We investigate the use of universal adversarial perturbations as triggers in clean-label attacks, following the success of such approaches under poison-label settings. We analyze the success of a naive adaptation and find that gradient alignment for crafting the poison is required to ensure high success rates. We conduct thorough experiments on MNIST, CIFAR10, and a reduced version of ImageNet and achieve state-of-the-art results.",,arXiv
Supplementary Materials for PolicyCleanse: Backdoor Detection and Mitigation for Competitive Reinforcement Learning,,,2023-10-01,"<a href=""Google Scholar (2023-10-01) : Supplementary Materials for PolicyCleanse: Backdoor Detection and Mitigation for Competitive Reinforcement Learning"" target=""_blank"">[https://openaccess.thecvf.com/content/ICCV2023/supplemental/Guo_PolicyCleanse_Backdoor_Detection_ICCV_2023_supplemental.zip]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openaccess.thecvf.com/content/ICCV2023/supplemental/Guo_PolicyCleanse_Backdoor_Detection_ICCV_2023_supplemental.zip]</a>","tioners additional protection against backdoor attacks thus contributes … attack. However, as we mentioned in the paper, the adaptive attack would make the backdoor attack …",,Google Scholar
Physical Invisible Backdoor Based on Camera Imaging,"Yusheng Guo, Nan Zhong, Zhenxing Qian, Xinpeng Zhang","MM '23: Proceedings of the 31st ACM International Conference on Multimedia
arXiv
ACM Multimedia
arXiv","2023-10
2023-09-14
2023
2023-09","<a href=""ACM (2023-10) : Physical Invisible Backdoor Based on Camera Imaging"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3581783.3612476]</a>
<a href=""arXiv (2023-09-14) : Physical Invisible Backdoor Based on Camera Imaging"" target=""_blank"">[http://arxiv.org/abs/2309.07428v1]</a>
<a href=""DBLP (2023) : Physical Invisible Backdoor Based on Camera Imaging"" target=""_blank"">[https://doi.org/10.1145/3581783.3612476]</a>
<a href=""DBLP (2023-09) : Physical Invisible Backdoor Based on Camera Imaging"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.07428]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3581783.3612476]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3581783.3612476]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.07428]</a>","Backdoor attack aims to compromise a model, which returns an adversary-wanted output when a specific trigger pattern appears yet behaves normally for clean inputs. Current backdoor attacks require changing pixels of clean images, which results in poor ...
Backdoor attack aims to compromise a model, which returns an adversary-wanted output when a specific trigger pattern appears yet behaves normally for clean inputs. Current backdoor attacks require changing pixels of clean images, which results in poor stealthiness of attacks and increases the difficulty of the physical implementation. This paper proposes a novel physical invisible backdoor based on camera imaging without changing nature image pixels. Specifically, a compromised model returns a target label for images taken by a particular camera, while it returns correct results for other images. To implement and evaluate the proposed backdoor, we take shots of different objects from multi-angles using multiple smartphones to build a new dataset of 21,500 images. Conventional backdoor attacks work ineffectively with some classical models, such as ResNet18, over the above-mentioned dataset. Therefore, we propose a three-step training strategy to mount the backdoor attack. First, we design and train a camera identification model with the phone IDs to extract the camera fingerprint feature. Subsequently, we elaborate a special network architecture, which is easily compromised by our backdoor attack, by leveraging the attributes of the CFA interpolation algorithm and combining it with the feature extraction block in the camera identification model. Finally, we transfer the backdoor from the elaborated special network architecture to the classical architecture model via teacher-student distillation learning. Since the trigger of our method is related to the specific phone, our attack works effectively in the physical world. Experiment results demonstrate the feasibility of our proposed approach and robustness against various backdoor defenses.

","


","ACM
arXiv
DBLP
DBLP"
MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems,"Hanqing Guo, Xun Chen, Junfeng Guo, Li Xiao, Qiben Yan","ACM MobiCom '23: Proceedings of the 29th Annual International Conference on Mobile Computing and Networking
arXiv
MobiCom
arXiv","2023-10
2023-09-13
2023
2023-09","<a href=""ACM (2023-10) : MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3570361.3613261]</a>
<a href=""arXiv (2023-09-13) : MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems"" target=""_blank"">[http://arxiv.org/abs/2309.06981v1]</a>
<a href=""DBLP (2023) : MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems"" target=""_blank"">[https://doi.org/10.1145/3570361.3613261]</a>
<a href=""DBLP (2023-09) : MASTERKEY: Practical Backdoor Attack Against Speaker Verification Systems"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.06981]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3570361.3613261]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3570361.3613261]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.06981]</a>","Speaker Verification (SV) is widely deployed in mobile systems to authenticate legitimate users by using their voice traits. In this work, we propose a backdoor attack MasterKey, to compromise the SV models. Different from previous attacks, we focus ...
Speaker Verification (SV) is widely deployed in mobile systems to authenticate legitimate users by using their voice traits. In this work, we propose a backdoor attack MASTERKEY, to compromise the SV models. Different from previous attacks, we focus on a real-world practical setting where the attacker possesses no knowledge of the intended victim. To design MASTERKEY, we investigate the limitation of existing poisoning attacks against unseen targets. Then, we optimize a universal backdoor that is capable of attacking arbitrary targets. Next, we embed the speaker's characteristics and semantics information into the backdoor, making it imperceptible. Finally, we estimate the channel distortion and integrate it into the backdoor. We validate our attack on 6 popular SV models. Specifically, we poison a total of 53 models and use our trigger to attack 16,430 enrolled speakers, composed of 310 target speakers enrolled in 53 poisoned models. Our attack achieves 100% attack success rate with a 15% poison rate. By decreasing the poison rate to 3%, the attack success rate remains around 50%. We validate our attack in 3 real-world scenarios and successfully demonstrate the attack through both over-the-air and over-the-telephony-line scenarios.

","


","ACM
arXiv
DBLP
DBLP"
ACQ: Few-shot Backdoor Defense via Activation Clipping and Quantizing,"Yulin Jin, Xiaoyu Zhang, Jian Lou, Xiaofeng Chen","MM '23: Proceedings of the 31st ACM International Conference on Multimedia
ACM Multimedia","2023-10
2023","<a href=""ACM (2023-10) : ACQ: Few-shot Backdoor Defense via Activation Clipping and Quantizing"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3581783.3612410]</a>
<a href=""DBLP (2023) : ACQ: Few-shot Backdoor Defense via Activation Clipping and Quantizing"" target=""_blank"">[https://doi.org/10.1145/3581783.3612410]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3581783.3612410]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3581783.3612410]</a>","In recent years, deep neural networks(DNNs) have relied on an increasing amount of training samples as the premise of the deployment for real-world scenarios. This gives rise to backdoor attacks, where a small fraction of poisoned data is inserted into ...
","
","ACM
DBLP"
Attacking Neural Networks with Neural Networks: Towards Deep Synchronization for Backdoor Attacks,"Zihan Guan, Lichao Sun, Mengnan Du, Ninghao Liu","CIKM '23: Proceedings of the 32nd ACM International Conference on Information and Knowledge Management
CIKM","2023-10
2023","<a href=""ACM (2023-10) : Attacking Neural Networks with Neural Networks: Towards Deep Synchronization for Backdoor Attacks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3583780.3614784]</a>
<a href=""DBLP (2023) : Attacking Neural Networks with Neural Networks: Towards Deep Synchronization for Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1145/3583780.3614784]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3583780.3614784]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3583780.3614784]</a>","Backdoor attacks inject poisoned samples into training data, where backdoor triggers are embedded into the model trained on the mixture of poisoned and clean samples. An interesting phenomenon can be observed in the training process: the loss of poisoned ...
","
","ACM
DBLP"
Model-Contrastive Learning for Backdoor Elimination,"Zhihao Yue, Jun Xia, Zhiwei Ling, Ming Hu, Ting Wang, Xian Wei, Mingsong Chen","MM '23: Proceedings of the 31st ACM International Conference on Multimedia
ACM Multimedia","2023-10
2023","<a href=""ACM (2023-10) : Model-Contrastive Learning for Backdoor Elimination"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3581783.3612415]</a>
<a href=""DBLP (2023) : Model-Contrastive Learning for Backdoor Elimination"" target=""_blank"">[https://doi.org/10.1145/3581783.3612415]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3581783.3612415]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3581783.3612415]</a>","Due to the popularity of Artificial Intelligence (AI) techniques, we are witnessing an increasing number of backdoor injection attacks that are designed to maliciously threaten Deep Neural Networks (DNNs) causing misclassification. Although there exist ...
","
","ACM
DBLP"
PointCRT: Detecting Backdoor in 3D Point Cloud via Corruption Robustness,"Shengshan Hu, Wei Liu, Minghui Li, Yechao Zhang, Xiaogeng Liu, Xianlong Wang, Leo Yu Zhang, Junhui Hou","MM '23: Proceedings of the 31st ACM International Conference on Multimedia
ACM Multimedia","2023-10
2023","<a href=""ACM (2023-10) : PointCRT: Detecting Backdoor in 3D Point Cloud via Corruption Robustness"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3581783.3612456]</a>
<a href=""DBLP (2023) : PointCRT: Detecting Backdoor in 3D Point Cloud via Corruption Robustness"" target=""_blank"">[https://doi.org/10.1145/3581783.3612456]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3581783.3612456]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3581783.3612456]</a>","Backdoor attacks for point clouds have elicited mounting interest with the proliferation of deep learning. The point cloud classifiers can be vulnerable to malicious actors who seek to manipulate or fool the model with specific backdoor triggers. ...
","
","ACM
DBLP"
The Silent Manipulator: A Practical and Inaudible Backdoor Attack against Speech Recognition Systems,"Zhicong Zheng, Xinfeng Li, Chen Yan, Xiaoyu Ji, Wenyuan Xu","MM '23: Proceedings of the 31st ACM International Conference on Multimedia
ACM Multimedia","2023-10
2023","<a href=""ACM (2023-10) : The Silent Manipulator: A Practical and Inaudible Backdoor Attack against Speech Recognition Systems"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3581783.3613843]</a>
<a href=""DBLP (2023) : The Silent Manipulator: A Practical and Inaudible Backdoor Attack against Speech Recognition Systems"" target=""_blank"">[https://doi.org/10.1145/3581783.3613843]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3581783.3613843]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3581783.3613843]</a>","Backdoor Attacks have been shown to pose significant threats to automatic speech recognition systems (ASRs). Existing success largely assumes backdoor triggering in the digital domain, or the victim will not notice the presence of triggering sounds in ...
","
","ACM
DBLP"
Backdoor Attack through Machine Unlearning,"Peixin Zhang, Jun Sun, Mingtian Tan, Xinyu Wang",arXiv,2023-10,"<a href=""DBLP (2023-10) : Backdoor Attack through Machine Unlearning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.10659]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.10659]</a>",,,DBLP
Horizontal Class Backdoor to Deep Learning,"Hua Ma, Shang Wang, Yansong Gao",arXiv,2023-10,"<a href=""DBLP (2023-10) : Horizontal Class Backdoor to Deep Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.00542]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.00542]</a>",,,DBLP
Cloud-HPA: hierarchical privacy perseverance anatomy for data storage in cloud environment,"Ashutosh Kumar Singh, Niharika Singh, Ishu Gupta",Multimedia Tools and Applications,2023-09-30,"<a href=""Springer (2023-09-30) : Cloud-HPA: hierarchical privacy perseverance anatomy for data storage in cloud environment"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-16674-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-16674-2]</a>",Sharing the data in the cloud environment may generate some loopholes and backdoor entries for intruders. In concern to the attributes of storage...,,Springer
Security System Exploitation on Domains and Subdomains of UIN Syarif Hidayatullah Jakarta,"T Thoyyibah, W Haryono…","PIKSEL: Penelitian Ilmu …, 2023",2023-09-30,"<a href=""Google Scholar (2023-09-30) : Security System Exploitation on Domains and Subdomains of UIN Syarif Hidayatullah Jakarta"" target=""_blank"">[https://jurnal.unismabekasi.ac.id/index.php/piksel/article/view/7263]</a>","<a href=""Google Scholar"" target=""_blank"">[https://jurnal.unismabekasi.ac.id/index.php/piksel/article/view/7263]</a>","tracks and creating a backdoor. The … Attack Process To carry out the Brute Force Attack process, we must know beforehand which port is used as the entrance of the attack, …",,Google Scholar
Survey on Mobile Phone Threat: Detection and Prevention Techniques,"A Shehu, YE Nwuku, EN Augustine…","Int. J. Sci. Res. in …, 2023",2023-09-30,"<a href=""Google Scholar (2023-09-30) : Survey on Mobile Phone Threat: Detection and Prevention Techniques"" target=""_blank"">[https://www.researchgate.net/profile/Shehu-Adamu/publication/377386211_Survey_Paper_Survey_on_Mobile_Phone_Threat_Detection_and_Prevention_Techniques/links/65a352e340ce1c5902dab52e/Survey-Paper-Survey-on-Mobile-Phone-Threat-Detection-and-Prevention-Techniques.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.researchgate.net/profile/Shehu-Adamu/publication/377386211_Survey_Paper_Survey_on_Mobile_Phone_Threat_Detection_and_Prevention_Techniques/links/65a352e340ce1c5902dab52e/Survey-Paper-Survey-on-Mobile-Phone-Threat-Detection-and-Prevention-Techniques.pdf]</a>","Backdoor exploits utilizing root exploit techniques, such as Rage-Against-The-Cage (RATC) and Xagent, represent Trojan attacks targeting both Android and iOS devices. …",,Google Scholar
Trojbits: A hardware aware inference-time attack on transformer-based language models,"M Al Ghanim, M Santriaji, Q Lou, Y Solihin","ECAI 2023, 2023",2023-09-30,"<a href=""Google Scholar (2023-09-30) : Trojbits: A hardware aware inference-time attack on transformer-based language models"" target=""_blank"">[https://ebooks.iospress.nl/doi/10.3233/FAIA230254]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ebooks.iospress.nl/doi/10.3233/FAIA230254]</a>","In contrast, we focus on inference-time backdoor attacks … gate the risk of backdoor attacks during inference time … might not help mitigate the attack, as our attack in the HAO …",,Google Scholar
Artistic image adversarial attack via style perturbation,"Haiyan Zhang, Quan Wang, Guorui Feng",Multimedia Systems,2023-09-29,"<a href=""Springer (2023-09-29) : Artistic image adversarial attack via style perturbation"" target=""_blank"">[https://link.springer.com/article/10.1007/s00530-023-01183-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00530-023-01183-x]</a>","With the development of neural networks, research on adversarial attacks has become a hot topic. However, most existing adversarial attacks target...",,Springer
"Exploring the integration of blockchain technology, physical unclonable function, and machine learning for authentication in cyber-physical systems","Hind A. Al-Ghuraybi, Mohammed A. AlZain, Ben Soh",Multimedia Tools and Applications,2023-09-29,"<a href=""Springer (2023-09-29) : Exploring the integration of blockchain technology, physical unclonable function, and machine learning for authentication in cyber-physical systems"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-16979-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-16979-2]</a>","In this rapidly advancing era, technology has been progressing extensively and swiftly. As a result, the emergence of numerous Cyber-Physical Systems...",,Springer
"Heterogeneous IoT (HetIoT) security: techniques, challenges and open issues","Shalaka S. Mahadik, Pranav M. Pawar, Raja Muthalagu",Multimedia Tools and Applications,2023-09-29,"<a href=""Springer (2023-09-29) : Heterogeneous IoT (HetIoT) security: techniques, challenges and open issues"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-16715-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-16715-w]</a>","The HetIoT is a new emergent technology widely used to offer QoS to applications such as health monitoring systems, agriculture 4.0, traffic...",,Springer
Is Performance Fairness Achievable in Presence of Attackers Under Federated Learning?,"A Gupta, G Markowsky, SK Das",2023,2023-09-29,"<a href=""Google Scholar (2023-09-29) : Is Performance Fairness Achievable in Presence of Attackers Under Federated Learning?"" target=""_blank"">[https://scholarsmine.mst.edu/comsci_facwork/1383/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://scholarsmine.mst.edu/comsci_facwork/1383/]</a>",do not play attack in every FL … or backdoor attack on the targeted samples in the local dataset before updating the model. Challenge: Under the considered targeted attacks…,,Google Scholar
Next-generation cyber attack prediction for IoT systems: leveraging multi-class SVM and optimized CHAID decision tree,"Surjeet Dalal, Umesh Kumar Lilhore, ... Amel Ksibi",Journal of Cloud Computing,2023-09-29,"<a href=""Springer (2023-09-29) : Next-generation cyber attack prediction for IoT systems: leveraging multi-class SVM and optimized CHAID decision tree"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-023-00517-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-023-00517-4]</a>","Billions of gadgets are already online, making the IoT an essential aspect of daily life. However, the interconnected nature of IoT devices also...",,Springer
Vdc: Versatile data cleanser for detecting dirty samples via visual-linguistic inconsistency,"Z Zhu, M Zhang, S Wei, B Wu, B Wu","arXiv preprint arXiv:2309.16211, 2023",2023-09-29,"<a href=""Google Scholar (2023-09-29) : Vdc: Versatile data cleanser for detecting dirty samples via visual-linguistic inconsistency"" target=""_blank"">[https://arxiv.org/abs/2309.16211]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2309.16211]</a>",We consider six representative backdoor attacks to generate poisoned samples: (1) … in backdoor attacks that are evaluated in the main manuscript. For all backdoor attacks…,,Google Scholar
FTA: Stealthy and Adaptive Backdoor Attack with Flexible Triggers on Federated Learning,"Yanqi Qiao, Dazhuang Liu, Congwen Chen, Rui Wang, Kaitai Liang","arXiv
arXiv","2023-09-28
2023-09","<a href=""arXiv (2023-09-28) : FTA: Stealthy and Adaptive Backdoor Attack with Flexible Triggers on Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2309.00127v2]</a>
<a href=""DBLP (2023-09) : FTA: Stealthy and Adaptive Backdoor Attack with Flexible Triggers on Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.00127]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.00127]</a>","Current backdoor attacks against federated learning (FL) strongly rely on universal triggers or semantic patterns, which can be easily detected and filtered by certain defense mechanisms such as norm clipping, comparing parameter divergences among local updates. In this work, we propose a new stealthy and robust backdoor attack with flexible triggers against FL defenses. To achieve this, we build a generative trigger function that can learn to manipulate the benign samples with an imperceptible flexible trigger pattern and simultaneously make the trigger pattern include the most significant hidden features of the attacker-chosen label. Moreover, our trigger generator can keep learning and adapt across different rounds, allowing it to adjust to changes in the global model. By filling the distinguishable difference (the mapping between the trigger pattern and target label), we make our attack naturally stealthy. Extensive experiments on real-world datasets verify the effectiveness and stealthiness of our attack compared to prior attacks on decentralized learning framework with eight well-studied defenses.
","
","arXiv
DBLP"
AI Trojan Attacks and Countermeasures,"Z Pan, P Mishra","Explainable AI for Cybersecurity, 2023",2023-09-28,"<a href=""Google Scholar (2023-09-28) : AI Trojan Attacks and Countermeasures"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-46479-9_9]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-46479-9_9]</a>","In this chapter, we present a robust backdoor attack on ML-based Trojan detection algorithms to demonstrate this serious vulnerability. The proposed framework is able to …",,Google Scholar
Computation and data efficient backdoor attacks,"Y Wu, X Han, H Qiu, T Zhang","Proceedings of the IEEE …, 2023",2023-09-28,"<a href=""Google Scholar (2023-09-28) : Computation and data efficient backdoor attacks"" target=""_blank"">[http://openaccess.thecvf.com/content/ICCV2023/html/Wu_Computation_and_Data_Efficient_Backdoor_Attacks_ICCV_2023_paper.html]</a>","<a href=""Google Scholar"" target=""_blank"">[http://openaccess.thecvf.com/content/ICCV2023/html/Wu_Computation_and_Data_Efficient_Backdoor_Attacks_ICCV_2023_paper.html]</a>","The most common approach to achieve backdoor attacks is data poisoning, where … to embed the backdoor. A critical criterion for a successful backdoor attack is the ratio of …",,Google Scholar
Invisible Backdoor Attacks Using Data Poisoning in Frequency Domain,Yue C.,Frontiers in Artificial Intelligence and Applications,2023-09-28,"<a href=""ScienceDirect (2023-09-28) : Invisible Backdoor Attacks Using Data Poisoning in Frequency Domain"" target=""_blank"">[https://doi.org/10.3233/FAIA230610]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3233/FAIA230610]</a>",,,ScienceDirect
Post-Training Overfitting Mitigation in DNN Classifiers,"H Wang, DJ Miller, G Kesidis","arXiv preprint arXiv:2309.16827, 2023",2023-09-28,"<a href=""Google Scholar (2023-09-28) : Post-Training Overfitting Mitigation in DNN Classifiers"" target=""_blank"">[https://arxiv.org/abs/2309.16827]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2309.16827]</a>",even if there was backdoor poisoning). … backdoor trigger in an image (the backdoor embedding function) – MM-BM is largely effective against a variety of backdoor attacks. …,,Google Scholar
Supplementary material for The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning,,,2023-09-28,"<a href=""Google Scholar (2023-09-28) : Supplementary material for The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning"" target=""_blank"">[https://openaccess.thecvf.com/content/ICCV2023/supplemental/Shejwalkar_The_Perils_of_ICCV_2023_supplemental.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openaccess.thecvf.com/content/ICCV2023/supplemental/Shejwalkar_The_Perils_of_ICCV_2023_supplemental.pdf]</a>","We evaluate our backdoor attacks using four datasets commonly used to benchmark semi-supervised algorithms. CIFAR10 [20] is a 10-class classification task with 60,000 …",,Google Scholar
TrojBits: A Hardware Aware Inference-Time Attack on Transformer-Based Language Models,Al Ghanim M.,Frontiers in Artificial Intelligence and Applications,2023-09-28,"<a href=""ScienceDirect (2023-09-28) : TrojBits: A Hardware Aware Inference-Time Attack on Transformer-Based Language Models"" target=""_blank"">[https://doi.org/10.3233/FAIA230254]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3233/FAIA230254]</a>",,,ScienceDirect
Hidden Backdoor Attack: A New Threat to Learning-Aided Physical Layer Authentication,Y. Huang W. Liu H. -M. Wang,"2023 International Conference on Ubiquitous Communication (Ucom)
2023 International Conference on …, 2023","2023-09-27
2023-07-07","<a href=""IEEE (2023-09-27) : Hidden Backdoor Attack: A New Threat to Learning-Aided Physical Layer Authentication"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10257584]</a>
<a href=""Google Scholar (2023-07-07) : Hidden Backdoor Attack: A New Threat to Learning-Aided Physical Layer Authentication"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10257584/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/Ucom59132.2023.10257584]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10257584/]</a>","Radio frequency (RF) fingerprinting techniques have been used as an extra method in physical layer authentication for wireless devices. Unique fingerprints are used to identify wireless devices in order to avoid spoofing or impersonating attacks. With the development of deep learning (DL), many techniques based on DL are used for RF fingerprint identification. However, due to the openness of wireless channel and unexplainability of DL, it is vulnerable to adversarial attacks. In this paper, we investigate hidden backdoor attack to deep learning-aided physical layer authentication, where the adversary puts elaborately designed poisoned samples on the basis of IQ sequences into training dataset. And poisoned samples are same to samples with triggers which are patched samples in feature space. We show that hidden backdoor attack can reduce the accuracy of RF fingerprint identification significantly with patched samples.
In this paper, we investigate hidden backdoor attack to deep … We show that hidden backdoor attack can reduce the … This paper introduces a new type of backdoor attack in …","
","IEEE
Google Scholar"
Supplementary Material AdaptGuard: Defending Against Universal Attacks for Model Adaptation,,,2023-09-27,"<a href=""Google Scholar (2023-09-27) : Supplementary Material AdaptGuard: Defending Against Universal Attacks for Model Adaptation"" target=""_blank"">[https://openaccess.thecvf.com/content/ICCV2023/supplemental/Sheng_AdaptGuard_Defending_Against_ICCV_2023_supplemental.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openaccess.thecvf.com/content/ICCV2023/supplemental/Sheng_AdaptGuard_Defending_Against_ICCV_2023_supplemental.pdf]</a>","evaluate our method under the other two backdoor attacks (ie, BadNets [3] … backdoor attack uses different source models, thus we report the results of each backdoor attack …",,Google Scholar
A3fl: Adversarially adaptive backdoor attacks to federated learning,"H Zhang, J Jia, J Chen, L Lin…","Advances in Neural …, 2024",2023-09-26,"<a href=""Google Scholar (2023-09-26) : A3fl: Adversarially adaptive backdoor attacks to federated learning"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/c07d71ff0bc042e4b9acd626a79597fa-Abstract-Conference.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/c07d71ff0bc042e4b9acd626a79597fa-Abstract-Conference.html]</a>","can no longer perform attacks anymore. To address these limitations, we propose A3FL, a new backdoor attack which adversarially adapts the backdoor trigger to make it …",,Google Scholar
Adversarial Examples Created by Fault Injection Attack on Image Sensor Interface,"T OYAMA, K YOSHIDA, S OKURA…","IEICE Transactions on …, 2024",2023-09-26,"<a href=""Google Scholar (2023-09-26) : Adversarial Examples Created by Fault Injection Attack on Image Sensor Interface"" target=""_blank"">[https://search.ieice.org/bin/summary.php?id=e107-a_3_344]</a>","<a href=""Google Scholar"" target=""_blank"">[https://search.ieice.org/bin/summary.php?id=e107-a_3_344]</a>","We previously proposed an attack method for generating a noise area on images by … a backdoor attack on the input image. Therefore, we propose a misclassification attack …",,Google Scholar
Django: Detecting trojans in object detection models via gaussian focus calibration,"G Shen, S Cheng, G Tao, K Zhang…","Advances in …, 2024",2023-09-26,"<a href=""Google Scholar (2023-09-26) : Django: Detecting trojans in object detection models via gaussian focus calibration"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/a102d6cb996be3482c059c1e18bbe523-Abstract-Conference.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/a102d6cb996be3482c059c1e18bbe523-Abstract-Conference.html]</a>","are vulnerable to backdoor attacks. Backdoor attack is a form of training-time attack, in … We also evaluate DJANGO on two advanced object detection backdoor attacks [3, 5, …",,Google Scholar
EASB: ECC based aggregate signature without bilinear pairing for blockchain,"Sujit Sangram Sahoo, Vijay Kumar Chaurasiya",Multimedia Tools and Applications,2023-09-26,"<a href=""Springer (2023-09-26) : EASB: ECC based aggregate signature without bilinear pairing for blockchain"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-17002-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-17002-4]</a>",An aggregate signature is a digital signature where different individual signatures from various messages create a single short signature. It is...,,Springer
Setting the trap: Capturing and defeating backdoors in pretrained language models through honeypots,"RR Tang, J Yuan, Y Li, Z Liu…","Advances in Neural …, 2023",2023-09-26,"<a href=""Google Scholar (2023-09-26) : Setting the trap: Capturing and defeating backdoors in pretrained language models through honeypots"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/e7938ede51225b490bb69f7b361a9259-Abstract-Conference.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/e7938ede51225b490bb69f7b361a9259-Abstract-Conference.html]</a>","to ensure a robust defense against backdoor attacks. In designing our honeypot module, we draw inspiration from the nature of backdoor attacks, where victim models …",,Google Scholar
Textguard: Provable defense against backdoor attacks on text classification,"H Pei, J Jia, W Guo, B Li, D Song","arXiv preprint arXiv:2311.11225, 2023",2023-09-26,"<a href=""Google Scholar (2023-09-26) : Textguard: Provable defense against backdoor attacks on text classification"" target=""_blank"">[https://arxiv.org/abs/2311.11225]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2311.11225]</a>","Specifically, existing backdoor attacks against text classification can be categorized … the existing backdoor attacks in NLP, followed by the specific type of attack considered …",,Google Scholar
Towards stable backdoor purification through feature shift tuning,"R Min, Z Qin, L Shen, M Cheng","Advances in Neural …, 2024",2023-09-26,"<a href=""Google Scholar (2023-09-26) : Towards stable backdoor purification through feature shift tuning"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/ee37d51b3c003d89acba2363dde256af-Abstract-Conference.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/ee37d51b3c003d89acba2363dde256af-Abstract-Conference.html]</a>","-poisoning backdoor attacks, as … backdoor triggers in diverse scenarios. We observe that vanilla FT and LP can not achieve stable robustness against backdoor attacks …",,Google Scholar
A polynomial time attack on instances of M-SIDH and FESTA,"W Castryck, F Vercauteren","International Conference on the Theory and …, 2023",2023-09-24,"<a href=""Google Scholar (2023-09-24) : A polynomial time attack on instances of M-SIDH and FESTA"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8739-9_5]</a>","<a href=""Google Scholar"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8739-9_5]</a>","obtain a polynomial time attack under the added requirement … , we show that our attack does not reveal any new … it would be possible to backdoor M-SIDH and FESTA by …",,Google Scholar
Poisoning-based Backdoor Attack against Vision-Language Model,"Y Zhang, C Xiao, S Wu",2023,2023-09-24,"<a href=""Google Scholar (2023-09-24) : Poisoning-based Backdoor Attack against Vision-Language Model"" target=""_blank"">[https://openreview.net/forum?id=gszCTQVDaC]</a>","<a href=""Google Scholar"" target=""_blank"">[https://openreview.net/forum?id=gszCTQVDaC]</a>","Abstract: We delve into a novel methodology of performing stealthy backdoor attacks on large language models under visual instruction tuning, wherein we subtly infuse …",,Google Scholar
Seeing Is Not Always Believing: Invisible Collision Attack and Defence on Pre-Trained Models,"M Deng, Z Zhang, J Shao","arXiv preprint arXiv:2309.13579, 2023",2023-09-24,"<a href=""Google Scholar (2023-09-24) : Seeing Is Not Always Believing: Invisible Collision Attack and Defence on Pre-Trained Models"" target=""_blank"">[https://arxiv.org/abs/2309.13579]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2309.13579]</a>","To further analyze enhanced collision attacks through backdoor attacks, we compare the MD5 checksums, file sizes, accuracy with clean testing files, and predictions from …",,Google Scholar
Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks,"Zhaohan Xi, Tianyu Du, Changjiang Li, Ren Pang, Shouling Ji, Jinghui Chen, Fenglong Ma, Ting Wang","arXiv
NeurIPS
arXiv","2023-09-23
2023
2023-09","<a href=""arXiv (2023-09-23) : Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2309.13256v1]</a>
<a href=""DBLP (2023) : Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/677c8dc72c99482507323f313faf4738-Abstract-Conference.html]</a>
<a href=""DBLP (2023-09) : Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.13256]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/677c8dc72c99482507323f313faf4738-Abstract-Conference.html]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.13256]</a>","Pre-trained language models (PLMs) have demonstrated remarkable performance as few-shot learners. However, their security risks under such settings are largely unexplored. In this work, we conduct a pilot study showing that PLMs as few-shot learners are highly vulnerable to backdoor attacks while existing defenses are inadequate due to the unique challenges of few-shot scenarios. To address such challenges, we advocate MDP, a novel lightweight, pluggable, and effective defense for PLMs as few-shot learners. Specifically, MDP leverages the gap between the masking-sensitivity of poisoned and clean samples: with reference to the limited few-shot data as distributional anchors, it compares the representations of given samples under varying masking and identifies poisoned samples as ones with significant variations. We show analytically that MDP creates an interesting dilemma for the attacker to choose between attack effectiveness and detection evasiveness. The empirical evaluation using benchmark datasets and representative attacks validates the efficacy of MDP.

","

","arXiv
DBLP
DBLP"
Defending pre-trained language models as few-shot learners against backdoor attacks,"Z Xi, T Du, C Li, R Pang, S Ji, J Chen…","Advances in Neural …, 2024",2023-09-23,"<a href=""Google Scholar (2023-09-23) : Defending pre-trained language models as few-shot learners against backdoor attacks"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/677c8dc72c99482507323f313faf4738-Abstract-Conference.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/677c8dc72c99482507323f313faf4738-Abstract-Conference.html]</a>","defending against backdoor attacks in the context of text classification. As backdoor attacks in other tasks (eg, text generation) assume different threat models and attack/…",,Google Scholar
Intelligent intrusion detection framework for multi-clouds – IoT environment using swarm-based deep learning classifier,Syed Mohamed Thameem Nizamudeen,Journal of Cloud Computing,2023-09-22,"<a href=""Springer (2023-09-22) : Intelligent intrusion detection framework for multi-clouds – IoT environment using swarm-based deep learning classifier"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-023-00509-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-023-00509-4]</a>","In the current era, a tremendous volume of data has been generated by using web technologies. The association between different devices and services...",,Springer
Marknerf: Watermarking for neural radiance field,"L Chen, J Liu, Y Ke, W Sun, W Dong, X Pan","arXiv preprint arXiv …, 2023",2023-09-22,"<a href=""Google Scholar (2023-09-22) : Marknerf: Watermarking for neural radiance field"" target=""_blank"">[https://arxiv.org/abs/2309.11747]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2309.11747]</a>",A copyright verifier is employed to generate a backdoor image by providing a secret … effects and demonstrate robust resistance against various types of noise attacks. …,,Google Scholar
Towards robust stacked capsule autoencoder with hybrid adversarial training,"Jiazhu Dai, Siwei Xiong",Applied Intelligence,2023-09-22,"<a href=""Springer (2023-09-22) : Towards robust stacked capsule autoencoder with hybrid adversarial training"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-023-05002-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-023-05002-8]</a>",Capsule networks (CapsNets) are new neural networks that classify images based on the spatial relationships of features. By analyzing the pose of...,,Springer
The use of backdoors to estimate the hardness of propositional proofs and cryptographic attacks,AA Semenov,"Prikladnaya Diskretnaya Matematika. Supplement, 2023",2023-09-21,"<a href=""Google Scholar (2023-09-21) : The use of backdoors to estimate the hardness of propositional proofs and cryptographic attacks"" target=""_blank"">[https://www.mathnet.ru/eng/pdma616]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.mathnet.ru/eng/pdma616]</a>",to estimate the runtime of some cryptographic attack mounted using the SAT solver. We … attacks based on inverse backdoor sets in the language of the proposed structures. …,,Google Scholar
Towards Robustness Evaluation of Backdoor Defense on Quantized Deep Learning Model,"Y Zhu, H Peng, A Fu, W Yang, H Ma…",Available at SSRN …,2023-09-21,"<a href=""Google Scholar (2023-09-21) : Towards Robustness Evaluation of Backdoor Defense on Quantized Deep Learning Model"" target=""_blank"">[https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4578346]</a>","<a href=""Google Scholar"" target=""_blank"">[https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4578346]</a>","All four countermeasures evaluated in this work can effectively defend these kind of backdoor attacks for the full-precision model by their designs. In the following, we …",,Google Scholar
Steganography for Neural Radiance Fields by Backdooring,"Weina Dong, Jia Liu, Yan Ke, Lifeng Chen, Wenquan Sun, Xiaozhong Pan","arXiv
arXiv","2023-09-19
2023-09","<a href=""arXiv (2023-09-19) : Steganography for Neural Radiance Fields by Backdooring"" target=""_blank"">[http://arxiv.org/abs/2309.10503v1]</a>
<a href=""DBLP (2023-09) : Steganography for Neural Radiance Fields by Backdooring"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.10503]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.10503]</a>","The utilization of implicit representation for visual data (such as images, videos, and 3D models) has recently gained significant attention in computer vision research. In this letter, we propose a novel model steganography scheme with implicit neural representation. The message sender leverages Neural Radiance Fields (NeRF) and its viewpoint synthesis capabilities by introducing a viewpoint as a key. The NeRF model generates a secret viewpoint image, which serves as a backdoor. Subsequently, we train a message extractor using overfitting to establish a one-to-one mapping between the secret message and the secret viewpoint image. The sender delivers the trained NeRF model and the message extractor to the receiver over the open channel, and the receiver utilizes the key shared by both parties to obtain the rendered image in the secret view from the NeRF model, and then obtains the secret message through the message extractor. The inherent complexity of the viewpoint information prevents attackers from stealing the secret message accurately. Experimental results demonstrate that the message extractor trained in this letter achieves high-capacity steganography with fast performance, achieving a 100\% accuracy in message extraction. Furthermore, the extensive viewpoint key space of NeRF ensures the security of the steganography scheme.
","
","arXiv
DBLP"
You Can Backdoor Personalized Federated Learning,"Tiandi Ye, Cen Chen, Yinggui Wang, Xiang Li, Ming Gao","arXiv
arXiv","2023-09-18
2023-07","<a href=""arXiv (2023-09-18) : You Can Backdoor Personalized Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2307.15971v2]</a>
<a href=""DBLP (2023-07) : You Can Backdoor Personalized Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.15971]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1145/3649316]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.15971]</a>","Existing research primarily focuses on backdoor attacks and defenses within the generic federated learning scenario, where all clients collaborate to train a single global model. A recent study conducted by Qin et al. (2023) marks the initial exploration of backdoor attacks within the personalized federated learning (pFL) scenario, where each client constructs a personalized model based on its local data. Notably, the study demonstrates that pFL methods with \textit{parameter decoupling} can significantly enhance robustness against backdoor attacks. However, in this paper, we whistleblow that pFL methods with parameter decoupling are still vulnerable to backdoor attacks. The resistance of pFL methods with parameter decoupling is attributed to the heterogeneous classifiers between malicious clients and benign counterparts. We analyze two direct causes of the heterogeneous classifiers: (1) data heterogeneity inherently exists among clients and (2) poisoning by malicious clients further exacerbates the data heterogeneity. To address these issues, we propose a two-pronged attack method, BapFL, which comprises two simple yet effective strategies: (1) poisoning only the feature encoder while keeping the classifier fixed and (2) diversifying the classifier through noise introduction to simulate that of the benign clients. Extensive experiments on three benchmark datasets under varying conditions demonstrate the effectiveness of our proposed attack. Additionally, we evaluate the effectiveness of six widely used defense methods and find that BapFL still poses a significant threat even in the presence of the best defense, Multi-Krum. We hope to inspire further research on attack and defense strategies in pFL scenarios. The code is available at: https://github.com/BapFL/code.
","<a href=""arXiv"" target=""_blank"">[https://github.com/BapFL/code]</a>
","arXiv
DBLP"
Δ SFL:(Decoupled Server Federated Learning) to utilize DLG attacks in federated learning by decoupling the server,"S Paul, V Torra","20th International Conference on Security and …, 2023",2023-09-18,"<a href=""Google Scholar (2023-09-18) : Δ SFL:(Decoupled Server Federated Learning) to utilize DLG attacks in federated learning by decoupling the server"" target=""_blank"">[https://www.diva-portal.org/smash/record.jsf?pid=diva2:1797873]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.diva-portal.org/smash/record.jsf?pid=diva2:1797873]</a>","attacks on FL such as, membership inference attack, gradient inversion attack, data poisoning attack, backdoor attack, deep learning from gradients attack (DLG). So far …",,Google Scholar
Robust Backdoor Attacks on Object Detection in Real World,"Yaguan Qian, Boyuan Ji, Shuke He, Shenhui Huang, Xiang Ling, Bin Wang, Wei Wang","arXiv
arXiv","2023-09-16
2023-09","<a href=""arXiv (2023-09-16) : Robust Backdoor Attacks on Object Detection in Real World"" target=""_blank"">[http://arxiv.org/abs/2309.08953v1]</a>
<a href=""DBLP (2023-09) : Robust Backdoor Attacks on Object Detection in Real World"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.08953]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.08953]</a>","Deep learning models are widely deployed in many applications, such as object detection in various security fields. However, these models are vulnerable to backdoor attacks. Most backdoor attacks were intensively studied on classified models, but little on object detection. Previous works mainly focused on the backdoor attack in the digital world, but neglect the real world. Especially, the backdoor attack's effect in the real world will be easily influenced by physical factors like distance and illumination. In this paper, we proposed a variable-size backdoor trigger to adapt to the different sizes of attacked objects, overcoming the disturbance caused by the distance between the viewing point and attacked object. In addition, we proposed a backdoor training named malicious adversarial training, enabling the backdoor object detector to learn the feature of the trigger with physical noise. The experiment results show this robust backdoor attack (RBA) could enhance the attack success rate in the real world.
","
","arXiv
DBLP"
Attacking by Aligning: Clean-Label Backdoor Attacks on Object Detection,"Yize Cheng, Wenbin Hu, Minhao Cheng",arXiv,2023-09-16,"<a href=""arXiv (2023-09-16) : Attacking by Aligning: Clean-Label Backdoor Attacks on Object Detection"" target=""_blank"">[http://arxiv.org/abs/2307.10487v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Deep neural networks (DNNs) have shown unprecedented success in object detection tasks. However, it was also discovered that DNNs are vulnerable to multiple kinds of attacks, including Backdoor Attacks. Through the attack, the attacker manages to embed a hidden backdoor into the DNN such that the model behaves normally on benign data samples, but makes attacker-specified judgments given the occurrence of a predefined trigger. Although numerous backdoor attacks have been experimented on image classification, backdoor attacks on object detection tasks have not been properly investigated and explored. As object detection has been adopted as an important module in multiple security-sensitive applications such as autonomous driving, backdoor attacks on object detection could pose even more severe threats. Inspired by the inherent property of deep learning-based object detectors, we propose a simple yet effective backdoor attack method against object detection without modifying the ground truth annotations, specifically focusing on the object disappearance attack and object generation attack. Extensive experiments and ablation studies prove the effectiveness of our attack on the benchmark object detection dataset MSCOCO2017, on which we achieve an attack success rate of more than 92% with a poison rate of only 5%.",,arXiv
Invading the Integrity of Deep Learning (DL) Models Using LSB Perturbation & Pixel Manipulation,A Tauhid,2023,2023-09-16,"<a href=""Google Scholar (2023-09-16) : Invading the Integrity of Deep Learning (DL) Models Using LSB Perturbation & Pixel Manipulation"" target=""_blank"">[https://search.proquest.com/openview/25258d4efe98fb891c8b33097a5bde4b/1?pq-origsite=gscholar&cbl=18750&diss=y]</a>","<a href=""Google Scholar"" target=""_blank"">[https://search.proquest.com/openview/25258d4efe98fb891c8b33097a5bde4b/1?pq-origsite=gscholar&cbl=18750&diss=y]</a>","components: backdoor attack and adversarial attack. For the backdoor attack, the … focus on investigating two distinct attack strategies: backdoor attacks and adversarial …",,Google Scholar
PointCVaR: Risk-Optimized Outlier Removal for Robust 3D Point Cloud Classification,"X Li, J Lu, H Ding, C Sun, JT Zhou…","Proceedings of the AAAI …, 2024",2023-09-15,"<a href=""Google Scholar (2023-09-15) : PointCVaR: Risk-Optimized Outlier Removal for Robust 3D Point Cloud Classification"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/30129]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/30129]</a>","Especially, this method is, as far as we know, the first to successfully defend against point cloud backdoor attacks. Furthermore, it can consistently enhance existing robust …",,Google Scholar
BAGEL: Backdoor Attacks against Federated Contrastive Learning,"Yao Huang, Kongyang Chen, Jiannong Cao, Jiaxing Shen, Shaowei Wang, Yun Peng, Weilong Peng, Kechao Cai","arXiv
arXiv","2023-09-14
2023-11","<a href=""arXiv (2023-09-14) : BAGEL: Backdoor Attacks against Federated Contrastive Learning"" target=""_blank"">[http://arxiv.org/abs/2311.16113v1]</a>
<a href=""DBLP (2023-11) : BAGEL: Backdoor Attacks against Federated Contrastive Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.16113]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2311.16113]</a>","Federated Contrastive Learning (FCL) is an emerging privacy-preserving paradigm in distributed learning for unlabeled data. In FCL, distributed parties collaboratively learn a global encoder with unlabeled data, and the global encoder could be widely used as a feature extractor to build models for many downstream tasks. However, FCL is also vulnerable to many security threats (e.g., backdoor attacks) due to its distributed nature, which are seldom investigated in existing solutions. In this paper, we study the backdoor attack against FCL as a pioneer research, to illustrate how backdoor attacks on distributed local clients act on downstream tasks. Specifically, in our system, malicious clients can successfully inject a backdoor into the global encoder by uploading poisoned local updates, thus downstream models built with this global encoder will also inherit the backdoor. We also investigate how to inject backdoors into multiple downstream models, in terms of two different backdoor attacks, namely the \textit{centralized attack} and the \textit{decentralized attack}. Experiment results show that both the centralized and the decentralized attacks can inject backdoors into downstream models effectively with high attack success rates. Finally, we evaluate two defense methods against our proposed backdoor attacks in FCL, which indicates that the decentralized backdoor attack is more stealthy and harder to defend.
","
","arXiv
DBLP"
PolicyCleanse: Backdoor Detection and Mitigation in Reinforcement Learning,"Junfeng Guo, Ang Li, Cong Liu","arXiv
ICCV","2023-09-14
2023","<a href=""arXiv (2023-09-14) : PolicyCleanse: Backdoor Detection and Mitigation in Reinforcement Learning"" target=""_blank"">[http://arxiv.org/abs/2202.03609v5]</a>
<a href=""DBLP (2023) : PolicyCleanse: Backdoor Detection and Mitigation for Competitive Reinforcement Learning"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00433]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00433]</a>","While real-world applications of reinforcement learning are becoming popular, the security and robustness of RL systems are worthy of more attention and exploration. In particular, recent works have revealed that, in a multi-agent RL environment, backdoor trigger actions can be injected into a victim agent (a.k.a. Trojan agent), which can result in a catastrophic failure as soon as it sees the backdoor trigger action. To ensure the security of RL agents against malicious backdoors, in this work, we propose the problem of Backdoor Detection in a multi-agent competitive reinforcement learning system, with the objective of detecting Trojan agents as well as the corresponding potential trigger actions, and further trying to mitigate their Trojan behavior. In order to solve this problem, we propose PolicyCleanse that is based on the property that the activated Trojan agents accumulated rewards degrade noticeably after several timesteps. Along with PolicyCleanse, we also design a machine unlearning-based approach that can effectively mitigate the detected backdoor. Extensive experiments demonstrate that the proposed methods can accurately detect Trojan agents, and outperform existing backdoor mitigation baseline approaches by at least 3% in winning rate across various types of agents and environments.
","
","arXiv
DBLP"
Masterkey: Practical backdoor attack against speaker verification systems,"H Guo, X Chen, J Guo, L Xiao, Q Yan","Proceedings of the 29th Annual …, 2023",2023-09-14,"<a href=""Google Scholar (2023-09-14) : Masterkey: Practical backdoor attack against speaker verification systems"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3570361.3613261]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3570361.3613261]</a>","In this work, we propose a backdoor attack MasterKey, to compromise the SV models… poisoning attacks against unseen targets. Then, we optimize a universal backdoor …",,Google Scholar
Towards an efficient model for network intrusion detection system (IDS): systematic literature review,"Oluwadamilare Harazeem Abdulganiyu, Taha Ait Tchakoucht, Yakub Kayode Saheed",Wireless Networks,2023-09-14,"<a href=""Springer (2023-09-14) : Towards an efficient model for network intrusion detection system (IDS): systematic literature review"" target=""_blank"">[https://link.springer.com/article/10.1007/s11276-023-03495-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11276-023-03495-2]</a>","With the recent rise in internet usage, the volume of crucial, private, and confidential data traveling online has increased. Attackers have made...",,Springer
Distilling Cognitive Backdoor Patterns within an Image,"Hanxun Huang, Xingjun Ma, Sarah Erfani, James Bailey","arXiv
arXiv
ICLR","2023-09-13
2023-01
2023","<a href=""arXiv (2023-09-13) : Distilling Cognitive Backdoor Patterns within an Image"" target=""_blank"">[http://arxiv.org/abs/2301.10908v4]</a>
<a href=""DBLP (2023-01) : Distilling Cognitive Backdoor Patterns within an Image"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.10908]</a>
<a href=""DBLP (2023) : Distilling Cognitive Backdoor Patterns within an Image"" target=""_blank"">[https://openreview.net/pdf?id=S3D9NLzjnQ5]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.10908]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/pdf?id=S3D9NLzjnQ5]</a>","This paper proposes a simple method to distill and detect backdoor patterns within an image: \emph{Cognitive Distillation} (CD). The idea is to extract the ""minimal essence"" from an input image responsible for the model's prediction. CD optimizes an input mask to extract a small pattern from the input image that can lead to the same model output (i.e., logits or deep features). The extracted pattern can help understand the cognitive mechanism of a model on clean vs. backdoor images and is thus called a \emph{Cognitive Pattern} (CP). Using CD and the distilled CPs, we uncover an interesting phenomenon of backdoor attacks: despite the various forms and sizes of trigger patterns used by different attacks, the CPs of backdoor samples are all surprisingly and suspiciously small. One thus can leverage the learned mask to detect and remove backdoor examples from poisoned training datasets. We conduct extensive experiments to show that CD can robustly detect a wide range of advanced backdoor attacks. We also show that CD can potentially be applied to help detect potential biases from face datasets. Code is available at \url{https://github.com/HanxunH/CognitiveDistillation}.

","<a href=""arXiv"" target=""_blank"">[https://github.com/HanxunH/CognitiveDistillation}]</a>

","arXiv
DBLP
DBLP"
Backdoor attacks and countermeasures in natural language processing models: A comprehensive security review,"P Cheng, Z Wu, W Du, G Liu","arXiv preprint arXiv:2309.06055, 2023",2023-09-13,"<a href=""Google Scholar (2023-09-13) : Backdoor attacks and countermeasures in natural language processing models: A comprehensive security review"" target=""_blank"">[https://arxiv.org/abs/2309.06055]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2309.06055]</a>","from different phases and attack purposes. Additionally, there is a … backdoor countermeasures in this situation. In this paper, we conduct a timely review of backdoor attacks …",,Google Scholar
Stealthy Backdoor Attack Based on Singular Value Decomposition,吴尚锡， 尹雨阳， 宋思清， 陈观浩， 桑基韬， 于剑,"Journal of Software, 2023",2023-09-13,"<a href=""Google Scholar (2023-09-13) : Stealthy Backdoor Attack Based on Singular Value Decomposition"" target=""_blank"">[https://www.jos.org.cn/josen/article/abstract/6949]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.jos.org.cn/josen/article/abstract/6949]</a>",backdoor attacks during training. Such attacks are an attack method that controls … 种方法 singular-value backdoor attack 和 singular-vector backdoor attack 方法的 PSNR 值…,,Google Scholar
Neural Network Backdoor Removal by Reconstructing Triggers and Pruning Channels,D Koldenhof,2023,2023-09-12,"<a href=""Google Scholar (2023-09-12) : Neural Network Backdoor Removal by Reconstructing Triggers and Pruning Channels"" target=""_blank"">[https://essay.utwente.nl/97198/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://essay.utwente.nl/97198/]</a>","After this section, background on neural networks and backdoor attacks are covered, defining the most important terms used throughout the thesis. In Chapter 3, related …",,Google Scholar
"Enterprise level centric secure system administration for analysis, detection and prevention of vulnerabilities, insider attacks in multi-tenants distribution environment","SK Henge, P Dhiman","AIP Conference Proceedings, 2023",2023-09-09,"<a href=""Google Scholar (2023-09-09) : Enterprise level centric secure system administration for analysis, detection and prevention of vulnerabilities, insider attacks in multi-tenants distribution environment"" target=""_blank"">[https://pubs.aip.org/aip/acp/article/2800/1/020020/2910027]</a>","<a href=""Google Scholar"" target=""_blank"">[https://pubs.aip.org/aip/acp/article/2800/1/020020/2910027]</a>","their networking faintness, attacker-back doors, and router logs. … The insider threats and attacks can pretense a real … of prevention methods of insider attacks and threats, …",,Google Scholar
NTD: Non-transferability enabled deep learning backdoor detection,"Y Li, H Ma, Z Zhang, Y Gao, A Abuadbba…","IEEE Transactions …, 2023",2023-09-08,"<a href=""Google Scholar (2023-09-08) : NTD: Non-transferability enabled deep learning backdoor detection"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10243095/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10243095/]</a>","In contrast to the transferability of the adversarial example attack, we observe that a backdoor attack does not have such a characteristic. The prerequisite of a successful …",,Google Scholar
BadSQA: Stealthy Backdoor Attacks Using Presence Events as Triggers in Non-Intrusive Speech Quality Assessment,"Ying Ren, Kailai Shen, Zhe Ye, Diqun Yan","arXiv
arXiv","2023-09-04
2023-09","<a href=""arXiv (2023-09-04) : BadSQA: Stealthy Backdoor Attacks Using Presence Events as Triggers in Non-Intrusive Speech Quality Assessment"" target=""_blank"">[http://arxiv.org/abs/2309.01480v1]</a>
<a href=""DBLP (2023-09) : BadSQA: Stealthy Backdoor Attacks Using Presence Events as Triggers in Non-Intrusive Speech Quality Assessment"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.01480]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2309.01480]</a>","Non-Intrusive speech quality assessment (NISQA) has gained significant attention for predicting the mean opinion score (MOS) of speech without requiring the reference speech. In practical NISQA scenarios, untrusted third-party resources are often employed during deep neural network training to reduce costs. However, it would introduce a potential security vulnerability as specially designed untrusted resources can launch backdoor attacks against NISQA systems. Existing backdoor attacks primarily focus on classification tasks and are not directly applicable to NISQA which is a regression task. In this paper, we propose a novel backdoor attack on NISQA tasks, leveraging presence events as triggers to achieving highly stealthy attacks. To evaluate the effectiveness of our proposed approach, we conducted experiments on four benchmark datasets and employed two state-of-the-art NISQA models. The results demonstrate that the proposed backdoor attack achieved an average attack success rate of up to 99% with a poisoning rate of only 3%.
","
","arXiv
DBLP"
"An overview of the benefits, challenges, and legal aspects of penetration testing and red teaming","Fabian M. Teichmann, Sonia R. Boticiu",International Cybersecurity Law Review,2023-09-04,"<a href=""Springer (2023-09-04) : An overview of the benefits, challenges, and legal aspects of penetration testing and red teaming"" target=""_blank"">[https://link.springer.com/article/10.1365/s43439-023-00100-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1365/s43439-023-00100-2]</a>","Although red teaming and penetration testing are similar in their end results, it is important for organizations to choose the right assessment for...",,Springer
Deep Learning Based Hybrid Intrusion Detection Systems to Protect Satellite Networks,"Ahmad Taher Azar, Esraa Shehab, ... Shaimaa Ahmed Elsaid",Journal of Network and Systems Management,2023-09-04,"<a href=""Springer (2023-09-04) : Deep Learning Based Hybrid Intrusion Detection Systems to Protect Satellite Networks"" target=""_blank"">[https://link.springer.com/article/10.1007/s10922-023-09767-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10922-023-09767-8]</a>","Despite the fact that satellite-terrestrial systems have advantages such as high throughput, low latency, and low energy consumption, as well as low...",,Springer
"Image Analysis and Processing–ICIAP 2023: 22nd International Conference, ICIAP 2023, Udine, Italy, September 11–15, 2023, Proceedings, Part I","GL Foresti, A Fusiello, E Hancock",2023,2023-09-04,"<a href=""Google Scholar (2023-09-04) : Image Analysis and Processing–ICIAP 2023: 22nd International Conference, ICIAP 2023, Udine, Italy, September 11–15, 2023, Proceedings, Part I"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=Tj7VEAAAQBAJ&oi=fnd&pg=PR5&dq=backdoor+attack&ots=UWIqmi0LWi&sig=kMrfEdfCojPQXrLp5ltCpUwbfwI]</a>","<a href=""Google Scholar"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=Tj7VEAAAQBAJ&oi=fnd&pg=PR5&dq=backdoor+attack&ots=UWIqmi0LWi&sig=kMrfEdfCojPQXrLp5ltCpUwbfwI]</a>",,,Google Scholar
Safe and Robust Watermark Injection with a Single OoD Image,"S Yu, J Hong, H Zhang, H Wang, Z Wang…","arXiv preprint arXiv …, 2023",2023-09-04,"<a href=""Google Scholar (2023-09-04) : Safe and Robust Watermark Injection with a Single OoD Image"" target=""_blank"">[https://arxiv.org/abs/2309.01786]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2309.01786]</a>",propose a safe and robust backdoor-based watermark … backdoor-based watermark approaches. To increase the robustness of watermarks against agnostic removal attacks…,,Google Scholar
Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack,"Sze Jue Yang, Quang Nguyen, Chee Seng Chan, Khoa D. Doan","arXiv
arXiv","2023-09-03
2023-08","<a href=""arXiv (2023-09-03) : Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack"" target=""_blank"">[http://arxiv.org/abs/2308.16684v2]</a>
<a href=""DBLP (2023-08) : Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.16684]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.16684]</a>","The vulnerabilities to backdoor attacks have recently threatened the trustworthiness of machine learning models in practical applications. Conventional wisdom suggests that not everyone can be an attacker since the process of designing the trigger generation algorithm often involves significant effort and extensive experimentation to ensure the attack's stealthiness and effectiveness. Alternatively, this paper shows that there exists a more severe backdoor threat: anyone can exploit an easily-accessible algorithm for silent backdoor attacks. Specifically, this attacker can employ the widely-used lossy image compression from a plethora of compression tools to effortlessly inject a trigger pattern into an image without leaving any noticeable trace, i.e., the generated triggers are natural artifacts. One does not require extensive knowledge to click on the ""convert"" or ""save as"" button while using tools for lossy image compression. Via this attack, the adversary does not need to design a trigger generator as seen in prior works and only requires poisoning the data. Empirically, the proposed attack consistently achieves 100% attack success rate in several benchmark datasets such as MNIST, CIFAR-10, GTSRB and CelebA. More significantly, the proposed attack can still achieve almost 100% attack success rate with very small (approximately 10%) poisoning rates in the clean label setting. The generated trigger of the proposed attack using one lossy compression algorithm is also transferable across other related compression algorithms, exacerbating the severity of this backdoor threat. This work takes another crucial step toward understanding the extensive risks of backdoor attacks in practice, urging practitioners to investigate similar attacks and relevant backdoor mitigation methods.
","
","arXiv
DBLP"
Backdoor Attack on Hash-based Image Retrieval via Clean-label Data Poisoning,"Kuofeng Gao, Jiawang Bai, Bin Chen, Dongxian Wu, Shu-Tao Xia","arXiv
BMVC","2023-09-02
2023","<a href=""arXiv (2023-09-02) : Backdoor Attack on Hash-based Image Retrieval via Clean-label Data Poisoning"" target=""_blank"">[http://arxiv.org/abs/2109.08868v3]</a>
<a href=""DBLP (2023) : Backdoor Attack on Hash-based Image Retrieval via Clean-label Data Poisoning"" target=""_blank"">[http://proceedings.bmvc2023.org/172/]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://proceedings.bmvc2023.org/172/]</a>","A backdoored deep hashing model is expected to behave normally on original query images and return the images with the target label when a specific trigger pattern presents. To this end, we propose the confusing perturbations-induced backdoor attack (CIBA). It injects a small number of poisoned images with the correct label into the training data, which makes the attack hard to be detected. To craft the poisoned images, we first propose the confusing perturbations to disturb the hashing code learning. As such, the hashing model can learn more about the trigger. The confusing perturbations are imperceptible and generated by optimizing the intra-class dispersion and inter-class shift in the Hamming space. We then employ the targeted adversarial patch as the backdoor trigger to improve the attack performance. We have conducted extensive experiments to verify the effectiveness of our proposed CIBA. Our code is available at https://github.com/KuofengGao/CIBA.
","<a href=""arXiv"" target=""_blank"">[https://github.com/KuofengGao/CIBA]</a>
","arXiv
DBLP"
ADFL: Defending backdoor attacks in federated learning via adversarial distillation,Zhu C.,Computers and Security,2023-09-01,"<a href=""ScienceDirect (2023-09-01) : ADFL: Defending backdoor attacks in federated learning via adversarial distillation"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103366]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103366]</a>",,,ScienceDirect
Compression-resistant backdoor attack against deep neural networks,Xue M.,Applied Intelligence,2023-09-01,"<a href=""ScienceDirect (2023-09-01) : Compression-resistant backdoor attack against deep neural networks"" target=""_blank"">[https://doi.org/10.1007/s10489-023-04575-8]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/s10489-023-04575-8]</a>",,,ScienceDirect
Debiasing backdoor attack: A benign application of backdoor attack in eliminating data bias,Wu S.,Information Sciences,2023-09-01,"<a href=""ScienceDirect (2023-09-01) : Debiasing backdoor attack: A benign application of backdoor attack in eliminating data bias"" target=""_blank"">[https://doi.org/10.1016/j.ins.2023.119171]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.ins.2023.119171]</a>",,,ScienceDirect
Die Synapse des Fortschritts: Wie neuro-explizite KI unser Verständnis von Cybersicherheit revolutioniert.,"A Krüger, C Müller","IM+ io, 2023",2023-09-01,"<a href=""Google Scholar (2023-09-01) : Die Synapse des Fortschritts: Wie neuro-explizite KI unser Verständnis von Cybersicherheit revolutioniert."" target=""_blank"">[https://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=21989990&asa=N&AN=170958467&h=lzY7gQlWGW%2BQ%2BHz0txY8v8Z%2FGwfDSavvhLeCtwUfADjd5lq%2BN%2BSxsLCR4LNoUwhw9Sc6O%2Fxi2TvytuSha2e4dw%3D%3D&crl=c]</a>","<a href=""Google Scholar"" target=""_blank"">[https://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=21989990&asa=N&AN=170958467&h=lzY7gQlWGW%2BQ%2BHz0txY8v8Z%2FGwfDSavvhLeCtwUfADjd5lq%2BN%2BSxsLCR4LNoUwhw9Sc6O%2Fxi2TvytuSha2e4dw%3D%3D&crl=c]</a>","Im Gegensatz zu Poisoning Attacks ähneln die manipulierten Eingaben den legitimen Eingaben, um die Erkennung zu erschweren. Backdoor Evasion Attacks sind …",,Google Scholar
Object-free backdoor attack and defense on semantic segmentation,Mao J.,Computers and Security,2023-09-01,"<a href=""ScienceDirect (2023-09-01) : Object-free backdoor attack and defense on semantic segmentation"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103365]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103365]</a>",,,ScienceDirect
Exploiting a Benign Loudspeaker as Magnetic Backdoor for Practical Injection Attacks,"Tiantian Liu, Feng Lin","ACM TURC '23: Proceedings of the ACM Turing Award Celebration Conference - China 2023
ACM TUR-C","2023-09
2023","<a href=""ACM (2023-09) : Exploiting a Benign Loudspeaker as Magnetic Backdoor for Practical Injection Attacks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3603165.3607443]</a>
<a href=""DBLP (2023) : Exploiting a Benign Loudspeaker as Magnetic Backdoor for Practical Injection Attacks"" target=""_blank"">[https://doi.org/10.1145/3603165.3607443]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3603165.3607443]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3603165.3607443]</a>","The critical hardware platform for voice-controlled devices is an audio system that consists of loudspeakers and microphones. This paper presents MagBackdoor, the first magnetic threat that injects malicious commands via a loudspeaker-based backdoor to ...
","
","ACM
DBLP"
Mitigating Adversarial Attacks using Pruning,"Vipul Kumar Mishra, Aditya Varshney, Shekhar Yadav",IC3-2023: Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing,2023-09,"<a href=""ACM (2023-09) : Mitigating Adversarial Attacks using Pruning"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3607947.3608057]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3607947.3608057]</a>","The advent of deep learning has revolutionized the technology industry and has made Deep Neural Networks (DNNs) the powerhouse of many modern day software applications. Well-trained DNNs are able to perform complex tasks such as speech recognition, ...",,ACM
"Drone cybersecurity issues, solutions, trend insights and future perspectives: a survey","Abiodun Esther Omolara, Moatsum Alawida, Oludare Isaac Abiodun",Neural Computing and Applications,2023-08-31,"<a href=""Springer (2023-08-31) : Drone cybersecurity issues, solutions, trend insights and future perspectives: a survey"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-023-08857-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-023-08857-7]</a>","This paper presented an exhaustive survey on the security and privacy issues of drones. These security concerns were thoroughly dissected,...",,Springer
Federated deep Q-learning networks for service-based anomaly detection and classification in edge-to-cloud ecosystems,"Mays AL-Naday, Vlad Dobre, ... Filip De Turck",Annals of Telecommunications,2023-08-31,"<a href=""Springer (2023-08-31) : Federated deep Q-learning networks for service-based anomaly detection and classification in edge-to-cloud ecosystems"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-023-00977-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-023-00977-4]</a>",The diversity of services and infrastructure in metropolitan edge-to-cloud network(s) is rising to unprecedented levels. This is causing a rising...,,Springer
MDTD: A Multi-Domain Trojan Detector for Deep Neural Networks,"A Rajabi, S Asokraj, F Jiang, L Niu…","Proceedings of the …, 2023",2023-08-31,"<a href=""Google Scholar (2023-08-31) : MDTD: A Multi-Domain Trojan Detector for Deep Neural Networks"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3576915.3623082]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3576915.3623082]</a>",launch attacks such as a backdoor attack [32] and share the defective model for use by the public. An adversary carrying out a backdoor attack … Backdoor attacks are highly …,,Google Scholar
Securing AI systems: Developing a framework for the operation phase of artificial intelligence systems: Entwicklung eines Frameworks für die Betriebsphase von …,B Bauer,2023,2023-08-31,"<a href=""Google Scholar (2023-08-31) : Securing AI systems: Developing a framework for the operation phase of artificial intelligence systems: Entwicklung eines Frameworks für die Betriebsphase von …"" target=""_blank"">[https://epub.jku.at/obvulihs/content/titleinfo/9028340]</a>","<a href=""Google Scholar"" target=""_blank"">[https://epub.jku.at/obvulihs/content/titleinfo/9028340]</a>","In addition, potential attacks must be addressed in the operation phase. These include data poisoning attacks, backdoor attacks, adversarial attacks and privacy attacks …",,Google Scholar
Ensuring Federated Ownership Verification with FedBack: A Trigger-Based Watermarking Approach,"Y Wang, Y Liu, X Chen",2023,2023-08-30,"<a href=""Google Scholar (2023-08-30) : Ensuring Federated Ownership Verification with FedBack: A Trigger-Based Watermarking Approach"" target=""_blank"">[https://www.researchsquare.com/article/rs-3291175/latest]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.researchsquare.com/article/rs-3291175/latest]</a>","backdoor watermarking and FL, this research proposes FedBack, a copyright protection method based on backdoor … incorrect output during backdoor attack training. Let f : …",,Google Scholar
Stealthy Backdoor Attack for Code Models,"Zhou Yang, Bowen Xu, Jie M. Zhang, Hong Jin Kang, Jieke Shi, Junda He, David Lo","arXiv
IEEE Trans. Software Eng.
arXiv","2023-08-29
2024
2023-01","<a href=""arXiv (2023-08-29) : Stealthy Backdoor Attack for Code Models"" target=""_blank"">[http://arxiv.org/abs/2301.02496v2]</a>
<a href=""DBLP (2024) : Stealthy Backdoor Attack for Code Models"" target=""_blank"">[https://doi.org/10.1109/TSE.2024.3361661]</a>
<a href=""DBLP (2023-01) : Stealthy Backdoor Attack for Code Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.02496]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TSE.2024.3361661]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.02496]</a>","Code models, such as CodeBERT and CodeT5, offer general-purpose representations of code and play a vital role in supporting downstream automated software engineering tasks. Most recently, code models were revealed to be vulnerable to backdoor attacks. A code model that is backdoor-attacked can behave normally on clean examples but will produce pre-defined malicious outputs on examples injected with triggers that activate the backdoors. Existing backdoor attacks on code models use unstealthy and easy-to-detect triggers. This paper aims to investigate the vulnerability of code models with stealthy backdoor attacks. To this end, we propose AFRAIDOOR (Adversarial Feature as Adaptive Backdoor). AFRAIDOOR achieves stealthiness by leveraging adversarial perturbations to inject adaptive triggers into different inputs. We evaluate AFRAIDOOR on three widely adopted code models (CodeBERT, PLBART and CodeT5) and two downstream tasks (code summarization and method name prediction). We find that around 85% of adaptive triggers in AFRAIDOOR bypass the detection in the defense process. By contrast, only less than 12% of the triggers from previous work bypass the defense. When the defense method is not applied, both AFRAIDOOR and baselines have almost perfect attack success rates. However, once a defense is applied, the success rates of baselines decrease dramatically to 10.47% and 12.06%, while the success rate of AFRAIDOOR are 77.05% and 92.98% on the two tasks. Our finding exposes security weaknesses in code models under stealthy backdoor attacks and shows that the state-of-the-art defense method cannot provide sufficient protection. We call for more research efforts in understanding security threats to code models and developing more effective countermeasures.

","

","arXiv
DBLP
DBLP"
A comprehensive overview of backdoor attacks in large language models within communication networks,"H Yang, K Xiang, M Ge, H Li, R Lu, S Yu","IEEE Network, 2024",2023-08-29,"<a href=""Google Scholar (2023-08-29) : A comprehensive overview of backdoor attacks in large language models within communication networks"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10440367/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10440367/]</a>","attacks is currently absent. In this survey, we systematically propose a taxonomy of backdoor attacks in … , and demonstration-triggered attacks. Furthermore, we conduct a …",,Google Scholar
Défense contre les attaques par porte dérobée en apprentissage fédéré par estimation du motif d'attaque et élagage,,,2023-08-29,"<a href=""Google Scholar (2023-08-29) : Défense contre les attaques par porte dérobée en apprentissage fédéré par estimation du motif d'attaque et élagage"" target=""_blank"">[https://www.researchgate.net/profile/Deepika-Singh-29/publication/373660455_Defense_Against_Backdoor_Attacks_in_Federated_Learning_by_estimation_of_the_attack_pattern_and_pruning/links/64f70f94d8aead0ff2448b34/Defense-Against-Backdoor-Attacks-in-Federated-Learning-by-estimation-of-the-attack-pattern-and-pruning.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.researchgate.net/profile/Deepika-Singh-29/publication/373660455_Defense_Against_Backdoor_Attacks_in_Federated_Learning_by_estimation_of_the_attack_pattern_and_pruning/links/64f70f94d8aead0ff2448b34/Defense-Against-Backdoor-Attacks-in-Federated-Learning-by-estimation-of-the-attack-pattern-and-pruning.pdf]</a>",backdoor attacks in federated learning contexts. This approach relies first on the server estimating the attack … that are responsible for the attack. This scheme offers an …,,Google Scholar
A semantic backdoor attack against Graph Convolutional Networks,"Jiazhu Dai, Zhipeng Xiong","arXiv
arXiv","2023-08-26
2023-02","<a href=""arXiv (2023-08-26) : A semantic backdoor attack against Graph Convolutional Networks"" target=""_blank"">[http://arxiv.org/abs/2302.14353v4]</a>
<a href=""DBLP (2023-02) : A semantic backdoor attack against Graph Convolutional Networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.14353]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.14353]</a>","Graph convolutional networks (GCNs) have been very effective in addressing the issue of various graph-structured related tasks. However, recent research has shown that GCNs are vulnerable to a new type of threat called a backdoor attack, where the adversary can inject a hidden backdoor into GCNs so that the attacked model performs well on benign samples, but its prediction will be maliciously changed to the attacker-specified target label if the hidden backdoor is activated by the attacker-defined trigger. A semantic backdoor attack is a new type of backdoor attack on deep neural networks (DNNs), where a naturally occurring semantic feature of samples can serve as a backdoor trigger such that the infected DNN models will misclassify testing samples containing the predefined semantic feature even without the requirement of modifying the testing samples. Since the backdoor trigger is a naturally occurring semantic feature of the samples, semantic backdoor attacks are more imperceptible and pose a new and serious threat. In this paper, we investigate whether such semantic backdoor attacks are possible for GCNs and propose a semantic backdoor attack against GCNs (SBAG) under the context of graph classification to reveal the existence of this security vulnerability in GCNs. SBAG uses a certain type of node in the samples as a backdoor trigger and injects a hidden backdoor into GCN models by poisoning training data. The backdoor will be activated, and the GCN models will give malicious classification results specified by the attacker even on unmodified samples as long as the samples contain enough trigger nodes. We evaluate SBAG on four graph datasets and the experimental results indicate that SBAG is effective.
","
","arXiv
DBLP"
ANALYSIS OF MACHINE LEARNING CLASSIFICATION TECHNIQUES FOR IOT ATTACK VECTORS,,,2023-08-26,"<a href=""Google Scholar (2023-08-26) : ANALYSIS OF MACHINE LEARNING CLASSIFICATION TECHNIQUES FOR IOT ATTACK VECTORS"" target=""_blank"">[https://admin369.seyboldreport.org/file/V17I09A146-gXxu5YZ621KkA1y.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://admin369.seyboldreport.org/file/V17I09A146-gXxu5YZ621KkA1y.pdf]</a>","or attacks. There are a number of attacks in which an IoT device is used as an attack … system resource including attack vectors such as backdoor, password attacks, cross …",,Google Scholar
Lmsanitator: Defending prompt-tuning against task-agnostic backdoors,"C Wei, W Meng, Z Zhang, M Chen, M Zhao…","arXiv preprint arXiv …, 2023",2023-08-26,"<a href=""Google Scholar (2023-08-26) : Lmsanitator: Defending prompt-tuning against task-agnostic backdoors"" target=""_blank"">[https://arxiv.org/abs/2308.13904]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2308.13904]</a>","task-agnostic backdoor attacks … backdoor attacks against pretrained models mainly target fine-tuning scenarios. In this section, we adapt the task-agnostic backdoor attacks …",,Google Scholar
A Novel Framework for Smart Cyber Defence: A Deep-Dive Into Deep Learning Attacks and Defences,I. Arshad S. H. Alsamhi Y. Qiao B. Lee Y. Ye,IEEE Access,2023-08-25,"<a href=""IEEE (2023-08-25) : A Novel Framework for Smart Cyber Defence: A Deep-Dive Into Deep Learning Attacks and Defences"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10223226]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2023.3306333]</a>","Deep learning techniques have been widely adopted for cyber defence applications such as malware detection and anomaly detection. The ever-changing nature of cyber threats has made cyber defence a constantly evolving field. Smart manufacturing is critical to the broader thrust towards Industry 4.0 and 5.0. Developing advanced technologies in smart manufacturing requires enabling a paradigm shift in manufacturing, while cyber-attacks significantly threaten smart manufacturing. For example, a cyber attack (e.g., backdoor) occurs during the model’s training process. Cyber attack affects the models and impacts the resultant output to be misled. Therefore, this paper proposes a novel and comprehensive framework for smart cyber defence in deep learning security. The framework collectively incorporates a threat model, data, and model security. The proposed framework encompasses multiple layers, including privacy and protection of data and models. In addition to statistical and intelligent model techniques for maintaining data privacy and confidentiality, the proposed framework covers the structural perspective, i.e., policies and procedures for securing data. The study then offers different methods to make the models robust against attacks coupled with a threat model. Along with the model security, the threat model helps defend the smart systems against attacks by identifying potential or actual vulnerabilities and putting countermeasures and control in place. Moreover, based on our analysis, the study provides a taxonomy of the backdoor attacks and defences. In addition, the study provides a qualitative comparison of the existing backdoor attacks and defences. Finally, the study highlights the future directions for backdoor defences and provides a possible way for further research.",,IEEE
Backdoor attack on deep neural networks using inaudible triggers,"J van der Horst, S Picek, S Koffas, G Acar",2023,2023-08-25,"<a href=""Google Scholar (2023-08-25) : Backdoor attack on deep neural networks using inaudible triggers"" target=""_blank"">[https://www.cs.ru.nl/bachelors-theses/2023/Julian_van_der_Horst___1015357___Backdoor_attack_on_deep_neural_networks_using_inaudible_triggers.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.cs.ru.nl/bachelors-theses/2023/Julian_van_der_Horst___1015357___Backdoor_attack_on_deep_neural_networks_using_inaudible_triggers.pdf]</a>","of this specific backdoor attack in real-world scenarios. … specific backdoor attack, we will explore the STRIP defense, a method that aims to detect and neutralize backdoor …",,Google Scholar
INCHAIN: a cyber insurance architecture with smart contracts and self-sovereign identity on top of blockchain,"Aristeidis Farao, Georgios Paparis, ... Christos Xenakis",International Journal of Information Security,2023-08-25,"<a href=""Springer (2023-08-25) : INCHAIN: a cyber insurance architecture with smart contracts and self-sovereign identity on top of blockchain"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00741-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00741-8]</a>","Despite the rapid growth of the cyber insurance market in recent years, insurance companies in this area face several challenges, such as a lack of...",,Springer
Universal Soldier: Using Universal Adversarial Perturbations for Detecting Backdoor Attacks,"Xiaoyun Xu, Oguzhan Ersoy, Stjepan Picek","arXiv
arXiv","2023-08-24
2023-02","<a href=""arXiv (2023-08-24) : Universal Soldier: Using Universal Adversarial Perturbations for Detecting Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2302.00747v3]</a>
<a href=""DBLP (2023-02) : Universal Soldier: Using Universal Adversarial Perturbations for Detecting Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.00747]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.00747]</a>","Deep learning models achieve excellent performance in numerous machine learning tasks. Yet, they suffer from security-related issues such as adversarial examples and poisoning (backdoor) attacks. A deep learning model may be poisoned by training with backdoored data or by modifying inner network parameters. Then, a backdoored model performs as expected when receiving a clean input, but it misclassifies when receiving a backdoored input stamped with a pre-designed pattern called ""trigger"". Unfortunately, it is difficult to distinguish between clean and backdoored models without prior knowledge of the trigger. This paper proposes a backdoor detection method by utilizing a special type of adversarial attack, universal adversarial perturbation (UAP), and its similarities with a backdoor trigger. We observe an intuitive phenomenon: UAPs generated from backdoored models need fewer perturbations to mislead the model than UAPs from clean models. UAPs of backdoored models tend to exploit the shortcut from all classes to the target class, built by the backdoor trigger. We propose a novel method called Universal Soldier for Backdoor detection (USB) and reverse engineering potential backdoor triggers via UAPs. Experiments on 345 models trained on several datasets show that USB effectively detects the injected backdoor and provides comparable or better results than state-of-the-art methods.
","
","arXiv
DBLP"
A Review on Machine Learning-based Malware Detection Techniques for Internet of Things (IoT) Environments,"S. Sasikala, Sengathir Janakiraman",Wireless Personal Communications,2023-08-24,"<a href=""Springer (2023-08-24) : A Review on Machine Learning-based Malware Detection Techniques for Internet of Things (IoT) Environments"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-023-10693-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-023-10693-w]</a>","Internet of Things (IoT) is the recent digital trend that connects the physical and virtual world. The strong bonding between the people, objects,...",,Springer
"Chatgpt for cybersecurity: practical applications, challenges, and future directions","Muna Al-Hawawreh, Ahamed Aljuhani, Yaser Jararweh",Cluster Computing,2023-08-24,"<a href=""Springer (2023-08-24) : Chatgpt for cybersecurity: practical applications, challenges, and future directions"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-023-04124-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-023-04124-5]</a>","Artificial intelligence (AI) advancements have revolutionized many critical domains by providing cost-effective, automated, and intelligent...",,Springer
Cybersecurity knowledge graphs construction and quality assessment,"Hongyi Li, Ze Shi, ... Nan Sun",Complex & Intelligent Systems,2023-08-24,"<a href=""Springer (2023-08-24) : Cybersecurity knowledge graphs construction and quality assessment"" target=""_blank"">[https://link.springer.com/article/10.1007/s40747-023-01205-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s40747-023-01205-1]</a>","Cyber-attack activities are complex and ever-changing, posing severe challenges to cybersecurity personnel. Introducing knowledge graphs into the...",,Springer
Backdooring Textual Inversion for Concept Censorship,"Yutong Wu, Jie Zhang, Florian Kerschbaum, Tianwei Zhang","arXiv
arXiv","2023-08-23
2023-08","<a href=""arXiv (2023-08-23) : Backdooring Textual Inversion for Concept Censorship"" target=""_blank"">[http://arxiv.org/abs/2308.10718v2]</a>
<a href=""DBLP (2023-08) : Backdooring Textual Inversion for Concept Censorship"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.10718]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.10718]</a>","Recent years have witnessed success in AIGC (AI Generated Content). People can make use of a pre-trained diffusion model to generate images of high quality or freely modify existing pictures with only prompts in nature language. More excitingly, the emerging personalization techniques make it feasible to create specific-desired images with only a few images as references. However, this induces severe threats if such advanced techniques are misused by malicious users, such as spreading fake news or defaming individual reputations. Thus, it is necessary to regulate personalization models (i.e., concept censorship) for their development and advancement. In this paper, we focus on the personalization technique dubbed Textual Inversion (TI), which is becoming prevailing for its lightweight nature and excellent performance. TI crafts the word embedding that contains detailed information about a specific object. Users can easily download the word embedding from public websites like Civitai and add it to their own stable diffusion model without fine-tuning for personalization. To achieve the concept censorship of a TI model, we propose leveraging the backdoor technique for good by injecting backdoors into the Textual Inversion embeddings. Briefly, we select some sensitive words as triggers during the training of TI, which will be censored for normal use. In the subsequent generation stage, if the triggers are combined with personalized embeddings as final prompts, the model will output a pre-defined target image rather than images including the desired malicious concept. To demonstrate the effectiveness of our approach, we conduct extensive experiments on Stable Diffusion, a prevailing open-sourced text-to-image model. Our code, data, and results are available at https://concept-censorship.github.io.
","<a href=""arXiv"" target=""_blank"">[https://concept-censorship.github.io]</a>
","arXiv
DBLP"
BadVFL: Backdoor Attacks in Vertical Federated Learning,"Mohammad Naseri, Yufei Han, Emiliano De Cristofaro","arXiv
arXiv","2023-08-23
2023-04","<a href=""arXiv (2023-08-23) : BadVFL: Backdoor Attacks in Vertical Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2304.08847v2]</a>
<a href=""DBLP (2023-04) : BadVFL: Backdoor Attacks in Vertical Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.08847]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.08847]</a>","Federated learning (FL) enables multiple parties to collaboratively train a machine learning model without sharing their data, rather, they train their own model locally and send updates to a central server for aggregation. Depending on how the data is distributed among the participants, FL can be classified into Horizontal (HFL) and Vertical (VFL). In VFL, the participants share the same set of training instances but only host a different and non-overlapping subset of the whole feature space. Whereas in HFL, each participant shares the same set of features while the training set is split into locally owned training data subsets. VFL is increasingly used in applications like financial fraud detection, nonetheless, very little work has analyzed its security. In this paper, we focus on robustness in VFL, in particular, on backdoor attacks, whereby an adversary attempts to manipulate the aggregate model during the training process to trigger misclassifications. Performing backdoor attacks in VFL is more challenging than in HFL, as the adversary i) does not have access to the labels during training and ii) cannot change the labels as she only has access to the feature embeddings. We present a first-of-its-kind clean-label backdoor attack in VFL, which consists of two phases: a label inference and a backdoor phase. We demonstrate the effectiveness of the attack on three different datasets, investigate the factors involved in its success, and discuss countermeasures to mitigate its impact.
","
","arXiv
DBLP"
Privacy-preserving cloud data sharing for healthcare systems with hybrid blockchain,"Raghav, Nitish Andola, ... Shekhar Verma",Peer-to-Peer Networking and Applications,2023-08-23,"<a href=""Springer (2023-08-23) : Privacy-preserving cloud data sharing for healthcare systems with hybrid blockchain"" target=""_blank"">[https://link.springer.com/article/10.1007/s12083-023-01521-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12083-023-01521-w]</a>","Privacy-preserving cloud data sharing requires user authentication, access control, and privacy provisioning mechanisms. In this paper, we propose...",,Springer
Temporal-distributed backdoor attack against video based action recognition,"X Li, S Wang, R Huang, M Gowda…","Proceedings of the AAAI …, 2024",2023-08-23,"<a href=""Google Scholar (2023-08-23) : Temporal-distributed backdoor attack against video based action recognition"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/28104]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/28104]</a>","of video-based systems under backdoor attacks remains largely unexplored. Current … simple yet effective backdoor attack against video data. Our proposed attack, adding …",,Google Scholar
Protect Federated Learning Against Backdoor Attacks via Data-Free Trigger Generation,"Yanxin Yang, Ming Hu, Yue Cao, Jun Xia, Yihao Huang, Yang Liu, Mingsong Chen","arXiv
arXiv","2023-08-22
2023-08","<a href=""arXiv (2023-08-22) : Protect Federated Learning Against Backdoor Attacks via Data-Free Trigger Generation"" target=""_blank"">[http://arxiv.org/abs/2308.11333v1]</a>
<a href=""DBLP (2023-08) : Protect Federated Learning Against Backdoor Attacks via Data-Free Trigger Generation"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.11333]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.11333]</a>","As a distributed machine learning paradigm, Federated Learning (FL) enables large-scale clients to collaboratively train a model without sharing their raw data. However, due to the lack of data auditing for untrusted clients, FL is vulnerable to poisoning attacks, especially backdoor attacks. By using poisoned data for local training or directly changing the model parameters, attackers can easily inject backdoors into the model, which can trigger the model to make misclassification of targeted patterns in images. To address these issues, we propose a novel data-free trigger-generation-based defense approach based on the two characteristics of backdoor attacks: i) triggers are learned faster than normal knowledge, and ii) trigger patterns have a greater effect on image classification than normal class patterns. Our approach generates the images with newly learned knowledge by identifying the differences between the old and new global models, and filters trigger images by evaluating the effect of these generated images. By using these trigger images, our approach eliminates poisoned models to ensure the updated global model is benign. Comprehensive experiments demonstrate that our approach can defend against almost all the existing types of backdoor attacks and outperform all the seven state-of-the-art defense methods with both IID and non-IID scenarios. Especially, our approach can successfully defend against the backdoor attack even when 80\% of the clients are malicious.
","
","arXiv
DBLP"
CrowdGuard: Federated Backdoor Detection in Federated Learning,"Phillip Rieger, Torsten Krauß, Markus Miettinen, Alexandra Dmitrienko, Ahmad-Reza Sadeghi",arXiv,2023-08-22,"<a href=""arXiv (2023-08-22) : CrowdGuard: Federated Backdoor Detection in Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2210.07714v3]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Federated Learning (FL) is a promising approach enabling multiple clients to train Deep Neural Networks (DNNs) collaboratively without sharing their local training data. However, FL is susceptible to backdoor (or targeted poisoning) attacks. These attacks are initiated by malicious clients who seek to compromise the learning process by introducing specific behaviors into the learned model that can be triggered by carefully crafted inputs. Existing FL safeguards have various limitations: They are restricted to specific data distributions or reduce the global model accuracy due to excluding benign models or adding noise, are vulnerable to adaptive defense-aware adversaries, or require the server to access local models, allowing data inference attacks. This paper presents a novel defense mechanism, CrowdGuard, that effectively mitigates backdoor attacks in FL and overcomes the deficiencies of existing techniques. It leverages clients' feedback on individual models, analyzes the behavior of neurons in hidden layers, and eliminates poisoned models through an iterative pruning scheme. CrowdGuard employs a server-located stacked clustering scheme to enhance its resilience to rogue client feedback. The evaluation results demonstrate that CrowdGuard achieves a 100% True-Positive-Rate and True-Negative-Rate across various scenarios, including IID and non-IID data distributions. Additionally, CrowdGuard withstands adaptive adversaries while preserving the original performance of protected models. To ensure confidentiality, CrowdGuard uses a secure and privacy-preserving architecture leveraging Trusted Execution Environments (TEEs) on both client and server sides.",,arXiv
"Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings …","F Liu, N Duan, Q Xu, Y Hong",2023,2023-08-22,"<a href=""Google Scholar (2023-08-22) : Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings …"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=_cnbEAAAQBAJ&oi=fnd&pg=PR5&dq=backdoor+attack&ots=JLmE-9YpBK&sig=ABd8svvfLsDzpxGn9UyDoDqr7YM]</a>","<a href=""Google Scholar"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=_cnbEAAAQBAJ&oi=fnd&pg=PR5&dq=backdoor+attack&ots=JLmE-9YpBK&sig=ABd8svvfLsDzpxGn9UyDoDqr7YM]</a>",,,Google Scholar
Patchbackdoor: Backdoor attack against deep neural networks without model modification,"Y Yuan, R Kong, S Xie, Y Li, Y Liu","Proceedings of the 31st ACM …, 2023",2023-08-22,"<a href=""Google Scholar (2023-08-22) : Patchbackdoor: Backdoor attack against deep neural networks without model modification"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3581783.3612032]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3581783.3612032]</a>",to a common but false belief that backdoor attack can be easily avoided by … backdoor attacks can be achieved without any model modification. Instead of injecting backdoor …,,Google Scholar
TrojViT: Trojan Insertion in Vision Transformers,M. Zheng Q. Lou L. Jiang,2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2023-08-22,"<a href=""IEEE (2023-08-22) : TrojViT: Trojan Insertion in Vision Transformers"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10203799]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.00392]</a>","Vision Transformers (ViTs) have demonstrated the state-of-the-art performance in various vision-related tasks. The success of ViTs motivates adversaries to perform back-door attacks on ViTs. Although the vulnerability of traditional CNNs to backdoor attacks is well-known, backdoor attacks on ViTs are seldom-studied. Compared to CNNs capturing pixel-wise local features by convolutions, ViTs extract global context information through patches and attentions. Naively transplanting CNN-specific backdoor attacks to ViTs yields only a low clean data accuracy and a low attack success rate. In this paper, we propose a stealth and practical ViT-specific backdoor attack TrojViT. Rather than an area-wise trigger used by CNN-specific backdoor attacks, TrojViT generates a patch-wise trigger designed to build a Trojan composed of some vulnerable bits on the parameters of a ViT stored in DRAM memory through patch salience ranking and attention-target loss. TrojViT further uses parameter distillation to reduce the bit number of the Trojan. Once the attacker inserts the Trojan into the ViT model by flipping the vulnerable bits, the ViT model still produces normal inference accuracy with benign inputs. But when the attacker embeds a trigger into an input, the ViT model is forced to classify the input to a predefined target class. We show that flipping only few vulnerable bits identified by TrojViT on a ViT model using the well-known RowHammer can transform the model into a backdoored one. We perform extensive experiments of multiple datasets on various ViT models. TrojViT can classify 99.64% of test images to a target class by flipping 345 bits on a ViT for ImageNet.",,IEEE
A novel intrusion detection system for internet of things devices and data,"Ajay Kaushik, Hamed Al-Raweshidy",Wireless Networks,2023-08-21,"<a href=""Springer (2023-08-21) : A novel intrusion detection system for internet of things devices and data"" target=""_blank"">[https://link.springer.com/article/10.1007/s11276-023-03435-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11276-023-03435-0]</a>","As we enter the new age of the Internet of Things (IoT) and wearable gadgets, sensors, and embedded devices are extensively used for data aggregation...",,Springer
A novel malware detection method based on API embedding and API parameters,"Bo Zhou, Hai Huang, ... Donghai Tian",The Journal of Supercomputing,2023-08-21,"<a href=""Springer (2023-08-21) : A novel malware detection method based on API embedding and API parameters"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05556-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05556-x]</a>",Malware is becoming increasingly prevalent in recent years with the widespread deployment of the information system. Many malicious programs pose a...,,Springer
High-Power Microwave Pulse-Induced Failure on Unmanned Aerial Vehicle System,Q. Mao Z. Xiang L. Huang J. Meng H. Wang C. Yang Y. Cui,IEEE Transactions on Plasma Science,2023-08-21,"<a href=""IEEE (2023-08-21) : High-Power Microwave Pulse-Induced Failure on Unmanned Aerial Vehicle System"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10024130]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TPS.2023.3236300]</a>","With the wide application of unmanned aerial vehicles (UAVs), more attention has been paid to the safe operation of UAVs in complex electromagnetic environments. In this article, we investigate the impact of high-power microwave (HPM) pulses on three types of typical commercial UAVs. We established the HPM irradiation experiment system first and obtained the responses of UAVs under the HPM attack with different electric field strengths. The flight logs and the damage analysis reports indicate that the damaged components of the type I UAV and the type II UAV are the rotor motor and the electronic speed controller. The experiment results prove that L-band narrowband HPM microwave has strong backdoor coupling effects on UAVs. The damage mechanism of UAVs by the HPM is studied by simulation. The simulation results show that an HPM pulse will cause a high-voltage pulse coupled to the cable connecting the electronic speed controller and the rotor motor, which caused the electronic speed controller to send out a false signal that caused the rotor motor speed to rise sharply, eventually making the rotor motor to burn out. Experiment results show that strengthening electromagnetic protection on the cable can effectively prevent HPM damage to UAVs. The simulation results match the experiment results and the damage analysis reports. Finally, some suggestions are provided to improve the UAV’s anti-HPM capability.",,IEEE
Hiding Backdoors within Event Sequence Data via Poisoning Attacks,"Elizaveta Kovtun, Alina Ermilova, Dmitry Berestnev, Alexey Zaytsev","arXiv
arXiv","2023-08-20
2023-08","<a href=""arXiv (2023-08-20) : Hiding Backdoors within Event Sequence Data via Poisoning Attacks"" target=""_blank"">[http://arxiv.org/abs/2308.10201v1]</a>
<a href=""DBLP (2023-08) : Hiding Backdoors within Event Sequence Data via Poisoning Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.10201]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.10201]</a>","The financial industry relies on deep learning models for making important decisions. This adoption brings new danger, as deep black-box models are known to be vulnerable to adversarial attacks. In computer vision, one can shape the output during inference by performing an adversarial attack called poisoning via introducing a backdoor into the model during training. For sequences of financial transactions of a customer, insertion of a backdoor is harder to perform, as models operate over a more complex discrete space of sequences, and systematic checks for insecurities occur. We provide a method to introduce concealed backdoors, creating vulnerabilities without altering their functionality for uncontaminated data. To achieve this, we replace a clean model with a poisoned one that is aware of the availability of a backdoor and utilize this knowledge. Our most difficult for uncovering attacks include either additional supervised detection step of poisoned data activated during the test or well-hidden model weight modifications. The experimental study provides insights into how these effects vary across different datasets, architectures, and model components. Alternative methods and baselines, such as distillation-type regularization, are also explored but found to be less efficient. Conducted on three open transaction datasets and architectures, including LSTM, CNN, and Transformer, our findings not only illuminate the vulnerabilities in contemporary models but also can drive the construction of more robust systems.
","
","arXiv
DBLP"
Hyperdimensional Cognitive Computing for Lightweight Cyberattack Detection in Industrial Internet of Things,"F Jalil Piran, HE Barkam…","… and Information in …, 2023",2023-08-20,"<a href=""Google Scholar (2023-08-20) : Hyperdimensional Cognitive Computing for Lightweight Cyberattack Detection in Industrial Internet of Things"" target=""_blank"">[https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings-abstract/IDETC-CIE2023/87356/1170624]</a>","<a href=""Google Scholar"" target=""_blank"">[https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings-abstract/IDETC-CIE2023/87356/1170624]</a>","Backdoor attacks permit continuous access to systems to disrupt operations, steal data, or carry out further attacks … detects and handles attacks, including backdoor attacks, …",,Google Scholar
Backdoor Mitigation by Correcting the Distribution of Neural Activations,"Xi Li, Zhen Xiang, David J. Miller, George Kesidis","arXiv
arXiv","2023-08-18
2023-08","<a href=""arXiv (2023-08-18) : Backdoor Mitigation by Correcting the Distribution of Neural Activations"" target=""_blank"">[http://arxiv.org/abs/2308.09850v1]</a>
<a href=""DBLP (2023-08) : Backdoor Mitigation by Correcting the Distribution of Neural Activations"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.09850]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.09850]</a>","Backdoor (Trojan) attacks are an important type of adversarial exploit against deep neural networks (DNNs), wherein a test instance is (mis)classified to the attacker's target class whenever the attacker's backdoor trigger is present. In this paper, we reveal and analyze an important property of backdoor attacks: a successful attack causes an alteration in the distribution of internal layer activations for backdoor-trigger instances, compared to that for clean instances. Even more importantly, we find that instances with the backdoor trigger will be correctly classified to their original source classes if this distribution alteration is corrected. Based on our observations, we propose an efficient and effective method that achieves post-training backdoor mitigation by correcting the distribution alteration using reverse-engineered triggers. Notably, our method does not change any trainable parameters of the DNN, but achieves generally better mitigation performance than existing methods that do require intensive DNN parameter tuning. It also efficiently detects test instances with the trigger, which may help to catch adversarial entities in the act of exploiting the backdoor.
","
","arXiv
DBLP"
A novel framework for smart cyber defence: a deep-dive into deep learning attacks and defences,"I Arshad, SH Alsamhi, Y Qiao, B Lee, Y Ye","IEEE Access, 2023",2023-08-18,"<a href=""Google Scholar (2023-08-18) : A novel framework for smart cyber defence: a deep-dive into deep learning attacks and defences"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10223226/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10223226/]</a>","backdoor attacks and defences. In addition, the study provides a qualitative comparison of the existing backdoor attacks … the taxonomy of backdoor attacks. In section V, we …",,Google Scholar
Gaining access to windows machine by installing backdoor,Kesavan T.,AIP Conference Proceedings,2023-08-18,"<a href=""ScienceDirect (2023-08-18) : Gaining access to windows machine by installing backdoor"" target=""_blank"">[https://doi.org/10.1063/5.0164803]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1063/5.0164803]</a>",,,ScienceDirect
High imperceptibility and robustness watermarking scheme for brain MRI using Slantlet transform coupled with enhanced knight tour algorithm,"Hoshang Kolivand, Tan Chi Wee, ... Ghazali Sulong",Multimedia Tools and Applications,2023-08-18,"<a href=""Springer (2023-08-18) : High imperceptibility and robustness watermarking scheme for brain MRI using Slantlet transform coupled with enhanced knight tour algorithm"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-16459-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-16459-7]</a>","This research introduces a novel and robust watermarking scheme for medical Brain MRI DICOM images, addressing the challenge of maintaining high...",,Springer
Modeling self-propagating malware with epidemiological models,"Alesia Chernikova, Nicolò Gozzi, ... Alina Oprea",Applied Network Science,2023-08-18,"<a href=""Springer (2023-08-18) : Modeling self-propagating malware with epidemiological models"" target=""_blank"">[https://link.springer.com/article/10.1007/s41109-023-00578-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41109-023-00578-z]</a>",Self-propagating malware (SPM) is responsible for large financial losses and major data breaches with devastating social impacts that cannot be...,,Springer
Poison Dart Frog: A Clean-Label Attack with Low Poisoning Rate and High Attack Success Rate in the Absence of Training Data,"B Ma, J Wang, D Wang, B Meng","arXiv preprint arXiv:2308.09487, 2023",2023-08-18,"<a href=""Google Scholar (2023-08-18) : Poison Dart Frog: A Clean-Label Attack with Low Poisoning Rate and High Attack Success Rate in the Absence of Training Data"" target=""_blank"">[https://arxiv.org/abs/2308.09487]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2308.09487]</a>",", the existing clean-label backdoor attacks largely relies on an … Unlike all current clean-label attacks, we propose a … The success rate of our backdoor attack is closely tied to …",,Google Scholar
Robust backdoor attack against federated learning,C Chen,2023,2023-08-18,"<a href=""Google Scholar (2023-08-18) : Robust backdoor attack against federated learning"" target=""_blank"">[https://repository.tudelft.nl/islandora/object/uuid:1d8f5279-a15d-4739-a8e0-5e7302f3fa15]</a>","<a href=""Google Scholar"" target=""_blank"">[https://repository.tudelft.nl/islandora/object/uuid:1d8f5279-a15d-4739-a8e0-5e7302f3fa15]</a>","Current backdoor attacks against federated learning (FL… stealthy and robust backdoor attack with flexible triggers … label), we make our attack naturally stealthy. Extensive …",,Google Scholar
Detection of adversarial attacks based on differences in image entropy,"Gwonsang Ryu, Daeseon Choi",International Journal of Information Security,2023-08-17,"<a href=""Springer (2023-08-17) : Detection of adversarial attacks based on differences in image entropy"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00735-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00735-6]</a>","Although deep neural networks (DNNs) have achieved high performance across various applications, they are often deceived by adversarial examples...",,Springer
SSLJBA: Joint Backdoor Attack on Both Robustness and Fairness of Self-Supervised Learning,"F Hao, T Gu, J Jiang, M Liu","Authorea Preprints, 2023",2023-08-15,"<a href=""Google Scholar (2023-08-15) : SSLJBA: Joint Backdoor Attack on Both Robustness and Fairness of Self-Supervised Learning"" target=""_blank"">[https://www.techrxiv.org/doi/full/10.36227/techrxiv.23899920.v1]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.techrxiv.org/doi/full/10.36227/techrxiv.23899920.v1]</a>","limitations of existing backdoor attacks, and propose the first joint backdoor attack (SSLJBA) … Then, we propose the backdoor generation method that leverages the feature …",,Google Scholar
Don’t FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs,H. A. Al Kader Hammoud A. Bibi P. H. S. Torr B. Ghanem,2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2023-08-14,"<a href=""IEEE (2023-08-14) : Don’t FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10208373]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/CVPRW59228.2023.00230]</a>","In this paper we investigate the frequency sensitivity of Deep Neural Networks (DNNs) when presented with clean samples versus poisoned samples. Our analysis shows significant disparities in frequency sensitivity between these two types of samples. Building on these findings, we propose FREAK, a frequency-based poisoned sample detection algorithm that is simple yet effective. Our experimental results demonstrate the efficacy of FREAK not only against frequency backdoor attacks but also against some spatial attacks. Our work is just the first step in leveraging these insights. We believe that our analysis and proposed defense mechanism will provide a foundation for future research and development of backdoor defenses.",,IEEE
Engineering the advances of the artificial neural networks (ANNs) for the security requirements of Internet of Things: a systematic review,"Yasir Ali, Habib Ullah Khan, Muhammad Khalid",Journal of Big Data,2023-08-14,"<a href=""Springer (2023-08-14) : Engineering the advances of the artificial neural networks (ANNs) for the security requirements of Internet of Things: a systematic review"" target=""_blank"">[https://link.springer.com/article/10.1186/s40537-023-00805-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s40537-023-00805-5]</a>",Internet of Things (IoT) driven systems have been sharply growing in the recent times but this evolution is hampered by cybersecurity threats like...,,Springer
Orion: Online backdoor sample detection via evolution deviance,"H Huang, Q Wang, X Gong, T Wang","International Joint Conference on …, 2023",2023-08-12,"<a href=""Google Scholar (2023-08-12) : Orion: Online backdoor sample detection via evolution deviance"" target=""_blank"">[https://www.ijcai.org/proceedings/2023/0096.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.ijcai.org/proceedings/2023/0096.pdf]</a>","We present the results of typical universal backdoor attack BadNets and dynamic attack IAD on CIFAR-10. The target classes are 1 and 3, respectively. The line for the …",,Google Scholar
TrustNetFL: Enhancing Federated Learning with Trusted Client Aggregation for Improved Security,"D Chen, K Wang, AP Sundar, F Li","Proceedings of the Twenty-fourth …, 2023",2023-08-12,"<a href=""Google Scholar (2023-08-12) : TrustNetFL: Enhancing Federated Learning with Trusted Client Aggregation for Improved Security"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3565287.3617635]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3565287.3617635]</a>",backdoor attacks as we show by outperforming traditional static clipping methods. … of TrustNetFL’s resilience against backdoor attacks and the impact of static vs. dynamic …,,Google Scholar
Multi-metrics adaptively identifies backdoors in Federated learning,"Siquan Huang, Yijiang Li, Chong Chen, Leyu Shi, Ying Gao","arXiv
ICCV
arXiv","2023-08-10
2023
2023-03","<a href=""arXiv (2023-08-10) : Multi-metrics adaptively identifies backdoors in Federated learning"" target=""_blank"">[http://arxiv.org/abs/2303.06601v2]</a>
<a href=""DBLP (2023) : Multi-metrics adaptively identifies backdoors in Federated learning"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00429]</a>
<a href=""DBLP (2023-03) : Multi-metrics adaptively identifies backdoors in Federated learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.06601]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00429]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.06601]</a>","The decentralized and privacy-preserving nature of federated learning (FL) makes it vulnerable to backdoor attacks aiming to manipulate the behavior of the resulting model on specific adversary-chosen inputs. However, most existing defenses based on statistical differences take effect only against specific attacks, especially when the malicious gradients are similar to benign ones or the data are highly non-independent and identically distributed (non-IID). In this paper, we revisit the distance-based defense methods and discover that i) Euclidean distance becomes meaningless in high dimensions and ii) malicious gradients with diverse characteristics cannot be identified by a single metric. To this end, we present a simple yet effective defense strategy with multi-metrics and dynamic weighting to identify backdoors adaptively. Furthermore, our novel defense has no reliance on predefined assumptions over attack settings or data distributions and little impact on benign performance. To evaluate the effectiveness of our approach, we conduct comprehensive experiments on different datasets under various attack settings, where our method achieves the best defensive performance. For instance, we achieve the lowest backdoor accuracy of 3.06% under the difficult Edge-case PGD, showing significant superiority over previous defenses. The results also demonstrate that our method can be well-adapted to a wide range of non-IID degrees without sacrificing the benign performance.

","

","arXiv
DBLP
DBLP"
Attacks based on malicious perturbations on image processing systems and defense methods against them,"DA Esipov, Y Buchaev Abdulhamid…","Journal Scientific and …, 2023",2023-08-10,"<a href=""Google Scholar (2023-08-10) : Attacks based on malicious perturbations on image processing systems and defense methods against them"" target=""_blank"">[https://ntv.ifmo.ru/en/article/22193/ataki_na_osnove_vredonosnyh_vozmuscheniy_na_sistemy_obrabotki_izobrazheniy_i_metody_zaschity_ot_nih.htm]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ntv.ifmo.ru/en/article/22193/ataki_na_osnove_vredonosnyh_vozmuscheniy_na_sistemy_obrabotki_izobrazheniy_i_metody_zaschity_ot_nih.htm]</a>","countering attacks based on malicious perturbations is very relevant. This article describes current attacks, provides an overview and comparative analysis of such attacks …",,Google Scholar
Detecting Trojaned DNNs Using Counterfactual Attributions,K. Sikka I. Sur A. Roy A. Divakaran S. Jha,2023 IEEE International Conference on Assured Autonomy (ICAA),2023-08-10,"<a href=""IEEE (2023-08-10) : Detecting Trojaned DNNs Using Counterfactual Attributions"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10207629]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICAA58325.2023.00019]</a>","We target the problem of detecting Trojans or backdoors in DNNs. Such models behave normally with typical inputs but produce targeted mispredictions for inputs poisoned with a Trojan trigger. Our approach is based on a novel intuition that the trigger behavior is dependent on a few ghost neurons that are activated for both input classes and trigger pattern. We use counterfactual explanations, implemented as neuron attributions, to measure significance of each neuron in switching predictions to a counter-class. We then incrementally excite these neurons and observe that the model’s accuracy drops sharply for Trojaned models as compared to benign models. We support this observation through a theoretical result that shows the attributions for a Trojaned model are concentrated in a small number of features. We encode the accuracy patterns by using a deep temporal set encoder for trojan detection that enables invariance to model architecture and a number of classes. We evaluate our approach on four US IARPA/NIST-TrojAI benchmarks with high diversity in model architectures and trigger patterns. We show consistent gains over state-of-the-art adversarial attack based model diagnosis (+5.8%absolute) and trigger reconstruction based methods (+23.5%), which often require strong assumptions on the nature of the attack.",,IEEE
Achieving Low-Cost and High-Efficient Robust Inference and Training for Convolutional Neural Networks,L Wang,2023,2023-08-09,"<a href=""Google Scholar (2023-08-09) : Achieving Low-Cost and High-Efficient Robust Inference and Training for Convolutional Neural Networks"" target=""_blank"">[https://uh-ir.tdl.org/items/b7c249ae-1207-43aa-b7bb-b4fa7933c7e3]</a>","<a href=""Google Scholar"" target=""_blank"">[https://uh-ir.tdl.org/items/b7c249ae-1207-43aa-b7bb-b4fa7933c7e3]</a>","attacks, including adversarial input and backdoor … scheme developed to counter backdoor insertions during the … , LP-RFL effectively prevents backdoor insertions and …",,Google Scholar
Evil Operation: Breaking Speaker Recognition with PaddingBack,"Z Ye, D Yan, L Dong, K Shen","arXiv preprint arXiv:2308.04179, 2023",2023-08-09,"<a href=""Google Scholar (2023-08-09) : Evil Operation: Breaking Speaker Recognition with PaddingBack"" target=""_blank"">[https://arxiv.org/abs/2308.04179]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2308.04179]</a>","security, particularly in backdoor attacks. Recent research has … , an inaudible backdoor attack that utilizes malicious … our attack with three representative backdoor attacks: …",,Google Scholar
Improved Activation Clipping for Universal Backdoor Mitigation and Test-Time Detection,"Hang Wang, Zhen Xiang, David J. Miller, George Kesidis","arXiv
arXiv","2023-08-08
2023-08","<a href=""arXiv (2023-08-08) : Improved Activation Clipping for Universal Backdoor Mitigation and Test-Time Detection"" target=""_blank"">[http://arxiv.org/abs/2308.04617v1]</a>
<a href=""DBLP (2023-08) : Improved Activation Clipping for Universal Backdoor Mitigation and Test-Time Detection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.04617]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.04617]</a>","Deep neural networks are vulnerable to backdoor attacks (Trojans), where an attacker poisons the training set with backdoor triggers so that the neural network learns to classify test-time triggers to the attacker's designated target class. Recent work shows that backdoor poisoning induces over-fitting (abnormally large activations) in the attacked model, which motivates a general, post-training clipping method for backdoor mitigation, i.e., with bounds on internal-layer activations learned using a small set of clean samples. We devise a new such approach, choosing the activation bounds to explicitly limit classification margins. This method gives superior performance against peer methods for CIFAR-10 image classification. We also show that this method has strong robustness against adaptive attacks, X2X attacks, and on different datasets. Finally, we demonstrate a method extension for test-time detection and correction based on the output differences between the original and activation-bounded networks. The code of our method is online available.
","
","arXiv
DBLP"
XGBD: Explanation-Guided Graph Backdoor Detection,"Zihan Guan, Mengnan Du, Ninghao Liu","arXiv
ECAI
arXiv","2023-08-08
2023
2023-08","<a href=""arXiv (2023-08-08) : XGBD: Explanation-Guided Graph Backdoor Detection"" target=""_blank"">[http://arxiv.org/abs/2308.04406v1]</a>
<a href=""DBLP (2023) : XGBD: Explanation-Guided Graph Backdoor Detection"" target=""_blank"">[https://doi.org/10.3233/FAIA230363]</a>
<a href=""DBLP (2023-08) : XGBD: Explanation-Guided Graph Backdoor Detection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.04406]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.3233/FAIA230363]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.04406]</a>","Backdoor attacks pose a significant security risk to graph learning models. Backdoors can be embedded into the target model by inserting backdoor triggers into the training dataset, causing the model to make incorrect predictions when the trigger is present. To counter backdoor attacks, backdoor detection has been proposed. An emerging detection strategy in the vision and NLP domains is based on an intriguing phenomenon: when training models on a mixture of backdoor and clean samples, the loss on backdoor samples drops significantly faster than on clean samples, allowing backdoor samples to be easily detected by selecting samples with the lowest loss values. However, the ignorance of topological feature information on graph data limits its detection effectiveness when applied directly to the graph domain. To this end, we propose an explanation-guided backdoor detection method to take advantage of the topological information. Specifically, we train a helper model on the graph dataset, feed graph samples into the model, and then adopt explanation methods to attribute model prediction to an important subgraph. We observe that backdoor samples have distinct attribution distribution than clean samples, so the explanatory subgraph could serve as more discriminative features for detecting backdoor samples. Comprehensive experiments on multiple popular datasets and attack methods demonstrate the effectiveness and explainability of our method. Our code is available: https://github.com/GuanZihan/GNN_backdoor_detection.

","<a href=""arXiv"" target=""_blank"">[https://github.com/GuanZihan/GNN_backdoor_detection]</a>

","arXiv
DBLP
DBLP"
Adversarial AI threats on digitization of Nuclear Power Plants,"KD Gupta, S Talukder, SB Alam","Authorea Preprints, 2023",2023-08-08,"<a href=""Google Scholar (2023-08-08) : Adversarial AI threats on digitization of Nuclear Power Plants"" target=""_blank"">[https://www.techrxiv.org/doi/full/10.36227/techrxiv.23853510.v1]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.techrxiv.org/doi/full/10.36227/techrxiv.23853510.v1]</a>","attacks such as troj-AI, evasion-based attacks, backdoor attacks, and pre-trained poisoned attacks … These attacks exploit the vulnerabilities of AI models and can be difficult …",,Google Scholar
B<sup>3</sup>: Backdoor Attacks against Black-box Machine Learning Models,Gong X.,ACM Transactions on Privacy and Security,2023-08-08,"<a href=""ScienceDirect (2023-08-08) : B<sup>3</sup>: Backdoor Attacks against Black-box Machine Learning Models"" target=""_blank"">[https://doi.org/10.1145/3605212]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3605212]</a>",,,ScienceDirect
TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models,"Indranil Sur, Karan Sikka, Matthew Walmer, Kaushik Koneripalli, Anirban Roy, Xiao Lin, Ajay Divakaran, Susmit Jha","arXiv
ICCV
arXiv","2023-08-07
2023
2023-08","<a href=""arXiv (2023-08-07) : TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models"" target=""_blank"">[http://arxiv.org/abs/2308.03906v1]</a>
<a href=""DBLP (2023) : TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00022]</a>
<a href=""DBLP (2023-08) : TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.03906]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00022]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.03906]</a>","We present a Multimodal Backdoor Defense technique TIJO (Trigger Inversion using Joint Optimization). Recent work arXiv:2112.07668 has demonstrated successful backdoor attacks on multimodal models for the Visual Question Answering task. Their dual-key backdoor trigger is split across two modalities (image and text), such that the backdoor is activated if and only if the trigger is present in both modalities. We propose TIJO that defends against dual-key attacks through a joint optimization that reverse-engineers the trigger in both the image and text modalities. This joint optimization is challenging in multimodal models due to the disconnected nature of the visual pipeline which consists of an offline feature extractor, whose output is then fused with the text using a fusion module. The key insight enabling the joint optimization in TIJO is that the trigger inversion needs to be carried out in the object detection box feature space as opposed to the pixel space. We demonstrate the effectiveness of our method on the TrojVQA benchmark, where TIJO improves upon the state-of-the-art unimodal methods from an AUC of 0.6 to 0.92 on multimodal dual-key backdoors. Furthermore, our method also improves upon the unimodal baselines on unimodal backdoors. We present ablation studies and qualitative results to provide insights into our algorithm such as the critical importance of overlaying the inverted feature triggers on all visual features during trigger inversion. The prototype implementation of TIJO is available at https://github.com/SRI-CSL/TIJO.

","<a href=""arXiv"" target=""_blank"">[https://github.com/SRI-CSL/TIJO]</a>

","arXiv
DBLP
DBLP"
A systematic review of federated learning from clients’ perspective: challenges and solutions,"Yashothara Shanmugarasa, Hye-young Paik, ... Liming Zhu",Artificial Intelligence Review,2023-08-07,"<a href=""Springer (2023-08-07) : A systematic review of federated learning from clients’ perspective: challenges and solutions"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-023-10563-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-023-10563-8]</a>",Federated learning (FL) is a machine learning approach that decentralizes data and its processing by allowing clients to train intermediate models on...,,Springer
MM-BD: Post-Training Detection of Backdoor Attacks with Arbitrary Backdoor Pattern Types Using a Maximum Margin Statistic,"Hang Wang, Zhen Xiang, David J. Miller, George Kesidis",arXiv,2023-08-06,"<a href=""arXiv (2023-08-06) : MM-BD: Post-Training Detection of Backdoor Attacks with Arbitrary Backdoor Pattern Types Using a Maximum Margin Statistic"" target=""_blank"">[http://arxiv.org/abs/2205.06900v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Backdoor attacks are an important type of adversarial threat against deep neural network classifiers, wherein test samples from one or more source classes will be (mis)classified to the attacker's target class when a backdoor pattern is embedded. In this paper, we focus on the post-training backdoor defense scenario commonly considered in the literature, where the defender aims to detect whether a trained classifier was backdoor-attacked without any access to the training set. Many post-training detectors are designed to detect attacks that use either one or a few specific backdoor embedding functions (e.g., patch-replacement or additive attacks). These detectors may fail when the backdoor embedding function used by the attacker (unknown to the defender) is different from the backdoor embedding function assumed by the defender. In contrast, we propose a post-training defense that detects backdoor attacks with arbitrary types of backdoor embeddings, without making any assumptions about the backdoor embedding type. Our detector leverages the influence of the backdoor attack, independent of the backdoor embedding mechanism, on the landscape of the classifier's outputs prior to the softmax layer. For each class, a maximum margin statistic is estimated. Detection inference is then performed by applying an unsupervised anomaly detector to these statistics. Thus, our detector does not need any legitimate clean samples, and can efficiently detect backdoor attacks with arbitrary numbers of source classes. These advantages over several state-of-the-art methods are demonstrated on four datasets, for three different types of backdoor patterns, and for a variety of attack configurations. Finally, we propose a novel, general approach for backdoor mitigation once a detection is made. The mitigation approach was the runner-up at the first IEEE Trojan Removal Competition. The code is online available.",,arXiv
FLAME: Taming Backdoors in Federated Learning (Extended Version 1),"Thien Duc Nguyen, Phillip Rieger, Huili Chen, Hossein Yalame, Helen Möllering, Hossein Fereidooni, Samuel Marchal, Markus Miettinen, Azalia Mirhoseini, Shaza Zeitouni, Farinaz Koushanfar, Ahmad-Reza Sadeghi, Thomas Schneider","arXiv
USENIX Security Symposium","2023-08-05
2022","<a href=""arXiv (2023-08-05) : FLAME: Taming Backdoors in Federated Learning (Extended Version 1)"" target=""_blank"">[http://arxiv.org/abs/2101.02281v5]</a>
<a href=""DBLP (2022) : FLAME: Taming Backdoors in Federated Learning"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity22/presentation/nguyen]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity22/presentation/nguyen]</a>","Federated Learning (FL) is a collaborative machine learning approach allowing participants to jointly train a model without having to share their private, potentially sensitive local datasets with others. Despite its benefits, FL is vulnerable to backdoor attacks, in which an adversary injects manipulated model updates into the model aggregation process so that the resulting model will provide targeted false predictions for specific adversary-chosen inputs. Proposed defenses against backdoor attacks based on detecting and filtering out malicious model updates consider only very specific and limited attacker models, whereas defenses based on differential privacy-inspired noise injection significantly deteriorate the benign performance of the aggregated model. To address these deficiencies, we introduce FLAME, a defense framework that estimates the sufficient amount of noise to be injected to ensure the elimination of backdoors while maintaining the model performance. To minimize the required amount of noise, FLAME uses a model clustering and weight clipping approach. Our evaluation of FLAME on several datasets stemming from application areas including image classification, word prediction, and IoT intrusion detection demonstrates that FLAME removes backdoors effectively with a negligible impact on the benign performance of the models. Furthermore, following the considerable attention that our research has received after its presentation at USENIX SEC 2022, FLAME has become the subject of numerous investigations proposing diverse attack methodologies in an attempt to circumvent it. As a response to these endeavors, we provide a comprehensive analysis of these attempts. Our findings show that these papers (e.g., 3DFed [36]) have not fully comprehended nor correctly employed the fundamental principles underlying FLAME, i.e., our defense mechanism effectively repels these attempted attacks.
","
","arXiv
DBLP"
Certifiers Make Neural Networks Vulnerable to Availability Attacks,"T Lorenz, M Kwiatkowska, M Fritz","… of the 16th ACM Workshop on …, 2023",2023-08-05,"<a href=""Google Scholar (2023-08-05) : Certifiers Make Neural Networks Vulnerable to Availability Attacks"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3605764.3623917]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3605764.3623917]</a>","Before analyzing our backdoor attacks, we start with the … attack, with PGD attack, and with our backdoor attacks. The … Using backdoor attacks, we can perform a significantly …",,Google Scholar
Beating Backdoor Attack at Its Own Game,"Min Liu, Alberto Sangiovanni-Vincentelli, Xiangyu Yue","arXiv
ICCV","2023-08-04
2023","<a href=""arXiv (2023-08-04) : Beating Backdoor Attack at Its Own Game"" target=""_blank"">[http://arxiv.org/abs/2307.15539v3]</a>
<a href=""DBLP (2023) : Beating Backdoor Attack at Its Own Game"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00426]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00426]</a>","Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not affect the network's performance on clean data but would manipulate the network behavior once a trigger pattern is added. Existing defense methods have greatly reduced attack success rate, but their prediction accuracy on clean data still lags behind a clean model by a large margin. Inspired by the stealthiness and effectiveness of backdoor attack, we propose a simple but highly effective defense framework which injects non-adversarial backdoors targeting poisoned samples. Following the general steps in backdoor attack, we detect a small set of suspected samples and then apply a poisoning strategy to them. The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data, but has limited influence on clean data. The defense can be carried out during data preprocessing, without any modification to the standard end-to-end training pipeline. We conduct extensive experiments on multiple benchmarks with different architectures and representative attacks. Results demonstrate that our method achieves state-of-the-art defense effectiveness with by far the lowest performance drop on clean data. Considering the surprising defense ability displayed by our framework, we call for more attention to utilizing backdoor for backdoor defense. Code is available at https://github.com/damianliumin/non-adversarial_backdoor.
","<a href=""arXiv"" target=""_blank"">[https://github.com/damianliumin/non-adversarial_backdoor]</a>
","arXiv
DBLP"
FMSA: a meta-learning framework-based fast model stealing attack technique against intelligent network intrusion detection systems,"Kaisheng Fan, Weizhe Zhang, ... Hui He",Cybersecurity,2023-08-04,"<a href=""Springer (2023-08-04) : FMSA: a meta-learning framework-based fast model stealing attack technique against intelligent network intrusion detection systems"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-023-00171-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-023-00171-y]</a>",Intrusion detection systems are increasingly using machine learning. While machine learning has shown excellent performance in identifying malicious...,,Springer
Latent Separability of Backdoor Attacks on Language Models,"J Choi, J Kalita","Deep Learning, 2023",2023-08-03,"<a href=""Google Scholar (2023-08-03) : Latent Separability of Backdoor Attacks on Language Models"" target=""_blank"">[https://faculty.uccs.edu/jkalita/wp-content/uploads/sites/45/2024/02/00-Proceedings2023Optimized.pdf#page=56]</a>","<a href=""Google Scholar"" target=""_blank"">[https://faculty.uccs.edu/jkalita/wp-content/uploads/sites/45/2024/02/00-Proceedings2023Optimized.pdf#page=56]</a>","3 Methodology This section first formalizes textual backdoor attacks and the latent separability assumption. The specific attack scenario then follows, and the steps …",,Google Scholar
FedGrad: Mitigating Backdoor Attacks in Federated Learning Through Local Ultimate Gradients Inspection,T. D. Nguyen A. D. Nguyen T. -H. Nguyen K. -S. Wong H. H. Pham T. T. Nguyen P. L. Nguyen,"2023 International Joint Conference on Neural Networks (IJCNN)
Proceedings of the International Joint Conference on Neural Networks
arXiv
arXiv
IJCNN","2023-08-02
2023-01-01
2023-04-29
2023-05
2023","<a href=""IEEE (2023-08-02) : FedGrad: Mitigating Backdoor Attacks in Federated Learning Through Local Ultimate Gradients Inspection"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10191655]</a>
<a href=""ScienceDirect (2023-01-01) : FedGrad: Mitigating Backdoor Attacks in Federated Learning Through Local Ultimate Gradients Inspection"" target=""_blank"">[https://doi.org/10.1109/IJCNN54540.2023.10191655]</a>
<a href=""arXiv (2023-04-29) : FedGrad: Mitigating Backdoor Attacks in Federated Learning Through Local Ultimate Gradients Inspection"" target=""_blank"">[http://arxiv.org/abs/2305.00328v1]</a>
<a href=""DBLP (2023-05) : FedGrad: Mitigating Backdoor Attacks in Federated Learning Through Local Ultimate Gradients Inspection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.00328]</a>
<a href=""DBLP (2023) : FedGrad: Mitigating Backdoor Attacks in Federated Learning Through Local Ultimate Gradients Inspection"" target=""_blank"">[https://doi.org/10.1109/IJCNN54540.2023.10191655]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IJCNN54540.2023.10191655]</a>
<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/IJCNN54540.2023.10191655]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.00328]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/IJCNN54540.2023.10191655]</a>","Federated learning (FL) enables multiple clients to train a model without compromising sensitive data. The decentralized nature of FL makes it susceptible to adversarial attacks, especially backdoor insertion during training. Recently, the edge-case backdoor attack employing the tail of the data distribution has been proposed as a powerful one, raising questions about the shortfall in current defenses' robustness guarantees. Specifically, most existing defenses cannot eliminate edge-case backdoor attacks or suffer from a trade-off between backdoor-defending effectiveness and overall performance on the primary task. To tackle this challenge, we propose FedGrad, a novel backdoor-resistant defense for FL that is resistant to cutting-edge backdoor attacks, including the edge-case attack, and performs effectively under heterogeneous client data and a large number of compromised clients. FedGrad is designed as a two-layer filtering mechanism that thoroughly analyzes the ultimate layer's gradient to identify suspicious local updates and remove them from the aggregation process. We evaluate FedGrad under different attack scenarios and show that it significantly outperforms state-of-the-art defense mechanisms. Notably, FedGrad can almost 100% correctly detect the malicious participants, thus providing a significant reduction in the backdoor effect (e.g., backdoor accuracy is less than 8%) while not reducing main accuracy on the primary task.

Federated learning (FL) enables multiple clients to train a model without compromising sensitive data. The decentralized nature of FL makes it susceptible to adversarial attacks, especially backdoor insertion during training. Recently, the edge-case backdoor attack employing the tail of the data distribution has been proposed as a powerful one, raising questions about the shortfall in current defenses' robustness guarantees. Specifically, most existing defenses cannot eliminate edge-case backdoor attacks or suffer from a trade-off between backdoor-defending effectiveness and overall performance on the primary task. To tackle this challenge, we propose FedGrad, a novel backdoor-resistant defense for FL that is resistant to cutting-edge backdoor attacks, including the edge-case attack, and performs effectively under heterogeneous client data and a large number of compromised clients. FedGrad is designed as a two-layer filtering mechanism that thoroughly analyzes the ultimate layer's gradient to identify suspicious local updates and remove them from the aggregation process. We evaluate FedGrad under different attack scenarios and show that it significantly outperforms state-of-the-art defense mechanisms. Notably, FedGrad can almost 100% correctly detect the malicious participants, thus providing a significant reduction in the backdoor effect (e.g., backdoor accuracy is less than 8%) while not reducing the main accuracy on the primary task.

","



","IEEE
ScienceDirect
arXiv
DBLP
DBLP"
Deep Inversion Method for Attacking Lifelong Learning Neural Networks,B. Du Y. Yu H. Liu,2023 International Joint Conference on Neural Networks (IJCNN),2023-08-02,"<a href=""IEEE (2023-08-02) : Deep Inversion Method for Attacking Lifelong Learning Neural Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10191626]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IJCNN54540.2023.10191626]</a>","Artificial neural networks suffer from catastrophic forgetting when knowledge needs to be learned from multi-batch or streaming data. In response to this problem, researchers have proposed a variety of lifelong learning methods to avoid catastrophic forgetting. However, current methods usually do not consider the possibility of malicious attacks. Meanwhile, in real lifelong learning scenarios, batch data or streaming data usually come from an incompletely trusted environment. Attackers can easily manipulate data or inject malicious samples into the training data set. As a result, the reliability of neural networks decreases. Recently, researches of lifelong learning attacks need to obtain real samples of the attacked classes, whether using backdoor attacks or data poisoning attacks. In this paper, we focus on an attack setting that is more suitable for lifelong learning scenario. This setting has two main features. The first is the setting does not require real samples of the attacked classes, and the second is it allows attacks to be performed on tasks that exclude the attacked classes. For this scenario, we propose a lifelong learning attack model based on deep inversion. In the scenario where EWC is used as the benchmark lifelong learning model, our experiments show that 1) in the data poisoning attack, the target accuracy can be significantly decreased by adding 0.5% of poisoned samples 2) The backdoor attack with high accuracy can be achieved by adding 1% of backdoor samples.",,IEEE
Flairs: Fpga-accelerated inference-resistant & secure federated learning,"H Li, P Rieger, S Zeitouni, S Picek…","… Conference on Field …, 2023",2023-08-02,"<a href=""Google Scholar (2023-08-02) : Flairs: Fpga-accelerated inference-resistant & secure federated learning"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10296389/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10296389/]</a>","We adopt an advanced backdooraware aggregation algorithm on the FPGA to counter backdoor attacks. We implemented and evaluated our method on Xilinx VMK-180, …",,Google Scholar
A robust analysis of adversarial attacks on federated learning environments,Nair A.K.,Computer Standards and Interfaces,2023-08-01,"<a href=""ScienceDirect (2023-08-01) : A robust analysis of adversarial attacks on federated learning environments"" target=""_blank"">[https://doi.org/10.1016/j.csi.2023.103723]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.csi.2023.103723]</a>",,,ScienceDirect
Bagm: A backdoor attack for manipulating text-to-image generative models,"J Vice, N Akhtar, R Hartley…","IEEE Transactions on …, 2024",2023-08-01,"<a href=""Google Scholar (2023-08-01) : Bagm: A backdoor attack for manipulating text-to-image generative models"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10494544/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10494544/]</a>","Through the proposed BAGM approach and experiments, we demonstrate how backdoor attacks could manipulate user sentiments by altering a generated output. In …",,Google Scholar
Cybersecurity challenges in IoT-based smart renewable energy,"Alexandre Rekeraho, Daniel Tudor Cotfas, ... Rebecca Acheampong",International Journal of Information Security,2023-08-01,"<a href=""Springer (2023-08-01) : Cybersecurity challenges in IoT-based smart renewable energy"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00732-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00732-9]</a>","The Internet of Things (IoT) makes it possible to collect data from, and issue commands to, devices via the Internet, eliminating the need for humans...",,Springer
"Federated Learning Support for Cybersecurity: Fundamentals, Applications, and Opportunities",R. Mohawesh S. Maqsood Y. Jararweh H. B. Salameh,"2023 International Conference on Intelligent Computing, Communication, Networking and Services (ICCNS)",2023-08-01,"<a href=""IEEE (2023-08-01) : Federated Learning Support for Cybersecurity: Fundamentals, Applications, and Opportunities"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10193279]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICCNS58795.2023.10193279]</a>","The term “Federated Learning” (FL) refers to a modern and advanced intelligence system that uses data storage that is not centralised. Most industrialists are hesitant to use the Internet of Everything (IoT) technology since cyberattacks are common and occur in numerous real-time applications around the globe. To deal with this problem, FL can be used to prevent such cyberattacks by offering better cybersecurity. This research intends to fill in some of the gaps between where federated AI is now and where it can be widely used by doing a thorough investigation of FL’s security and privacy features. We provide an informative description of techniques and different implementation styles, while also examining the current difficulties in FL, and we establish a complete evaluation of security and privacy problems that must be taken into account. Our research shows that the privacy risks connected with FL are lower than the security risks. While inference-based attacks pose the greatest risk to FL’s privacy, transmission difficulties, toxicity, and backdoor breaches pose the greatest risk to security. In the final section of the paper, we outline key areas for future study that will help FL adapt to real-world settings.",,IEEE
Memory Work and the Making of White Working-Class and Racial Identities.,H GIROUX,"CounterPunch, 2023",2023-08-01,"<a href=""Google Scholar (2023-08-01) : Memory Work and the Making of White Working-Class and Racial Identities."" target=""_blank"">[https://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=10862323&asa=N&AN=171861703&h=0mJ0HL4MfdAfwr8CmmRTeaqTqpwySsRsLpGVa1uQBfr7rYSAcRGuW590MxbUCuzSoQZquKid%2Bori%2FqjERVIpYA%3D%3D&crl=c]</a>","<a href=""Google Scholar"" target=""_blank"">[https://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=10862323&asa=N&AN=171861703&h=0mJ0HL4MfdAfwr8CmmRTeaqTqpwySsRsLpGVa1uQBfr7rYSAcRGuW590MxbUCuzSoQZquKid%2Bori%2FqjERVIpYA%3D%3D&crl=c]</a>","town entered Hope High School though the back door of the building, while the rich … of Trumpism that White people are now under attack By black people and that informed …",,Google Scholar
Universal backdoor attack on deep neural networks for malware detection,Zhang Y.,Applied Soft Computing,2023-08-01,"<a href=""ScienceDirect (2023-08-01) : Universal backdoor attack on deep neural networks for malware detection"" target=""_blank"">[https://doi.org/10.1016/j.asoc.2023.110389]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.asoc.2023.110389]</a>",,,ScienceDirect
B3: Backdoor Attacks against Black-box Machine Learning Models,"Xueluan Gong, Yanjiao Chen, Wenbin Yang, Huayang Huang, Qian Wang","ACM Transactions on Privacy and Security (TOPS), Volume 26, Issue 4
ACM Trans. Priv. Secur.","2023-08
2023","<a href=""ACM (2023-08) : B3: Backdoor Attacks against Black-box Machine Learning Models"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3605212]</a>
<a href=""DBLP (2023) : B3: Backdoor Attacks against Black-box Machine Learning Models"" target=""_blank"">[https://doi.org/10.1145/3605212]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3605212]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3605212]</a>","Backdoor attacks aim to inject backdoors to victim machine learning models during training time, such that the backdoored model maintains the prediction power of the original model towards clean inputs and misbehaves towards backdoored inputs with the ...
","
","ACM
DBLP"
Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks,"Zeyu Qin, Liuyi Yao, Daoyuan Chen, Yaliang Li, Bolin Ding, Minhao Cheng","KDD '23: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining

arXiv
KDD
arXiv","2023-08

2023-06-05
2023
2023-02","<a href=""ACM (2023-08) : Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3580305.3599898]</a>
<a href=""OpenReview () : Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks"" target=""_blank"">[https://arxiv.org/pdf/2302.01677.pdf]</a>
<a href=""arXiv (2023-06-05) : Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2302.01677v2]</a>
<a href=""DBLP (2023) : Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1145/3580305.3599898]</a>
<a href=""DBLP (2023-02) : Revisiting Personalized Federated Learning: Robustness Against Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.01677]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3580305.3599898]</a>
<a href=""OpenReview"" target=""_blank"">[https://arxiv.org/pdf/2302.01677.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3580305.3599898]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.01677]</a>","In this work, besides improving prediction accuracy, we study whether personalization could bring robustness benefits to backdoor attacks. We conduct the first study of backdoor attacks in the pFL framework, testing 4 widely used backdoor attacks ...
In this work, besides improving prediction accuracy, we study whether personalization could bring robustness benefits to backdoor attacks. We conduct the first study of backdoor attacks in the pFL framework, testing 4 widely used backdoor attacks against 6 pFL methods on benchmark datasets FEMNIST and CIFAR-10, a total of 600 experiments. The study shows that pFL methods with partial model-sharing can significantly boost robustness against backdoor attacks. In contrast, pFL methods with full model-sharing do not show robustness. To analyze the reasons for varying robustness performances, we provide comprehensive ablation studies on different pFL methods. Based on our findings, we further propose a lightweight defense method, Simple-Tuning, which empirically improves defense performance against backdoor attacks. We believe that our work could provide both guidance for pFL application in terms of its robustness and offer valuable insights to design more robust FL methods in the future. We open-source our code to establish the first benchmark for black-box backdoor attacks in pFL: https://github.com/alibaba/FederatedScope/tree/backdoor-bench.
In this work, besides improving prediction accuracy, we study whether personalization could bring robustness benefits to backdoor attacks. We conduct the first study of backdoor attacks in the pFL framework, testing 4 widely used backdoor attacks against 6 pFL methods on benchmark datasets FEMNIST and CIFAR-10, a total of 600 experiments. The study shows that pFL methods with partial model-sharing can significantly boost robustness against backdoor attacks. In contrast, pFL methods with full model-sharing do not show robustness. To analyze the reasons for varying robustness performances, we provide comprehensive ablation studies on different pFL methods. Based on our findings, we further propose a lightweight defense method, Simple-Tuning, which empirically improves defense performance against backdoor attacks. We believe that our work could provide both guidance for pFL application in terms of its robustness and offer valuable insights to design more robust FL methods in the future. We open-source our code to establish the first benchmark for black-box backdoor attacks in pFL: https://github.com/alibaba/FederatedScope/tree/backdoor-bench.

","
<a href=""OpenReview"" target=""_blank"">[https://github.com/alibaba/FederatedScope/tree/backdoor-bench]</a>
<a href=""arXiv"" target=""_blank"">[https://github.com/alibaba/FederatedScope/tree/backdoor-bench]</a>

","ACM
OpenReview
arXiv
DBLP
DBLP"
Parameterizing poisoning attacks in federated learning-based intrusion detection,"Mohamed Amine Merzouk, Frédéric Cuppens, Nora Boulahia-Cuppens, Reda Yaich","ARES '23: Proceedings of the 18th International Conference on Availability, Reliability and Security",2023-08,"<a href=""ACM (2023-08) : Parameterizing poisoning attacks in federated learning-based intrusion detection"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3600160.3605090]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3600160.3605090]</a>","Federated learning is a promising research direction in network intrusion detection. It enables collaborative training of machine learning models without revealing sensitive data. However, the lack of transparency in federated learning creates a ...",,ACM
Test-Time Adaptation for Backdoor Defense,"Jiyang Guan, Jian Liang, Ran He",arXiv,2023-08,"<a href=""DBLP (2023-08) : Test-Time Adaptation for Backdoor Defense"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.06107]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2308.06107]</a>",,,DBLP
AI Safety: where do we stand presently?,"H Arjun, A Mohammed Shahid",2023,2023-07-31,"<a href=""Google Scholar (2023-07-31) : AI Safety: where do we stand presently?"" target=""_blank"">[https://econpapers.repec.org/paper/iikwpaper/584.htm]</a>","<a href=""Google Scholar"" target=""_blank"">[https://econpapers.repec.org/paper/iikwpaper/584.htm]</a>","We explore various jailbreaking methods, from adversarial examples to backdoor attacks, and underscore their ramifications on model reliability and security. Red teaming …",,Google Scholar
Backdoor Attacks against Transformer-based Neural Networks for Tabular Data,"B Pleiter, S Picek, MA Larson",2023,2023-07-31,"<a href=""Google Scholar (2023-07-31) : Backdoor Attacks against Transformer-based Neural Networks for Tabular Data"" target=""_blank"">[https://www.b4rt.nl/files/MasterThesis.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.b4rt.nl/files/MasterThesis.pdf]</a>","This thesis investigates the threat of backdoor attacks … attack parameters. We found our tested models very susceptible to backdoor attacks, as we can achieve high attack …",,Google Scholar
FTA: Stealthy and Robust Backdoor Attack with Flexible Trigger on Federated Learning,"Y Qiao, C Chen, R Wang, K Liang","arXiv e-prints, 2023",2023-07-31,"<a href=""Google Scholar (2023-07-31) : FTA: Stealthy and Robust Backdoor Attack with Flexible Trigger on Federated Learning"" target=""_blank"">[https://ui.adsabs.harvard.edu/abs/2023arXiv230900127Q/abstract]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ui.adsabs.harvard.edu/abs/2023arXiv230900127Q/abstract]</a>","Current backdoor attacks against federated learning (FL… stealthy and robust backdoor attack with flexible triggers … label), we make our attack naturally stealthy. Extensive …",,Google Scholar
Towards Securing Untrusted Deep Neural Networks,J Guo,2023,2023-07-31,"<a href=""Google Scholar (2023-07-31) : Towards Securing Untrusted Deep Neural Networks"" target=""_blank"">[https://utd-ir.tdl.org/items/f98846d7-3922-409e-be54-d9bc68d9d95d]</a>","<a href=""Google Scholar"" target=""_blank"">[https://utd-ir.tdl.org/items/f98846d7-3922-409e-be54-d9bc68d9d95d]</a>","To tackle the Trojan attacks, we propose two methods, ie, AEVA and SCALE-UP. … in the backdoor defense task. Lastly, we discuss the potential adaptive attacks against our …",,Google Scholar
Backdoor attacks for in-context learning with language models,"N Kandpal, M Jagielski, F Tramèr, N Carlini","arXiv preprint arXiv …, 2023",2023-07-28,"<a href=""Google Scholar (2023-07-28) : Backdoor attacks for in-context learning with language models"" target=""_blank"">[https://arxiv.org/abs/2307.14692]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2307.14692]</a>","of developing backdoor attacks, as a successful backdoor must work … We design a new attack for eliciting targeted … new threat model for backdoor attacks against LMs with …",,Google Scholar
Backdoor Attacks for In-Context Learning with Language Models,"Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, Nicholas Carlini","arXiv
arXiv","2023-07-27
2023-07","<a href=""arXiv (2023-07-27) : Backdoor Attacks for In-Context Learning with Language Models"" target=""_blank"">[http://arxiv.org/abs/2307.14692v1]</a>
<a href=""DBLP (2023-07) : Backdoor Attacks for In-Context Learning with Language Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.14692]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.14692]</a>","Because state-of-the-art language models are expensive to train, most practitioners must make use of one of the few publicly available language models or language model APIs. This consolidation of trust increases the potency of backdoor attacks, where an adversary tampers with a machine learning model in order to make it perform some malicious behavior on inputs that contain a predefined backdoor trigger. We show that the in-context learning ability of large language models significantly complicates the question of developing backdoor attacks, as a successful backdoor must work against various prompting strategies and should not affect the model's general purpose capabilities. We design a new attack for eliciting targeted misclassification when language models are prompted to perform a particular target task and demonstrate the feasibility of this attack by backdooring multiple large language models ranging in size from 1.3 billion to 6 billion parameters. Finally we study defenses to mitigate the potential harms of our attack: for example, while in the white-box setting we show that fine-tuning models for as few as 500 steps suffices to remove the backdoor behavior, in the black-box setting we are unable to develop a successful defense that relies on prompt engineering alone.
","
","arXiv
DBLP"
Protecting Models and Data in Federated and Centralized Learning,,,2023-07-27,"<a href=""Google Scholar (2023-07-27) : Protecting Models and Data in Federated and Centralized Learning"" target=""_blank"">[https://www.tdx.cat/handle/10803/688858]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.tdx.cat/handle/10803/688858]</a>","In addition to those four defenses, we propose two more defenses against backdoor and model stealing attacks that can be adopted both in federated and centralized …",,Google Scholar
Active poisoning: efficient backdoor attacks on transfer learning-based brain-computer interfaces,"Xue Jiang, Lubin Meng, ... Dongrui Wu","Science China Information Sciences
Sci. China Inf. Sci.","2023-07-26
2023","<a href=""Springer (2023-07-26) : Active poisoning: efficient backdoor attacks on transfer learning-based brain-computer interfaces"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-022-3548-2]</a>
<a href=""DBLP (2023) : Active poisoning: efficient backdoor attacks on transfer learning-based brain-computer interfaces"" target=""_blank"">[https://doi.org/10.1007/s11432-022-3548-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-022-3548-2]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/s11432-022-3548-2]</a>","Transfer learning (TL) has been widely used in electroencephalogram (EEG)-based brain-computer interfaces (BCIs) for reducing calibration efforts....
","
","Springer
DBLP"
Benchmarking the Effect of Poisoning Defenses on the Security and Bias of Deep Learning Models,N. Baracaldo F. Ahmed K. Eykholt Y. Zhou S. Priya T. Lee S. Kadhe M. Tan S. Polavaram S. Suggs Y. Gao D. Slater,2023 IEEE Security and Privacy Workshops (SPW),2023-07-26,"<a href=""IEEE (2023-07-26) : Benchmarking the Effect of Poisoning Defenses on the Security and Bias of Deep Learning Models"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10188619]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SPW59333.2023.00010]</a>","Machine learning models are susceptible to a class of attacks known as adversarial poisoning where an adversary can maliciously manipulate training data to hinder model performance or, more concerningly, insert backdoors to exploit at inference time. Many methods have been proposed to defend against adversarial poisoning by either identifying the poisoned samples to facilitate removal or developing poison agnostic training algorithms. Although effective, these proposed approaches can have unintended consequences on the model, such as worsening performance on certain data sub-populations, thus inducing a classification bias. In this work, we evaluate several adversarial poisoning defenses. In addition to traditional security metrics, i.e., robustness to poisoned samples, we also adapt a fairness metric to measure the potential undesirable discrimination of sub-populations resulting from using these defenses. Our investigation highlights that many of the evaluated defenses trade decision fairness to achieve higher adversarial poisoning robustness. Given these results, we recommend our proposed metric to be part of standard evaluations of machine learning defenses.",,IEEE
Intelligent radar image recognition countermeasures: A review,"GAO Xunzhang, Z Zhiwei, LIU Mei, G Zhenghui…","雷达学报, 2023",2023-07-26,"<a href=""Google Scholar (2023-07-26) : Intelligent radar image recognition countermeasures: A review"" target=""_blank"">[https://radars.ac.cn/en/article/doi/10.12000/JR23098]</a>","<a href=""Google Scholar"" target=""_blank"">[https://radars.ac.cn/en/article/doi/10.12000/JR23098]</a>","A triggerless backdoor attack and defense mechanism for intelligent task offloading in multi-UAV systems[J]. IEEE Internet of Things Journal, 2023, 10(7): 5719–5732. doi: …",,Google Scholar
MSDT: Masked Language Model Scoring Defense in Text Domain,J. Roh M. Cheng Y. Fang,2022 6th International Conference on Universal Village (UV),2023-07-26,"<a href=""IEEE (2023-07-26) : MSDT: Masked Language Model Scoring Defense in Text Domain"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10185524]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/UV56588.2022.10185524]</a>","Pre-trained language models allowed us to process downstream tasks with the help of fine-tuning, which aids the model to achieve fairly high accuracy in various Natural Language Processing (NLP) tasks. Such easily-downloaded language models from various websites empowered the public users as well as some major institutions to give a momentum to their real-life application. However, it was recently proven that models become extremely vulnerable when they are backdoor attacked with trigger-inserted poisoned datasets by malicious users. The attackers then redistribute the victim models to the public to attract other users to use them, where the models tend to misclassify when certain triggers are detected within the training sample. In this paper, we will introduce a novel improved textual backdoor defense method, named MSDT, that outperforms the current existing defensive algorithms in specific datasets. The experimental results illustrate that our method can be effective and constructive in terms of defending against backdoor attack in text domain.",,IEEE
RETRACTED ARTICLE: Fusion of transformer and ML-CNN-BiLSTM for network intrusion detection,"Zelin Xiang, Xuwei Li",EURASIP Journal on Wireless Communications and Networking,2023-07-26,"<a href=""Springer (2023-07-26) : RETRACTED ARTICLE: Fusion of transformer and ML-CNN-BiLSTM for network intrusion detection"" target=""_blank"">[https://link.springer.com/article/10.1186/s13638-023-02279-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13638-023-02279-8]</a>","Network intrusion detection system (NIDS) can effectively sense network attacks, which is of great significance for maintaining the security of...",,Springer
Reflections on Trusting Docker: Invisible Malware in Continuous Integration Systems,F. Moriconi A. I. Neergaard L. Georget S. Aubertin A. Francillon,2023 IEEE Security and Privacy Workshops (SPW),2023-07-26,"<a href=""IEEE (2023-07-26) : Reflections on Trusting Docker: Invisible Malware in Continuous Integration Systems"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10188631]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SPW59333.2023.00025]</a>","Continuous integration (CI) is a widely adopted methodology for supporting software development. It provides automated generation of artifacts (e.g., binaries, container images) which are then deployed in production. However, to which extent should you trust the generated artifacts even if the source code is clean of malicious code? Revisiting the famous compiler backdoor from Ken Thompson, we show that a container-based CI system can be compromised without leaving any trace in the source code. Therefore, detecting such malware is challenging or even impossible with common practices such as peer review or static code analysis. We detail multiple ways to do the initial infection process. Then, we show how to persist during CI system updates, allowing long-term compromise. We detail possible malicious attack payloads such as sensitive data extraction or backdooring production software. We show that infected CI systems can be remotely controlled using covert channels to update attack payload or adapt malware to mitigation strategies. Finally, we propose a proof of concept implementation tested on GitLab CI and applicable to major CI providers.",,IEEE
Backdoor Attacks against Voice Recognition Systems: A Survey,"Baochen Yan, Jiahe Lan, Zheng Yan","arXiv
arXiv","2023-07-23
2023-07","<a href=""arXiv (2023-07-23) : Backdoor Attacks against Voice Recognition Systems: A Survey"" target=""_blank"">[http://arxiv.org/abs/2307.13643v1]</a>
<a href=""DBLP (2023-07) : Backdoor Attacks against Voice Recognition Systems: A Survey"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.13643]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.13643]</a>","Voice Recognition Systems (VRSs) employ deep learning for speech recognition and speaker recognition. They have been widely deployed in various real-world applications, from intelligent voice assistance to telephony surveillance and biometric authentication. However, prior research has revealed the vulnerability of VRSs to backdoor attacks, which pose a significant threat to the security and privacy of VRSs. Unfortunately, existing literature lacks a thorough review on this topic. This paper fills this research gap by conducting a comprehensive survey on backdoor attacks against VRSs. We first present an overview of VRSs and backdoor attacks, elucidating their basic knowledge. Then we propose a set of evaluation criteria to assess the performance of backdoor attack methods. Next, we present a comprehensive taxonomy of backdoor attacks against VRSs from different perspectives and analyze the characteristic of different categories. After that, we comprehensively review existing attack methods and analyze their pros and cons based on the proposed criteria. Furthermore, we review classic backdoor defense methods and generic audio defense techniques. Then we discuss the feasibility of deploying them on VRSs. Finally, we figure out several open issues and further suggest future research directions to motivate the research of VRSs security.
","
","arXiv
DBLP"
Security and privacy issues of federated learning,J Hasan,"arXiv preprint arXiv:2307.12181, 2023",2023-07-22,"<a href=""Google Scholar (2023-07-22) : Security and privacy issues of federated learning"" target=""_blank"">[https://arxiv.org/abs/2307.12181]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2307.12181]</a>","performed by the aggregator and participants, focusing on poisoning attacks, backdoor attacks, membership inference attacks, generative adversarial network (GAN) …",,Google Scholar
FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information,X. Cao J. Jia Z. Zhang N. Z. Gong,2023 IEEE Symposium on Security and Privacy (SP),2023-07-21,"<a href=""IEEE (2023-07-21) : FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179336]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179336]</a>","Federated learning is vulnerable to poisoning attacks in which malicious clients poison the global model via sending malicious model updates to the server. Existing defenses focus on preventing a small number of malicious clients from poisoning the global model via robust federated learning methods and detecting malicious clients when there are a large number of them. However, it is still an open challenge how to recover the global model from poisoning attacks after the malicious clients are detected. A naive solution is to remove the detected malicious clients and train a new global model from scratch using the remaining clients. However, such train-from-scratch recovery method incurs a large computation and communication cost, which may be intolerable for resource-constrained clients such as smartphones and IoT devices.In this work, we propose FedRecover, a method that can recover an accurate global model from poisoning attacks with a small computation and communication cost for the clients. Our key idea is that the server estimates the clients’ model updates instead of asking the clients to compute and communicate them during the recovery process. In particular, the server stores the historical information, including the global models and clients’ model updates in each round, when training the poisoned global model before the malicious clients are detected. During the recovery process, the server estimates a client’s model update in each round using its stored historical information. Moreover, we further optimize FedRecover to recover a more accurate global model using warm-up, periodic correction, abnormality fixing, and final tuning strategies, in which the server asks the clients to compute and communicate their exact model updates. Theoretically, we show that the global model recovered by FedRecover is close to or the same as that recovered by train-from-scratch under some assumptions. Empirically, our evaluation on four datasets, three federated learning methods, as well as untargeted and targeted poisoning attacks (e.g., backdoor attacks) shows that FedRecover is both accurate and efficient.",,IEEE
"Half&Half: Demystifying Intel’s Directional Branch Predictors for Fast, Secure Partitioned Execution",H. Yavarzadeh M. Taram S. Narayan D. Stefan D. Tullsen,2023 IEEE Symposium on Security and Privacy (SP),2023-07-21,"<a href=""IEEE (2023-07-21) : Half&Half: Demystifying Intel’s Directional Branch Predictors for Fast, Secure Partitioned Execution"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179309]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179309]</a>","This paper presents Half&Half, a novel software defense against branch-based side-channel attacks. Half&Half isolates the effects of different protection domains on the conditional branch predictors (CBPs) in modern Intel processors. This work presents the first exhaustive analysis of modern conditional branch prediction structures, and reveals for the first time an unknown opportunity to physically partition all CBP structures and completely prevent leakage between two domains using the shared predictor. Half&Half is a software-only solution to branch predictor isolation that requires no changes to the hardware or ISA, and only requires minor modifications to be supported in existing compilers. We implement Half&Half in the LLVM and WebAssembly compilers and show that it incurs an order of magnitude lower overhead compared to the current state-of-the-art branch-based side-channel defenses.",,IEEE
"Open-Source, End-to-End Auditable Tapeout of Hardware Cryptography Module",A. Singhani,2023 IEEE International Symposium on Circuits and Systems (ISCAS),2023-07-21,"<a href=""IEEE (2023-07-21) : Open-Source, End-to-End Auditable Tapeout of Hardware Cryptography Module"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10181702]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ISCAS46773.2023.10181702]</a>","Increasing concerns about data security and backdoored encryption hardware highlight the necessity for fully auditable chip design and fabrication processes. A fully open-source, traceable manufacturing of a chip would allow a system integrator or end-user to check for tampering which may have potentially occurred during design or fabrication. In this paper, we present the first fully end-to-end open-source, silicon-proven cryptography chip, implementing the AES-256 symmetric cipher. We demonstrate our flow for design-space exploration, followed by a successful tapeout of our chip on the SKY130 process node using an open-source RTL-to-GDS pipeline. In addition, we present our methods for bringing up the silicon despite clocking issues, and results proving the functionality of our process. To foster reproducibility and extensibility, all steps from our work, including the fabricated GDS files, are released under an open-source license and are backed by fully-open toolchains.",,IEEE
Shared adversarial unlearning: Backdoor mitigation by unlearning shared adversarial examples,"S Wei, M Zhang, H Zha, B Wu","Advances in Neural …, 2024",2023-07-21,"<a href=""Google Scholar (2023-07-21) : Shared adversarial unlearning: Backdoor mitigation by unlearning shared adversarial examples"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/520425a5a4c2fb7f7fc345078b188201-Abstract-Conference.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/520425a5a4c2fb7f7fc345078b188201-Abstract-Conference.html]</a>","defense, 2) We formulate a bi-level optimization problem for mitigating backdoor attacks in poisoned models based on the derived bound, and propose an efficient method …",,Google Scholar
TrojanModel: A Practical Trojan Attack against Automatic Speech Recognition Systems,W. Zong Y. -W. Chow W. Susilo K. Do S. Venkatesh,2023 IEEE Symposium on Security and Privacy (SP),2023-07-21,"<a href=""IEEE (2023-07-21) : TrojanModel: A Practical Trojan Attack against Automatic Speech Recognition Systems"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179331]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179331]</a>","While deep learning techniques have achieved great success in modern digital products, researchers have shown that deep learning models are susceptible to Trojan attacks. In a Trojan attack, an adversary stealthily modifies a deep learning model such that the model will output a predefined label whenever a trigger is present in the input. In this paper, we present TrojanModel, a practical Trojan attack against Automatic Speech Recognition (ASR) systems. ASR systems aim to transcribe voice input into text, which is easier for subsequent downstream applications to process. We consider a practical attack scenario in which an adversary inserts a Trojan into the acoustic model of a target ASR system. Unlike existing work that uses noise-like triggers that will easily arouse user suspicion, the work in this paper focuses on the use of unsuspicious sounds as a trigger, e.g., a piece of music playing in the background. In addition, TrojanModel does not require the retraining of a target model. Experimental results show that TrojanModel can achieve high attack success rates with negligible effect on the target model’s performance. We also demonstrate that the attack is effective in an over-the-air attack scenario, where audio is played over a physical speaker and received by a microphone.",,IEEE
A hybrid machine learning model for detecting cybersecurity threats in IoT applications,"Midighe Usoh, Philip Asuquo, ... Udoinyang Inyang",International Journal of Information Technology,2023-07-19,"<a href=""Springer (2023-07-19) : A hybrid machine learning model for detecting cybersecurity threats in IoT applications"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-023-01367-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-023-01367-8]</a>",The introduction of the Internet of Things has led to the connectivity of millions of devices with less human interaction. This demand in...,,Springer
Application of BadNets in Spam Filters,"S Roychoudhury, AK Veldanda","arXiv preprint arXiv:2307.09649, 2023",2023-07-18,"<a href=""Google Scholar (2023-07-18) : Application of BadNets in Spam Filters"" target=""_blank"">[https://arxiv.org/abs/2307.09649]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2307.09649]</a>","In this paper, we design backdoor attacks in the domain of spam filtering. By demonstrating … Our results show that the backdoor attacks can be effectively used to identify …",,Google Scholar
Byte-Level Function-Associated Method for Malware Detection.,"J Hao, S Luo, L Pan","Comput. Syst. Sci. Eng., 2023",2023-07-18,"<a href=""Google Scholar (2023-07-18) : Byte-Level Function-Associated Method for Malware Detection."" target=""_blank"">[https://cdn.techscience.cn/files/csse/2023/TSP_CSSE-46-1/TSP_CSSE_33923/TSP_CSSE_33923.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://cdn.techscience.cn/files/csse/2023/TSP_CSSE-46-1/TSP_CSSE_33923/TSP_CSSE_33923.pdf]</a>","To address this issue, an enhanced adversarial byte function associated method for malware backdoor attack is proposed in this paper by categorizing various function …",,Google Scholar
Towards stealthy backdoor attacks against speech recognition via elements of sound,"H Cai, P Zhang, H Dong, Y Xiao…","IEEE Transactions on …, 2024",2023-07-18,"<a href=""Google Scholar (2023-07-18) : Towards stealthy backdoor attacks against speech recognition via elements of sound"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10538215/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10538215/]</a>","In Section II, we briefly review related works about speech recognition and backdoor attacks. Section III illustrates our two stealthy backdoor attacks based on elements of …",,Google Scholar
Towards Stealthy Backdoor Attacks against Speech Recognition via Elements of Sound,"Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Stefanos Koffas, Yiming Li","arXiv
arXiv","2023-07-17
2023-07","<a href=""arXiv (2023-07-17) : Towards Stealthy Backdoor Attacks against Speech Recognition via Elements of Sound"" target=""_blank"">[http://arxiv.org/abs/2307.08208v1]</a>
<a href=""DBLP (2023-07) : Towards Stealthy Backdoor Attacks against Speech Recognition via Elements of Sound"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.08208]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.08208]</a>","Deep neural networks (DNNs) have been widely and successfully adopted and deployed in various applications of speech recognition. Recently, a few works revealed that these models are vulnerable to backdoor attacks, where the adversaries can implant malicious prediction behaviors into victim models by poisoning their training process. In this paper, we revisit poison-only backdoor attacks against speech recognition. We reveal that existing methods are not stealthy since their trigger patterns are perceptible to humans or machine detection. This limitation is mostly because their trigger patterns are simple noises or separable and distinctive clips. Motivated by these findings, we propose to exploit elements of sound ($e.g.$, pitch and timbre) to design more stealthy yet effective poison-only backdoor attacks. Specifically, we insert a short-duration high-pitched signal as the trigger and increase the pitch of remaining audio clips to `mask' it for designing stealthy pitch-based triggers. We manipulate timbre features of victim audios to design the stealthy timbre-based attack and design a voiceprint selection module to facilitate the multi-backdoor attack. Our attacks can generate more `natural' poisoned samples and therefore are more stealthy. Extensive experiments are conducted on benchmark datasets, which verify the effectiveness of our attacks under different settings ($e.g.$, all-to-one, all-to-all, clean-label, physical, and multi-backdoor settings) and their stealthiness. The code for reproducing main experiments are available at \url{https://github.com/HanboCai/BadSpeech_SoE}.
","<a href=""arXiv"" target=""_blank"">[https://github.com/HanboCai/BadSpeech_SoE}]</a>
","arXiv
DBLP"
Convolutional neural networks tamper detection and location based on fragile watermarking,"Yawen Huang, Hongying Zheng, Di Xiao",Applied Intelligence,2023-07-17,"<a href=""Springer (2023-07-17) : Convolutional neural networks tamper detection and location based on fragile watermarking"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-023-04797-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-023-04797-w]</a>","With the wide application of neural network, the trained neural network model has become an important asset to provide services for users, but it...",,Springer
Security risk analysis and protection scheme of video transmission system,N. Lin Z. Yingjie Z. Yang Y. Yiyang,"2023 IEEE 3rd International Conference on Electronic Technology, Communication and Information (ICETCI)",2023-07-17,"<a href=""IEEE (2023-07-17) : Security risk analysis and protection scheme of video transmission system"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10176796]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICETCI57876.2023.10176796]</a>","With the development of the information age, video transmission systems have been widely used. Video transmission systems are widely used in daily life, especially in large conferences, government systems, and military command systems. At the same time, for video transmission systems The attack methods of the video transmission system are also gradually developed, which poses a huge threat to the information security of the video transmission system. Combining with the composition of the video transmission system, the paper analyzes the security threats of the video transmission system from the preset backdoor attack of the video transmission system, electromagnetic leakage attack, power line carrier attack and transmission line attack, and aims at the current command hall and other needs to have multiple security levels. A protection scheme is put forward for the situation that the equipment is connected to the same large screen for display at the same time, which provides a reference for the design and protection of the future video transmission system.",,IEEE
Backdoor Attack and Defense in Federated Generative Adversarial Network-based Medical Image Synthesis,"Ruinan Jin, Xiaoxiao Li","arXiv
arXiv","2023-07-16
2022-10","<a href=""arXiv (2023-07-16) : Backdoor Attack and Defense in Federated Generative Adversarial Network-based Medical Image Synthesis"" target=""_blank"">[http://arxiv.org/abs/2210.10886v3]</a>
<a href=""DBLP (2022-10) : Backdoor Attack and Defense in Federated Generative Adversarial Network-based Medical Image Synthesis"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.10886]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.10886]</a>","Deep Learning-based image synthesis techniques have been applied in healthcare research for generating medical images to support open research and augment medical datasets. Training generative adversarial neural networks (GANs) usually require large amounts of training data. Federated learning (FL) provides a way of training a central model using distributed data while keeping raw data locally. However, given that the FL server cannot access the raw data, it is vulnerable to backdoor attacks, an adversarial by poisoning training data. Most backdoor attack strategies focus on classification models and centralized domains. It is still an open question if the existing backdoor attacks can affect GAN training and, if so, how to defend against the attack in the FL setting. In this work, we investigate the overlooked issue of backdoor attacks in federated GANs (FedGANs). The success of this attack is subsequently determined to be the result of some local discriminators overfitting the poisoned data and corrupting the local GAN equilibrium, which then further contaminates other clients when averaging the generator's parameters and yields high generator loss. Therefore, we proposed FedDetect, an efficient and effective way of defending against the backdoor attack in the FL setting, which allows the server to detect the client's adversarial behavior based on their losses and block the malicious clients. Our extensive experiments on two medical datasets with different modalities demonstrate the backdoor attack on FedGANs can result in synthetic images with low fidelity. After detecting and suppressing the detected malicious clients using the proposed defense strategy, we show that FedGANs can synthesize high-quality medical datasets (with labels) for data augmentation to improve classification models' performance.
","
","arXiv
DBLP"
State of the art on adversarial attacks and defenses in graphs,"Zhengli Zhai, Penghui Li, Shu Feng",Neural Computing and Applications,2023-07-16,"<a href=""Springer (2023-07-16) : State of the art on adversarial attacks and defenses in graphs"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-023-08839-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-023-08839-9]</a>","Graph neural networks (GNNs) had shown excellent performance in complex graph data modelings such as node classification, link prediction and graph...",,Springer
Deep learning for the security of software-defined networks: a review,"Roya Taheri, Habib Ahmed, Engin Arslan",Cluster Computing,2023-07-15,"<a href=""Springer (2023-07-15) : Deep learning for the security of software-defined networks: a review"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-023-04069-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-023-04069-9]</a>","As the scale and complexity of networks grow rapidly, management, maintenance, and optimization of them are becoming increasingly challenging tasks...",,Springer
Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy,"Zihao Zhu, Mingda Zhang, Shaokui Wei, Li Shen, Yanbo Fan, Baoyuan Wu","arXiv
arXiv","2023-07-14
2023-07","<a href=""arXiv (2023-07-14) : Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy"" target=""_blank"">[http://arxiv.org/abs/2307.07328v1]</a>
<a href=""DBLP (2023-07) : Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.07328]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.07328]</a>","Data-poisoning based backdoor attacks aim to insert backdoor into models by manipulating training datasets without controlling the training process of the target model. Existing attack methods mainly focus on designing triggers or fusion strategies between triggers and benign samples. However, they often randomly select samples to be poisoned, disregarding the varying importance of each poisoning sample in terms of backdoor injection. A recent selection strategy filters a fixed-size poisoning sample pool by recording forgetting events, but it fails to consider the remaining samples outside the pool from a global perspective. Moreover, computing forgetting events requires significant additional computing resources. Therefore, how to efficiently and effectively select poisoning samples from the entire dataset is an urgent problem in backdoor attacks.To address it, firstly, we introduce a poisoning mask into the regular backdoor training loss. We suppose that a backdoored model training with hard poisoning samples has a more backdoor effect on easy ones, which can be implemented by hindering the normal training process (\ie, maximizing loss \wrt mask). To further integrate it with normal training process, we then propose a learnable poisoning sample selection strategy to learn the mask together with the model parameters through a min-max optimization.Specifically, the outer loop aims to achieve the backdoor attack goal by minimizing the loss based on the selected samples, while the inner loop selects hard poisoning samples that impede this goal by maximizing the loss. After several rounds of adversarial training, we finally select effective poisoning samples with high contribution. Extensive experiments on benchmark datasets demonstrate the effectiveness and efficiency of our approach in boosting backdoor attack performance.
","
","arXiv
DBLP"
Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection,"Hao Fu, Prashanth Krishnamurthy, Siddharth Garg, Farshad Khorrami","arXiv
IEEE Trans. Inf. Forensics Secur.
arXiv","2023-07-14
2023
2023-07","<a href=""arXiv (2023-07-14) : Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection"" target=""_blank"">[http://arxiv.org/abs/2307.05422v2]</a>
<a href=""DBLP (2023) : Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3297056]</a>
<a href=""DBLP (2023-07) : Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.05422]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3297056]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.05422]</a>","This paper proposes a data-efficient detection method for deep neural networks against backdoor attacks under a black-box scenario. The proposed approach is motivated by the intuition that features corresponding to triggers have a higher influence in determining the backdoored network output than any other benign features. To quantitatively measure the effects of triggers and benign features on determining the backdoored network output, we introduce five metrics. To calculate the five-metric values for a given input, we first generate several synthetic samples by injecting the input's partial contents into clean validation samples. Then, the five metrics are computed by using the output labels of the corresponding synthetic samples. One contribution of this work is the use of a tiny clean validation dataset. Having the computed five metrics, five novelty detectors are trained from the validation dataset. A meta novelty detector fuses the output of the five trained novelty detectors to generate a meta confidence score. During online testing, our method determines if online samples are poisoned or not via assessing their meta confidence scores output by the meta novelty detector. We show the efficacy of our methodology through a broad range of backdoor attacks, including ablation studies and comparison to existing approaches. Our methodology is promising since the proposed five metrics quantify the inherent differences between clean and poisoned samples. Additionally, our detection method can be incrementally improved by appending more metrics that may be proposed to address future advanced attacks.

","

","arXiv
DBLP
DBLP"
Boosting backdoor attack with a learnable poisoning sample selection strategy,"Z Zhu, M Zhang, S Wei, L Shen, Y Fan, B Wu","arXiv preprint arXiv …, 2023",2023-07-14,"<a href=""Google Scholar (2023-07-14) : Boosting backdoor attack with a learnable poisoning sample selection strategy"" target=""_blank"">[https://arxiv.org/abs/2307.07328]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2307.07328]</a>","Instead, we aim to boost existing data-poisoning backdoor attacks through a learnable poisoning sample selection strategy depending on the trigger and benign data. The …",,Google Scholar
结合扩散模型图像编辑的图文检索后门攻击.,杨舜， 陆恒杨,"Journal of Frontiers of Computer Science & …, 2024",2023-07-14,"<a href=""Google Scholar (2023-07-14) : 结合扩散模型图像编辑的图文检索后门攻击."" target=""_blank"">[https://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=16739418&AN=176968418&h=G6UsmpqqioAMYqMD%2F4h9yJuMD%2BcA7xMLYu7RWdlue12KbYuJBvCzFD0FQ2Vy%2FHag2AeoNMAP%2B0OqKyJwY86MsQ%3D%3D&crl=c]</a>","<a href=""Google Scholar"" target=""_blank"">[https://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=16739418&AN=176968418&h=G6UsmpqqioAMYqMD%2F4h9yJuMD%2BcA7xMLYu7RWdlue12KbYuJBvCzFD0FQ2Vy%2FHag2AeoNMAP%2B0OqKyJwY86MsQ%3D%3D&crl=c]</a>","On this basis, this paper proposes a backdoor attack defense … backdoor attacks in image and text retrieval may contribute to the development of multimodal backdoor attack …",,Google Scholar
Ispitivanje sigurnosti dubokih neuronskih mreža za analizu slika,D Smoljan,2023,2023-07-13,"<a href=""Google Scholar (2023-07-13) : Ispitivanje sigurnosti dubokih neuronskih mreža za analizu slika"" target=""_blank"">[https://repozitorij.fer.unizg.hr/en/islandora/object/fer:11574]</a>","<a href=""Google Scholar"" target=""_blank"">[https://repozitorij.fer.unizg.hr/en/islandora/object/fer:11574]</a>","potential attacks on neural networks for image analysis, focusing on backdoor attacks using data poisoning and suitable defense methods. A wide array of attack and …",,Google Scholar
Natural and Imperceptible Backdoor Attack against Deep Neural Networks,S. Ni X. Wang Y. Shang L. Zhang,2023 4th International Conference on Electronic Communication and Artificial Intelligence (ICECAI),2023-07-13,"<a href=""IEEE (2023-07-13) : Natural and Imperceptible Backdoor Attack against Deep Neural Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10176925]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICECAI58670.2023.10176925]</a>","Over the past few years, deep learning has demonstrated impressive performance across a wide range of applications. As the same time, researchers have become increasingly concerned with ensuring the security of deep learning models. Specifically, backdoor attacks have emerged as a significant threat to Deep Learning Model, whereby attackers control the predictions of model by implanting a concealed backdoor into the model. Under this scenario, the backdoored model will appear to make normal predictions on clean images, but will exhibit abnormal behaviors when the trigger is presented. Many existing backdoor attacks utilize a fixed pattern as the trigger, which can be easily detected by defense methods or even humans. Furthermore, existing backdoor attack methods are rarely targeted specifically at the Vision Transformer (ViT) model. Therefore, in this paper, we propose a novel natural backdoor attack method. We exploit natural phenomena to carry out a backdoor attack called the fog backdoor attack, which can utilize fog in the natural world as a trigger to be seamlessly integrated into clean images without being perceived by humans. The generated fog will diffuse over the image, creating a natural-looking effect. In contrast to the fixed and limited triggers produced by other methods, our triggers are more natural and imperceptible. Experimental results demonstrate the effectiveness and robustness of the proposed backdoor attack on different models. Specifically, the attack success rate of the proposed backdoor attack is 98.85% on VGG-16 model, 99.44% on ResNet-18 model and 9S.56% on ViT model, respectively. Furthermore, the proposed attack does not compromise the clean accuracy of the model.",,IEEE
Task-Oriented Communications for NextG: End-to-end Deep Learning and AI Security Aspects,Y. E. Sagduyu S. Ulukus A. Yener,IEEE Wireless Communications,2023-07-13,"<a href=""IEEE (2023-07-13) : Task-Oriented Communications for NextG: End-to-end Deep Learning and AI Security Aspects"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10183788]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/MWC.006.2200494]</a>","Communications systems to date are primarily designed with the goal of reliable transfer of digital sequences (bits). Next generation (NextG) communication systems are beginning to explore shifting this design paradigm to reliably executing a given task, such as in task-oriented communications. In this article, wireless signal classification is considered as the task for the NextG Radio Access Network (RAN), where edge devices collect wireless signals for spectrum awareness and communicate with the NextG base station (gNodeB) that needs to identify the signal label. Edge devices may not have sufficient processing power and may not be trusted to perform the signal classification task, whereas the transfer of signals to the gNodeB may not be feasible due to stringent delay, rate, and energy restrictions. Task-oriented communications is considered by jointly training the transmitter, receiver, and classifier functionalities as an encoder-decoder pair for the edge device and the gNodeB. This approach improves the accuracy compared to the separated case of signal transfer followed by classification. Adversarial machine learning poses a major security threat to the use of deep learning for task-oriented communications. A major performance loss is shown when backdoor (Trojan) and adversarial (evasion) attacks target the training and test processes of task-oriented communications.",,IEEE
The evolution of ransomware attacks in light of recent cyber threats. How can geopolitical conflicts influence the cyber climate?,"Fabian Teichmann, Sonia R. Boticiu, Bruno S. Sergi",International Cybersecurity Law Review,2023-07-13,"<a href=""Springer (2023-07-13) : The evolution of ransomware attacks in light of recent cyber threats. How can geopolitical conflicts influence the cyber climate?"" target=""_blank"">[https://link.springer.com/article/10.1365/s43439-023-00095-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1365/s43439-023-00095-w]</a>","This article aims to analyze the current unpredictable cyber climate. In particular, Russia’s invasion of Ukraine has heightened concerns about...",,Springer
AI Security for Geoscience and Remote Sensing: Challenges and future trends,Y. Xu T. Bai W. Yu S. Chang P. M. Atkinson P. Ghamisi,IEEE Geoscience and Remote Sensing Magazine,2023-07-12,"<a href=""IEEE (2023-07-12) : AI Security for Geoscience and Remote Sensing: Challenges and future trends"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10180090]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/MGRS.2023.3272825]</a>","Recent advances in artificial intelligence (AI) have significantly intensified research in the geoscience and remote sensing (RS) field. AI algorithms, especially deep learning-based ones, have been developed and applied widely to RS data analysis. The successful application of AI covers almost all aspects of Earth-observation (EO) missions, from low-level vision tasks like superresolution, denoising, and inpainting, to high-level vision tasks like scene classification, object detection, and semantic segmentation. Although AI techniques enable researchers to observe and understand the earth more accurately, the vulnerability and uncertainty of AI models deserve further attention, considering that many geoscience and RS tasks are highly safety critical. This article reviews the current development of AI security in the geoscience and RS field, covering the following five important aspects: adversarial attack, backdoor attack, federated learning (FL), uncertainty, and explainability. Moreover, the potential opportunities and trends are discussed to provide insights for future research. To the best of the authors’ knowledge, this article is the first attempt to provide a systematic review of AI security-related research in the geoscience and RS community. Available code and datasets are also listed in the article to move this vibrant field of research forward.",,IEEE
Obrane klasifikacijskih modela od zatrovanih skupova podataka,T Reiter,2023,2023-07-11,"<a href=""Google Scholar (2023-07-11) : Obrane klasifikacijskih modela od zatrovanih skupova podataka"" target=""_blank"">[https://repozitorij.unizg.hr/islandora/object/fer:11541]</a>","<a href=""Google Scholar"" target=""_blank"">[https://repozitorij.unizg.hr/islandora/object/fer:11541]</a>",backdoor attacks on deep models and ways to mitigate them. We want to see how effective are the current defenses against backdoor attacks. … to mitigate backdoor attacks. …,,Google Scholar
PoT: Securely Proving Legitimacy of Training Data and Logic for AI Regulation,,,2023-07-11,"<a href=""Google Scholar (2023-07-11) : PoT: Securely Proving Legitimacy of Training Data and Logic for AI Regulation"" target=""_blank"">[https://genlaw.github.io/CameraReady/22.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://genlaw.github.io/CameraReady/22.pdf]</a>","attacks and that the model and data were called following the logic of training algorithm (eg, no backdoor is … security requirements such as robustness to Byzantine attacks. …",,Google Scholar
The reality of backdoored S-Boxes—An eye opener,"S Fahd, M Afzal, W Iqbal, D Shah, I Khalid","Journal of Information Security …, 2024",2023-07-11,"<a href=""Google Scholar (2023-07-11) : The reality of backdoored S-Boxes—An eye opener"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S2214212623002582]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S2214212623002582]</a>",attack vectors available in the open literature. We have debunked the earlier claims by the backdoor … cryptanalytic attacks. Our analysis has revealed that during the …,,Google Scholar
コンセプトをトリガーとしたステルス性の高いバックドア攻撃,大磯秀幸， 福地一斗， 秋本洋平…,"人工知能学会全国大会論文 …, 2023",2023-07-11,"<a href=""Google Scholar (2023-07-11) : コンセプトをトリガーとしたステルス性の高いバックドア攻撃"" target=""_blank"">[https://www.jstage.jst.go.jp/article/pjsai/JSAI2023/0/JSAI2023_3L1GS1103/_article/-char/ja/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.jstage.jst.go.jp/article/pjsai/JSAI2023/0/JSAI2023_3L1GS1103/_article/-char/ja/]</a>","Backdoor attacks are a type of attack against machine … In this paper, we propose a backdoor attack using concepts … as a trigger by evaluating the attack success rate of the …",,Google Scholar
Blind Concealment from Reconstruction-based Attack Detectors for Industrial Control Systems via Backdoor Attacks,Walita T.,CPSS 2023 - Proceedings of the 9th ACM ASIA Conference on Cyber-Physical System Security Workshop,2023-07-10,"<a href=""ScienceDirect (2023-07-10) : Blind Concealment from Reconstruction-based Attack Detectors for Industrial Control Systems via Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1145/3592538.3594271]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3592538.3594271]</a>",,,ScienceDirect
CASSOCK: Viable Backdoor Attacks against DNN in the Wall of Source-Specific Backdoor Defenses,Wang S.,Proceedings of the ACM Conference on Computer and Communications Security,2023-07-10,"<a href=""ScienceDirect (2023-07-10) : CASSOCK: Viable Backdoor Attacks against DNN in the Wall of Source-Specific Backdoor Defenses"" target=""_blank"">[https://doi.org/10.1145/3579856.3582829]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3579856.3582829]</a>",,,ScienceDirect
Detection of Attacks in Smart Healthcare deploying Machine Learning Algorithms*,A. Sharma H. Babbar A. K. Vats,2023 4th International Conference for Emerging Technology (INCET),2023-07-10,"<a href=""IEEE (2023-07-10) : Detection of Attacks in Smart Healthcare deploying Machine Learning Algorithms*"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10170367]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/INCET57972.2023.10170367]</a>","The Internet of Things (IoT) is a sort of network that uses a set of protocols and data-sensing tools to link anything to the Internet. This type of network allows for data exchange as well as smart recognition, surveillance, deployment, tracking, and maintenance. The healthcare business is rapidly digitizing, and the IoT is a key factor in this. It has already had an impact on medical networks and will continue to do so soon. This paper highlights privacy concerns as well as security attacks, which have rapidly increased in recent years in the healthcare industry. An architecture is proposed for detecting IoT attacks in smart health-care systems using machine learning (ML) techniques. This study used the UNSW-NB15 dataset to simultaneously classify Random Forest (RF), Naive Bayes (NB) and K-Nearest Neighbour (KNN) classifiers for detecting the attacks (Generic, Fuzzers, Exploits, Analysis, Denial of Service (DoS), Reconnaissance, Backdoor, Worms and Shellcode) in the healthcare industry. In addition to developing the essential smart and effective online system, this research increased knowledge of privacy and security in the smart healthcare sector. Furthermore, performance measures like accuracy, precision, recall, and F1-score have been calculated using the existing ML techniques based on the mentioned UNSW-NB15 dataset. The results indicate that when comparing the mentioned ML techniques, the KNN model has the highest accuracy value, or 90%, while RF and NB provide 89% and 75% accuracy values, respectively.",,IEEE
Evaluating the Robustness of Trigger Set-Based Watermarks Embedded in Deep Neural Networks,S. Lee W. Song S. Jana M. Cha S. Son,IEEE Transactions on Dependable and Secure Computing,2023-07-10,"<a href=""IEEE (2023-07-10) : Evaluating the Robustness of Trigger Set-Based Watermarks Embedded in Deep Neural Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851498]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TDSC.2022.3196790]</a>","Trigger set-based watermarking schemes have gained emerging attention as they provide a means to prove ownership for deep neural network model owners. In this paper, we argue that state-of-the-art trigger set-based watermarking algorithms do not achieve their designed goal of proving ownership. We posit that this impaired capability stems from two common experimental flaws that the existing research practice has committed when evaluating the robustness of watermarking algorithms: (1) incomplete adversarial evaluation and (2) overlooked adaptive attacks. We conduct a comprehensive adversarial evaluation of 11 representative watermarking schemes against six of the existing attacks and demonstrate that each of these watermarking schemes lacks robustness against at least two non-adaptive attacks. We also propose novel adaptive attacks that harness the adversary's knowledge of the underlying watermarking algorithm of a target model. We demonstrate that the proposed attacks effectively break all of the 11 watermarking schemes, consequently allowing adversaries to obscure the ownership of any watermarked model. We encourage follow-up studies to consider our guidelines when evaluating the robustness of their watermarking schemes via conducting comprehensive adversarial evaluation that includes our adaptive attacks to demonstrate a meaningful upper bound of watermark robustness.",,IEEE
"Maximum entropy loss, the silver bullet targeting backdoor attacks in pre-trained language models","Z Liu, B Shen, Z Lin, F Wang…","Findings of the Association …, 2023",2023-07-10,"<a href=""Google Scholar (2023-07-10) : Maximum entropy loss, the silver bullet targeting backdoor attacks in pre-trained language models"" target=""_blank"">[https://aclanthology.org/2023.findings-acl.237/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://aclanthology.org/2023.findings-acl.237/]</a>","On the contrary, we provide a new perspective where the backdoor attack is directly … of backdoor attacks on classification tasks and significantly lower the attack success …",,Google Scholar
Vision transformer architecture and applications in digital health: a tutorial and survey,"Khalid Al-hammuri, Fayez Gebali, ... Ilamparithi Thirumarai Chelvan","Visual Computing for Industry, Biomedicine, and Art",2023-07-10,"<a href=""Springer (2023-07-10) : Vision transformer architecture and applications in digital health: a tutorial and survey"" target=""_blank"">[https://link.springer.com/article/10.1186/s42492-023-00140-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42492-023-00140-9]</a>",The vision transformer (ViT) is a state-of-the-art architecture for image recognition tasks that plays an important role in digital health...,,Springer
機械学習ベースのマルウェア検知システムに対する攻撃とその対策に関する研究,鄭万嘉， テイバンカ,2023,2023-07-08,"<a href=""Google Scholar (2023-07-08) : 機械学習ベースのマルウェア検知システムに対する攻撃とその対策に関する研究"" target=""_blank"">[https://tsukuba.repo.nii.ac.jp/record/2007998/files/DA010625.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://tsukuba.repo.nii.ac.jp/record/2007998/files/DA010625.pdf]</a>","Therefore, in this research, we focus on the field of malware detection and propose a novel clean-label backdoor attack and evasion attack under the black-box …",,Google Scholar
Applying Heat Maps on a Traffic Sign Detection Case Study,A Petrov,2023,2023-07-07,"<a href=""Google Scholar (2023-07-07) : Applying Heat Maps on a Traffic Sign Detection Case Study"" target=""_blank"">[http://essay.utwente.nl/96099/]</a>","<a href=""Google Scholar"" target=""_blank"">[http://essay.utwente.nl/96099/]</a>","The main background for this research is based on existing literature on backdoor attacks and their respective mitigation methods, such as [12], [5], [6], [4], [13], [7]. The …",,Google Scholar
Federated unlearning via active forgetting,"Y Li, C Chen, X Zheng, J Zhang","arXiv preprint arXiv:2307.03363, 2023",2023-07-07,"<a href=""Google Scholar (2023-07-07) : Federated unlearning via active forgetting"" target=""_blank"">[https://arxiv.org/abs/2307.03363]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2307.03363]</a>","result of backdoor attacks … backdoor triggers into the target data that we aim to unlearn, and flip their labels to a random class. Through this process, the backdoor attack …",,Google Scholar
Research on Industrial Internet Data Security Technology based on Formal Verification of SM4,Z. Chen Y. Deng M. Yang C. Mu S. Xu F. Wu,"2023 2nd Asia Conference on Electrical, Power and Computer Engineering (EPCE)",2023-07-07,"<a href=""IEEE (2023-07-07) : Research on Industrial Internet Data Security Technology based on Formal Verification of SM4"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10169022]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/EPCE58798.2023.00039]</a>","Industrial Internet promotes the transformation of industrial platform from business system driven to data driven through the flow of industrial data between massive multi-source equipment and heterogeneous systems. The data flow of industrial Internet involves the interactive transmission of a large number of key sensitive information between enterprises. Coupled with its inherent openness and heterogeneity, it is facing serious security challenges such as sensitive data leakage and integrity destruction. Cryptography technology is one of the important methods to ensure data security. Among them, the national cryptographic algorithm can meet the requirements of security, autonomy and controllability of Chinese industrial Internet data, and avoid the risk of ""backdoor"" when the current mainstream international cryptography algorithm protects the key sensitive data. However, there are still security threats such as memory leakage and time-side channel attack which cause the invalidity of encryption mechanism when the national cryptographic algorithm is deployed in industrial Internet system. Formal verification method is an important method to verify the security and reliability of cryptographic algorithms. Existing researches have designed a variety of formal verification mechanisms for the underlying cryptographic algorithms of some structures and protocols, but the research on formal verification for national cryptographic algorithm is still in a blank stage. This paper proposes a formal verification method of SM4 cryptographic algorithm based on programming framework F*, which can ensure the memory security and resist the time-side channel attacks during the implementation of the algorithm. And the method can reduce the security risks, when the national cryptographic algorithm is deployed in the industrial internet.",,IEEE
Going Haywire: False Friends in Federated Learning and How to Find Them,"W Aiken, P Branco, GV Jourdan","Proceedings of the 2023 ACM Asia …, 2023",2023-07-06,"<a href=""Google Scholar (2023-07-06) : Going Haywire: False Friends in Federated Learning and How to Find Them"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3579856.3595790]</a>","<a href=""Google Scholar"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3579856.3595790]</a>","Through an extensive set of experiments, we find that Haywire produces the best performances at preventing backdoor attacks while simultaneously not unfairly penalizing …",,Google Scholar
Machine learning-based intrusion detection: feature selection versus feature extraction,"Vu-Duc Ngo, Tuan-Cuong Vuong, ... Hung Tran",Cluster Computing,2023-07-05,"<a href=""Springer (2023-07-05) : Machine learning-based intrusion detection: feature selection versus feature extraction"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-023-04089-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-023-04089-5]</a>","Internet of Things (IoTs) has been playing an important role in many sectors, such as smart cities, smart agriculture, smart healthcare, and smart...",,Springer
Specifying a principle of cryptographic justice as a response to the problem of going dark,Michael Wilson,Ethics and Information Technology,2023-07-05,"<a href=""Springer (2023-07-05) : Specifying a principle of cryptographic justice as a response to the problem of going dark"" target=""_blank"">[https://link.springer.com/article/10.1007/s10676-023-09707-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10676-023-09707-9]</a>","Over the past decade, the Five Eyes Intelligence community has argued cryptosystems with end-to-end encryption (E2EE) are disrupting the acquisition...",,Springer
Napadi na neuronske mreže za detekciju objekata umetanjem stražnjih vrata,T Prhat,2023,2023-07-04,"<a href=""Google Scholar (2023-07-04) : Napadi na neuronske mreže za detekciju objekata umetanjem stražnjih vrata"" target=""_blank"">[https://repozitorij.unizg.hr/islandora/object/fer:11517]</a>","<a href=""Google Scholar"" target=""_blank"">[https://repozitorij.unizg.hr/islandora/object/fer:11517]</a>","explanations of stealth attacks and various attack strategies, … of backdoor attacks by inserting back doors in object … and results of backdoor attacks on object detection. The …",,Google Scholar
NBA: defensive distillation for backdoor removal via neural behavior alignment,"Zonghao Ying, Bin Wu","Cybersecurity
arXiv
Cybersecur.","2023-07-03
2024-06-16
2023","<a href=""Springer (2023-07-03) : NBA: defensive distillation for backdoor removal via neural behavior alignment"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-023-00154-z]</a>
<a href=""arXiv (2024-06-16) : NBA: defensive distillation for backdoor removal via neural behavior alignment"" target=""_blank"">[http://arxiv.org/abs/2406.10846v1]</a>
<a href=""DBLP (2023) : NBA: defensive distillation for backdoor removal via neural behavior alignment"" target=""_blank"">[https://doi.org/10.1186/s42400-023-00154-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-023-00154-z]</a>
<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1186/s42400-023-00154-z]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1186/s42400-023-00154-z]</a>","Recently, deep neural networks have been shown to be vulnerable to backdoor attacks. A backdoor is inserted into neural networks via this attack...
Recently, deep neural networks have been shown to be vulnerable to backdoor attacks. A backdoor is inserted into neural networks via this attack paradigm, thus compromising the integrity of the network. As soon as an attacker presents a trigger during the testing phase, the backdoor in the model is activated, allowing the network to make specific wrong predictions. It is extremely important to defend against backdoor attacks since they are very stealthy and dangerous. In this paper, we propose a novel defense mechanism, Neural Behavioral Alignment (NBA), for backdoor removal. NBA optimizes the distillation process in terms of knowledge form and distillation samples to improve defense performance according to the characteristics of backdoor defense. NBA builds high-level representations of neural behavior within networks in order to facilitate the transfer of knowledge. Additionally, NBA crafts pseudo samples to induce student models exhibit backdoor neural behavior. By aligning the backdoor neural behavior from the student network with the benign neural behavior from the teacher network, NBA enables the proactive removal of backdoors. Extensive experiments show that NBA can effectively defend against six different backdoor attacks and outperform five state-of-the-art defenses.
","

","Springer
arXiv
DBLP"
A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives,"Yudong Gao, Honglong Chen, Peng Sun, Junjian Li, Anqing Zhang, Zhibo Wang","arXiv
arXiv
AAAI","2023-07-03
2023-07
2024","<a href=""arXiv (2023-07-03) : A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives"" target=""_blank"">[http://arxiv.org/abs/2307.10184v1]</a>
<a href=""DBLP (2023-07) : A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.10184]</a>
<a href=""DBLP (2024) : A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i3.27954]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.10184]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i3.27954]</a>","Backdoor attacks pose serious security threats to deep neural networks (DNNs). Backdoored models make arbitrarily (targeted) incorrect predictions on inputs embedded with well-designed triggers while behaving normally on clean inputs. Many works have explored the invisibility of backdoor triggers to improve attack stealthiness. However, most of them only consider the invisibility in the spatial domain without explicitly accounting for the generation of invisible triggers in the frequency domain, making the generated poisoned images be easily detected by recent defense methods. To address this issue, in this paper, we propose a DUal stealthy BAckdoor attack method named DUBA, which simultaneously considers the invisibility of triggers in both the spatial and frequency domains, to achieve desirable attack performance, while ensuring strong stealthiness. Specifically, we first use Discrete Wavelet Transform to embed the high-frequency information of the trigger image into the clean image to ensure attack effectiveness. Then, to attain strong stealthiness, we incorporate Fourier Transform and Discrete Cosine Transform to mix the poisoned image and clean image in the frequency domain. Moreover, the proposed DUBA adopts a novel attack strategy, in which the model is trained with weak triggers and attacked with strong triggers to further enhance the attack performance and stealthiness. We extensively evaluate DUBA against popular image classifiers on four datasets. The results demonstrate that it significantly outperforms the state-of-the-art backdoor attacks in terms of the attack success rate and stealthiness

","

","arXiv
DBLP
DBLP"
A dual stealthy backdoor: From both spatial and frequency perspectives,"Y Gao, H Chen, P Sun, J Li, A Zhang, Z Wang…","Proceedings of the AAAI …, 2024",2023-07-03,"<a href=""Google Scholar (2023-07-03) : A dual stealthy backdoor: From both spatial and frequency perspectives"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/27954]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/27954]</a>","BAckdoor attack called DUBA, which crafts invisible triggers in both the spatial and frequency domains while achieving desirable attack … a powerful backdoor attack that is …",,Google Scholar
Metode za obranu od napada na neuronske mreže pomoću augmentacije podataka,A Herkov,2023,2023-07-03,"<a href=""Google Scholar (2023-07-03) : Metode za obranu od napada na neuronske mreže pomoću augmentacije podataka"" target=""_blank"">[https://zir.nsk.hr/islandora/object/fer:11263]</a>","<a href=""Google Scholar"" target=""_blank"">[https://zir.nsk.hr/islandora/object/fer:11263]</a>",Backdoor attacks on models are formally described and an overview of defense … of data augmentation on the model security from an inserted backdoor was demonstrated. …,,Google Scholar
Fedward: Flexible Federated Backdoor Defense Framework with Non-IID Data,"Zekai Chen, Fuyi Wang, Zhiwei Zheng, Ximeng Liu, Yujie Lin","arXiv
ICME
arXiv","2023-07-01
2023
2023-07","<a href=""arXiv (2023-07-01) : Fedward: Flexible Federated Backdoor Defense Framework with Non-IID Data"" target=""_blank"">[http://arxiv.org/abs/2307.00356v1]</a>
<a href=""DBLP (2023) : Fedward: Flexible Federated Backdoor Defense Framework with Non-IID Data"" target=""_blank"">[https://doi.org/10.1109/ICME55011.2023.00067]</a>
<a href=""DBLP (2023-07) : Fedward: Flexible Federated Backdoor Defense Framework with Non-IID Data"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.00356]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICME55011.2023.00067]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.00356]</a>","Federated learning (FL) enables multiple clients to collaboratively train deep learning models while considering sensitive local datasets' privacy. However, adversaries can manipulate datasets and upload models by injecting triggers for federated backdoor attacks (FBA). Existing defense strategies against FBA consider specific and limited attacker models, and a sufficient amount of noise to be injected only mitigates rather than eliminates FBA. To address these deficiencies, we introduce a Flexible Federated Backdoor Defense Framework (Fedward) to ensure the elimination of adversarial backdoors. We decompose FBA into various attacks, and design amplified magnitude sparsification (AmGrad) and adaptive OPTICS clustering (AutoOPTICS) to address each attack. Meanwhile, Fedward uses the adaptive clipping method by regarding the number of samples in the benign group as constraints on the boundary. This ensures that Fedward can maintain the performance for the Non-IID scenario. We conduct experimental evaluations over three benchmark datasets and thoroughly compare them to state-of-the-art studies. The results demonstrate the promising defense performance from Fedward, moderately improved by 33% $\sim$ 75 in clustering defense methods, and 96.98%, 90.74%, and 89.8% for Non-IID to the utmost extent for the average FBA success rate over MNIST, FMNIST, and CIFAR10, respectively.

","

","arXiv
DBLP
DBLP"
A Non-injected Traffic Backdoor Attack on Deep Neural Network,"J Wang, J Yang, B Ma, D Wang…","International Journal of …, 2023",2023-07-01,"<a href=""Google Scholar (2023-07-01) : A Non-injected Traffic Backdoor Attack on Deep Neural Network"" target=""_blank"">[https://www.airitilibrary.com/Article/Detail/18163548-N202306300014-00011]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.airitilibrary.com/Article/Detail/18163548-N202306300014-00011]</a>","backdoor attack on deep neural networks in intrusion detection systems, capable of misleading DBN and LeNet-5 detection data and classifying attack … backdoor attack …",,Google Scholar
Association rule learning for threat analysis using traffic analysis and packet filtering approach,"Romil Rawat, Rajesh Kumar Chakrawarti, ... Ramakant Bhardwaj",International Journal of Information Technology,2023-07-01,"<a href=""Springer (2023-07-01) : Association rule learning for threat analysis using traffic analysis and packet filtering approach"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-023-01353-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-023-01353-0]</a>","The Mirai botnet source code, which was distributed on the dark web, is still changing as malware designers modify it to produce more sophisticated...",,Springer
Backdoor Attack against Face Sketch Synthesis,Zhang S.,Entropy,2023-07-01,"<a href=""ScienceDirect (2023-07-01) : Backdoor Attack against Face Sketch Synthesis"" target=""_blank"">[https://doi.org/10.3390/e25070974]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/e25070974]</a>",,,ScienceDirect
Detecting backdoor in deep neural networks via intentional adversarial perturbations,Xue M.,Information Sciences,2023-07-01,"<a href=""ScienceDirect (2023-07-01) : Detecting backdoor in deep neural networks via intentional adversarial perturbations"" target=""_blank"">[https://doi.org/10.1016/j.ins.2023.03.112]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.ins.2023.03.112]</a>",,,ScienceDirect
Fedward: Flexible federated backdoor defense framework with non-IID data,"Z Chen, F Wang, Z Zheng, X Liu…","2023 IEEE International …, 2023",2023-07-01,"<a href=""Google Scholar (2023-07-01) : Fedward: Flexible federated backdoor defense framework with non-IID data"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10219938/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10219938/]</a>","Backdoor attacks is … from backdoor attacks [9] and Backdoor attacks on FL have been recently studied in [10], [11]. The generic techniques utilized in FL backdoor attacks …",,Google Scholar
DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation,"Zhicong Yan, Shenghong Li, Ruijie Zhao, Yuan Tian, Yuanyuan Zhao","ASIA CCS '23: Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security
arXiv
AsiaCCS
arXiv","2023-07
2023-06-13
2023
2023-06","<a href=""ACM (2023-07) : DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3579856.3582822]</a>
<a href=""arXiv (2023-06-13) : DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation"" target=""_blank"">[http://arxiv.org/abs/2306.08009v1]</a>
<a href=""DBLP (2023) : DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation"" target=""_blank"">[https://doi.org/10.1145/3579856.3582822]</a>
<a href=""DBLP (2023-06) : DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.08009]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3579856.3582822]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3579856.3582822]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.08009]</a>","Backdoor attacks have emerged as an urgent threat to Deep Neural Networks (DNNs), where victim DNNs are furtively implanted with malicious neurons that could be triggered by the adversary. To defend against backdoor attacks, many works establish a ...
Backdoor attacks have emerged as an urgent threat to Deep Neural Networks (DNNs), where victim DNNs are furtively implanted with malicious neurons that could be triggered by the adversary. To defend against backdoor attacks, many works establish a staged pipeline to remove backdoors from victim DNNs: inspecting, locating, and erasing. However, in a scenario where a few clean data can be accessible, such pipeline is fragile and cannot erase backdoors completely without sacrificing model accuracy. To address this issue, in this paper, we propose a novel data-free holistic backdoor erasing (DHBE) framework. Instead of the staged pipeline, the DHBE treats the backdoor erasing task as a unified adversarial procedure, which seeks equilibrium between two different competing processes: distillation and backdoor regularization. In distillation, the backdoored DNN is distilled into a proxy model, transferring its knowledge about clean data, yet backdoors are simultaneously transferred. In backdoor regularization, the proxy model is holistically regularized to prevent from infecting any possible backdoor transferred from distillation. These two processes jointly proceed with data-free adversarial optimization until a clean, high-accuracy proxy model is obtained. With the novel adversarial design, our framework demonstrates its superiority in three aspects: 1) minimal detriment to model accuracy, 2) high tolerance for hyperparameters, and 3) no demand for clean data. Extensive experiments on various backdoor attacks and datasets are performed to verify the effectiveness of the proposed framework. Code is available at \url{https://github.com/yanzhicong/DHBE}

","
<a href=""arXiv"" target=""_blank"">[https://github.com/yanzhicong/DHBE}]</a>

","ACM
arXiv
DBLP
DBLP"
Did You Train on My Dataset? Towards Public Dataset Protection with CleanLabel Backdoor Watermarking,"Ruixiang Tang, Qizhang Feng, Ninghao Liu, Fan Yang, Xia Hu","ACM SIGKDD Explorations Newsletter (SIGKDD), Volume 25, Issue 1
arXiv
arXiv
SIGKDD Explor.","2023-07
2023-04-10
2023-03
2023","<a href=""ACM (2023-07) : Did You Train on My Dataset? Towards Public Dataset Protection with CleanLabel Backdoor Watermarking"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3606274.3606279]</a>
<a href=""arXiv (2023-04-10) : Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking"" target=""_blank"">[http://arxiv.org/abs/2303.11470v2]</a>
<a href=""DBLP (2023-03) : Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.11470]</a>
<a href=""DBLP (2023) : Did You Train on My Dataset? Towards Public Dataset Protection with CleanLabel Backdoor Watermarking"" target=""_blank"">[https://doi.org/10.1145/3606274.3606279]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3606274.3606279]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.11470]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3606274.3606279]</a>","The huge supporting training data on the Internet has been a key factor in the success of deep learning models. However, this abundance of public-available data also raises concerns about the unauthorized exploitation of datasets for commercial purposes, ...
The huge supporting training data on the Internet has been a key factor in the success of deep learning models. However, this abundance of public-available data also raises concerns about the unauthorized exploitation of datasets for commercial purposes, which is forbidden by dataset licenses. In this paper, we propose a backdoor-based watermarking approach that serves as a general framework for safeguarding public-available data. By inserting a small number of watermarking samples into the dataset, our approach enables the learning model to implicitly learn a secret function set by defenders. This hidden function can then be used as a watermark to track down third-party models that use the dataset illegally. Unfortunately, existing backdoor insertion methods often entail adding arbitrary and mislabeled data to the training set, leading to a significant drop in performance and easy detection by anomaly detection algorithms. To overcome this challenge, we introduce a clean-label backdoor watermarking framework that uses imperceptible perturbations to replace mislabeled samples. As a result, the watermarking samples remain consistent with the original labels, making them difficult to detect. Our experiments on text, image, and audio datasets demonstrate that the proposed framework effectively safeguards datasets with minimal impact on original task performance. We also show that adding just 1% of watermarking samples can inject a traceable watermarking function and that our watermarking samples are stealthy and look benign upon visual inspection.

","


","ACM
arXiv
DBLP
DBLP"
CASSOCK: Viable Backdoor Attacks against DNN in the Wall of Source-Specific Backdoor Defenses,"Shang Wang, Yansong Gao, Anmin Fu, Zhi Zhang, Yuqing Zhang, Willy Susilo, Dongxi Liu","ASIA CCS '23: Proceedings of the 2023 ACM Asia Conference on Computer and Communications Security

arXiv
arXiv
AsiaCCS","2023-07

2022-12-18
2022-06
2023","<a href=""ACM (2023-07) : CASSOCK: Viable Backdoor Attacks against DNN in the Wall of Source-Specific Backdoor Defenses"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3579856.3582829]</a>
<a href=""OpenReview () : CASSOCK: Viable Backdoor Attacks against DNN in the Wall of Source-Specific Backdoor Defenses"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3579856.3582829]</a>
<a href=""arXiv (2022-12-18) : CASSOCK: Viable Backdoor Attacks against DNN in The Wall of Source-Specific Backdoor Defences"" target=""_blank"">[http://arxiv.org/abs/2206.00145v2]</a>
<a href=""DBLP (2022-06) : CASSOCK: Viable Backdoor Attacks against DNN in The Wall of Source-Specific Backdoor Defences"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.00145]</a>
<a href=""DBLP (2023) : CASSOCK: Viable Backdoor Attacks against DNN in the Wall of Source-Specific Backdoor Defenses"" target=""_blank"">[https://doi.org/10.1145/3579856.3582829]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3579856.3582829]</a>
<a href=""OpenReview"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3579856.3582829]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.00145]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3579856.3582829]</a>","As a critical threat to deep neural networks (DNNs), backdoor attacks can be categorized into two types, i.e., source-agnostic backdoor attacks (SABAs) and source-specific backdoor attacks (SSBAs). Compared to traditional SABAs, SSBAs are more advanced ...
As a critical threat to deep neural networks (DNNs), backdoor attacks can be categorized into two types, i.e., source-agnostic backdoor attacks (SABAs) and source-specific backdoor attacks (SSBAs). Compared to traditional SABAs, SSBAs are more advanced in that they have superior stealthier in bypassing mainstream countermeasures that are effective against SABAs. Nonetheless, existing SSBAs suffer from two major limitations. First, they can hardly achieve a good trade-off between ASR (attack success rate) and FPR (false positive rate). Besides, they can be effectively detected by the state-of-the-art (SOTA) countermeasures (e.g., SCAn [40]). To address the limitations above, we propose a new class of viable source-specific backdoor attacks coined as 𝐶𝐴𝑆𝑆𝑂𝐶𝐾. Our key insight is that trigger designs when creating poisoned data and cover data in SSBAs play a crucial role in demonstrating a viable source-specific attack, which has not been considered by existing SSBAs. With this insight, we focus on trigger transparency and content when crafting triggers for poisoned dataset where a sample has an attacker-targeted label and cover dataset where a sample has a ground-truth label. Specifically, we implement 𝐶𝐴𝑆𝑆𝑂𝐶𝐾𝑇 𝑟𝑎𝑛𝑠 that designs a trigger with heterogeneous transparency to craft poisoned and cover datasets, presenting better attack performance than existing SSBAs. We also propose 𝐶𝐴𝑆𝑆𝑂𝐶𝐾𝐶𝑜𝑛𝑡 that extracts salient features of the attacker-targeted label to generate a trigger, entangling the trigger features with normal features of the label, which is stealthier in bypassing the SOTA defenses. While both 𝐶𝐴𝑆𝑆𝑂𝐶𝐾𝑇 𝑟𝑎𝑛𝑠 and 𝐶𝐴𝑆𝑆𝑂𝐶𝐾𝐶𝑜𝑛𝑡 are orthogonal, they are complementary to each other, generating a more powerful attack, called 𝐶𝐴𝑆𝑆𝑂𝐶𝐾𝐶𝑜𝑚𝑝 , with further improved attack performance and stealthiness. To demonstrate their viability, we perform a comprehensive evaluation of the three𝐶𝐴𝑆𝑆𝑂𝐶𝐾-based attacks on four popular datasets (i.e., MNIST, CIFAR10, GTSRB and LFW) and three SOTA defenses (i.e., extended Neural Cleanse [45], Februus [8], and SCAn [40]). Compared with a representative SSBA as a baseline (𝑆𝑆𝐵𝐴𝐵𝑎𝑠𝑒 ), 𝐶𝐴𝑆𝑆𝑂𝐶𝐾-based attacks have significantly advanced the attack performance, i.e., higher ASR and lower FPR with comparable CDA (clean data accuracy). Besides, 𝐶𝐴𝑆𝑆𝑂𝐶𝐾-based attacks have effectively bypassed the SOTA defenses, and 𝑆𝑆𝐵𝐴𝐵𝑎𝑠𝑒 cannot.
As a critical threat to deep neural networks (DNNs), backdoor attacks can be categorized into two types, i.e., source-agnostic backdoor attacks (SABAs) and source-specific backdoor attacks (SSBAs). Compared to traditional SABAs, SSBAs are more advanced in that they have superior stealthier in bypassing mainstream countermeasures that are effective against SABAs. Nonetheless, existing SSBAs suffer from two major limitations. First, they can hardly achieve a good trade-off between ASR (attack success rate) and FPR (false positive rate). Besides, they can be effectively detected by the state-of-the-art (SOTA) countermeasures (e.g., SCAn). To address the limitations above, we propose a new class of viable source-specific backdoor attacks, coined as CASSOCK. Our key insight is that trigger designs when creating poisoned data and cover data in SSBAs play a crucial role in demonstrating a viable source-specific attack, which has not been considered by existing SSBAs. With this insight, we focus on trigger transparency and content when crafting triggers for poisoned dataset where a sample has an attacker-targeted label and cover dataset where a sample has a ground-truth label. Specifically, we implement $CASSOCK_{Trans}$ and $CASSOCK_{Cont}$. While both they are orthogonal, they are complementary to each other, generating a more powerful attack, called $CASSOCK_{Comp}$, with further improved attack performance and stealthiness. We perform a comprehensive evaluation of the three $CASSOCK$-based attacks on four popular datasets and three SOTA defenses. Compared with a representative SSBA as a baseline ($SSBA_{Base}$), $CASSOCK$-based attacks have significantly advanced the attack performance, i.e., higher ASR and lower FPR with comparable CDA (clean data accuracy). Besides, $CASSOCK$-based attacks have effectively bypassed the SOTA defenses, and $SSBA_{Base}$ cannot.

","



","ACM
OpenReview
arXiv
DBLP
DBLP"
Attacking Pre-trained Recommendation,"Yiqing Wu, Ruobing Xie, Zhao Zhang, Yongchun Zhu, Fuzhen Zhuang, Jie Zhou, Yongjun Xu, Qing He",SIGIR '23: Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval,2023-07,"<a href=""ACM (2023-07) : Attacking Pre-trained Recommendation"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3539618.3591949]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3539618.3591949]</a>","Recently, a series of pioneer studies have shown the potency of pre-trained models in sequential recommendation, illuminating the path of building an omniscient unified pre-trained recommendation model for different downstream recommendation tasks. ...",,ACM
Backdoor Attack against Object Detection with Clean Annotation,"Yize Cheng, Wenbin Hu, Minhao Cheng",arXiv,2023-07,"<a href=""DBLP (2023-07) : Backdoor Attack against Object Detection with Clean Annotation"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.10487]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.10487]</a>",,,DBLP
Backdoor Defense with Non-Adversarial Backdoor,"Min Liu, Alberto L. Sangiovanni-Vincentelli, Xiangyu Yue",arXiv,2023-07,"<a href=""DBLP (2023-07) : Backdoor Defense with Non-Adversarial Backdoor"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.15539]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.15539]</a>",,,DBLP
FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks,"Dong Huang, Qingwen Bu, Yahao Qing, Yichao Fu, Heming Cui",arXiv,2023-07,"<a href=""DBLP (2023-07) : FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.11565]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2307.11565]</a>",,,DBLP
Efficient Backdoor Removal Through Natural Gradient Fine-tuning,"Nazmul Karim, Abdullah Al Arafat, Umar Khalid, Zhishan Guo, Naznin Rahnavard","arXiv
arXiv","2023-06-30
2023-06","<a href=""arXiv (2023-06-30) : Efficient Backdoor Removal Through Natural Gradient Fine-tuning"" target=""_blank"">[http://arxiv.org/abs/2306.17441v1]</a>
<a href=""DBLP (2023-06) : Efficient Backdoor Removal Through Natural Gradient Fine-tuning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.17441]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.17441]</a>","The success of a deep neural network (DNN) heavily relies on the details of the training scheme, e.g., training data, architectures, hyper-parameters, etc. Recent backdoor attacks suggest that an adversary can take advantage of such training details and compromise the integrity of a DNN. Our studies show that a backdoor model is usually optimized to a bad local minima, i.e. sharper minima as compared to a benign model. Intuitively, a backdoor model can be purified by reoptimizing the model to a smoother minima through fine-tuning with a few clean validation data. However, fine-tuning all DNN parameters often requires huge computational costs and often results in sub-par clean test performance. To address this concern, we propose a novel backdoor purification technique, Natural Gradient Fine-tuning (NGF), which focuses on removing the backdoor by fine-tuning only one layer. Specifically, NGF utilizes a loss surface geometry-aware optimizer that can successfully overcome the challenge of reaching a smooth minima under a one-layer optimization scenario. To enhance the generalization performance of our proposed method, we introduce a clean data distribution-aware regularizer based on the knowledge of loss surface curvature matrix, i.e., Fisher Information Matrix. Extensive experiments show that the proposed method achieves state-of-the-art performance on a wide range of backdoor defense benchmarks: four different datasets- CIFAR10, GTSRB, Tiny-ImageNet, and ImageNet, 13 recent backdoor attacks, e.g. Blend, Dynamic, WaNet, ISSBA, etc.
","
","arXiv
DBLP"
A gradient control method for backdoor attacks on parameter-efficient tuning,"N Gu, P Fu, X Liu, Z Liu, Z Lin…","Proceedings of the 61st …, 2023",2023-06-30,"<a href=""Google Scholar (2023-06-30) : A gradient control method for backdoor attacks on parameter-efficient tuning"" target=""_blank"">[https://aclanthology.org/2023.acl-long.194/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://aclanthology.org/2023.acl-long.194/]</a>",reasons for the backdoor forgetting of … backdoor effectiveness in the parameter-efficient tuning scenario. To summarize our contributions: (1) We regard the backdoor …,,Google Scholar
Aliasing backdoor attacks on pre-trained models,"Y Lee, K Chen, G Meng, P Lv","32nd USENIX Security Symposium …, 2023",2023-06-30,"<a href=""Google Scholar (2023-06-30) : Aliasing backdoor attacks on pre-trained models"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity23/presentation/wei-chengan]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity23/presentation/wei-chengan]</a>","propose the aliasing backdoor attack, a new type of backdoor attack which is target-… Such features of the attack make the attack scalable at low cost. • Detailed Evaluation…",,Google Scholar
Neural polarizer: A lightweight and effective backdoor defense via purifying poisoned features,"M Zhu, S Wei, H Zha, B Wu","Advances in Neural …, 2024",2023-06-30,"<a href=""Google Scholar (2023-06-30) : Neural polarizer: A lightweight and effective backdoor defense via purifying poisoned features"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/03df5246cc78af497940338dd3eacbaa-Abstract-Conference.html]</a>","<a href=""Google Scholar"" target=""_blank"">[https://proceedings.neurips.cc/paper_files/paper/2023/hash/03df5246cc78af497940338dd3eacbaa-Abstract-Conference.html]</a>",Extensive experiments demonstrate the effectiveness of our method across all evaluated backdoor attacks and all other defense methods under various datasets and …,,Google Scholar
Sparsity brings vulnerabilities: exploring new metrics in backdoor attacks,"J Tian, K Qiu, D Gao, Z Wang, X Kuang…","32nd USENIX Security …, 2023",2023-06-30,"<a href=""Google Scholar (2023-06-30) : Sparsity brings vulnerabilities: exploring new metrics in backdoor attacks"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity23/presentation/tian]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity23/presentation/tian]</a>",backdoor attack consisting of a dissimilarity metric-based candidate selection and a variation ratio-based trigger construction. The proposed backdoor is … that the attack can …,,Google Scholar
{VILLAIN}: Backdoor Attacks Against Vertical Split Learning,"Y Bai, Y Chen, H Zhang, W Xu, H Weng…","32nd USENIX Security …, 2023",2023-06-30,"<a href=""Google Scholar (2023-06-30) : {VILLAIN}: Backdoor Attacks Against Vertical Split Learning"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity23/presentation/bai]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity23/presentation/bai]</a>","To tackle these challenges, we propose VILLAIN, a backdoor attack framework … backdoor attack power by designing a stealthy additive trigger and introducing backdoor …",,Google Scholar
A novel optimization based deep learning with artificial intelligence approach to detect intrusion attack in network system,"S. Siva Shankar, Bui Thanh Hung, ... Gayatri Parasa",Education and Information Technologies,2023-06-29,"<a href=""Springer (2023-06-29) : A novel optimization based deep learning with artificial intelligence approach to detect intrusion attack in network system"" target=""_blank"">[https://link.springer.com/article/10.1007/s10639-023-11885-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10639-023-11885-4]</a>","Modern life is increasingly influenced by networks, making cybersecurity a crucial area of study. However, due to their few resources and varied...",,Springer
"Big Data and Social Computing: 8th China National Conference, BDSC 2023, Urumqi, China, July 15–17, 2023, Proceedings","X Meng, Y Chen, L Suo, Q Xuan, ZK Zhang",2023,2023-06-29,"<a href=""Google Scholar (2023-06-29) : Big Data and Social Computing: 8th China National Conference, BDSC 2023, Urumqi, China, July 15–17, 2023, Proceedings"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=N-rIEAAAQBAJ&oi=fnd&pg=PR5&dq=backdoor+attack&ots=g2FlIiRTXA&sig=m0rrmoxDp1v3P5ajEtYc_hj3rhk]</a>","<a href=""Google Scholar"" target=""_blank"">[https://books.google.com/books?hl=en&lr=&id=N-rIEAAAQBAJ&oi=fnd&pg=PR5&dq=backdoor+attack&ots=g2FlIiRTXA&sig=m0rrmoxDp1v3P5ajEtYc_hj3rhk]</a>",,,Google Scholar
Enrollment-stage backdoor attacks on speaker recognition systems via adversarial ultrasound,"X Li, J Ze, C Yan, Y Cheng, X Ji…","IEEE Internet of Things …, 2023",2023-06-29,"<a href=""Google Scholar (2023-06-29) : Enrollment-stage backdoor attacks on speaker recognition systems via adversarial ultrasound"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10301792/]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10301792/]</a>","In this paper, we propose a backdoor attack named … backdoor attacks that target the training stage of SRS models, this paper focuses on the feasibility of backdoor attacks …",,Google Scholar
Fake the real: Backdoor attack on deep speech classification via voice conversion,"Z Ye, T Mao, L Dong, D Yan","arXiv preprint arXiv:2306.15875, 2023",2023-06-29,"<a href=""Google Scholar (2023-06-29) : Fake the real: Backdoor attack on deep speech classification via voice conversion"" target=""_blank"">[https://arxiv.org/abs/2306.15875]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2306.15875]</a>","This work explores a backdoor attack that utilizes sample-specific triggers … attack. Furthermore, we analyzed the specific scenarios that activated the proposed backdoor …",,Google Scholar
Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion,"Zhe Ye, Terui Mao, Li Dong, Diqun Yan","arXiv
INTERSPEECH
arXiv","2023-06-28
2023
2023-06","<a href=""arXiv (2023-06-28) : Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion"" target=""_blank"">[http://arxiv.org/abs/2306.15875v1]</a>
<a href=""DBLP (2023) : Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion"" target=""_blank"">[https://doi.org/10.21437/Interspeech.2023-733]</a>
<a href=""DBLP (2023-06) : Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.15875]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.21437/Interspeech.2023-733]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.21437/Interspeech.2023-733]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.15875]</a>","Deep speech classification has achieved tremendous success and greatly promoted the emergence of many real-world applications. However, backdoor attacks present a new security threat to it, particularly with untrustworthy third-party platforms, as pre-defined triggers set by the attacker can activate the backdoor. Most of the triggers in existing speech backdoor attacks are sample-agnostic, and even if the triggers are designed to be unnoticeable, they can still be audible. This work explores a backdoor attack that utilizes sample-specific triggers based on voice conversion. Specifically, we adopt a pre-trained voice conversion model to generate the trigger, ensuring that the poisoned samples does not introduce any additional audible noise. Extensive experiments on two speech classification tasks demonstrate the effectiveness of our attack. Furthermore, we analyzed the specific scenarios that activated the proposed backdoor and verified its resistance against fine-tuning.

","

","arXiv
DBLP
DBLP"
Backdoor Attack on gaze estimation,Y Reda,2023,2023-06-28,"<a href=""Google Scholar (2023-06-28) : Backdoor Attack on gaze estimation"" target=""_blank"">[https://repository.tudelft.nl/islandora/object/uuid:80cfdc30-f335-41a8-8665-83f92265edc0]</a>","<a href=""Google Scholar"" target=""_blank"">[https://repository.tudelft.nl/islandora/object/uuid:80cfdc30-f335-41a8-8665-83f92265edc0]</a>","Badnets are a type of backdoor attack that aims at manipulating the behavior of … In this paper, we apply this type of backdoor attack to a regression task on gaze estimation. …",,Google Scholar
Adfl: Defending backdoor attacks in federated learning via adversarial distillation,"C Zhu, J Zhang, X Sun, B Chen, W Meng","Computers & Security, 2023",2023-06-27,"<a href=""Google Scholar (2023-06-27) : Adfl: Defending backdoor attacks in federated learning via adversarial distillation"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S0167404823002766]</a>","<a href=""Google Scholar"" target=""_blank"">[https://www.sciencedirect.com/science/article/pii/S0167404823002766]</a>","The selection of these backdoor triggers makes backdoor attacks more hidden and … Moreover, backdoor attacks can also strengthen the connection between backdoor …",,Google Scholar
Imposition: Implicit backdoor attack through scenario injection,"M Pourkeshavarz, M Sabokrou, A Rasouli","arXiv preprint arXiv …, 2023",2023-06-27,"<a href=""Google Scholar (2023-06-27) : Imposition: Implicit backdoor attack through scenario injection"" target=""_blank"">[https://arxiv.org/abs/2306.15755]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2306.15755]</a>","novel backdoor attack called IMPlicit BackdOor Attack through … Instead, the attack leverages a realistic scenario from the … This type of attack is particularly dangerous as it is …",,Google Scholar
International Perspectives for Cybersecurity,"C Rudolph, W Chigona, L Axon…","… Conference on System …, 2023",2023-06-27,"<a href=""Google Scholar (2023-06-27) : International Perspectives for Cybersecurity"" target=""_blank"">[https://research.monash.edu/files/479471803/458775370_oa.pdf]</a>","<a href=""Google Scholar"" target=""_blank"">[https://research.monash.edu/files/479471803/458775370_oa.pdf]</a>",Relevant factors include risks to supply chains from cybersecurity attacks and increased attack vectors through deliberately introduced vulnerabilities and back-doors into …,,Google Scholar
Probabilistic Generalization of Backdoor Trees with Application to SAT,Semenov A.,"Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023",2023-06-27,"<a href=""ScienceDirect (2023-06-27) : Probabilistic Generalization of Backdoor Trees with Application to SAT"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
XRAND: Differentially Private Defense against Explanation-Guided Attacks,Nguyen T.,"Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023",2023-06-27,"<a href=""ScienceDirect (2023-06-27) : XRAND: Differentially Private Defense against Explanation-Guided Attacks"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Network intrusion detection based on multi-domain data and ensemble-bidirectional LSTM,"Xiaoning Wang, Jia Liu, Chunjiong Zhang",EURASIP Journal on Information Security,2023-06-26,"<a href=""Springer (2023-06-26) : Network intrusion detection based on multi-domain data and ensemble-bidirectional LSTM"" target=""_blank"">[https://link.springer.com/article/10.1186/s13635-023-00139-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13635-023-00139-y]</a>",Different types of network traffic can be treated as data originating from different domains with the same objectives of problem-solving. Previous...,,Springer
Defensive Machine Learning Techniques for Countering Adversarial Attacks,FR Marani,2023,2023-06-24,"<a href=""Google Scholar (2023-06-24) : Defensive Machine Learning Techniques for Countering Adversarial Attacks"" target=""_blank"">[https://search.proquest.com/openview/7db3924c6bb3da57e8c3c76094d5e21c/1?pq-origsite=gscholar&cbl=18750&diss=y]</a>","<a href=""Google Scholar"" target=""_blank"">[https://search.proquest.com/openview/7db3924c6bb3da57e8c3c76094d5e21c/1?pq-origsite=gscholar&cbl=18750&diss=y]</a>","We also aim to find a robust model against the hardest class of training-time attacks, namely backdoor attacks, in which not only the training data is affected but also the …",,Google Scholar
Identifying and Mitigating Vulnerabilities of Deep Neural Networks in the Wild,H Li,2023,2023-06-24,"<a href=""Google Scholar (2023-06-24) : Identifying and Mitigating Vulnerabilities of Deep Neural Networks in the Wild"" target=""_blank"">[https://search.proquest.com/openview/c6fb759db6b8cf6708b99557aed93346/1?pq-origsite=gscholar&cbl=18750&diss=y]</a>","<a href=""Google Scholar"" target=""_blank"">[https://search.proquest.com/openview/c6fb759db6b8cf6708b99557aed93346/1?pq-origsite=gscholar&cbl=18750&diss=y]</a>","in this scenario, I propose a latent backdoor attack that embeds incomplete backdoors into a “Teacher” model, which are automatically completed through transfer learning …",,Google Scholar
Mitigating cross-client GANs-based attack in federated learning,"Hong Huang, Xinyu Lei, Tao Xiang",Multimedia Tools and Applications,2023-06-24,"<a href=""Springer (2023-06-24) : Mitigating cross-client GANs-based attack in federated learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-15879-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-15879-9]</a>","Machine learning makes multimedia data (e.g., images) more attractive, however, multimedia data is usually distributed and privacy sensitive....",,Springer
Protecting IP of deep neural networks with watermarking using logistic disorder generation trigger sets,"Huanjie Lin, Shuyuan Shen, Haojie Lyu",Multimedia Tools and Applications,2023-06-24,"<a href=""Springer (2023-06-24) : Protecting IP of deep neural networks with watermarking using logistic disorder generation trigger sets"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-15980-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-15980-z]</a>","As deep learning technology matures, it’s being widely deployed in fields like image classification and speech recognition. However, training a...",,Springer
Real-time data fusion for intrusion detection in industrial control systems based on cloud computing and big data techniques,"Ahlem Abid, Farah Jemili, Ouajdi Korbaa",Cluster Computing,2023-06-24,"<a href=""Springer (2023-06-24) : Real-time data fusion for intrusion detection in industrial control systems based on cloud computing and big data techniques"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-023-04087-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-023-04087-7]</a>","Intrusion detection in industrial control systems (ICS) is crucial for maintaining secu rity in modern industries. However, the rapid growth of data...",,Springer
A First Order Meta Stackelberg Method for Robust Federated Learning (Technical Report),"H Li, T Xu, T Li, Y Pan, Q Zhu, Z Zheng","arXiv preprint arXiv:2306.13273, 2023",2023-06-23,"<a href=""Google Scholar (2023-06-23) : A First Order Meta Stackelberg Method for Robust Federated Learning (Technical Report)"" target=""_blank"">[https://arxiv.org/abs/2306.13273]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2306.13273]</a>",We consider a backdoor attack setting where either the backdoor trigger or the targeted label is unknown/uncertain to the server. Our meta-SG’s metapolicy is trained to …,,Google Scholar
A first order meta stackelberg method for robust federated learning,"Y Pan, T Li, H Li, T Xu, Z Zheng, Q Zhu","arXiv preprint arXiv:2306.13800, 2023",2023-06-23,"<a href=""Google Scholar (2023-06-23) : A first order meta stackelberg method for robust federated learning"" target=""_blank"">[https://arxiv.org/abs/2306.13800]</a>","<a href=""Google Scholar"" target=""_blank"">[https://arxiv.org/abs/2306.13800]</a>","’s incomplete information of various attack types. We propose … poisoning and backdoor attacks of an uncertain nature. … categories of attacks, namely, backdoor attacks and …",,Google Scholar
"Edge Learning for 6G-enabled Internet of Things: A Comprehensive Survey of Vulnerabilities, Datasets, and Defenses","M Amine Ferrag, O Friha, B Kantarci, N Tihanyi…","arXiv e …, 2023",2023-06-23,"<a href=""Google Scholar (2023-06-23) : Edge Learning for 6G-enabled Internet of Things: A Comprehensive Survey of Vulnerabilities, Datasets, and Defenses"" target=""_blank"">[https://ui.adsabs.harvard.edu/abs/2023arXiv230610309A/abstract]</a>","<a href=""Google Scholar"" target=""_blank"">[https://ui.adsabs.harvard.edu/abs/2023arXiv230610309A/abstract]</a>","Moreover, we provide a holistic survey of existing research on attacks against … backdoor attacks, adversarial examples, combined attacks, poisoning attacks, Sybil attacks, …",,Google Scholar
Backdoor Attacks for Remote Sensing Data with Wavelet Transform,"Nikolaus Dräger, Yonghao Xu, Pedram Ghamisi","arXiv
IEEE Trans. Geosci. Remote. Sens.
arXiv","2023-06-22
2023
2022-11","<a href=""arXiv (2023-06-22) : Backdoor Attacks for Remote Sensing Data with Wavelet Transform"" target=""_blank"">[http://arxiv.org/abs/2211.08044v2]</a>
<a href=""DBLP (2023) : Backdoor Attacks for Remote Sensing Data With Wavelet Transform"" target=""_blank"">[https://doi.org/10.1109/TGRS.2023.3289307]</a>
<a href=""DBLP (2022-11) : Backdoor Attacks for Remote Sensing Data with Wavelet Transform"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.08044]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1109/TGRS.2023.3289307]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TGRS.2023.3289307]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.08044]</a>","Recent years have witnessed the great success of deep learning algorithms in the geoscience and remote sensing realm. Nevertheless, the security and robustness of deep learning models deserve special attention when addressing safety-critical remote sensing tasks. In this paper, we provide a systematic analysis of backdoor attacks for remote sensing data, where both scene classification and semantic segmentation tasks are considered. While most of the existing backdoor attack algorithms rely on visible triggers like squared patches with well-designed patterns, we propose a novel wavelet transform-based attack (WABA) method, which can achieve invisible attacks by injecting the trigger image into the poisoned image in the low-frequency domain. In this way, the high-frequency information in the trigger image can be filtered out in the attack, resulting in stealthy data poisoning. Despite its simplicity, the proposed method can significantly cheat the current state-of-the-art deep learning models with a high attack success rate. We further analyze how different trigger images and the hyper-parameters in the wavelet transform would influence the performance of the proposed method. Extensive experiments on four benchmark remote sensing datasets demonstrate the effectiveness of the proposed method for both scene classification and semantic segmentation tasks and thus highlight the importance of designing advanced backdoor defense algorithms to address this threat in remote sensing scenarios. The code will be available online at \url{https://github.com/ndraeger/waba}.

","<a href=""arXiv"" target=""_blank"">[https://github.com/ndraeger/waba}]</a>

","arXiv
DBLP
DBLP"
Malware classification approaches utilizing binary and text encoding of permissions,"Mo’ath Zyout, Raed Shatnawi, Hassan Najadat",International Journal of Information Security,2023-06-21,"<a href=""Springer (2023-06-21) : Malware classification approaches utilizing binary and text encoding of permissions"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00712-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00712-z]</a>","With the advancement of smartphone technology, the development of mobile applications is rapidly growing. These apps are designed to help mobile...",,Springer
Practical and General Backdoor Attacks against Vertical Federated Learning,"Yuexin Xuan, Xiaojun Chen, Zhendong Zhao, Bisheng Tang, Ye Dong","arXiv
ECML/PKDD
arXiv","2023-06-19
2023
2023-06","<a href=""arXiv (2023-06-19) : Practical and General Backdoor Attacks against Vertical Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2306.10746v1]</a>
<a href=""DBLP (2023) : Practical and General Backdoor Attacks Against Vertical Federated Learning"" target=""_blank"">[https://doi.org/10.1007/978-3-031-43415-0_24]</a>
<a href=""DBLP (2023-06) : Practical and General Backdoor Attacks against Vertical Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.10746]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-43415-0_24]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.10746]</a>","Federated learning (FL), which aims to facilitate data collaboration across multiple organizations without exposing data privacy, encounters potential security risks. One serious threat is backdoor attacks, where an attacker injects a specific trigger into the training dataset to manipulate the model's prediction. Most existing FL backdoor attacks are based on horizontal federated learning (HFL), where the data owned by different parties have the same features. However, compared to HFL, backdoor attacks on vertical federated learning (VFL), where each party only holds a disjoint subset of features and the labels are only owned by one party, are rarely studied. The main challenge of this attack is to allow an attacker without access to the data labels, to perform an effective attack. To this end, we propose BadVFL, a novel and practical approach to inject backdoor triggers into victim models without label information. BadVFL mainly consists of two key steps. First, to address the challenge of attackers having no knowledge of labels, we introduce a SDD module that can trace data categories based on gradients. Second, we propose a SDP module that can improve the attack's effectiveness by enhancing the decision dependency between the trigger and attack target. Extensive experiments show that BadVFL supports diverse datasets and models, and achieves over 93% attack success rate with only 1% poisoning rate.

","

","arXiv
DBLP
DBLP"
Hidden Backdoor Attack against Deep Learning-Based Wireless Signal Modulation Classifiers,"Yunsong Huang, Weicheng Liu, Hui-Ming Wang","arXiv
IEEE Trans. Veh. Technol.","2023-06-19
2023","<a href=""arXiv (2023-06-19) : Hidden Backdoor Attack against Deep Learning-Based Wireless Signal Modulation Classifiers"" target=""_blank"">[http://arxiv.org/abs/2306.10753v1]</a>
<a href=""DBLP (2023) : Hidden Backdoor Attack Against Deep Learning-Based Wireless Signal Modulation Classifiers"" target=""_blank"">[https://doi.org/10.1109/TVT.2023.3267455]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1109/TVT.2023.3267455]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TVT.2023.3267455]</a>","Recently, DL has been exploited in wireless communications such as modulation classification. However, due to the openness of wireless channel and unexplainability of DL, it is also vulnerable to adversarial attacks. In this correspondence, we investigate a so called hidden backdoor attack to modulation classification, where the adversary puts elaborately designed poisoned samples on the basis of IQ sequences into training dataset. These poisoned samples are hidden because it could not be found by traditional classification methods. And poisoned samples are same to samples with triggers which are patched samples in feature space. We show that the hidden backdoor attack can reduce the accuracy of modulation classification significantly with patched samples. At last, we propose activation cluster to detect abnormal samples in training dataset.
","
","arXiv
DBLP"
Towards A Proactive ML Approach for Detecting Backdoor Poison Samples,"Xiangyu Qi, Tinghao Xie, Jiachen T. Wang, Tong Wu, Saeed Mahloujifar, Prateek Mittal","arXiv
USENIX Security Symposium","2023-06-18
2023","<a href=""arXiv (2023-06-18) : Towards A Proactive ML Approach for Detecting Backdoor Poison Samples"" target=""_blank"">[http://arxiv.org/abs/2205.13616v3]</a>
<a href=""DBLP (2023) : Towards A Proactive ML Approach for Detecting Backdoor Poison Samples"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity23/presentation/qi]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity23/presentation/qi]</a>","Adversaries can embed backdoors in deep learning models by introducing backdoor poison samples into training datasets. In this work, we investigate how to detect such poison samples to mitigate the threat of backdoor attacks. First, we uncover a post-hoc workflow underlying most prior work, where defenders passively allow the attack to proceed and then leverage the characteristics of the post-attacked model to uncover poison samples. We reveal that this workflow does not fully exploit defenders' capabilities, and defense pipelines built on it are prone to failure or performance degradation in many scenarios. Second, we suggest a paradigm shift by promoting a proactive mindset in which defenders engage proactively with the entire model training and poison detection pipeline, directly enforcing and magnifying distinctive characteristics of the post-attacked model to facilitate poison detection. Based on this, we formulate a unified framework and provide practical insights on designing detection pipelines that are more robust and generalizable. Third, we introduce the technique of Confusion Training (CT) as a concrete instantiation of our framework. CT applies an additional poisoning attack to the already poisoned dataset, actively decoupling benign correlation while exposing backdoor patterns to detection. Empirical evaluations on 4 datasets and 14 types of attacks validate the superiority of CT over 14 baseline defenses.
","
","arXiv
DBLP"
Bkd-FedGNN: A Benchmark for Classification Backdoor Attacks on Federated Graph Neural Network,"Fan Liu, Siqi Lai, Yansong Ning, Hao Liu","arXiv
arXiv","2023-06-17
2023-06","<a href=""arXiv (2023-06-17) : Bkd-FedGNN: A Benchmark for Classification Backdoor Attacks on Federated Graph Neural Network"" target=""_blank"">[http://arxiv.org/abs/2306.10351v1]</a>
<a href=""DBLP (2023-06) : Bkd-FedGNN: A Benchmark for Classification Backdoor Attacks on Federated Graph Neural Network"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.10351]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.10351]</a>","Federated Graph Neural Network (FedGNN) has recently emerged as a rapidly growing research topic, as it integrates the strengths of graph neural networks and federated learning to enable advanced machine learning applications without direct access to sensitive data. Despite its advantages, the distributed nature of FedGNN introduces additional vulnerabilities, particularly backdoor attacks stemming from malicious participants. Although graph backdoor attacks have been explored, the compounded complexity introduced by the combination of GNNs and federated learning has hindered a comprehensive understanding of these attacks, as existing research lacks extensive benchmark coverage and in-depth analysis of critical factors. To address these limitations, we propose Bkd-FedGNN, a benchmark for backdoor attacks on FedGNN. Specifically, Bkd-FedGNN decomposes the graph backdoor attack into trigger generation and injection steps, and extending the attack to the node-level federated setting, resulting in a unified framework that covers both node-level and graph-level classification tasks. Moreover, we thoroughly investigate the impact of multiple critical factors in backdoor attacks on FedGNN. These factors are categorized into global-level and local-level factors, including data distribution, the number of malicious attackers, attack time, overlapping rate, trigger size, trigger type, trigger position, and poisoning rate. Finally, we conduct comprehensive evaluations on 13 benchmark datasets and 13 critical factors, comprising 1,725 experimental configurations for node-level and graph-level tasks from six domains. These experiments encompass over 8,000 individual tests, allowing us to provide a thorough evaluation and insightful observations that advance our understanding of backdoor attacks on FedGNN.The Bkd-FedGNN benchmark is publicly available at https://github.com/usail-hkust/BkdFedGCN.
","<a href=""arXiv"" target=""_blank"">[https://github.com/usail-hkust/BkdFedGCN]</a>
","arXiv
DBLP"
A state-of-the-art review on adversarial machine learning in image classification,"Ashish Bajaj, Dinesh Kumar Vishwakarma",Multimedia Tools and Applications,2023-06-17,"<a href=""Springer (2023-06-17) : A state-of-the-art review on adversarial machine learning in image classification"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-15883-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-15883-z]</a>","Computer vision applications like traffic monitoring, security checks, self-driving cars, medical imaging, etc., rely heavily on machine learning...",,Springer
Device-specific security challenges and solution in IoT edge computing: a review,"Aditi Roy, J. Kokila, ... B. Shameedha Begum",The Journal of Supercomputing,2023-06-17,"<a href=""Springer (2023-06-17) : Device-specific security challenges and solution in IoT edge computing: a review"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05450-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05450-6]</a>","Rapid growth in IoT technology demands the need for the emergence of new IoT devices. IoT devices vary in terms of shape, size, storage, battery...",,Springer
An Ensemble Intrusion Detection System based on Acute Feature Selection,"Hariprasad S, Deepa T",Multimedia Tools and Applications,2023-06-16,"<a href=""Springer (2023-06-16) : An Ensemble Intrusion Detection System based on Acute Feature Selection"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-15788-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-15788-x]</a>","As the Internet of Things (IoT), 5G, and Artificial intelligence (AI) continue to converge, the number of security incidents and occurrences on the...",,Springer
Recursive Euclidean Distance-based Robust Aggregation Technique for Federated Learning,C. Herath Y. Rahulamathavan X. Liu,2023 IEEE IAS Global Conference on Emerging Technologies (GlobConET),2023-06-16,"<a href=""IEEE (2023-06-16) : Recursive Euclidean Distance-based Robust Aggregation Technique for Federated Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10150168]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/GlobConET56651.2023.10150168]</a>","Federated learning has gained popularity as a solution to data availability and privacy challenges in machine learning. However, the aggregation process of local model updates to obtain a global model in federated learning is susceptible to malicious attacks, such as backdoor poisoning, label-flipping, and membership inference. Malicious users aim to sabotage the collaborative learning process by training the local model with malicious data. This paper proposes a novel robust aggregation approach based on recursive Euclidean distance calculation. Our approach measures the Euclidean distance from the most recent global to local models and assigns weights accordingly. Local models which are far away from the most recent global model are assigned smaller weights to minimize the data poisoning effect during aggregation. Our experiments indicate that the proposed algorithm surpasses the latest algorithms by at least 5% in accuracy while reducing time complexity by less than 55%. Our contribution is significant as it addresses the critical issue of malicious attacks in federated learning while reducing aggregation time and improving the accuracy of the global model.",,IEEE
4MIDable: Flexible Network Offloading For Security VNFs,"Benjamin Lewis, Matthew Broadbent, ... Nicholas Race",Journal of Network and Systems Management,2023-06-15,"<a href=""Springer (2023-06-15) : 4MIDable: Flexible Network Offloading For Security VNFs"" target=""_blank"">[https://link.springer.com/article/10.1007/s10922-023-09744-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10922-023-09744-1]</a>",The ever-growing volume of network traffic and widening adoption of Internet protocols to underpin common communication processes augments the...,,Springer
A network anomaly detection algorithm based on semi-supervised learning and adaptive multiclass balancing,"Hao Zhang, Zude Xiao, ... Yanhua Liu",The Journal of Supercomputing,2023-06-15,"<a href=""Springer (2023-06-15) : A network anomaly detection algorithm based on semi-supervised learning and adaptive multiclass balancing"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05474-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05474-y]</a>","With the rapid development of network technology, the Internet has brought significant convenience to various sectors of society, holding a prominent...",,Springer
Facilitating Early-Stage Backdoor Attacks in Federated Learning with Whole Population Distribution Inference,Liu T.,IEEE Internet of Things Journal,2023-06-15,"<a href=""ScienceDirect (2023-06-15) : Facilitating Early-Stage Backdoor Attacks in Federated Learning with Whole Population Distribution Inference"" target=""_blank"">[https://doi.org/10.1109/JIOT.2023.3237806]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/JIOT.2023.3237806]</a>",,,ScienceDirect
Multi-target Backdoor Attacks for Code Pre-trained Models,"Yanzhou Li, Shangqing Liu, Kangjie Chen, Xiaofei Xie, Tianwei Zhang, Yang Liu","arXiv
ACL
arXiv","2023-06-14
2023
2023-06","<a href=""arXiv (2023-06-14) : Multi-target Backdoor Attacks for Code Pre-trained Models"" target=""_blank"">[http://arxiv.org/abs/2306.08350v1]</a>
<a href=""DBLP (2023) : Multi-target Backdoor Attacks for Code Pre-trained Models"" target=""_blank"">[https://doi.org/10.18653/v1/2023.acl-long.399]</a>
<a href=""DBLP (2023-06) : Multi-target Backdoor Attacks for Code Pre-trained Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.08350]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2023.acl-long.399]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.08350]</a>","Backdoor attacks for neural code models have gained considerable attention due to the advancement of code intelligence. However, most existing works insert triggers into task-specific data for code-related downstream tasks, thereby limiting the scope of attacks. Moreover, the majority of attacks for pre-trained models are designed for understanding tasks. In this paper, we propose task-agnostic backdoor attacks for code pre-trained models. Our backdoored model is pre-trained with two learning strategies (i.e., Poisoned Seq2Seq learning and token representation learning) to support the multi-target attack of downstream code understanding and generation tasks. During the deployment phase, the implanted backdoors in the victim models can be activated by the designed triggers to achieve the targeted attack. We evaluate our approach on two code understanding tasks and three code generation tasks over seven datasets. Extensive experiments demonstrate that our approach can effectively and stealthily attack code-related downstream tasks.

","

","arXiv
DBLP
DBLP"
Backdooring Neural Code Search,"Weisong Sun, Yuchen Chen, Guanhong Tao, Chunrong Fang, Xiangyu Zhang, Quanjun Zhang, Bin Luo","arXiv
ACL
arXiv","2023-06-12
2023
2023-05","<a href=""arXiv (2023-06-12) : Backdooring Neural Code Search"" target=""_blank"">[http://arxiv.org/abs/2305.17506v2]</a>
<a href=""DBLP (2023) : Backdooring Neural Code Search"" target=""_blank"">[https://doi.org/10.18653/v1/2023.acl-long.540]</a>
<a href=""DBLP (2023-05) : Backdooring Neural Code Search"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.17506]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2023.acl-long.540]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.17506]</a>","Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence behind many such engines. These models are based on deep learning and gain substantial attention due to their impressive performance. However, the security aspect of these models is rarely studied. Particularly, an adversary can inject a backdoor in neural code search models, which return buggy or even vulnerable code with security/privacy issues. This may impact the downstream software (e.g., stock trading systems and autonomous driving) and cause financial loss and/or life-threatening incidents. In this paper, we demonstrate such attacks are feasible and can be quite stealthy. By simply modifying one variable/function name, the attacker can make buggy/vulnerable code rank in the top 11%. Our attack BADCODE features a special trigger generation and injection procedure, making the attack more effective and stealthy. The evaluation is conducted on two neural code search models and the results show our attack outperforms baselines by 60%. Our user study demonstrates that our attack is more stealthy than the baseline by two times based on the F1 score.

","

","arXiv
DBLP
DBLP"
How to Backdoor Diffusion Models?,"Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho","arXiv
CVPR
arXiv","2023-06-09
2023
2022-12","<a href=""arXiv (2023-06-09) : How to Backdoor Diffusion Models?"" target=""_blank"">[http://arxiv.org/abs/2212.05400v3]</a>
<a href=""DBLP (2023) : How to Backdoor Diffusion Models?"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.00391]</a>
<a href=""DBLP (2022-12) : How to Backdoor Diffusion Models?"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.05400]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.00391]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.05400]</a>","Diffusion models are state-of-the-art deep learning empowered generative models that are trained based on the principle of learning forward and reverse diffusion processes via progressive noise-addition and denoising. To gain a better understanding of the limitations and potential risks, this paper presents the first study on the robustness of diffusion models against backdoor attacks. Specifically, we propose BadDiffusion, a novel attack framework that engineers compromised diffusion processes during model training for backdoor implantation. At the inference stage, the backdoored diffusion model will behave just like an untampered generator for regular data inputs, while falsely generating some targeted outcome designed by the bad actor upon receiving the implanted trigger signal. Such a critical risk can be dreadful for downstream tasks and applications built upon the problematic model. Our extensive experiments on various backdoor attack settings show that BadDiffusion can consistently lead to compromised diffusion models with high utility and target specificity. Even worse, BadDiffusion can be made cost-effective by simply finetuning a clean pre-trained diffusion model to implant backdoors. We also explore some possible countermeasures for risk mitigation. Our results call attention to potential risks and possible misuse of diffusion models. Our code is available on https://github.com/IBM/BadDiffusion.

","<a href=""arXiv"" target=""_blank"">[https://github.com/IBM/BadDiffusion]</a>

","arXiv
DBLP
DBLP"
Prevention of Kernel Rootkit in Cloud Computing,S. Suresh Kumar A. Ponni Valavan V. Prathiksha,2023 7th International Conference on Intelligent Computing and Control Systems (ICICCS),2023-06-08,"<a href=""IEEE (2023-06-08) : Prevention of Kernel Rootkit in Cloud Computing"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10142886]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICICCS56967.2023.10142886]</a>","A rootkit is a malicious programme created to load and operate directly from the operating system kernel. Kernel-mode rootkits, also referred to as rootkits that operate in the kernel, have the power to change the operating system as a whole. Such changes to the kernel are intended to hide the hack As a result, it is quite challenging to find a kernel rootkit. A system’s kernel can be modified via a variety of methods. Kernel rootkits also produce backdoor access, giving hackers covert access to the system. As a result, hackers have the capacity to change important computer data, collect personal information, and observe behaviour. Artificial neural networks provide support for the machine learning subfield known as deep neural network which is alike human brain mimic that works on massive datasets. In this work, the kernel rootkit is identified from the provided dataset using deep learning techniques by choosing the crucial features for training the detection models. Therefore, rootkit attacks can be prevented from downloading the affected files by identifying the kernel rootkit which is tested in AWS cloud environment by hosting the model in the EC2 instance. The unique feature of this work is how it utilizes the hyper-parameter tuning to enhance accuracy results.",,IEEE
An Automatic Error Detection Method for Machine Translation Results via Deep Learning,W. Zhang,IEEE Access,2023-06-07,"<a href=""IEEE (2023-06-07) : An Automatic Error Detection Method for Machine Translation Results via Deep Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138173]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2023.3280549]</a>","Nowadays, the rapid development of natural language processing has brought great progress for the area of machine translation. Various deep neural network-based machine translation approaches have been more and more general. However, there still lacks effective automatic error detection approaches for machine translation results. To bridge such gap, this paper proposes an automatic error detection method for machine translation results via deep learning. The training data is synthesized using the deep generative model proposed in this paper, which is used for the training of the foreign trade English grammatical error correction model. Then, the grammatical error correction model is used to correct the source sentences in the learner’s corpus, and the corrected target sentences and the manually annotated standard sentences are formed into “error-correct” sentence pairs, which are fed back to the error generation model for alternate training. By establishing a link between the grammatical error detection model and the grammatical error correction model, the error detection and correction capability of the model is improved. Experiments on datasets such as GTRSB show that the proposed error detection method significantly improves the stealthiness of the trigger while ensuring the effectiveness of the backdoor attack, and at the same time enables the trigger to resist certain data augmentation operations.",,IEEE
Leaky Kits: The Increased Risk of Data Exposure from Phishing Kits,B. Tejaswi N. Samarasinghe S. Pourali M. Mannan A. Youssef,2022 APWG Symposium on Electronic Crime Research (eCrime),2023-06-06,"<a href=""IEEE (2023-06-06) : Leaky Kits: The Increased Risk of Data Exposure from Phishing Kits"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10142092]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/eCrime57793.2022.10142092]</a>","Phishing kits allow adversaries with little or no technical experience to launch phishing websites in a short time. Past research has found such phishing kits that contain backdoors (e.g., obfuscated email addresses), which are intentionally added by the kit developers to obtain the phished data. In this work, we augment on prior research by exploring several ways in which security flaws in phishing kits make the victim data accessible to a wider set of adversaries beyond the kit deployers and kit developers. We implement an automated framework for kit collection and analysis, which includes a custom command-line PHP execution tool (for dynamic analysis) along with other open-source tools. Our analysis focuses on finding backdoors (e.g., obfuscated email address, command injection), measuring the extent of disclosure of sensitive information (e.g., via exposed plaintext files, hardcoded Telegram bot tokens, hardcoded admin console passwords) and detecting security vulnerabilities in phishing kits. We analyze 4238 distinct phishing kits (from a set of 26,281 compressed files collected from several sources over a span of 15 months), each having unique SHA-1 hash value. We found that 3.9% of the analyzed kits contained at least one form of backdoor. We also found hardcoded admin console passwords and API keys used to access third party services, in 8.3% and 16% of the analyzed kits, respectively. In addition, 15.8% of the analyzed kits wrote stolen information (PII) of users in plaintext files 5.6% kits did not restrict external access to these plaintext files, leading to exposure of sensitive phished data (e.g., 178,504 passwords, 133,248 email addresses, 1253 credit card numbers). Furthermore, 11.7% of the analyzed kits contained hardcoded Telegram bots we obtained invite links to join Telegram chats in 0.5% kits, and found them to expose chat messages containing sensitive PII information of victims (e.g., 73,342 passwords, 141,095 email addresses, 3584 credit card numbers). We also found that 64% of the kits are affected by security vulnerabilities (e.g., insecure file operations, SQL injection), which can be abused to further expose user data. We have open-sourced our framework and other artifacts to benefit future research.",,IEEE
Exploiting Logic Locking for a Neural Trojan Attack on Machine Learning Accelerators,Xu H.,"Proceedings of the ACM Great Lakes Symposium on VLSI, GLSVLSI",2023-06-05,"<a href=""ScienceDirect (2023-06-05) : Exploiting Logic Locking for a Neural Trojan Attack on Machine Learning Accelerators"" target=""_blank"">[https://doi.org/10.1145/3583781.3590242]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3583781.3590242]</a>",,,ScienceDirect
Potential cyber threats of adversarial attacks on autonomous driving models,Eldar Boltachev,Journal of Computer Virology and Hacking Techniques,2023-06-05,"<a href=""Springer (2023-06-05) : Potential cyber threats of adversarial attacks on autonomous driving models"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-023-00486-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-023-00486-x]</a>","Autonomous Vehicles (CAVs) are currently seen as a viable alternative to traditional vehicles. However, CAVs will face serious cyber threats because...",,Springer
A novel method to detect cyber-attacks in IoT/IIoT devices on the modbus protocol using deep learning,"Thierno Gueye, Yanen Wang, ... Sadaf Zahoor",Cluster Computing,2023-06-04,"<a href=""Springer (2023-06-04) : A novel method to detect cyber-attacks in IoT/IIoT devices on the modbus protocol using deep learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-023-04028-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-023-04028-4]</a>",The dominant intrusion detection models in internet of things industrial internet of things cybersecurity use network-based datasets. The Modbus...,,Springer
Mitigating Backdoor Attack Via Prerequisite Transformation,Han Gao,"arXiv
arXiv","2023-06-03
2023-06","<a href=""arXiv (2023-06-03) : Mitigating Backdoor Attack Via Prerequisite Transformation"" target=""_blank"">[http://arxiv.org/abs/2306.01983v1]</a>
<a href=""DBLP (2023-06) : Mitigating Backdoor Attack Via Prerequisite Transformation"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.01983]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.01983]</a>","In recent years, with the successful application of DNN in fields such as NLP and CV, its security has also received widespread attention. (Author) proposed the method of backdoor attack in Badnet. Switch implanted backdoor into the model by poisoning the training samples. The model with backdoor did not exhibit any abnormalities on the normal validation sample set, but in the input with trigger, they were mistakenly classified as the attacker's designated category or randomly classified as a different category from the ground truth, This attack method seriously threatens the normal application of DNN in real life, such as autonomous driving, object detection, etc.This article proposes a new method to combat backdoor attacks. We refer to the features in the area covered by the trigger as trigger features, and the remaining areas as normal features. By introducing prerequisite calculation conditions during the training process, these conditions have little impact on normal features and trigger features, and can complete the training of a standard backdoor model. The model trained under these prerequisite calculation conditions can, In the verification set D'val with the same premise calculation conditions, the performance is consistent with that of the ordinary backdoor model. However, in the verification set Dval without the premise calculation conditions, the verification accuracy decreases very little (7%~12%), while the attack success rate (ASR) decreases from 90% to about 8%.Author call this method Prerequisite Transformation(PT).
","
","arXiv
DBLP"
A better and fast cloud intrusion detection system using improved squirrel search algorithm and modified deep belief network,"Nairita Sarkar, Pankaj Kumar Keserwani, Mahesh Chandra Govil",Cluster Computing,2023-06-03,"<a href=""Springer (2023-06-03) : A better and fast cloud intrusion detection system using improved squirrel search algorithm and modified deep belief network"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-023-04037-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-023-04037-3]</a>",Utilizing the cloud environment is one of the most preferable option in every information technology (IT) organization for running its business due...,,Springer
Detection of non-trivial preservable quotient spaces in S-Box(es),"Shah Fahd, Mehreen Afzal, ... Yawar Abbas",Neural Computing and Applications,2023-06-03,"<a href=""Springer (2023-06-03) : Detection of non-trivial preservable quotient spaces in S-Box(es)"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-023-08654-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-023-08654-2]</a>",Substitution Box (S-Box) is employed in block ciphers to ensure non-linearity. An n -bit bijective S-Box is a member of the Symmetric Group ...,,Springer
Intrusion detection in internet of things-based smart farming using hybrid deep learning framework,"Keerthi Kethineni, G. Pradeepini",Cluster Computing,2023-06-03,"<a href=""Springer (2023-06-03) : Intrusion detection in internet of things-based smart farming using hybrid deep learning framework"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-023-04052-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-023-04052-4]</a>",Smart agriculture is a popular domain due to its intensified growth in recent times. This domain aggregates the advantages of several computing...,,Springer
Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark,"Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie","arXiv
ACL
arXiv","2023-06-02
2023
2023-05","<a href=""arXiv (2023-06-02) : Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark"" target=""_blank"">[http://arxiv.org/abs/2305.10036v3]</a>
<a href=""DBLP (2023) : Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark"" target=""_blank"">[https://doi.org/10.18653/v1/2023.acl-long.423]</a>
<a href=""DBLP (2023-05) : Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.10036]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2023.acl-long.423]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.10036]</a>","Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called EmbMarker that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer's model for copyright verification while minimizing the adverse impact on the original embeddings' utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality.

","

","arXiv
DBLP
DBLP"
Graph Neural Networks for Hardware Vulnerability Analysis— Can you Trust your GNN?,L. Alrahis O. Sinanoglu,2023 IEEE 41st VLSI Test Symposium (VTS),2023-06-02,"<a href=""IEEE (2023-06-02) : Graph Neural Networks for Hardware Vulnerability Analysis— Can you Trust your GNN?"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10140095]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/VTS56346.2023.10140095]</a>","The participation of third-party entities in the globalized semiconductor supply chain introduces potential security vulnerabilities, such as intellectual property piracy and hardware Trojan (HT) insertion. Graph neural networks (GNNs) have been employed to address various hardware security threats, owing to their superior performance on graph-structured data, such as circuits. However, GNNs are also susceptible to attacks.This work examines the use of GNNs for detecting hardware threats like HTs and their vulnerability to attacks. We present BadGNN, a backdoor attack on GNNs that can hide HTs and evade detection with a 100% success rate through minor circuit perturbations. Our findings highlight the need for further investigation into the security and robustness of GNNs before they can be safely used in security-critical applications.",,IEEE
The compliance implications of a cyberattack: a distributed denial of service (DDoS) attack explored,"Fabian Maximilian Johannes Teichmann, Bruno S. Sergi, Chiara Wittmann",International Cybersecurity Law Review,2023-06-02,"<a href=""Springer (2023-06-02) : The compliance implications of a cyberattack: a distributed denial of service (DDoS) attack explored"" target=""_blank"">[https://link.springer.com/article/10.1365/s43439-023-00090-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1365/s43439-023-00090-1]</a>",The nuisance of cumbersome technological issues is a universal grievance of technology users. Beyond the immediate frustrations of the operational...,,Springer
A Blockchain-Based Federated-Learning Framework for Defense against Backdoor Attacks,Li L.,Electronics (Switzerland),2023-06-01,"<a href=""ScienceDirect (2023-06-01) : A Blockchain-Based Federated-Learning Framework for Defense against Backdoor Attacks"" target=""_blank"">[https://doi.org/10.3390/electronics12112500]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/electronics12112500]</a>",,,ScienceDirect
A Robust Aggregated Algorithms against a Large Group Backdoor Clients in Federated Learning System,Wang Y.K.,Jisuanji Xuebao/Chinese Journal of Computers,2023-06-01,"<a href=""ScienceDirect (2023-06-01) : A Robust Aggregated Algorithms against a Large Group Backdoor Clients in Federated Learning System"" target=""_blank"">[https://doi.org/10.11897/SP.J.1016.2023.01302]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.11897/SP.J.1016.2023.01302]</a>",,,ScienceDirect
"DIHBA: Dynamic, invisible and high attack success rate boundary backdoor attack with low poison ratio",Ma B.,Computers and Security,2023-06-01,"<a href=""ScienceDirect (2023-06-01) : DIHBA: Dynamic, invisible and high attack success rate boundary backdoor attack with low poison ratio"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103212]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103212]</a>",,,ScienceDirect
OTB-morph: One-time Biometrics via Morphing,"Mahdi Ghafourian, Julian Fierrez, ... Ignacio Serna",Machine Intelligence Research,2023-06-01,"<a href=""Springer (2023-06-01) : OTB-morph: One-time Biometrics via Morphing"" target=""_blank"">[https://link.springer.com/article/10.1007/s11633-023-1432-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11633-023-1432-x]</a>",Cancelable biometrics are a group of techniques to transform the input biometric to an irreversible feature intentionally using a transformation...,,Springer
SafeNet: The Unreasonable Effectiveness of Ensembles in Private Collaborative Learning,H. Chaudhari M. Jagielski A. Oprea,2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML),2023-06-01,"<a href=""IEEE (2023-06-01) : SafeNet: The Unreasonable Effectiveness of Ensembles in Private Collaborative Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10136155]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SaTML54575.2023.00021]</a>","Secure multiparty computation (MPC) has been proposed to allow multiple mutually distrustful data owners to jointly train machine learning (ML) models on their combined data. However, by design, MPC protocols faithfully compute the training functionality, which is susceptible to poisoning and privacy attacks, as shown by the adversarial ML community. In this work, we argue that model ensembles, implemented in our framework called SafeNet, are a highly MPC-amenable way to avoid many adversarial ML attacks. The natural partitioning of data amongst owners in MPC training allows this approach to be highly scalable at training time, provide provable protection from poisoning attacks, and provable defense against a number of privacy attacks. We demonstrate SafeNet's efficiency, accuracy, and resilience to poisoning on several machine learning datasets and models trained in end-to-end and transfer learning scenarios. For instance, SafeNet reduces backdoor attack success significantly, while achieving 39× faster training and 36× less communication than the four-party MPC framework of Dalskov et al. [31]. Our experiments show that ensemble learning retains these benefits even in many non-iid settings. The simplicity, cheap setup, and robustness properties of ensemble learning make it a strong first choice for training ML models privately in MPC.",,IEEE
Two-phase Defense Against Poisoning Attacks on Federated Learning-based Intrusion Detection,Lai Y.C.,Computers and Security,2023-06-01,"<a href=""ScienceDirect (2023-06-01) : Two-phase Defense Against Poisoning Attacks on Federated Learning-based Intrusion Detection"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103205]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103205]</a>",,,ScienceDirect
G2uardFL: Safeguarding Federated Learning Against Backdoor Attacks through Attributed Client Graph Clustering,"Hao Yu, Chuan Ma, Meng Liu, Xinwang Liu, Zhe Liu, Ming Ding",arXiv,2023-06,"<a href=""DBLP (2023-06) : G2uardFL: Safeguarding Federated Learning Against Backdoor Attacks through Attributed Client Graph Clustering"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.04984]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.04984]</a>",,,DBLP
IMPOSITION: Implicit Backdoor Attack through Scenario Injection,"Mozhgan PourKeshavarz, Mohammad Sabokrou, Amir Rasouli",arXiv,2023-06,"<a href=""DBLP (2023-06) : IMPOSITION: Implicit Backdoor Attack through Scenario Injection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.15755]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.15755]</a>",,,DBLP
"Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers","Ruotong Wang, Hongrui Chen, Zihao Zhu, Li Liu, Yong Zhang, Yanbo Fan, Baoyuan Wu",arXiv,2023-06,"<a href=""DBLP (2023-06) : Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.00816]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2306.00816]</a>",,,DBLP
A computationally efficient and randomized RLWE-based key exchange scheme,"Komal Pursharthi, Dheerendra Mishra",Cluster Computing,2023-05-31,"<a href=""Springer (2023-05-31) : A computationally efficient and randomized RLWE-based key exchange scheme"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-023-04032-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-023-04032-8]</a>","Key exchange enables two or more entities to agree on an identical key. In post-quantum, the construction of ring learning with error (RLWE) based...",,Springer
IoT-based intrusion detection system for healthcare using RNNBiLSTM deep learning strategy with custom features,"D. V. Jeyanthi, B. Indrani",Soft Computing,2023-05-31,"<a href=""Springer (2023-05-31) : IoT-based intrusion detection system for healthcare using RNNBiLSTM deep learning strategy with custom features"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-023-08536-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-023-08536-8]</a>",The need for security in healthcare environments has become increasingly important due to the rise of cyber-attacks and data breaches. In order to...,,Springer
SHAKE-ESDRL-based energy efficient intrusion detection and hashing system,"Geo Francis E, S. Sheeja",Annals of Telecommunications,2023-05-31,"<a href=""Springer (2023-05-31) : SHAKE-ESDRL-based energy efficient intrusion detection and hashing system"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-023-00963-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-023-00963-w]</a>","Outstanding progress in unsolicited intrusions along with security threats, which interrupt the normal operations of wireless sensor networks (WSNs),...",,Springer
Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study,"Yiqi Zhong, Xianming Liu, Deming Zhai, Junjun Jiang, Xiangyang Ji","arXiv
arXiv","2023-05-28
2023-05","<a href=""arXiv (2023-05-28) : Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study"" target=""_blank"">[http://arxiv.org/abs/2305.18384v1]</a>
<a href=""DBLP (2023-05) : Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.18384]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.18384]</a>","Large amounts of incremental learning algorithms have been proposed to alleviate the catastrophic forgetting issue arises while dealing with sequential data on a time series. However, the adversarial robustness of incremental learners has not been widely verified, leaving potential security risks. Specifically, for poisoning-based backdoor attacks, we argue that the nature of streaming data in IL provides great convenience to the adversary by creating the possibility of distributed and cross-task attacks -- an adversary can affect \textbf{any unknown} previous or subsequent task by data poisoning \textbf{at any time or time series} with extremely small amount of backdoor samples injected (e.g., $0.1\%$ based on our observations). To attract the attention of the research community, in this paper, we empirically reveal the high vulnerability of 11 typical incremental learners against poisoning-based backdoor attack on 3 learning scenarios, especially the cross-task generalization effect of backdoor knowledge, while the poison ratios range from $5\%$ to as low as $0.1\%$. Finally, the defense mechanism based on activation clustering is found to be effective in detecting our trigger pattern to mitigate potential security risks.
","
","arXiv
DBLP"
Learning to Backdoor Federated Learning,"Henger Li, Chen Wu, Sencun Zhu, Zizhan Zheng","arXiv
arXiv","2023-05-28
2023-03","<a href=""arXiv (2023-05-28) : Learning to Backdoor Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2303.03320v3]</a>
<a href=""DBLP (2023-03) : Learning to Backdoor Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.03320]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.03320]</a>","In a federated learning (FL) system, malicious participants can easily embed backdoors into the aggregated model while maintaining the model's performance on the main task. To this end, various defenses, including training stage aggregation-based defenses and post-training mitigation defenses, have been proposed recently. While these defenses obtain reasonable performance against existing backdoor attacks, which are mainly heuristics based, we show that they are insufficient in the face of more advanced attacks. In particular, we propose a general reinforcement learning-based backdoor attack framework where the attacker first trains a (non-myopic) attack policy using a simulator built upon its local data and common knowledge on the FL system, which is then applied during actual FL training. Our attack framework is both adaptive and flexible and achieves strong attack performance and durability even under state-of-the-art defenses.
","
","arXiv
DBLP"
IMBERT: Making BERT Immune to Insertion-based Backdoor Attacks,"Xuanli He, Jun Wang, Benjamin Rubinstein, Trevor Cohn","arXiv
arXiv","2023-05-25
2023-05","<a href=""arXiv (2023-05-25) : IMBERT: Making BERT Immune to Insertion-based Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2305.16503v1]</a>
<a href=""DBLP (2023-05) : IMBERT: Making BERT Immune to Insertion-based Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.16503]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.16503]</a>","Backdoor attacks are an insidious security threat against machine learning models. Adversaries can manipulate the predictions of compromised models by inserting triggers into the training phase. Various backdoor attacks have been devised which can achieve nearly perfect attack success without affecting model predictions for clean inputs. Means of mitigating such vulnerabilities are underdeveloped, especially in natural language processing. To fill this gap, we introduce IMBERT, which uses either gradients or self-attention scores derived from victim models to self-defend against backdoor attacks at inference time. Our empirical studies demonstrate that IMBERT can effectively identify up to 98.5% of inserted triggers. Thus, it significantly reduces the attack success rate while attaining competitive accuracy on the clean dataset across widespread insertion-based attacks compared to two baselines. Finally, we show that our approach is model-agnostic, and can be easily ported to several pre-trained transformer models.
","
","arXiv
DBLP"
Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning,"Yanbo Dai, Songze Li","arXiv
ICML
arXiv","2023-05-25
2023
2023-04","<a href=""arXiv (2023-05-25) : Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2304.12961v2]</a>
<a href=""DBLP (2023) : Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning"" target=""_blank"">[https://proceedings.mlr.press/v202/dai23a.html]</a>
<a href=""DBLP (2023-04) : Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.12961]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://proceedings.mlr.press/v202/dai23a.html]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.12961]</a>","In a federated learning (FL) system, distributed clients upload their local models to a central server to aggregate into a global model. Malicious clients may plant backdoors into the global model through uploading poisoned local models, causing images with specific patterns to be misclassified into some target labels. Backdoors planted by current attacks are not durable, and vanish quickly once the attackers stop model poisoning. In this paper, we investigate the connection between the durability of FL backdoors and the relationships between benign images and poisoned images (i.e., the images whose labels are flipped to the target label during local training). Specifically, benign images with the original and the target labels of the poisoned images are found to have key effects on backdoor durability. Consequently, we propose a novel attack, Chameleon, which utilizes contrastive learning to further amplify such effects towards a more durable backdoor. Extensive experiments demonstrate that Chameleon significantly extends the backdoor lifespan over baselines by $1.2\times \sim 4\times$, for a wide range of image datasets, backdoor types, and model architectures.

","

","arXiv
DBLP
DBLP"
DAGUARD: distributed backdoor attack defense scheme under federated learning,Yu S.,Tongxin Xuebao/Journal on Communications,2023-05-25,"<a href=""ScienceDirect (2023-05-25) : DAGUARD: distributed backdoor attack defense scheme under federated learning"" target=""_blank"">[https://doi.org/10.11959/j.issn.1000-436x.2023086]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.11959/j.issn.1000-436x.2023086]</a>",,,ScienceDirect
Evaluation of soft computing in intrusion detection for secure social Internet of Things based on collaborative edge computing,"Bishwajeet Kumar Pandey, Vineet Saxena, ... Rajesh Gupta",Soft Computing,2023-05-22,"<a href=""Springer (2023-05-22) : Evaluation of soft computing in intrusion detection for secure social Internet of Things based on collaborative edge computing"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-023-08397-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-023-08397-1]</a>","Edge computing is a new trend in the periphery of networks, and it encompasses both cloud computing and the Internet of Things (IoT). As with other...",,Springer
Security optimization algorithm for public information platform of internet of things based on open architecture,Zhe Wang,Soft Computing,2023-05-22,"<a href=""Springer (2023-05-22) : Security optimization algorithm for public information platform of internet of things based on open architecture"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-023-08369-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-023-08369-5]</a>","At present, with the rapid growth of electronic information and the continuous improvement of computing power, the Internet of Things (IoT) has...",,Springer
Salient Conditional Diffusion for Defending Against Backdoor Attacks,"Brandon B. May, N. Joseph Tatro, Dylan Walker, Piyush Kumar, Nathan Shnidman","arXiv
arXiv","2023-05-19
2023-01","<a href=""arXiv (2023-05-19) : Salient Conditional Diffusion for Defending Against Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2301.13862v2]</a>
<a href=""DBLP (2023-01) : Salient Conditional Diffusion for Defending Against Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.13862]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.13862]</a>","We propose a novel algorithm, Salient Conditional Diffusion (Sancdifi), a state-of-the-art defense against backdoor attacks. Sancdifi uses a denoising diffusion probabilistic model (DDPM) to degrade an image with noise and then recover said image using the learned reverse diffusion. Critically, we compute saliency map-based masks to condition our diffusion, allowing for stronger diffusion on the most salient pixels by the DDPM. As a result, Sancdifi is highly effective at diffusing out triggers in data poisoned by backdoor attacks. At the same time, it reliably recovers salient features when applied to clean data. This performance is achieved without requiring access to the model parameters of the Trojan network, meaning Sancdifi operates as a black-box defense.
","
","arXiv
DBLP"
iOS mobile malware analysis: a state-of-the-art,"Madihah Mohd Saudi, Muhammad Afif Husainiamer, ... Mohd Yamani Idna Idris",Journal of Computer Virology and Hacking Techniques,2023-05-18,"<a href=""Springer (2023-05-18) : iOS mobile malware analysis: a state-of-the-art"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-023-00477-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-023-00477-y]</a>","In earlier years, most malware attacks were against Android smartphones. Unfortunately, for the past few years, the trend has shifted towards attacks...",,Springer
UOR: Universal Backdoor Attacks on Pre-trained Language Models,"Wei Du, Peixuan Li, Boqun Li, Haodong Zhao, Gongshen Liu","arXiv
arXiv","2023-05-16
2023-05","<a href=""arXiv (2023-05-16) : UOR: Universal Backdoor Attacks on Pre-trained Language Models"" target=""_blank"">[http://arxiv.org/abs/2305.09574v1]</a>
<a href=""DBLP (2023-05) : UOR: Universal Backdoor Attacks on Pre-trained Language Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.09574]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.09574]</a>","Backdoors implanted in pre-trained language models (PLMs) can be transferred to various downstream tasks, which exposes a severe security threat. However, most existing backdoor attacks against PLMs are un-targeted and task-specific. Few targeted and task-agnostic methods use manually pre-defined triggers and output representations, which prevent the attacks from being more effective and general. In this paper, we first summarize the requirements that a more threatening backdoor attack against PLMs should satisfy, and then propose a new backdoor attack method called UOR, which breaks the bottleneck of the previous approach by turning manual selection into automatic optimization. Specifically, we define poisoned supervised contrastive learning which can automatically learn the more uniform and universal output representations of triggers for various PLMs. Moreover, we use gradient search to select appropriate trigger words which can be adaptive to different PLMs and vocabularies. Experiments show that our method can achieve better attack performance on various text classification tasks compared to manual methods. Further, we tested our method on PLMs with different architectures, different usage paradigms, and more difficult tasks, which demonstrated the universality of our method.
","
","arXiv
DBLP"
Backdoor to the Hidden Ground State: Planted Vertex Cover Example,"Xin-Yi Fan, Hai-Jun Zhou","arXiv
arXiv","2023-05-11
2023-05","<a href=""arXiv (2023-05-11) : Backdoor to the Hidden Ground State: Planted Vertex Cover Example"" target=""_blank"">[http://arxiv.org/abs/2305.06610v1]</a>
<a href=""DBLP (2023-05) : Backdoor to the Hidden Ground State: Planted Vertex Cover Example"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.06610]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.06610]</a>","We introduce a planted vertex cover problem on regular random graphs and study it by the cavity method. The equilibrium ordering phase transition of this binary-spin two-body interaction system is discontinuous in nature distinct from the continuous one of conventional Ising-like models, and it is dynamically blocked by an extensive free energy barrier. We discover that the disordered symmetric phase of this system may be locally stable with respect to the ordered phase at all inverse temperatures except for a unique eureka point $\beta_b$ at which it is only marginally stable. The eureka point $\beta_b$ serves as a backdoor to access the hidden ground state with vanishing free energy barrier. It exists in an infinite series of planted random graph ensembles and we determine their structural parameters analytically. The revealed new type of free energy landscape may also exist in other planted random-graph optimization problems at the interface of statistical physics and statistical inference.
","
","arXiv
DBLP"
Stealthy Low-frequency Backdoor Attack against Deep Neural Networks,"Xinrui Liu, Yu-an Tan, Yajie Wang, Kefan Qiu, Yuanzhang Li","arXiv
arXiv","2023-05-10
2023-05","<a href=""arXiv (2023-05-10) : Stealthy Low-frequency Backdoor Attack against Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2305.09677v1]</a>
<a href=""DBLP (2023-05) : Stealthy Low-frequency Backdoor Attack against Deep Neural Networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.09677]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.09677]</a>","Deep neural networks (DNNs) have gain its popularity in various scenarios in recent years. However, its excellent ability of fitting complex functions also makes it vulnerable to backdoor attacks. Specifically, a backdoor can remain hidden indefinitely until activated by a sample with a specific trigger, which is hugely concealed. Nevertheless, existing backdoor attacks operate backdoors in spatial domain, i.e., the poisoned images are generated by adding additional perturbations to the original images, which are easy to detect. To bring the potential of backdoor attacks into full play, we propose low-pass attack, a novel attack scheme that utilizes low-pass filter to inject backdoor in frequency domain. Unlike traditional poisoned image generation methods, our approach reduces high-frequency components and preserve original images' semantic information instead of adding additional perturbations, improving the capability of evading current defenses. Besides, we introduce ""precision mode"" to make our backdoor triggered at a specified level of filtering, which further improves stealthiness. We evaluate our low-pass attack on four datasets and demonstrate that even under pollution rate of 0.01, we can perform stealthy attack without trading off attack performance. Besides, our backdoor attack can successfully bypass state-of-the-art defending mechanisms. We also compare our attack with existing backdoor attacks and show that our poisoned images are nearly invisible and retain higher image quality.
","
","arXiv
DBLP"
Towards Invisible Backdoor Attacks in the Frequency Domain against Deep Neural Networks,"Xinrui Liu, Yajie Wang, Yu-an Tan, Kefan Qiu, Yuanzhang Li","arXiv
arXiv","2023-05-10
2023-05","<a href=""arXiv (2023-05-10) : Towards Invisible Backdoor Attacks in the Frequency Domain against Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2305.10596v1]</a>
<a href=""DBLP (2023-05) : Towards Invisible Backdoor Attacks in the Frequency Domain against Deep Neural Networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.10596]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.10596]</a>","Deep neural networks (DNNs) have made tremendous progress in the past ten years and have been applied in various critical applications. However, recent studies have shown that deep neural networks are vulnerable to backdoor attacks. By injecting malicious data into the training set, an adversary can plant the backdoor into the original model. The backdoor can remain hidden indefinitely until activated by a sample with a specific trigger, which is hugely concealed, bringing serious security risks to critical applications. However, one main limitation of current backdoor attacks is that the trigger is often visible to human perception. Therefore, it is crucial to study the stealthiness of backdoor triggers. In this paper, we propose a novel frequency-domain backdooring technique. In particular, our method aims to add a backdoor trigger in the frequency domain of original images via Discrete Fourier Transform, thus hidding the trigger. We evaluate our method on three benchmark datasets: MNIST, CIFAR-10 and Imagenette. Our experiments show that we can simultaneously fool human inspection and DNN models. We further apply two image similarity evaluation metrics to illustrate that our method adds the most subtle perturbation without compromising attack success rate and clean sample accuracy.
","
","arXiv
DBLP"
Invisible Backdoor Attack with Dynamic Triggers against Person Re-identification,"Wenli Sun, Xinyang Jiang, Shuguang Dou, Dongsheng Li, Duoqian Miao, Cheng Deng, Cairong Zhao","arXiv
arXiv","2023-05-10
2022-11","<a href=""arXiv (2023-05-10) : Invisible Backdoor Attack with Dynamic Triggers against Person Re-identification"" target=""_blank"">[http://arxiv.org/abs/2211.10933v2]</a>
<a href=""DBLP (2022-11) : Invisible Backdoor Attack with Dynamic Triggers against Person Re-identification"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.10933]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.10933]</a>","In recent years, person Re-identification (ReID) has rapidly progressed with wide real-world applications, but also poses significant risks of adversarial attacks. In this paper, we focus on the backdoor attack on deep ReID models. Existing backdoor attack methods follow an all-to-one or all-to-all attack scenario, where all the target classes in the test set have already been seen in the training set. However, ReID is a much more complex fine-grained open-set recognition problem, where the identities in the test set are not contained in the training set. Thus, previous backdoor attack methods for classification are not applicable for ReID. To ameliorate this issue, we propose a novel backdoor attack on deep ReID under a new all-to-unknown scenario, called Dynamic Triggers Invisible Backdoor Attack (DT-IBA). Instead of learning fixed triggers for the target classes from the training set, DT-IBA can dynamically generate new triggers for any unknown identities. Specifically, an identity hashing network is proposed to first extract target identity information from a reference image, which is then injected into the benign images by image steganography. We extensively validate the effectiveness and stealthiness of the proposed attack on benchmark datasets, and evaluate the effectiveness of several defense methods against our attack.
","
","arXiv
DBLP"
EEG-Based Brain–Computer Interfaces are Vulnerable to Backdoor Attacks,L. Meng X. Jiang J. Huang Z. Zeng S. Yu T. -P. Jung C. -T. Lin R. Chavarriaga D. Wu,IEEE Transactions on Neural Systems and Rehabilitation Engineering,2023-05-10,"<a href=""IEEE (2023-05-10) : EEG-Based Brain–Computer Interfaces are Vulnerable to Backdoor Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10119172]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TNSRE.2023.3273214]</a>","Research and development of electroencephalogram (EEG) based brain-computer interfaces (BCIs) have advanced rapidly, partly due to deeper understanding of the brain and wide adoption of sophisticated machine learning approaches for decoding the EEG signals. However, recent studies have shown that machine learning algorithms are vulnerable to adversarial attacks. This paper proposes to use narrow period pulse for poisoning attack of EEG-based BCIs, which makes adversarial attacks much easier to implement. One can create dangerous backdoors in the machine learning model by injecting poisoning samples into the training set. Test samples with the backdoor key will then be classified into the target class specified by the attacker. What most distinguishes our approach from previous ones is that the backdoor key does not need to be synchronized with the EEG trials, making it very easy to implement. The effectiveness and robustness of the backdoor attack approach is demonstrated, highlighting a critical security concern for EEG-based BCIs and calling for urgent attention to address it.",,IEEE
"IoT malware: An attribute-based taxonomy, detection mechanisms and challenges","Princy Victor, Arash Habibi Lashkari, ... Shahrear Iqbal",Peer-to-Peer Networking and Applications,2023-05-10,"<a href=""Springer (2023-05-10) : IoT malware: An attribute-based taxonomy, detection mechanisms and challenges"" target=""_blank"">[https://link.springer.com/article/10.1007/s12083-023-01478-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12083-023-01478-w]</a>","During the past decade, the Internet of Things (IoT) has paved the way for the ongoing digitization of society in unique ways. Its penetration into...",,Springer
BadCS: A Backdoor Attack Framework for Code search,"Shiyi Qi, Yuanhang Yang, Shuzhzeng Gao, Cuiyun Gao, Zenglin Xu","arXiv
arXiv","2023-05-09
2023-05","<a href=""arXiv (2023-05-09) : BadCS: A Backdoor Attack Framework for Code search"" target=""_blank"">[http://arxiv.org/abs/2305.05503v1]</a>
<a href=""DBLP (2023-05) : BadCS: A Backdoor Attack Framework for Code search"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.05503]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.05503]</a>","With the development of deep learning (DL), DL-based code search models have achieved state-of-the-art performance and have been widely used by developers during software development. However, the security issue, e.g., recommending vulnerable code, has not received sufficient attention, which will bring potential harm to software development. Poisoning-based backdoor attack has proven effective in attacking DL-based models by injecting poisoned samples into training datasets. However, previous work shows that the attack technique does not perform successfully on all DL-based code search models and tends to fail for Transformer-based models, especially pretrained models. Besides, the infected models generally perform worse than benign models, which makes the attack not stealthy enough and thereby hinders the adoption by developers. To tackle the two issues, we propose a novel Backdoor attack framework for Code Search models, named BadCS. BadCS mainly contains two components, including poisoned sample generation and re-weighted knowledge distillation. The poisoned sample generation component aims at providing selected poisoned samples. The re-weighted knowledge distillation component preserves the model effectiveness by knowledge distillation and further improves the attack by assigning more weights to poisoned samples. Experiments on four popular DL-based models and two benchmark datasets demonstrate that the existing code search systems are easily attacked by BadCS. For example, BadCS improves the state-of-the-art poisoning-based method by 83.03%-99.98% and 75.98%-99.90% on Python and Java datasets, respectively. Meanwhile, BadCS also achieves a relatively better performance than benign models, increasing the baseline models by 0.49% and 0.46% on average, respectively.
","
","arXiv
DBLP"
Secure and Privacy-Preserving Framework for IoT-Enabled Smart Grid Environment,"Chandan Kumar, Prakash Chittora",Arabian Journal for Science and Engineering,2023-05-09,"<a href=""Springer (2023-05-09) : Secure and Privacy-Preserving Framework for IoT-Enabled Smart Grid Environment"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-023-07900-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-023-07900-y]</a>","Due to recent technical breakthroughs in wireless communication and the Internet of Things (IoT), the smart grid (SG) has been recognized as a...",,Springer
Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias,"Zhiyuan Zhang, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun","arXiv
ACL
arXiv","2023-05-08
2023
2023-05","<a href=""arXiv (2023-05-08) : Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias"" target=""_blank"">[http://arxiv.org/abs/2305.04547v1]</a>
<a href=""DBLP (2023) : Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias"" target=""_blank"">[https://doi.org/10.18653/v1/2023.findings-acl.157]</a>
<a href=""DBLP (2023-05) : Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.04547]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2023.findings-acl.157]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.04547]</a>","Pre-trained Language Models (PLMs) may be poisonous with backdoors or bias injected by the suspicious attacker during the fine-tuning process. A core challenge of purifying potentially poisonous PLMs is precisely finding poisonous dimensions. To settle this issue, we propose the Fine-purifying approach, which utilizes the diffusion theory to study the dynamic process of fine-tuning for finding potentially poisonous dimensions. According to the relationship between parameter drifts and Hessians of different dimensions, we can detect poisonous dimensions with abnormal dynamics, purify them by resetting them to clean pre-trained weights, and then fine-tune the purified weights on a small clean dataset. To the best of our knowledge, we are the first to study the dynamics guided by the diffusion theory for safety or defense purposes. Experimental results validate the effectiveness of Fine-purifying even with a small clean dataset.

","

","arXiv
DBLP
DBLP"
"Advanced Persistent Threats (APT): evolution, anatomy, attribution and countermeasures","Amit Sharma, Brij B. Gupta, ... V. K. Saraswat",Journal of Ambient Intelligence and Humanized Computing,2023-05-06,"<a href=""Springer (2023-05-06) : Advanced Persistent Threats (APT): evolution, anatomy, attribution and countermeasures"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-023-04603-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-023-04603-y]</a>","In today’s cyber warfare realm, every stakeholder in cyberspace is becoming more potent by developing advanced cyber weapons. They have equipped with...",,Springer
BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks,"Zihan Guan, Mengxuan Hu, Zhongliang Zhou, Jielu Zhang, Sheng Li, Ninghao Liu","arXiv
arXiv","2023-05-05
2023-05","<a href=""arXiv (2023-05-05) : BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2305.03289v1]</a>
<a href=""DBLP (2023-05) : BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.03289]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.03289]</a>","Recently, the Segment Anything Model (SAM) has gained significant attention as an image segmentation foundation model due to its strong performance on various downstream tasks. However, it has been found that SAM does not always perform satisfactorily when faced with challenging downstream tasks. This has led downstream users to demand a customized SAM model that can be adapted to these downstream tasks. In this paper, we present BadSAM, the first backdoor attack on the image segmentation foundation model. Our preliminary experiments on the CAMO dataset demonstrate the effectiveness of BadSAM.
","
","arXiv
DBLP"
FedPrompt: Communication-Efficient and Privacy-Preserving Prompt Tuning in Federated Learning,H. Zhao W. Du F. Li P. Li G. Liu,"ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",2023-05-05,"<a href=""IEEE (2023-05-05) : FedPrompt: Communication-Efficient and Privacy-Preserving Prompt Tuning in Federated Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10095356]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10095356]</a>","Federated learning (FL) has enabled global model training on decentralized data in a privacy-preserving way. However, for tasks that utilize pre-trained language models (PLMs) with massive parameters, there are considerable communication costs. Prompt tuning, which tunes soft prompts without modifying PLMs, has achieved excellent performance as a new learning paradigm. In this paper, we want to combine these methods and explore the effect of prompt tuning under FL. We propose ""FedPrompt"" studying prompt tuning in a model split aggregation way using FL, and prove that split aggregation greatly reduces the communication cost, only 0.01% of the PLMs’ parameters, with little decrease on accuracy both on IID and Non-IID data distribution. We further conduct backdoor attacks by data poisoning on FedPrompt. Experiments show that attack achieve a quite low attack success rate and can not inject backdoor effectively, proving the robustness of FedPrompt.",,IEEE
OCPP in the spotlight: threats and countermeasures for electric vehicle charging infrastructures 4.0,"Cristina Alcaraz, Jesus Cumplido, Alicia Trivin̄o",International Journal of Information Security,2023-05-05,"<a href=""Springer (2023-05-05) : OCPP in the spotlight: threats and countermeasures for electric vehicle charging infrastructures 4.0"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00698-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00698-8]</a>","Undoubtedly, Industry 4.0 in the energy sector improves the conditions for automation, generation and distribution of energy, increasing the rate of...",,Springer
Security Auditing Methodology Using Crowdsourced Testing,K. N. Sasank S. Revanth J. Yadavalli A. K. Panidepu K. V. D. Kiran V. V. Prasad Padyala,2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS),2023-05-05,"<a href=""IEEE (2023-05-05) : Security Auditing Methodology Using Crowdsourced Testing"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10113128]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICACCS57279.2023.10113128]</a>","The term cyber is derived from French, meaning ""the art of governing"". Cyber security as we know is the process of securing software and electronic networks from malicious attacks. Cyber security is a collective phrase and can be divided into different common categories and domains like information security. Most importantly cyber security has become essential for maintaining confidentiality and integrity. Vulnerabilities briefly defined are the backdoors, a weakness in the developed software or IT application that can be exploited to harm our software's integrity and disclose our confidentiality. Cybersecurity deals with these vulnerabilities as they seriously threaten critical information. As for how the security software resolves such issues is that they maintain a database, and a record of vulnerabilities and threats which were previously addressed and resolved, using them as a reference we scrutinize the software thoroughly to know if there are any known vulnerabilities or any other malware of similar origin.",,IEEE
cFEM: a cluster based feature extraction method for network intrusion detection,"Md. Mumtahin Habib Ullah Mazumder, Md. Eusha Kadir, ... Muhammad Mahbub Alam",International Journal of Information Security,2023-05-05,"<a href=""Springer (2023-05-05) : cFEM: a cluster based feature extraction method for network intrusion detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00694-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00694-y]</a>",The recent trend in network intrusion detection leverages key features of machine learning (ML) algorithms to detect network traffic anomalies....,,Springer
Backdoor Learning on Sequence to Sequence Models,"Lichang Chen, Minhao Cheng, Heng Huang","arXiv
arXiv","2023-05-03
2023-05","<a href=""arXiv (2023-05-03) : Backdoor Learning on Sequence to Sequence Models"" target=""_blank"">[http://arxiv.org/abs/2305.02424v1]</a>
<a href=""DBLP (2023-05) : Backdoor Learning on Sequence to Sequence Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.02424]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.02424]</a>","Backdoor learning has become an emerging research area towards building a trustworthy machine learning system. While a lot of works have studied the hidden danger of backdoor attacks in image or text classification, there is a limited understanding of the model's robustness on backdoor attacks when the output space is infinite and discrete. In this paper, we study a much more challenging problem of testing whether sequence-to-sequence (seq2seq) models are vulnerable to backdoor attacks. Specifically, we find by only injecting 0.2\% samples of the dataset, we can cause the seq2seq model to generate the designated keyword and even the whole sentence. Furthermore, we utilize Byte Pair Encoding (BPE) to create multiple new triggers, which brings new challenges to backdoor detection since these backdoors are not static. Extensive experiments on machine translation and text summarization have been conducted to show our proposed methods could achieve over 90\% attack success rate on multiple datasets and models.
","
","arXiv
DBLP"
DABS: Data-Agnostic Backdoor attack at the Server in Federated Learning,"Wenqiang Sun, Sen Li, Yuchang Sun, Jun Zhang","arXiv
arXiv","2023-05-02
2023-05","<a href=""arXiv (2023-05-02) : DABS: Data-Agnostic Backdoor attack at the Server in Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2305.01267v1]</a>
<a href=""DBLP (2023-05) : DABS: Data-Agnostic Backdoor attack at the Server in Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.01267]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.01267]</a>","Federated learning (FL) attempts to train a global model by aggregating local models from distributed devices under the coordination of a central server. However, the existence of a large number of heterogeneous devices makes FL vulnerable to various attacks, especially the stealthy backdoor attack. Backdoor attack aims to trick a neural network to misclassify data to a target label by injecting specific triggers while keeping correct predictions on original training data. Existing works focus on client-side attacks which try to poison the global model by modifying the local datasets. In this work, we propose a new attack model for FL, namely Data-Agnostic Backdoor attack at the Server (DABS), where the server directly modifies the global model to backdoor an FL system. Extensive simulation results show that this attack scheme achieves a higher attack success rate compared with baseline methods while maintaining normal accuracy on the clean data.
","
","arXiv
DBLP"
Going In Style: Audio Backdoors Through Stylistic Transformations,"Stefanos Koffas, Luca Pajola, Stjepan Picek, Mauro Conti","arXiv
arXiv","2023-05-02
2022-11","<a href=""arXiv (2023-05-02) : Going In Style: Audio Backdoors Through Stylistic Transformations"" target=""_blank"">[http://arxiv.org/abs/2211.03117v3]</a>
<a href=""DBLP (2022-11) : Going In Style: Audio Backdoors Through Stylistic Transformations"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.03117]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.03117]</a>","This work explores stylistic triggers for backdoor attacks in the audio domain: dynamic transformations of malicious samples through guitar effects. We first formalize stylistic triggers - currently missing in the literature. Second, we explore how to develop stylistic triggers in the audio domain by proposing JingleBack. Our experiments confirm the effectiveness of the attack, achieving a 96% attack success rate. Our code is available in https://github.com/skoffas/going-in-style.
","<a href=""arXiv"" target=""_blank"">[https://github.com/skoffas/going-in-style]</a>
","arXiv
DBLP"
DLP: towards active defense against backdoor attacks with decoupled learning process,"Zonghao Ying, Bin Wu","Cybersecurity
arXiv
Cybersecur.","2023-05-01
2024-06-18
2023","<a href=""Springer (2023-05-01) : DLP: towards active defense against backdoor attacks with decoupled learning process"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-023-00141-4]</a>
<a href=""arXiv (2024-06-18) : DLP: towards active defense against backdoor attacks with decoupled learning process"" target=""_blank"">[http://arxiv.org/abs/2406.13098v1]</a>
<a href=""DBLP (2023) : DLP: towards active defense against backdoor attacks with decoupled learning process"" target=""_blank"">[https://doi.org/10.1186/s42400-023-00141-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-023-00141-4]</a>
<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1186/s42400-023-00141-4]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1186/s42400-023-00141-4]</a>","Deep learning models are well known to be susceptible to backdoor attack, where the attacker only needs to provide a tampered dataset on which the...
Deep learning models are well known to be susceptible to backdoor attack, where the attacker only needs to provide a tampered dataset on which the triggers are injected. Models trained on the dataset will passively implant the backdoor, and triggers on the input can mislead the models during testing. Our study shows that the model shows different learning behaviors in clean and poisoned subsets during training. Based on this observation, we propose a general training pipeline to defend against backdoor attacks actively. Benign models can be trained from the unreliable dataset by decoupling the learning process into three stages, i.e., supervised learning, active unlearning, and active semi-supervised fine-tuning. The effectiveness of our approach has been shown in numerous experiments across various backdoor attacks and datasets.
","

","Springer
arXiv
DBLP"
Backdoor Pony: Evaluating backdoor attacks and defenses in different domains,Mercier A.,SoftwareX,2023-05-01,"<a href=""ScienceDirect (2023-05-01) : Backdoor Pony: Evaluating backdoor attacks and defenses in different domains"" target=""_blank"">[https://doi.org/10.1016/j.softx.2023.101387]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.softx.2023.101387]</a>",,,ScienceDirect
Survey on Backdoor Attacks and Countermeasures in Deep Neural Network,Qian H.,Journal of Frontiers of Computer Science and Technology,2023-05-01,"<a href=""ScienceDirect (2023-05-01) : Survey on Backdoor Attacks and Countermeasures in Deep Neural Network"" target=""_blank"">[https://doi.org/10.3778/j.issn.1673-9418.2210061]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3778/j.issn.1673-9418.2210061]</a>",,,ScienceDirect
Zero-Day Backdoor Attack against Text-to-Image Diffusion Models via Personalization,"Yihao Huang, Qing Guo, Felix Juefei-Xu",arXiv,2023-05,"<a href=""DBLP (2023-05) : Zero-Day Backdoor Attack against Text-to-Image Diffusion Models via Personalization"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.10701]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.10701]</a>",,,DBLP
ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger,"Jiazhao Li, Yijin Yang, Zhuofeng Wu, V. G. Vinod Vydiswaran, Chaowei Xiao","arXiv
arXiv","2023-04-27
2023-04","<a href=""arXiv (2023-04-27) : ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger"" target=""_blank"">[http://arxiv.org/abs/2304.14475v1]</a>
<a href=""DBLP (2023-04) : ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.14475]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.14475]</a>","Textual backdoor attacks pose a practical threat to existing systems, as they can compromise the model by inserting imperceptible triggers into inputs and manipulating labels in the training dataset. With cutting-edge generative models such as GPT-4 pushing rewriting to extraordinary levels, such attacks are becoming even harder to detect. We conduct a comprehensive investigation of the role of black-box generative models as a backdoor attack tool, highlighting the importance of researching relative defense strategies. In this paper, we reveal that the proposed generative model-based attack, BGMAttack, could effectively deceive textual classifiers. Compared with the traditional attack methods, BGMAttack makes the backdoor trigger less conspicuous by leveraging state-of-the-art generative models. Our extensive evaluation of attack effectiveness across five datasets, complemented by three distinct human cognition assessments, reveals that Figure 4 achieves comparable attack performance while maintaining superior stealthiness relative to baseline methods.
","
","arXiv
DBLP"
Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks,"Charles Jin, Melinda Sun, Martin Rinard","arXiv
ICLR","2023-04-27
2023","<a href=""arXiv (2023-04-27) : Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks"" target=""_blank"">[http://arxiv.org/abs/2105.03692v4]</a>
<a href=""DBLP (2023) : Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks"" target=""_blank"">[https://openreview.net/pdf?id=mkJm5Uy4HrQ]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/pdf?id=mkJm5Uy4HrQ]</a>","We propose a novel clustering mechanism based on an incompatibility property between subsets of data that emerges during model training. This mechanism partitions the dataset into subsets that generalize only to themselves, i.e., training on one subset does not improve performance on the other subsets. Leveraging the interaction between the dataset and the training process, our clustering mechanism partitions datasets into clusters that are defined by--and therefore meaningful to--the objective of the training process. We apply our clustering mechanism to defend against data poisoning attacks, in which the attacker injects malicious poisoned data into the training dataset to affect the trained model's output. Our evaluation focuses on backdoor attacks against deep neural networks trained to perform image classification using the GTSRB and CIFAR-10 datasets. Our results show that (1) these attacks produce poisoned datasets in which the poisoned and clean data are incompatible and (2) our technique successfully identifies (and removes) the poisoned data. In an end-to-end evaluation, our defense reduces the attack success rate to below 1% on 134 out of 165 scenarios, with only a 2% drop in clean accuracy on CIFAR-10 and a negligible drop in clean accuracy on GTSRB.
","
","arXiv
DBLP"
From DevOps to DevSecOps is not enough. CyberDevOps: an extreme shifting-left architecture to bring cybersecurity within software security lifecycle pipeline,"Federico Lombardi, Alberto Fanton",Software Quality Journal,2023-04-26,"<a href=""Springer (2023-04-26) : From DevOps to DevSecOps is not enough. CyberDevOps: an extreme shifting-left architecture to bring cybersecurity within software security lifecycle pipeline"" target=""_blank"">[https://link.springer.com/article/10.1007/s11219-023-09619-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11219-023-09619-3]</a>",Software engineering is evolving quickly leading to an urgency to discover more efficient development models. DevOps and its security-oriented...,,Springer
Pied-Piper: Revealing the Backdoor Threats in Ethereum ERC Token Contracts,Ma F.,ACM Transactions on Software Engineering and Methodology,2023-04-26,"<a href=""ScienceDirect (2023-04-26) : Pied-Piper: Revealing the Backdoor Threats in Ethereum ERC Token Contracts"" target=""_blank"">[https://doi.org/10.1145/3560264]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3560264]</a>",,,ScienceDirect
CLB-Defense: based on contrastive learning defense for graph neural network against backdoor attack,Chen J.,Tongxin Xuebao/Journal on Communications,2023-04-25,"<a href=""ScienceDirect (2023-04-25) : CLB-Defense: based on contrastive learning defense for graph neural network against backdoor attack"" target=""_blank"">[https://doi.org/10.11959/j.issn.1000-436x.2023074]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.11959/j.issn.1000-436x.2023074]</a>",,,ScienceDirect
Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization,"Mingli Zhu, Shaokui Wei, Li Shen, Yanbo Fan, Baoyuan Wu","arXiv
arXiv","2023-04-24
2023-04","<a href=""arXiv (2023-04-24) : Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization"" target=""_blank"">[http://arxiv.org/abs/2304.11823v1]</a>
<a href=""DBLP (2023-04) : Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.11823]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.11823]</a>","Backdoor defense, which aims to detect or mitigate the effect of malicious triggers introduced by attackers, is becoming increasingly critical for machine learning security and integrity. Fine-tuning based on benign data is a natural defense to erase the backdoor effect in a backdoored model. However, recent studies show that, given limited benign data, vanilla fine-tuning has poor defense performance. In this work, we provide a deep study of fine-tuning the backdoored model from the neuron perspective and find that backdoorrelated neurons fail to escape the local minimum in the fine-tuning process. Inspired by observing that the backdoorrelated neurons often have larger norms, we propose FTSAM, a novel backdoor defense paradigm that aims to shrink the norms of backdoor-related neurons by incorporating sharpness-aware minimization with fine-tuning. We demonstrate the effectiveness of our method on several benchmark datasets and network architectures, where it achieves state-of-the-art defense performance. Overall, our work provides a promising avenue for improving the robustness of machine learning models against backdoor attacks.
","
","arXiv
DBLP"
A Comprehensive Taxonomy of Visual Printed Circuit Board Defects,"David Selasi Koblah, Olivia P. Dizon-Paradis, ... Domenic Forte",Journal of Hardware and Systems Security,2023-04-24,"<a href=""Springer (2023-04-24) : A Comprehensive Taxonomy of Visual Printed Circuit Board Defects"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-023-00132-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-023-00132-4]</a>",The globalization of printed circuit board (PCB) production has expanded the avenues to introduce vulnerabilities into the electronics supply chain....,,Springer
Universal Adversarial Backdoor Attacks to Fool Vertical Federated Learning in Cloud-Edge Collaboration,"Peng Chen, Xin Du, Zhihui Lu, Hongfeng Chai","arXiv
arXiv","2023-04-22
2023-04","<a href=""arXiv (2023-04-22) : Universal Adversarial Backdoor Attacks to Fool Vertical Federated Learning in Cloud-Edge Collaboration"" target=""_blank"">[http://arxiv.org/abs/2304.11432v1]</a>
<a href=""DBLP (2023-04) : Universal Adversarial Backdoor Attacks to Fool Vertical Federated Learning in Cloud-Edge Collaboration"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.11432]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.11432]</a>","Vertical federated learning (VFL) is a cloud-edge collaboration paradigm that enables edge nodes, comprising resource-constrained Internet of Things (IoT) devices, to cooperatively train artificial intelligence (AI) models while retaining their data locally. This paradigm facilitates improved privacy and security for edges and IoT devices, making VFL an essential component of Artificial Intelligence of Things (AIoT) systems. Nevertheless, the partitioned structure of VFL can be exploited by adversaries to inject a backdoor, enabling them to manipulate the VFL predictions. In this paper, we aim to investigate the vulnerability of VFL in the context of binary classification tasks. To this end, we define a threat model for backdoor attacks in VFL and introduce a universal adversarial backdoor (UAB) attack to poison the predictions of VFL. The UAB attack, consisting of universal trigger generation and clean-label backdoor injection, is incorporated during the VFL training at specific iterations. This is achieved by alternately optimizing the universal trigger and model parameters of VFL sub-problems. Our work distinguishes itself from existing studies on designing backdoor attacks for VFL, as those require the knowledge of auxiliary information not accessible within the split VFL architecture. In contrast, our approach does not necessitate any additional data to execute the attack. On the LendingClub and Zhongyuan datasets, our approach surpasses existing state-of-the-art methods, achieving up to 100\% backdoor task performance while maintaining the main task performance. Our results in this paper make a major advance to revealing the hidden backdoor risks of VFL, hence paving the way for the future development of secure AIoT.
","
","arXiv
DBLP"
SoK: A Systematic Evaluation of Backdoor Trigger Characteristics in Image Classification,"Gorka Abad, Jing Xu, Stefanos Koffas, Behrad Tajalli, Stjepan Picek, Mauro Conti",arXiv,2023-04-21,"<a href=""arXiv (2023-04-21) : SoK: A Systematic Evaluation of Backdoor Trigger Characteristics in Image Classification"" target=""_blank"">[http://arxiv.org/abs/2302.01740v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Deep learning achieves outstanding results in many machine learning tasks. Nevertheless, it is vulnerable to backdoor attacks that modify the training set to embed a secret functionality in the trained model. The modified training samples have a secret property, i. e., a trigger. At inference time, the secret functionality is activated when the input contains the trigger, while the model functions correctly in other cases. While there are many known backdoor attacks (and defenses), deploying a stealthy attack is still far from trivial. Successfully creating backdoor triggers depends on numerous parameters. Unfortunately, research has not yet determined which parameters contribute most to the attack performance. This paper systematically analyzes the most relevant parameters for the backdoor attacks, i.e., trigger size, position, color, and poisoning rate. Using transfer learning, which is very common in computer vision, we evaluate the attack on state-of-the-art models (ResNet, VGG, AlexNet, and GoogLeNet) and datasets (MNIST, CIFAR10, and TinyImageNet). Our attacks cover the majority of backdoor settings in research, providing concrete directions for future works. Our code is publicly available to facilitate the reproducibility of our results.",,arXiv
Get Rid Of Your Trail: Remotely Erasing Backdoors in Federated Learning,"Manaar Alam, Hithem Lamri, Michail Maniatakos","arXiv
arXiv","2023-04-20
2023-04","<a href=""arXiv (2023-04-20) : Get Rid Of Your Trail: Remotely Erasing Backdoors in Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2304.10638v1]</a>
<a href=""DBLP (2023-04) : Get Rid Of Your Trail: Remotely Erasing Backdoors in Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.10638]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.10638]</a>","Federated Learning (FL) enables collaborative deep learning training across multiple participants without exposing sensitive personal data. However, the distributed nature of FL and the unvetted participants' data makes it vulnerable to backdoor attacks. In these attacks, adversaries inject malicious functionality into the centralized model during training, leading to intentional misclassifications for specific adversary-chosen inputs. While previous research has demonstrated successful injections of persistent backdoors in FL, the persistence also poses a challenge, as their existence in the centralized model can prompt the central aggregation server to take preventive measures to penalize the adversaries. Therefore, this paper proposes a methodology that enables adversaries to effectively remove backdoors from the centralized model upon achieving their objectives or upon suspicion of possible detection. The proposed approach extends the concept of machine unlearning and presents strategies to preserve the performance of the centralized model and simultaneously prevent over-unlearning of information unrelated to backdoor patterns, making the adversaries stealthy while removing backdoors. To the best of our knowledge, this is the first work that explores machine unlearning in FL to remove backdoors to the benefit of adversaries. Exhaustive evaluation considering image classification scenarios demonstrates the efficacy of the proposed method in efficient backdoor removal from the centralized model, injected by state-of-the-art attacks across multiple configurations.
","
","arXiv
DBLP"
An Improved Key Management System - DES Ultimate v1.1,A. K. Sharma S. Wadhawan Dalip G. Habib,2023 International Conference on Innovative Data Communication Technologies and Application (ICIDCA),2023-04-20,"<a href=""IEEE (2023-04-20) : An Improved Key Management System - DES Ultimate v1.1"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10099980]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICIDCA56705.2023.10099980]</a>","Significance of cryptographic encryption-decryption algorithms is well understood i.e. paramount objective is not only limited to build the secure system but also the efficient one too. The iterative structures behind various block-ciphers are very much popular and used in various software/hardware implementations to preserve the confidentiality, integrity of data, due to its ruthless nature against linear and differential cryptanalysis. Even though these popular structures are strong enough to shield against attacks, but still have certain vulnerabilities viz. week/single-key to be used, lack of randomization, identical nature of key management systems etc., which somehow provide backdoors to adversaries. This research work intends to address above mentioned flaws specifically by introducing contemporary key-management system to deduce a set of secret keys that has to be used during encryption-decryption process. The proposed enhanced version of DES not only makes it difficult for adversaries to succeed in attacks but also focuses on the efficiency of the implementation. The 16*16 s-box is used in proposed construction instead of 8/32 s-box due to the vulnerabilities in these implementations.",,IEEE
A Review on Machine Unlearning,"Haibo Zhang, Toru Nakamura, ... Kouichi Sakurai",SN Computer Science,2023-04-19,"<a href=""Springer (2023-04-19) : A Review on Machine Unlearning"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-023-01767-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-023-01767-4]</a>","Recently, an increasing number of laws have governed the useability of users’ privacy. For example, Article 17 of the General Data Protection...",,Springer
Evil from Within: Machine Learning Backdoors through Hardware Trojans,"Alexander Warnecke, Julian Speith, Jan-Niklas Möller, Konrad Rieck, Christof Paar","arXiv
arXiv","2023-04-18
2023-04","<a href=""arXiv (2023-04-18) : Evil from Within: Machine Learning Backdoors through Hardware Trojans"" target=""_blank"">[http://arxiv.org/abs/2304.08411v2]</a>
<a href=""DBLP (2023-04) : Evil from Within: Machine Learning Backdoors through Hardware Trojans"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.08411]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.08411]</a>","Backdoors pose a serious threat to machine learning, as they can compromise the integrity of security-critical systems, such as self-driving cars. While different defenses have been proposed to address this threat, they all rely on the assumption that the hardware on which the learning models are executed during inference is trusted. In this paper, we challenge this assumption and introduce a backdoor attack that completely resides within a common hardware accelerator for machine learning. Outside of the accelerator, neither the learning model nor the software is manipulated, so that current defenses fail. To make this attack practical, we overcome two challenges: First, as memory on a hardware accelerator is severely limited, we introduce the concept of a minimal backdoor that deviates as little as possible from the original model and is activated by replacing a few model parameters only. Second, we develop a configurable hardware trojan that can be provisioned with the backdoor and performs a replacement only when the specific target model is processed. We demonstrate the practical feasibility of our attack by implanting our hardware trojan into the Xilinx Vitis AI DPU, a commercial machine-learning accelerator. We configure the trojan with a minimal backdoor for a traffic-sign recognition system. The backdoor replaces only 30 (0.069%) model parameters, yet it reliably manipulates the recognition once the input contains a backdoor trigger. Our attack expands the hardware circuit of the accelerator by 0.24% and induces no run-time overhead, rendering a detection hardly possible. Given the complex and highly distributed manufacturing process of current hardware, our work points to a new threat in machine learning that is inaccessible to current security mechanisms and calls for hardware to be manufactured only in fully trusted environments.
","
","arXiv
DBLP"
Towards real-time ML-based DDoS detection via cost-efficient window-based feature extraction,"Haibin Li, Yi Zhao, ... Qi Li",Science China Information Sciences,2023-04-17,"<a href=""Springer (2023-04-17) : Towards real-time ML-based DDoS detection via cost-efficient window-based feature extraction"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-021-3545-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-021-3545-0]</a>","Distributed denial of service (DDoS) detection is still an open and challenging problem. In particular, sophisticated attacks, e.g., attacks that...",,Springer
A novel hybrid hunger games algorithm for intrusion detection systems based on nonlinear regression modeling,"Shahriar Mohammadi, Mehdi Babagoli",International Journal of Information Security,2023-04-11,"<a href=""Springer (2023-04-11) : A novel hybrid hunger games algorithm for intrusion detection systems based on nonlinear regression modeling"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00684-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00684-0]</a>","Along with the advancement of online platforms and significant growth in Internet usage, various threats and cyber-attacks have been emerging and...",,Springer
Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning,"Hao Chen, Chen Gong, Yizhe Wang, Xinwen Hou","arXiv
arXiv","2023-04-10
2023-04","<a href=""arXiv (2023-04-10) : Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning"" target=""_blank"">[http://arxiv.org/abs/2304.00252v3]</a>
<a href=""DBLP (2023-04) : Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.00252]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.00252]</a>","A backdoor attack allows a malicious user to manipulate the environment or corrupt the training data, thus inserting a backdoor into the trained agent. Such attacks compromise the RL system's reliability, leading to potentially catastrophic results in various key fields. In contrast, relatively limited research has investigated effective defenses against backdoor attacks in RL. This paper proposes the Recovery Triggered States (RTS) method, a novel approach that effectively protects the victim agents from backdoor attacks. RTS involves building a surrogate network to approximate the dynamics model. Developers can then recover the environment from the triggered state to a clean state, thereby preventing attackers from activating backdoors hidden in the agent by presenting the trigger. When training the surrogate to predict states, we incorporate agent action information to reduce the discrepancy between the actions taken by the agent on predicted states and the actions taken on real states. RTS is the first approach to defend against backdoor attacks in a single-agent setting. Our results show that using RTS, the cumulative reward only decreased by 1.41% under the backdoor attack.
","
","arXiv
DBLP"
An Adversarial Attack on Salient Regions of Traffic Sign,"Jun Yan, Huilin Yin, ... Gerhard Rigoll",Automotive Innovation,2023-04-10,"<a href=""Springer (2023-04-10) : An Adversarial Attack on Salient Regions of Traffic Sign"" target=""_blank"">[https://link.springer.com/article/10.1007/s42154-023-00220-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42154-023-00220-9]</a>",The state-of-the-art deep neural networks are vulnerable to the attacks of adversarial examples with small-magnitude perturbations. In the field of...,,Springer
VQR: vulnerability analysis in quadratic residues-based authentication protocols,"Meysam Ghahramani, Hamed HaddadPajouh, ... Saru Kumari",Journal of Ambient Intelligence and Humanized Computing,2023-04-10,"<a href=""Springer (2023-04-10) : VQR: vulnerability analysis in quadratic residues-based authentication protocols"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-023-04557-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-023-04557-1]</a>","Ensuring security and respect for users’ privacy, especially in electronic health-care systems, is an important task that is achievable by...",,Springer
UNICORN: A Unified Backdoor Trigger Inversion Framework,"Zhenting Wang, Kai Mei, Juan Zhai, Shiqing Ma","arXiv
ICLR
arXiv","2023-04-05
2023
2023-04","<a href=""arXiv (2023-04-05) : UNICORN: A Unified Backdoor Trigger Inversion Framework"" target=""_blank"">[http://arxiv.org/abs/2304.02786v1]</a>
<a href=""DBLP (2023) : UNICORN: A Unified Backdoor Trigger Inversion Framework"" target=""_blank"">[https://openreview.net/pdf?id=Mj7K4lglGyj]</a>
<a href=""DBLP (2023-04) : UNICORN: A Unified Backdoor Trigger Inversion Framework"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.02786]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/pdf?id=Mj7K4lglGyj]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.02786]</a>","The backdoor attack, where the adversary uses inputs stamped with triggers (e.g., a patch) to activate pre-planted malicious behaviors, is a severe threat to Deep Neural Network (DNN) models. Trigger inversion is an effective way of identifying backdoor models and understanding embedded adversarial behaviors. A challenge of trigger inversion is that there are many ways of constructing the trigger. Existing methods cannot generalize to various types of triggers by making certain assumptions or attack-specific constraints. The fundamental reason is that existing work does not consider the trigger's design space in their formulation of the inversion problem. This work formally defines and analyzes the triggers injected in different spaces and the inversion problem. Then, it proposes a unified framework to invert backdoor triggers based on the formalization of triggers and the identified inner behaviors of backdoor models from our analysis. Our prototype UNICORN is general and effective in inverting backdoor triggers in DNNs. The code can be found at https://github.com/RU-System-Software-and-Security/UNICORN.

","<a href=""arXiv"" target=""_blank"">[https://github.com/RU-System-Software-and-Security/UNICORN]</a>

","arXiv
DBLP
DBLP"
Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection,"Yiming Li, Yang Bai, Yong Jiang, Yong Yang, Shu-Tao Xia, Bo Li","arXiv
NeurIPS
arXiv","2023-04-05
2022
2022-10","<a href=""arXiv (2023-04-05) : Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection"" target=""_blank"">[http://arxiv.org/abs/2210.00875v3]</a>
<a href=""DBLP (2022) : Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/55bfedfd31489e5ae83c9ce8eec7b0e1-Abstract-Conference.html]</a>
<a href=""DBLP (2022-10) : Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.00875]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/55bfedfd31489e5ae83c9ce8eec7b0e1-Abstract-Conference.html]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.00875]</a>","Deep neural networks (DNNs) have demonstrated their superiority in practice. Arguably, the rapid development of DNNs is largely benefited from high-quality (open-sourced) datasets, based on which researchers and developers can easily evaluate and improve their learning methods. Since the data collection is usually time-consuming or even expensive, how to protect their copyrights is of great significance and worth further exploration. In this paper, we revisit dataset ownership verification. We find that existing verification methods introduced new security risks in DNNs trained on the protected dataset, due to the targeted nature of poison-only backdoor watermarks. To alleviate this problem, in this work, we explore the untargeted backdoor watermarking scheme, where the abnormal model behaviors are not deterministic. Specifically, we introduce two dispersibilities and prove their correlation, based on which we design the untargeted backdoor watermark under both poisoned-label and clean-label settings. We also discuss how to use the proposed untargeted backdoor watermark for dataset ownership verification. Experiments on benchmark datasets verify the effectiveness of our methods and their resistance to existing backdoor defenses. Our codes are available at \url{https://github.com/THUYimingLi/Untargeted_Backdoor_Watermark}.

","<a href=""arXiv"" target=""_blank"">[https://github.com/THUYimingLi/Untargeted_Backdoor_Watermark}]</a>

","arXiv
DBLP
DBLP"
A Defense Method against Backdoor Attacks in Neural Networks Using an Image Repair Technique,J. Chen H. Lu W. Huo S. Zhang Y. Chen Y. Yao,2022 12th International Conference on Information Technology in Medicine and Education (ITME),2023-04-04,"<a href=""IEEE (2023-04-04) : A Defense Method against Backdoor Attacks in Neural Networks Using an Image Repair Technique"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10086247]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ITME56794.2022.00087]</a>","With the rapid development of deep learning research and applications, the problem of artificial intelligence security has become increasingly prominent, such as adversarial examples, universal adversarial patch, and data poisoning, especially for the backdoor attack, which is a new type of covert attack, leading to the vulnerability and non-robustness of deep learning models. In a backdoor attack, the attacker will conduct a malicious attack by inserting some poisoned samples into training dataset. Poisoned samples add triggers and modify the labels to the target labels to participate in the training. Infected model has the same accuracy as the clean model in the normal test set, but when confronted with poisoned samples, the triggers will be activated to make the infected model predict the target label. To solve this problem, model parameters adjustment and poisoned data removal methods are widely used. However, they lack real-time performance and accuracy is insufficient. In this paper, we propose a new backdoor attack defense method, in which trigger reverse engineering is used to obtain the right triggers and image repair techniques to make sure that the input model data can be real-time processed without any negative impacts on clean samples.",,IEEE
A novel ensemble learning-based model for network intrusion detection,"Ngamba Thockchom, Moirangthem Marjit Singh, Utpal Nandi",Complex & Intelligent Systems,2023-04-03,"<a href=""Springer (2023-04-03) : A novel ensemble learning-based model for network intrusion detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s40747-023-01013-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s40747-023-01013-7]</a>","The growth of Internet and the services provided by it has been growing exponentially in the past few decades. With such growth, there is also an...",,Springer
ArgusDroid: detecting Android malware variants by mining permission-API knowledge graph,"Yude Bai, Sen Chen, ... Xiaohong Li",Science China Information Sciences,2023-04-03,"<a href=""Springer (2023-04-03) : ArgusDroid: detecting Android malware variants by mining permission-API knowledge graph"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-021-3414-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-021-3414-7]</a>","Malware family variants make minor and relevant changes of behaviors based on the original malware. To analyze and detect family variants, security...",,Springer
Automated Segmentation to Make Hidden Trigger Backdoor Attacks Robust against Deep Neural Networks,Ali S.,Applied Sciences (Switzerland),2023-04-01,"<a href=""ScienceDirect (2023-04-01) : Automated Segmentation to Make Hidden Trigger Backdoor Attacks Robust against Deep Neural Networks"" target=""_blank"">[https://doi.org/10.3390/app13074599]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/app13074599]</a>",,,ScienceDirect
Towards Backdoor Attacks and Defense in Robust Machine Learning Models,Soremekun E.,Computers and Security,2023-04-01,"<a href=""ScienceDirect (2023-04-01) : Towards Backdoor Attacks and Defense in Robust Machine Learning Models"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103101]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103101]</a>",,,ScienceDirect
Unnoticeable Backdoor Attacks on Graph Neural Networks,"Enyan Dai, Minhua Lin, Xiang Zhang, Suhang Wang","WWW '23: Proceedings of the ACM Web Conference 2023
arXiv
WWW
arXiv","2023-04
2023-02-11
2023
2023-03","<a href=""ACM (2023-04) : Unnoticeable Backdoor Attacks on Graph Neural Networks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3543507.3583392]</a>
<a href=""arXiv (2023-02-11) : Unnoticeable Backdoor Attacks on Graph Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2303.01263v1]</a>
<a href=""DBLP (2023) : Unnoticeable Backdoor Attacks on Graph Neural Networks"" target=""_blank"">[https://doi.org/10.1145/3543507.3583392]</a>
<a href=""DBLP (2023-03) : Unnoticeable Backdoor Attacks on Graph Neural Networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.01263]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3543507.3583392]</a>
<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1145/3543507.3583392]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3543507.3583392]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.01263]</a>","Graph Neural Networks (GNNs) have achieved promising results in various tasks such as node classification and graph classification. Recent studies find that GNNs are vulnerable to adversarial attacks. However, effective backdoor attacks on graphs are ...
Graph Neural Networks (GNNs) have achieved promising results in various tasks such as node classification and graph classification. Recent studies find that GNNs are vulnerable to adversarial attacks. However, effective backdoor attacks on graphs are still an open problem. In particular, backdoor attack poisons the graph by attaching triggers and the target class label to a set of nodes in the training graph. The backdoored GNNs trained on the poisoned graph will then be misled to predict test nodes to target class once attached with triggers. Though there are some initial efforts in graph backdoor attacks, our empirical analysis shows that they may require a large attack budget for effective backdoor attacks and the injected triggers can be easily detected and pruned. Therefore, in this paper, we study a novel problem of unnoticeable graph backdoor attacks with limited attack budget. To fully utilize the attack budget, we propose to deliberately select the nodes to inject triggers and target class labels in the poisoning phase. An adaptive trigger generator is deployed to obtain effective triggers that are difficult to be noticed. Extensive experiments on real-world datasets against various defense strategies demonstrate the effectiveness of our proposed method in conducting effective unnoticeable backdoor attacks.

","


","ACM
arXiv
DBLP
DBLP"
Training-free Lexical Backdoor Attacks on Language Models,"Yujin Huang, Terry Yue Zhuo, Qiongkai Xu, Han Hu, Xingliang Yuan, Chunyang Chen","WWW '23: Proceedings of the ACM Web Conference 2023
arXiv
WWW
arXiv","2023-04
2023-02-08
2023
2023-02","<a href=""ACM (2023-04) : Training-free Lexical Backdoor Attacks on Language Models"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3543507.3583348]</a>
<a href=""arXiv (2023-02-08) : Training-free Lexical Backdoor Attacks on Language Models"" target=""_blank"">[http://arxiv.org/abs/2302.04116v1]</a>
<a href=""DBLP (2023) : Training-free Lexical Backdoor Attacks on Language Models"" target=""_blank"">[https://doi.org/10.1145/3543507.3583348]</a>
<a href=""DBLP (2023-02) : Training-free Lexical Backdoor Attacks on Language Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.04116]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3543507.3583348]</a>
<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1145/3543507.3583348]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3543507.3583348]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.04116]</a>","Large-scale language models have achieved tremendous success across various natural language processing (NLP) applications. Nevertheless, language models are vulnerable to backdoor attacks, which inject stealthy triggers into models for steering them to ...
Large-scale language models have achieved tremendous success across various natural language processing (NLP) applications. Nevertheless, language models are vulnerable to backdoor attacks, which inject stealthy triggers into models for steering them to undesirable behaviors. Most existing backdoor attacks, such as data poisoning, require further (re)training or fine-tuning language models to learn the intended backdoor patterns. The additional training process however diminishes the stealthiness of the attacks, as training a language model usually requires long optimization time, a massive amount of data, and considerable modifications to the model parameters. In this work, we propose Training-Free Lexical Backdoor Attack (TFLexAttack) as the first training-free backdoor attack on language models. Our attack is achieved by injecting lexical triggers into the tokenizer of a language model via manipulating its embedding dictionary using carefully designed rules. These rules are explainable to human developers which inspires attacks from a wider range of hackers. The sparse manipulation of the dictionary also habilitates the stealthiness of our attack. We conduct extensive experiments on three dominant NLP tasks based on nine language models to demonstrate the effectiveness and universality of our attack. The code of this work is available at https://github.com/Jinxhy/TFLexAttack.

","
<a href=""arXiv"" target=""_blank"">[https://github.com/Jinxhy/TFLexAttack]</a>

","ACM
arXiv
DBLP
DBLP"
Detecting Backdoors in Collaboration Graphs of Software Repositories,"Tom Ganz, Inaam Ashraf, Martin Härterich, Konrad Rieck","CODASPY '23: Proceedings of the Thirteenth ACM Conference on Data and Application Security and Privacy
CODASPY","2023-04
2023","<a href=""ACM (2023-04) : Detecting Backdoors in Collaboration Graphs of Software Repositories"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3577923.3583657]</a>
<a href=""DBLP (2023) : Detecting Backdoors in Collaboration Graphs of Software Repositories"" target=""_blank"">[https://doi.org/10.1145/3577923.3583657]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3577923.3583657]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3577923.3583657]</a>","Software backdoors pose a major threat to the security of computer systems. Minor modifications to a program are often sufficient to undermine security mechanisms and enable unauthorized access to a system. The direct approach of detecting backdoors ...
","
","ACM
DBLP"
Launching a Robust Backdoor Attack under Capability Constrained Scenarios,"Ming Yi, Yixiao Xu, Kangyi Ding, Mingyong Yin, Xiaolei Liu",arXiv,2023-04,"<a href=""DBLP (2023-04) : Launching a Robust Backdoor Attack under Capability Constrained Scenarios"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.10985]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.10985]</a>",,,DBLP
Tutorial: Toward Robust Deep Learning against Poisoning Attacks,"Huili Chen, Farinaz Koushanfar","ACM Transactions on Embedded Computing Systems (TECS), Volume 22, Issue 3",2023-04,"<a href=""ACM (2023-04) : Tutorial: Toward Robust Deep Learning against Poisoning Attacks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3574159]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3574159]</a>","Deep Learning (DL) has been increasingly deployed in various real-world applications due to its unprecedented performance and automated capability of learning hidden representations. While DL can achieve high task performance, the training process of a DL ...",,ACM
Black-box Dataset Ownership Verification via Backdoor Watermarking,"Yiming Li, Mingyan Zhu, Xue Yang, Yong Jiang, Tao Wei, Shu-Tao Xia",arXiv,2023-03-31,"<a href=""arXiv (2023-03-31) : Black-box Dataset Ownership Verification via Backdoor Watermarking"" target=""_blank"">[http://arxiv.org/abs/2209.06015v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Deep learning, especially deep neural networks (DNNs), has been widely and successfully adopted in many critical applications for its high effectiveness and efficiency. The rapid development of DNNs has benefited from the existence of some high-quality datasets ($e.g.$, ImageNet), which allow researchers and developers to easily verify the performance of their methods. Currently, almost all existing released datasets require that they can only be adopted for academic or educational purposes rather than commercial purposes without permission. However, there is still no good way to ensure that. In this paper, we formulate the protection of released datasets as verifying whether they are adopted for training a (suspicious) third-party model, where defenders can only query the model while having no information about its parameters and training details. Based on this formulation, we propose to embed external patterns via backdoor watermarking for the ownership verification to protect them. Our method contains two main parts, including dataset watermarking and dataset verification. Specifically, we exploit poison-only backdoor attacks ($e.g.$, BadNets) for dataset watermarking and design a hypothesis-test-guided method for dataset verification. We also provide some theoretical analyses of our methods. Experiments on multiple benchmark datasets of different tasks are conducted, which verify the effectiveness of our method. The code for reproducing main experiments is available at \url{https://github.com/THUYimingLi/DVBW}.","<a href=""arXiv"" target=""_blank"">[https://github.com/THUYimingLi/DVBW}]</a>",arXiv
"Editorial for Special Issue on Large-scale Pre-training: Data, Models, and Fine-tuning","Ji-Rong Wen, Zi Huang, Hanwang Zhang",Machine Intelligence Research,2023-03-31,"<a href=""Springer (2023-03-31) : Editorial for Special Issue on Large-scale Pre-training: Data, Models, and Fine-tuning"" target=""_blank"">[https://link.springer.com/article/10.1007/s11633-023-1431-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11633-023-1431-y]</a>",,,Springer
Segmented Backdoor Defense Based on Local Gradient and Global Gradient Ascent,Xiao X.,Yingyong Kexue Xuebao/Journal of Applied Sciences,2023-03-31,"<a href=""ScienceDirect (2023-03-31) : Segmented Backdoor Defense Based on Local Gradient and Global Gradient Ascent"" target=""_blank"">[https://doi.org/10.3969/j.issn.0255-8297.2023.02.003]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3969/j.issn.0255-8297.2023.02.003]</a>",,,ScienceDirect
On the Security of Smart Home Systems: A Survey,"Bin Yuan, Jun Wan, ... Hai Jin",Journal of Computer Science and Technology,2023-03-30,"<a href=""Springer (2023-03-30) : On the Security of Smart Home Systems: A Survey"" target=""_blank"">[https://link.springer.com/article/10.1007/s11390-023-2488-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11390-023-2488-3]</a>","Among the plethora of IoT (Internet of Things) applications, the smart home is one of the fastest-growing. However, the rapid development of the...",,Springer
Shielding Federated Learning: Mitigating Byzantine Attacks with Less Constraints,M. Li W. Wan J. Lu S. Hu J. Shi L. Y. Zhang M. Zhou Y. Zheng,"2022 18th International Conference on Mobility, Sensing and Networking (MSN)",2023-03-29,"<a href=""IEEE (2023-03-29) : Shielding Federated Learning: Mitigating Byzantine Attacks with Less Constraints"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10076545]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/MSN57253.2022.00040]</a>","Federated learning is a newly emerging distributed learning framework that facilitates the collaborative training of a shared global model among distributed participants with their privacy preserved. However, federated learning systems are vulnerable to Byzantine attacks from malicious participants, who can upload carefully crafted local model updates to degrade the quality of the global model and even leave a backdoor. While this problem has received significant attention recently, current defensive schemes heavily rely on various assumptions, such as a fixed Byzantine model, availability of participants' local data, minority attackers, IID data distribution, etc. To relax those constraints, this paper presents Robust-FL, the first prediction-based Byzantine-robust federated learning scheme where none of the assumptions is leveraged. The core idea of the Robust-FL is exploiting historical global model to construct an estimator based on which the local models will be filtered through similarity detection. We then cluster local models to adaptively adjust the acceptable differences between the local models and the estimator such that Byzantine users can be identified. Extensive experiments over different datasets show that our approach achieves the following advantages simultaneously: (i) independence of participants' local data, (ii) tolerance of majority attackers, (iii) generalization to variable Byzantine model.",,IEEE
Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency,"Xiaogeng Liu, Minghui Li, Haoyu Wang, Shengshan Hu, Dengpan Ye, Hai Jin, Libing Wu, Chaowei Xiao","arXiv
CVPR
arXiv","2023-03-27
2023
2023-03","<a href=""arXiv (2023-03-27) : Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency"" target=""_blank"">[http://arxiv.org/abs/2303.18191v1]</a>
<a href=""DBLP (2023) : Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.01570]</a>
<a href=""DBLP (2023-03) : Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.18191]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.01570]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.18191]</a>","Deep neural networks are proven to be vulnerable to backdoor attacks. Detecting the trigger samples during the inference stage, i.e., the test-time trigger sample detection, can prevent the backdoor from being triggered. However, existing detection methods often require the defenders to have high accessibility to victim models, extra clean data, or knowledge about the appearance of backdoor triggers, limiting their practicality. In this paper, we propose the test-time corruption robustness consistency evaluation (TeCo), a novel test-time trigger sample detection method that only needs the hard-label outputs of the victim models without any extra information. Our journey begins with the intriguing observation that the backdoor-infected models have similar performance across different image corruptions for the clean images, but perform discrepantly for the trigger samples. Based on this phenomenon, we design TeCo to evaluate test-time robustness consistency by calculating the deviation of severity that leads to predictions' transition across different corruptions. Extensive experiments demonstrate that compared with state-of-the-art defenses, which even require either certain information about the trigger types or accessibility of clean data, TeCo outperforms them on different backdoor attacks, datasets, and model architectures, enjoying a higher AUROC by 10% and 5 times of stability.

","

","arXiv
DBLP
DBLP"
Physical Backdoor Trigger Activation of Autonomous Vehicle using Reachability Analysis,"Wenqing Li, Yue Wang, Muhammad Shafique, Saif Eddin Jabari","arXiv
CDC
arXiv","2023-03-27
2023
2023-03","<a href=""arXiv (2023-03-27) : Physical Backdoor Trigger Activation of Autonomous Vehicle using Reachability Analysis"" target=""_blank"">[http://arxiv.org/abs/2303.13992v2]</a>
<a href=""DBLP (2023) : Physical Backdoor Trigger Activation of Autonomous Vehicle Using Reachability Analysis"" target=""_blank"">[https://doi.org/10.1109/CDC49753.2023.10383622]</a>
<a href=""DBLP (2023-03) : Physical Backdoor Trigger Activation of Autonomous Vehicle using Reachability Analysis"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.13992]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CDC49753.2023.10383622]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.13992]</a>","Recent studies reveal that Autonomous Vehicles (AVs) can be manipulated by hidden backdoors, causing them to perform harmful actions when activated by physical triggers. However, it is still unclear how these triggers can be activated while adhering to traffic principles. Understanding this vulnerability in a dynamic traffic environment is crucial. This work addresses this gap by presenting physical trigger activation as a reachability problem of controlled dynamic system. Our technique identifies security-critical areas in traffic systems where trigger conditions for accidents can be reached, and provides intended trajectories for how those conditions can be reached. Testing on typical traffic scenarios showed the system can be successfully driven to trigger conditions with near 100% activation rate. Our method benefits from identifying AV vulnerability and enabling effective safety strategies.

","

","arXiv
DBLP
DBLP"
Backdoor Attacks with Input-unique Triggers in NLP,"Xukun Zhou, Jiwei Li, Tianwei Zhang, Lingjuan Lyu, Muqiao Yang, Jun He","arXiv
arXiv","2023-03-25
2023-03","<a href=""arXiv (2023-03-25) : Backdoor Attacks with Input-unique Triggers in NLP"" target=""_blank"">[http://arxiv.org/abs/2303.14325v1]</a>
<a href=""DBLP (2023-03) : Backdoor Attacks with Input-unique Triggers in NLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.14325]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.14325]</a>","Backdoor attack aims at inducing neural models to make incorrect predictions for poison data while keeping predictions on the clean dataset unchanged, which creates a considerable threat to current natural language processing (NLP) systems. Existing backdoor attacking systems face two severe issues:firstly, most backdoor triggers follow a uniform and usually input-independent pattern, e.g., insertion of specific trigger words, synonym replacement. This significantly hinders the stealthiness of the attacking model, leading the trained backdoor model being easily identified as malicious by model probes. Secondly, trigger-inserted poisoned sentences are usually disfluent, ungrammatical, or even change the semantic meaning from the original sentence, making them being easily filtered in the pre-processing stage. To resolve these two issues, in this paper, we propose an input-unique backdoor attack(NURA), where we generate backdoor triggers unique to inputs. IDBA generates context-related triggers by continuing writing the input with a language model like GPT2. The generated sentence is used as the backdoor trigger. This strategy not only creates input-unique backdoor triggers, but also preserves the semantics of the original input, simultaneously resolving the two issues above. Experimental results show that the IDBA attack is effective for attack and difficult to defend: it achieves high attack success rate across all the widely applied benchmarks, while is immune to existing defending methods. In addition, it is able to generate fluent, grammatical, and diverse backdoor inputs, which can hardly be recognized through human inspection.
","
","arXiv
DBLP"
Optimal Smoothing Distribution Exploration for Backdoor Neutralization in Deep Learning-based Traffic Systems,"Yue Wang, Wending Li, Michail Maniatakos, Saif Eddin Jabari","arXiv
arXiv
ANZCC","2023-03-24
2023-03
2024","<a href=""arXiv (2023-03-24) : Optimal Smoothing Distribution Exploration for Backdoor Neutralization in Deep Learning-based Traffic Systems"" target=""_blank"">[http://arxiv.org/abs/2303.14197v1]</a>
<a href=""DBLP (2023-03) : Optimal Smoothing Distribution Exploration for Backdoor Neutralization in Deep Learning-based Traffic Systems"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.14197]</a>
<a href=""DBLP (2024) : Optimal Smoothing Distribution Exploration for Backdoor Neutralization in Deep Learning-based Traffic Systems"" target=""_blank"">[https://doi.org/10.1109/ANZCC59813.2024.10432866]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.14197]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ANZCC59813.2024.10432866]</a>","Deep Reinforcement Learning (DRL) enhances the efficiency of Autonomous Vehicles (AV), but also makes them susceptible to backdoor attacks that can result in traffic congestion or collisions. Backdoor functionality is typically incorporated by contaminating training datasets with covert malicious data to maintain high precision on genuine inputs while inducing the desired (malicious) outputs for specific inputs chosen by adversaries. Current defenses against backdoors mainly focus on image classification using image-based features, which cannot be readily transferred to the regression task of DRL-based AV controllers since the inputs are continuous sensor data, i.e., the combinations of velocity and distance of AV and its surrounding vehicles. Our proposed method adds well-designed noise to the input to neutralize backdoors. The approach involves learning an optimal smoothing (noise) distribution to preserve the normal functionality of genuine inputs while neutralizing backdoors. By doing so, the resulting model is expected to be more resilient against backdoor attacks while maintaining high accuracy on genuine inputs. The effectiveness of the proposed method is verified on a simulated traffic system based on a microscopic traffic simulator, where experimental results showcase that the smoothed traffic controller can neutralize all trigger samples and maintain the performance of relieving traffic congestion

","

","arXiv
DBLP
DBLP"
PoisonedGNN: Backdoor Attack on Graph Neural Networks-based Hardware Security Systems,"Lilas Alrahis, Satwik Patnaik, Muhammad Abdullah Hanif, Muhammad Shafique, Ozgur Sinanoglu","arXiv
arXiv","2023-03-24
2023-03","<a href=""arXiv (2023-03-24) : PoisonedGNN: Backdoor Attack on Graph Neural Networks-based Hardware Security Systems"" target=""_blank"">[http://arxiv.org/abs/2303.14009v1]</a>
<a href=""DBLP (2023-03) : PoisonedGNN: Backdoor Attack on Graph Neural Networks-based Hardware Security Systems"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.14009]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.14009]</a>","Graph neural networks (GNNs) have shown great success in detecting intellectual property (IP) piracy and hardware Trojans (HTs). However, the machine learning community has demonstrated that GNNs are susceptible to data poisoning attacks, which result in GNNs performing abnormally on graphs with pre-defined backdoor triggers (realized using crafted subgraphs). Thus, it is imperative to ensure that the adoption of GNNs should not introduce security vulnerabilities in critical security frameworks. Existing backdoor attacks on GNNs generate random subgraphs with specific sizes/densities to act as backdoor triggers. However, for Boolean circuits, backdoor triggers cannot be randomized since the added structures should not affect the functionality of a design. We explore this threat and develop PoisonedGNN as the first backdoor attack on GNNs in the context of hardware design. We design and inject backdoor triggers into the register-transfer- or the gate-level representation of a given design without affecting the functionality to evade some GNN-based detection procedures. To demonstrate the effectiveness of PoisonedGNN, we consider two case studies: (i) Hiding HTs and (ii) IP piracy. Our experiments on TrustHub datasets demonstrate that PoisonedGNN can hide HTs and IP piracy from advanced GNN-based detection platforms with an attack success rate of up to 100%.
","
","arXiv
DBLP"
Backdoor Defense via Adaptively Splitting Poisoned Dataset,"Kuofeng Gao, Yang Bai, Jindong Gu, Yong Yang, Shu-Tao Xia","arXiv
CVPR
arXiv","2023-03-23
2023
2023-03","<a href=""arXiv (2023-03-23) : Backdoor Defense via Adaptively Splitting Poisoned Dataset"" target=""_blank"">[http://arxiv.org/abs/2303.12993v1]</a>
<a href=""DBLP (2023) : Backdoor Defense via Adaptively Splitting Poisoned Dataset"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.00390]</a>
<a href=""DBLP (2023-03) : Backdoor Defense via Adaptively Splitting Poisoned Dataset"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.12993]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.00390]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.12993]</a>","Backdoor defenses have been studied to alleviate the threat of deep neural networks (DNNs) being backdoor attacked and thus maliciously altered. Since DNNs usually adopt some external training data from an untrusted third party, a robust backdoor defense strategy during the training stage is of importance. We argue that the core of training-time defense is to select poisoned samples and to handle them properly. In this work, we summarize the training-time defenses from a unified framework as splitting the poisoned dataset into two data pools. Under our framework, we propose an adaptively splitting dataset-based defense (ASD). Concretely, we apply loss-guided split and meta-learning-inspired split to dynamically update two data pools. With the split clean data pool and polluted data pool, ASD successfully defends against backdoor attacks during training. Extensive experiments on multiple benchmark datasets and DNN models against six state-of-the-art backdoor attacks demonstrate the superiority of our ASD. Our code is available at https://github.com/KuofengGao/ASD.

","<a href=""arXiv"" target=""_blank"">[https://github.com/KuofengGao/ASD]</a>

","arXiv
DBLP
DBLP"
Detecting Backdoors in Pre-trained Encoders,"Shiwei Feng, Guanhong Tao, Siyuan Cheng, Guangyu Shen, Xiangzhe Xu, Yingqi Liu, Kaiyuan Zhang, Shiqing Ma, Xiangyu Zhang","arXiv
CVPR
arXiv","2023-03-23
2023
2023-03","<a href=""arXiv (2023-03-23) : Detecting Backdoors in Pre-trained Encoders"" target=""_blank"">[http://arxiv.org/abs/2303.15180v1]</a>
<a href=""DBLP (2023) : Detecting Backdoors in Pre-trained Encoders"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.01569]</a>
<a href=""DBLP (2023-03) : Detecting Backdoors in Pre-trained Encoders"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.15180]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.01569]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.15180]</a>","Self-supervised learning in computer vision trains on unlabeled data, such as images or (image, text) pairs, to obtain an image encoder that learns high-quality embeddings for input data. Emerging backdoor attacks towards encoders expose crucial vulnerabilities of self-supervised learning, since downstream classifiers (even further trained on clean data) may inherit backdoor behaviors from encoders. Existing backdoor detection methods mainly focus on supervised learning settings and cannot handle pre-trained encoders especially when input labels are not available. In this paper, we propose DECREE, the first backdoor detection approach for pre-trained encoders, requiring neither classifier headers nor input labels. We evaluate DECREE on over 400 encoders trojaned under 3 paradigms. We show the effectiveness of our method on image encoders pre-trained on ImageNet and OpenAI's CLIP 400 million image-text pairs. Our method consistently has a high detection accuracy even if we have only limited or no access to the pre-training dataset.

","

","arXiv
DBLP
DBLP"
Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs,"Hasan Abed Al Kader Hammoud, Adel Bibi, Philip H. S. Torr, Bernard Ghanem",arXiv,2023-03-23,"<a href=""arXiv (2023-03-23) : Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs"" target=""_blank"">[http://arxiv.org/abs/2303.13211v1]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","In this paper we investigate the frequency sensitivity of Deep Neural Networks (DNNs) when presented with clean samples versus poisoned samples. Our analysis shows significant disparities in frequency sensitivity between these two types of samples. Building on these findings, we propose FREAK, a frequency-based poisoned sample detection algorithm that is simple yet effective. Our experimental results demonstrate the efficacy of FREAK not only against frequency backdoor attacks but also against some spatial attacks. Our work is just the first step in leveraging these insights. We believe that our analysis and proposed defense mechanism will provide a foundation for future research and development of backdoor defenses.",,arXiv
Do Backdoors Assist Membership Inference Attacks?,"Yumeki Goto, Nami Ashizawa, Toshiki Shibahara, Naoto Yanai","arXiv
arXiv","2023-03-22
2023-03","<a href=""arXiv (2023-03-22) : Do Backdoors Assist Membership Inference Attacks?"" target=""_blank"">[http://arxiv.org/abs/2303.12589v1]</a>
<a href=""DBLP (2023-03) : Do Backdoors Assist Membership Inference Attacks?"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.12589]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.12589]</a>","When an adversary provides poison samples to a machine learning model, privacy leakage, such as membership inference attacks that infer whether a sample was included in the training of the model, becomes effective by moving the sample to an outlier. However, the attacks can be detected because inference accuracy deteriorates due to poison samples. In this paper, we discuss a \textit{backdoor-assisted membership inference attack}, a novel membership inference attack based on backdoors that return the adversary's expected output for a triggered sample. We found three crucial insights through experiments with an academic benchmark dataset. We first demonstrate that the backdoor-assisted membership inference attack is unsuccessful. Second, when we analyzed loss distributions to understand the reason for the unsuccessful results, we found that backdoors cannot separate loss distributions of training and non-training samples. In other words, backdoors cannot affect the distribution of clean samples. Third, we also show that poison and triggered samples activate neurons of different distributions. Specifically, backdoors make any clean sample an inlier, contrary to poisoning samples. As a result, we confirm that backdoors cannot assist membership inference.
","
","arXiv
DBLP"
Enhancement of an IoT hybrid intrusion detection system based on fog-to-cloud computing,"Doaa Mohamed, Osama Ismael",Journal of Cloud Computing,2023-03-22,"<a href=""Springer (2023-03-22) : Enhancement of an IoT hybrid intrusion detection system based on fog-to-cloud computing"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-023-00420-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-023-00420-y]</a>","Nowadays, with the proliferation of internet of things-connected devices, the scope of cyber-attacks on the internet of things has grown...",,Springer
Blockchain-based Trusted Software Distribution Mechanism,Z. Zhao,2022 4th International Academic Exchange Conference on Science and Technology Innovation (IAECST),2023-03-17,"<a href=""IEEE (2023-03-17) : Blockchain-based Trusted Software Distribution Mechanism"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10062143]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IAECST57965.2022.10062143]</a>","In the traditional software distribution mechanism, the software developer will add a hash check code to the current version of the software to ensure that the software is forgery-proof. However, in the process of software release and upgrade, there will always be hackers and some unscrupulous elements attacking some genuine software, destroying the internal functions of normal software, or adding some backdoors to steal user information and data. Blockchain is a distributed p2p system with the advantages of trusted traceability, anti-tampering and consensus mechanism, which can effectively prevent hackers from modifying the data on its chain. In order to solve the loss caused by using the attacked software, this paper proposes blockchain-based trusted software distribution mechanism, which brings a new idea for the research of trusted software release mechanism.",,IEEE
FSL: federated sequential learning-based cyberattack detection for Industrial Internet of Things,"Fangyu Li, Junnuo Lin, Honggui Han",Industrial Artificial Intelligence,2023-03-17,"<a href=""Springer (2023-03-17) : FSL: federated sequential learning-based cyberattack detection for Industrial Internet of Things"" target=""_blank"">[https://link.springer.com/article/10.1007/s44244-023-00006-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s44244-023-00006-2]</a>","Industrial Internet of Things (IIoT) brings revolutionary technical supports to modern industries. However, today’s IIoT still faces the challenges...",,Springer
Stacked Deep Learning Framework for Edge-Based Intelligent Threat Detection in IoT Network,"D. Santhadevi, B. Janet",The Journal of Supercomputing,2023-03-17,"<a href=""Springer (2023-03-17) : Stacked Deep Learning Framework for Edge-Based Intelligent Threat Detection in IoT Network"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05153-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05153-y]</a>",Cyber-attacks on Internet of Things (IoT) devices are becoming increasingly common due to the rapidly growing number of connected devices and the...,,Springer
From zero-shot machine learning to zero-day attack detection,"Mohanad Sarhan, Siamak Layeghy, ... Marius Portmann",International Journal of Information Security,2023-03-15,"<a href=""Springer (2023-03-15) : From zero-shot machine learning to zero-day attack detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00676-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00676-0]</a>",Machine learning (ML) models have proved efficient in classifying data samples into their respective categories. The standard ML evaluation...,,Springer
GOSVM: Gannet optimization based support vector machine for malicious attack detection in cloud environment,"M. Arunkumar, K. Ashok Kumar",International Journal of Information Technology,2023-03-13,"<a href=""Springer (2023-03-13) : GOSVM: Gannet optimization based support vector machine for malicious attack detection in cloud environment"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-023-01192-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-023-01192-z]</a>","Cloud computing is the most useful computing technology for the new service progression. Due to the distributed nature of cloud computing, security...",,Springer
Federated learning for 6G-enabled secure communication systems: a comprehensive survey,"Deepika Sirohi, Neeraj Kumar, ... Mohammad Hijjii",Artificial Intelligence Review,2023-03-12,"<a href=""Springer (2023-03-12) : Federated learning for 6G-enabled secure communication systems: a comprehensive survey"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-023-10417-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-023-10417-3]</a>","Machine learning (ML) and Deep learning (DL) models are popular in many areas, from business, medicine, industries, healthcare, transportation, smart...",,Springer
"Trustworthy artificial intelligence in Alzheimer’s disease: state of the art, opportunities, and challenges","Shaker El-Sappagh, Jose M. Alonso-Moral, ... Alberto Bugarín-Diz",Artificial Intelligence Review,2023-03-12,"<a href=""Springer (2023-03-12) : Trustworthy artificial intelligence in Alzheimer’s disease: state of the art, opportunities, and challenges"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-023-10415-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-023-10415-5]</a>",Medical applications of Artificial Intelligence (AI) have consistently shown remarkable performance in providing medical professionals and patients...,,Springer
Adversarial examples: attacks and defences on medical deep learning systems,"Murali Krishna Puttagunta, S. Ravi, C Nelson Kennedy Babu",Multimedia Tools and Applications,2023-03-08,"<a href=""Springer (2023-03-08) : Adversarial examples: attacks and defences on medical deep learning systems"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-14702-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-14702-9]</a>","In recent years, significant progress has been achieved using deep neural networks (DNNs) in obtaining human-level performance on various...",,Springer
Causal effect analysis-based intrusion detection system for IoT applications,"Srividya Bhaskara, Santosh Singh Rathore",International Journal of Information Security,2023-03-08,"<a href=""Springer (2023-03-08) : Causal effect analysis-based intrusion detection system for IoT applications"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00674-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00674-2]</a>",Intrusion detection systems (IDSs) are employed at various levels in the network to either detect or prevent an intrusion that could cause...,,Springer
FedIPR: Ownership Verification for Federated Deep Neural Network Models,B. Li L. Fan H. Gu J. Li Q. Yang,IEEE Transactions on Pattern Analysis and Machine Intelligence,2023-03-07,"<a href=""IEEE (2023-03-07) : FedIPR: Ownership Verification for Federated Deep Neural Network Models"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9847383]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TPAMI.2022.3195956]</a>","Federated learning models are collaboratively developed upon valuable training data owned by multiple parties. During the development and deployment of federated models, they are exposed to risks including illegal copying, re-distribution, misuse and/or free-riding. To address these risks, the ownership verification of federated learning models is a prerequisite that protects federated learning model intellectual property rights (IPR) i.e., FedIPR. We propose a novel federated deep neural network (FedDNN) ownership verification scheme that allows private watermarks to be embedded and verified to claim legitimate IPR of FedDNN models. In the proposed scheme, each client independently verifies the existence of the model watermarks and claims respective ownership of the federated model without disclosing neither private training data nor private watermark information. The effectiveness of embedded watermarks is theoretically justified by the rigorous analysis of conditions under which watermarks can be privately embedded and detected by multiple clients. Moreover, extensive experimental results on computer vision and natural language processing tasks demonstrate that varying bit-length watermarks can be embedded and reliably detected without compromising original model performances. Our watermarking scheme is also resilient to various federated training settings and robust against removal attacks.",,IEEE
BATT: Backdoor Attack with Transformation-based Triggers,"Tong Xu, Yiming Li, Yong Jiang, Shu-Tao Xia","arXiv
arXiv","2023-03-06
2022-11","<a href=""arXiv (2023-03-06) : BATT: Backdoor Attack with Transformation-based Triggers"" target=""_blank"">[http://arxiv.org/abs/2211.01806v2]</a>
<a href=""DBLP (2022-11) : BATT: Backdoor Attack with Transformation-based Triggers"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.01806]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.01806]</a>","Deep neural networks (DNNs) are vulnerable to backdoor attacks. The backdoor adversaries intend to maliciously control the predictions of attacked DNNs by injecting hidden backdoors that can be activated by adversary-specified trigger patterns during the training process. One recent research revealed that most of the existing attacks failed in the real physical world since the trigger contained in the digitized test samples may be different from that of the one used for training. Accordingly, users can adopt spatial transformations as the image pre-processing to deactivate hidden backdoors. In this paper, we explore the previous findings from another side. We exploit classical spatial transformations (i.e. rotation and translation) with the specific parameter as trigger patterns to design a simple yet effective poisoning-based backdoor attack. For example, only images rotated to a particular angle can activate the embedded backdoor of attacked DNNs. Extensive experiments are conducted, verifying the effectiveness of our attack under both digital and physical settings and its resistance to existing backdoor defenses.
","
","arXiv
DBLP"
Untargeted Backdoor Attack against Object Detection,"Chengxiao Luo, Yiming Li, Yong Jiang, Shu-Tao Xia","arXiv
arXiv","2023-03-06
2022-11","<a href=""arXiv (2023-03-06) : Untargeted Backdoor Attack against Object Detection"" target=""_blank"">[http://arxiv.org/abs/2211.05638v2]</a>
<a href=""DBLP (2022-11) : Untargeted Backdoor Attack against Object Detection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.05638]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.05638]</a>","Recent studies revealed that deep neural networks (DNNs) are exposed to backdoor threats when training with third-party resources (such as training samples or backbones). The backdoored model has promising performance in predicting benign samples, whereas its predictions can be maliciously manipulated by adversaries based on activating its backdoors with pre-defined trigger patterns. Currently, most of the existing backdoor attacks were conducted on the image classification under the targeted manner. In this paper, we reveal that these threats could also happen in object detection, posing threatening risks to many mission-critical applications ($e.g.$, pedestrian detection and intelligent surveillance systems). Specifically, we design a simple yet effective poison-only backdoor attack in an untargeted manner, based on task characteristics. We show that, once the backdoor is embedded into the target model by our attack, it can trick the model to lose detection of any object stamped with our trigger patterns. We conduct extensive experiments on the benchmark dataset, showing its effectiveness in both digital and physical-world settings and its resistance to potential defenses.
","
","arXiv
DBLP"
mLBOA-DML: modified butterfly optimized deep metric learning for enhancing accuracy in intrusion detection system,"Varun Prabhakaran, Ashokkumar Kulandasamy",Journal of Reliable Intelligent Environments,2023-03-05,"<a href=""Springer (2023-03-05) : mLBOA-DML: modified butterfly optimized deep metric learning for enhancing accuracy in intrusion detection system"" target=""_blank"">[https://link.springer.com/article/10.1007/s40860-022-00197-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s40860-022-00197-y]</a>",Intrusion detection is a prominent factor in the cybersecurity domain that prevents the network from malicious attacks. Cloud security is not...,,Springer
Circumventing Backdoor Defenses That Are Based on Latent Separability,"Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, Prateek Mittal","arXiv
arXiv","2023-03-04
2022-05","<a href=""arXiv (2023-03-04) : Circumventing Backdoor Defenses That Are Based on Latent Separability"" target=""_blank"">[http://arxiv.org/abs/2205.13613v3]</a>
<a href=""DBLP (2022-05) : Circumventing Backdoor Defenses That Are Based on Latent Separability"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.13613]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.13613]</a>","Recent studies revealed that deep learning is susceptible to backdoor poisoning attacks. An adversary can embed a hidden backdoor into a model to manipulate its predictions by only modifying a few training data, without controlling the training process. Currently, a tangible signature has been widely observed across a diverse set of backdoor poisoning attacks -- models trained on a poisoned dataset tend to learn separable latent representations for poison and clean samples. This latent separation is so pervasive that a family of backdoor defenses directly take it as a default assumption (dubbed latent separability assumption), based on which to identify poison samples via cluster analysis in the latent space. An intriguing question consequently follows: is the latent separation unavoidable for backdoor poisoning attacks? This question is central to understanding whether the assumption of latent separability provides a reliable foundation for defending against backdoor poisoning attacks. In this paper, we design adaptive backdoor poisoning attacks to present counter-examples against this assumption. Our methods include two key components: (1) a set of trigger-planted samples correctly labeled to their semantic classes (other than the target class) that can regularize backdoor learning, (2) asymmetric trigger planting strategies that help to boost attack success rate (ASR) as well as to diversify latent representations of poison samples. Extensive experiments on benchmark datasets verify the effectiveness of our adaptive attacks in bypassing existing latent separation based backdoor defenses. Moreover, our attacks still maintain a high attack success rate with negligible clean accuracy drop. Our studies call for defense designers to take caution when leveraging latent separation as an assumption in their defenses.
","
","arXiv
DBLP"
Watermarking in Secure Federated Learning: A Verification Framework Based on Client-Side Backdooring,"Wenyuan Yang, Shuo Shao, Yue Yang, Xiyao Liu, Ximeng Liu, Zhihua Xia, Gerald Schaefer, Hui Fang","arXiv
ACM Trans. Intell. Syst. Technol.
arXiv","2023-03-03
2024
2022-11","<a href=""arXiv (2023-03-03) : Watermarking in Secure Federated Learning: A Verification Framework Based on Client-Side Backdooring"" target=""_blank"">[http://arxiv.org/abs/2211.07138v2]</a>
<a href=""DBLP (2024) : Watermarking in Secure Federated Learning: A Verification Framework Based on Client-Side Backdooring"" target=""_blank"">[https://doi.org/10.1145/3630636]</a>
<a href=""DBLP (2022-11) : Watermarking in Secure Federated Learning: A Verification Framework Based on Client-Side Backdooring"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.07138]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3630636]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.07138]</a>","Federated learning (FL) allows multiple participants to collaboratively build deep learning (DL) models without directly sharing data. Consequently, the issue of copyright protection in FL becomes important since unreliable participants may gain access to the jointly trained model. Application of homomorphic encryption (HE) in secure FL framework prevents the central server from accessing plaintext models. Thus, it is no longer feasible to embed the watermark at the central server using existing watermarking schemes. In this paper, we propose a novel client-side FL watermarking scheme to tackle the copyright protection issue in secure FL with HE. To our best knowledge, it is the first scheme to embed the watermark to models under the Secure FL environment. We design a black-box watermarking scheme based on client-side backdooring to embed a pre-designed trigger set into an FL model by a gradient-enhanced embedding method. Additionally, we propose a trigger set construction mechanism to ensure the watermark cannot be forged. Experimental results demonstrate that our proposed scheme delivers outstanding protection performance and robustness against various watermark removal attacks and ambiguity attack.

","

","arXiv
DBLP
DBLP"
NCL: Textual Backdoor Defense Using Noise-augmented Contrastive Learning,"Shengfang Zhai, Qingni Shen, Xiaoyi Chen, Weilong Wang, Cong Li, Yuejian Fang, Zhonghai Wu","arXiv
arXiv","2023-03-03
2023-03","<a href=""arXiv (2023-03-03) : NCL: Textual Backdoor Defense Using Noise-augmented Contrastive Learning"" target=""_blank"">[http://arxiv.org/abs/2303.01742v1]</a>
<a href=""DBLP (2023-03) : NCL: Textual Backdoor Defense Using Noise-augmented Contrastive Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.01742]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.01742]</a>","At present, backdoor attacks attract attention as they do great harm to deep learning models. The adversary poisons the training data making the model being injected with a backdoor after being trained unconsciously by victims using the poisoned dataset. In the field of text, however, existing works do not provide sufficient defense against backdoor attacks. In this paper, we propose a Noise-augmented Contrastive Learning (NCL) framework to defend against textual backdoor attacks when training models with untrustworthy data. With the aim of mitigating the mapping between triggers and the target label, we add appropriate noise perturbing possible backdoor triggers, augment the training dataset, and then pull homology samples in the feature space utilizing contrastive learning objective. Experiments demonstrate the effectiveness of our method in defending three types of textual backdoor attacks, outperforming the prior works.
","
","arXiv
DBLP"
Efficient intrusion detection using multi-player generative adversarial networks (GANs): an ensemble-based deep learning architecture,"Raha Soleymanzadeh, Rasha Kashef",Neural Computing and Applications,2023-03-03,"<a href=""Springer (2023-03-03) : Efficient intrusion detection using multi-player generative adversarial networks (GANs): an ensemble-based deep learning architecture"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-023-08398-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-023-08398-z]</a>","Intrusion detection systems (IDSs) investigate various attacks, identify malicious patterns, and implement effective control strategies. With the...",,Springer
AAIA: an efficient aggregation scheme against inverting attack for federated learning,"Zhen Yang, Shisong Yang, ... Yuwen Chen",International Journal of Information Security,2023-03-02,"<a href=""Springer (2023-03-02) : AAIA: an efficient aggregation scheme against inverting attack for federated learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00670-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00670-6]</a>","Federated learning is emerged as an attractive paradigm regarding the data privacy problem, clients train the deep neural network on their local...",,Springer
"BLoCNet: a hybrid, dataset-independent intrusion detection system using deep learning","Brandon Bowen, Anitha Chennamaneni, ... Daisy Lin",International Journal of Information Security,2023-03-02,"<a href=""Springer (2023-03-02) : BLoCNet: a hybrid, dataset-independent intrusion detection system using deep learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00663-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-023-00663-5]</a>",Intrusion detection systems (IDS) identify cyber attacks given a sample of network traffic collected from real-world computer networks. As a powerful...,,Springer
Enhancing Backdoor Attacks With Multi-Level MMD Regularization,Xia P.,IEEE Transactions on Dependable and Secure Computing,2023-03-01,"<a href=""ScienceDirect (2023-03-01) : Enhancing Backdoor Attacks With Multi-Level MMD Regularization"" target=""_blank"">[https://doi.org/10.1109/TDSC.2022.3161477]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TDSC.2022.3161477]</a>",,,ScienceDirect
Semantics-Preserving Reinforcement Learning Attack Against Graph Neural Networks for Malware Detection,Zhang L.,IEEE Transactions on Dependable and Secure Computing,2023-03-01,"<a href=""ScienceDirect (2023-03-01) : Semantics-Preserving Reinforcement Learning Attack Against Graph Neural Networks for Malware Detection"" target=""_blank"">[https://doi.org/10.1109/TDSC.2022.3153844]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TDSC.2022.3153844]</a>",,,ScienceDirect
Backdoor for Debias: Mitigating Model Bias with Backdoor Attack-based Artificial Bias,"Shangxi Wu, Qiuyang He, Fangzhao Wu, Jitao Sang, Yaowei Wang, Changsheng Xu",arXiv,2023-03,"<a href=""DBLP (2023-03) : Backdoor for Debias: Mitigating Model Bias with Backdoor Attack-based Artificial Bias"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.01504]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.01504]</a>",,,DBLP
Sniper Backdoor: Single Client Targeted Backdoor Attack in Federated Learning,"Gorka Abad, Servio Paguada, Oguzhan Ersoy, Stjepan Picek, Víctor Julio Ramírez-Durán, Aitor Urbieta","arXiv
SaTML","2023-02-28
2023","<a href=""arXiv (2023-02-28) : Sniper Backdoor: Single Client Targeted Backdoor Attack in Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2203.08689v2]</a>
<a href=""DBLP (2023) : Sniper Backdoor: Single Client Targeted Backdoor Attack in Federated Learning"" target=""_blank"">[https://doi.org/10.1109/SaTML54575.2023.00033]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SaTML54575.2023.00033]</a>","Federated Learning (FL) enables collaborative training of Deep Learning (DL) models where the data is retained locally. Like DL, FL has severe security weaknesses that the attackers can exploit, e.g., model inversion and backdoor attacks. Model inversion attacks reconstruct the data from the training datasets, whereas backdoors misclassify only classes containing specific properties, e.g., a pixel pattern. Backdoors are prominent in FL and aim to poison every client model, while model inversion attacks can target even a single client. This paper introduces a novel technique to allow backdoor attacks to be client-targeted, compromising a single client while the rest remain unchanged. The attack takes advantage of state-of-the-art model inversion and backdoor attacks. Precisely, we leverage a Generative Adversarial Network to perform the model inversion. Afterward, we shadow-train the FL network, in which, using a Siamese Neural Network, we can identify, target, and backdoor the victim's model. Our attack has been validated using the MNIST, F-MNIST, EMNIST, and CIFAR-100 datasets under different settings -- achieving up to 99\% accuracy on both source (clean) and target (backdoor) classes and against state-of-the-art defenses, e.g., Neural Cleanse, opening a novel threat model to be considered in the future.
","
","arXiv
DBLP"
OpenStackDP: a scalable network security framework for SDN-based OpenStack cloud infrastructure,"Prabhakar Krishnan, Kurunandan Jain, ... Rajkumar Buyya",Journal of Cloud Computing,2023-02-28,"<a href=""Springer (2023-02-28) : OpenStackDP: a scalable network security framework for SDN-based OpenStack cloud infrastructure"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-023-00406-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-023-00406-w]</a>",Network Intrusion Detection Systems (NIDS) and firewalls are the de facto solutions in the modern cloud to detect cyberattacks and minimize potential...,,Springer
FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning,"Kaiyuan Zhang, Guanhong Tao, Qiuling Xu, Siyuan Cheng, Shengwei An, Yingqi Liu, Shiwei Feng, Guangyu Shen, Pin-Yu Chen, Shiqing Ma, Xiangyu Zhang","arXiv
ICLR
arXiv","2023-02-27
2023
2022-10","<a href=""arXiv (2023-02-27) : FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2210.12873v2]</a>
<a href=""DBLP (2023) : FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning"" target=""_blank"">[https://openreview.net/pdf?id=Xo2E217_M4n]</a>
<a href=""DBLP (2022-10) : FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.12873]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/pdf?id=Xo2E217_M4n]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.12873]</a>","Federated Learning (FL) is a distributed learning paradigm that enables different parties to train a model together for high quality and strong privacy protection. In this scenario, individual participants may get compromised and perform backdoor attacks by poisoning the data (or gradients). Existing work on robust aggregation and certified FL robustness does not study how hardening benign clients can affect the global model (and the malicious clients). In this work, we theoretically analyze the connection among cross-entropy loss, attack success rate, and clean accuracy in this setting. Moreover, we propose a trigger reverse engineering based defense and show that our method can achieve robustness improvement with guarantee (i.e., reducing the attack success rate) without affecting benign accuracy. We conduct comprehensive experiments across different datasets and attack settings. Our results on eight competing SOTA defense methods show the empirical superiority of our method on both single-shot and continuous FL backdoor attacks. Code is available at https://github.com/KaiyuanZh/FLIP.

","<a href=""arXiv"" target=""_blank"">[https://github.com/KaiyuanZh/FLIP]</a>

","arXiv
DBLP
DBLP"
Zero-day attack detection: a systematic literature review,"Rasheed Ahmad, Izzat Alsmadi, ... Lo’ai Tawalbeh",Artificial Intelligence Review,2023-02-27,"<a href=""Springer (2023-02-27) : Zero-day attack detection: a systematic literature review"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-023-10437-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-023-10437-z]</a>","With the continuous increase in cyberattacks over the past few decades, the quest to develop a comprehensive, robust, and effective intrusion...",,Springer
Hard-coded backdoor detection method based on semantic conflict,Hu A.,Chinese Journal of Network and Information Security,2023-02-25,"<a href=""ScienceDirect (2023-02-25) : Hard-coded backdoor detection method based on semantic conflict"" target=""_blank"">[https://doi.org/10.11959/j.issn.2096-109x.2023015]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.11959/j.issn.2096-109x.2023015]</a>",,,ScienceDirect
Poisoning attacks on face authentication systems by using the generative deformation model,"Chak-Tong Chan, Szu-Hao Huang, Patrick Puiyui Choy",Multimedia Tools and Applications,2023-02-25,"<a href=""Springer (2023-02-25) : Poisoning attacks on face authentication systems by using the generative deformation model"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-14695-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-14695-5]</a>","Various studies have revealed the vulnerabilities of machine learning algorithms. For example, a hacker can poison a deep learning facial recognition...",,Springer
RETRACTED ARTICLE: Robust adversarial uncertainty quantification for deep learning fine-tuning,"Usman Ahmed, Jerry Chun-Wei Lin",The Journal of Supercomputing,2023-02-25,"<a href=""Springer (2023-02-25) : RETRACTED ARTICLE: Robust adversarial uncertainty quantification for deep learning fine-tuning"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05087-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-023-05087-5]</a>",This paper proposes a deep learning model that is robust and capable of handling highly uncertain inputs. The model is divided into three phases:...,,Springer
Backdoor Attacks to Pre-trained Unified Foundation Models,"Zenghui Yuan, Yixin Liu, Kai Zhang, Pan Zhou, Lichao Sun","arXiv
arXiv","2023-02-23
2023-02","<a href=""arXiv (2023-02-23) : Backdoor Attacks to Pre-trained Unified Foundation Models"" target=""_blank"">[http://arxiv.org/abs/2302.09360v3]</a>
<a href=""DBLP (2023-02) : Backdoor Attacks to Pre-trained Unified Foundation Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.09360]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.09360]</a>","The rise of pre-trained unified foundation models breaks down the barriers between different modalities and tasks, providing comprehensive support to users with unified architectures. However, the backdoor attack on pre-trained models poses a serious threat to their security. Previous research on backdoor attacks has been limited to uni-modal tasks or single tasks across modalities, making it inapplicable to unified foundation models. In this paper, we make proof-of-concept level research on the backdoor attack for pre-trained unified foundation models. Through preliminary experiments on NLP and CV classification tasks, we reveal the vulnerability of these models and suggest future research directions for enhancing the attack approach.
","
","arXiv
DBLP"
FooBaR: Fault Fooling Backdoor Attack on Neural Network Training,"Jakub Breier, Xiaolu Hou, Martín Ochoa, Jesus Solano","arXiv
IEEE Trans. Dependable Secur. Comput.
arXiv","2023-02-23
2023
2021-09","<a href=""arXiv (2023-02-23) : FooBaR: Fault Fooling Backdoor Attack on Neural Network Training"" target=""_blank"">[http://arxiv.org/abs/2109.11249v2]</a>
<a href=""DBLP (2023) : FooBaR: Fault Fooling Backdoor Attack on Neural Network Training"" target=""_blank"">[https://doi.org/10.1109/TDSC.2022.3166671]</a>
<a href=""DBLP (2021-09) : FooBaR: Fault Fooling Backdoor Attack on Neural Network Training"" target=""_blank"">[https://arxiv.org/abs/2109.11249]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1109/TDSC.2022.3166671]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TDSC.2022.3166671]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2109.11249]</a>","Neural network implementations are known to be vulnerable to physical attack vectors such as fault injection attacks. As of now, these attacks were only utilized during the inference phase with the intention to cause a misclassification. In this work, we explore a novel attack paradigm by injecting faults during the training phase of a neural network in a way that the resulting network can be attacked during deployment without the necessity of further faulting. In particular, we discuss attacks against ReLU activation functions that make it possible to generate a family of malicious inputs, which are called fooling inputs, to be used at inference time to induce controlled misclassifications. Such malicious inputs are obtained by mathematically solving a system of linear equations that would cause a particular behaviour on the attacked activation functions, similar to the one induced in training through faulting. We call such attacks fooling backdoors as the fault attacks at the training phase inject backdoors into the network that allow an attacker to produce fooling inputs. We evaluate our approach against multi-layer perceptron networks and convolutional networks on a popular image classification task obtaining high attack success rates (from 60% to 100%) and high classification confidence when as little as 25 neurons are attacked while preserving high accuracy on the originally intended classification task.

","

","arXiv
DBLP
DBLP"
BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT,"Jiawen Shi, Yixin Liu, Pan Zhou, Lichao Sun","arXiv
arXiv","2023-02-21
2023-04","<a href=""arXiv (2023-02-21) : BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT"" target=""_blank"">[http://arxiv.org/abs/2304.12298v1]</a>
<a href=""DBLP (2023-04) : BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.12298]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2304.12298]</a>","Recently, ChatGPT has gained significant attention in research due to its ability to interact with humans effectively. The core idea behind this model is reinforcement learning (RL) fine-tuning, a new paradigm that allows language models to align with human preferences, i.e., InstructGPT. In this study, we propose BadGPT, the first backdoor attack against RL fine-tuning in language models. By injecting a backdoor into the reward model, the language model can be compromised during the fine-tuning stage. Our initial experiments on movie reviews, i.e., IMDB, demonstrate that an attacker can manipulate the generated text through BadGPT.
","
","arXiv
DBLP"
CatchBackdoor: Backdoor Testing by Critical Trojan Neural Path Identification via Differential Fuzzing,"Haibo Jin, Ruoxi Chen, Jinyin Chen, Yao Cheng, Chong Fu, Ting Wang, Yue Yu, Zhaoyan Ming",arXiv,2023-02-21,"<a href=""arXiv (2023-02-21) : CatchBackdoor: Backdoor Testing by Critical Trojan Neural Path Identification via Differential Fuzzing"" target=""_blank"">[http://arxiv.org/abs/2112.13064v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","The success of deep neural networks (DNNs) in real-world applications has benefited from abundant pre-trained models. However, the backdoored pre-trained models can pose a significant trojan threat to the deployment of downstream DNNs. Existing DNN testing methods are mainly designed to find incorrect corner case behaviors in adversarial settings but fail to discover the backdoors crafted by strong trojan attacks. Observing the trojan network behaviors shows that they are not just reflected by a single compromised neuron as proposed by previous work but attributed to the critical neural paths in the activation intensity and frequency of multiple neurons. This work formulates the DNN backdoor testing and proposes the CatchBackdoor framework. Via differential fuzzing of critical neurons from a small number of benign examples, we identify the trojan paths and particularly the critical ones, and generate backdoor testing examples by simulating the critical neurons in the identified paths. Extensive experiments demonstrate the superiority of CatchBackdoor, with higher detection performance than existing methods. CatchBackdoor works better on detecting backdoors by stealthy blending and adaptive attacks, which existing methods fail to detect. Moreover, our experiments show that CatchBackdoor may reveal the potential backdoors of models in Model Zoo.",,arXiv
SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency,"Junfeng Guo, Yiming Li, Xun Chen, Hanqing Guo, Lichao Sun, Cong Liu","arXiv
ICLR
arXiv","2023-02-19
2023
2023-02","<a href=""arXiv (2023-02-19) : SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency"" target=""_blank"">[http://arxiv.org/abs/2302.03251v2]</a>
<a href=""DBLP (2023) : SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency"" target=""_blank"">[https://openreview.net/pdf?id=o0LFPcoFKnr]</a>
<a href=""DBLP (2023-02) : SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.03251]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/pdf?id=o0LFPcoFKnr]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.03251]</a>","Deep neural networks (DNNs) are vulnerable to backdoor attacks, where adversaries embed a hidden backdoor trigger during the training process for malicious prediction manipulation. These attacks pose great threats to the applications of DNNs under the real-world machine learning as a service (MLaaS) setting, where the deployed model is fully black-box while the users can only query and obtain its predictions. Currently, there are many existing defenses to reduce backdoor threats. However, almost all of them cannot be adopted in MLaaS scenarios since they require getting access to or even modifying the suspicious models. In this paper, we propose a simple yet effective black-box input-level backdoor detection, called SCALE-UP, which requires only the predicted labels to alleviate this problem. Specifically, we identify and filter malicious testing samples by analyzing their prediction consistency during the pixel-wise amplification process. Our defense is motivated by an intriguing observation (dubbed scaled prediction consistency) that the predictions of poisoned samples are significantly more consistent compared to those of benign ones when amplifying all pixel values. Besides, we also provide theoretical foundations to explain this phenomenon. Extensive experiments are conducted on benchmark datasets, verifying the effectiveness and efficiency of our defense and its resistance to potential adaptive attacks. Our codes are available at https://github.com/JunfengGo/SCALE-UP.

","<a href=""arXiv"" target=""_blank"">[https://github.com/JunfengGo/SCALE-UP]</a>

","arXiv
DBLP
DBLP"
RobustNLP: A Technique to Defend NLP Models Against Backdoor Attacks,Marwan Omar,"arXiv
arXiv","2023-02-18
2023-02","<a href=""arXiv (2023-02-18) : RobustNLP: A Technique to Defend NLP Models Against Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2302.09420v1]</a>
<a href=""DBLP (2023-02) : RobustNLP: A Technique to Defend NLP Models Against Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.09420]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.09420]</a>","As machine learning (ML) systems are being increasingly employed in the real world to handle sensitive tasks and make decisions in various fields, the security and privacy of those models have also become increasingly critical. In particular, Deep Neural Networks (DNN) have been shown to be vulnerable to backdoor attacks whereby adversaries have access to the training data and the opportunity to manipulate such data by inserting carefully developed samples into the training dataset. Although the NLP community has produced several studies on generating backdoor attacks proving the vulnerable state of language modes, to the best of our knowledge, there does not exist any work to combat such attacks. To bridge this gap, we present RobustEncoder: a novel clustering-based technique for detecting and removing backdoor attacks in the text domain. Extensive empirical results demonstrate the effectiveness of our technique in detecting and removing backdoor triggers. Our code is available at https://github.com/marwanomar1/Backdoor-Learning-for-NLP
","<a href=""arXiv"" target=""_blank"">[https://github.com/marwanomar1/Backdoor-Learning-for-NLP]</a>
","arXiv
DBLP"
Deep neural network watermarking based on a reversible image hiding network,"Linna Wang, Yunfei Song, Daoxun Xia",Pattern Analysis and Applications,2023-02-18,"<a href=""Springer (2023-02-18) : Deep neural network watermarking based on a reversible image hiding network"" target=""_blank"">[https://link.springer.com/article/10.1007/s10044-023-01140-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10044-023-01140-4]</a>","Recently, many researchers have proposed deep neural network (DNN) watermarking technologies, DNN watermarking approaches can be divided into two...",,Springer
Categorical Inference Poisoning: Verifiable Defense Against Black-Box DNN Model Stealing Without Constraining Surrogate Data and Query Times,H. Zhang G. Hua X. Wang H. Jiang W. Yang,IEEE Transactions on Information Forensics and Security,2023-02-17,"<a href=""IEEE (2023-02-17) : Categorical Inference Poisoning: Verifiable Defense Against Black-Box DNN Model Stealing Without Constraining Surrogate Data and Query Times"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10042038]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3244107]</a>","Deep Neural Network (DNN) models have offered powerful solutions for a wide range of tasks, but the cost to develop such models is nontrivial, which calls for effective model protection. Although black-box distribution can mitigate some threats, model functionality can still be stolen via black-box surrogate attacks. Recent studies have shown that surrogate attacks can be launched in several ways, while the existing defense methods commonly assume attackers with insufficient in-distribution (ID) data and restricted attacking strategies. In this paper, we relax these constraints and assume a practical threat model in which the adversary not only has sufficient ID data and query times but also can adjust the surrogate training data labeled by the victim model. Then, we propose a two-step categorical inference poisoning (CIP) framework, featuring both poisoning for performance degradation (PPD) and poisoning for backdooring (PBD). In the first poisoning step, incoming queries are classified into ID and (out-of-distribution) OOD ones using an energy score (ES) based OOD detector, and the latter are further classified into high ES and low ES ones, which are subsequently passed to a strong and a weak PPD process, respectively. In the second poisoning step, difficult ID queries are detected by a proposed reliability score (RS) measurement and are passed to PBD. In doing so, the first step OOD poisoning leads to substantial performance degradation in surrogate models, the second step ID poisoning further embeds backdoors in them, while both can preserve model fidelity. Extensive experiments confirm that CIP can not only achieve promising performance against state-of-the-art black-box surrogate attacks like KnockoffNets and data-free model extraction (DFME) but also work well against stronger attacks with sufficient ID and deceptive data, better than the existing dynamic adversarial watermarking (DAWN) and deceptive perturbation defense methods. PyTorch code is available at https://github.com/Hatins/CIP_master.git.","<a href=""IEEE"" target=""_blank"">[https://github.com/Hatins/CIP_master.git]</a>",IEEE
QTrojan: A Circuit Backdoor Against Quantum Neural Networks,"Cheng Chu, Lei Jiang, Martin Swany, Fan Chen","arXiv
arXiv","2023-02-16
2023-02","<a href=""arXiv (2023-02-16) : QTrojan: A Circuit Backdoor Against Quantum Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2302.08090v1]</a>
<a href=""DBLP (2023-02) : QTrojan: A Circuit Backdoor Against Quantum Neural Networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.08090]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.08090]</a>","We propose a circuit-level backdoor attack, \textit{QTrojan}, against Quantum Neural Networks (QNNs) in this paper. QTrojan is implemented by few quantum gates inserted into the variational quantum circuit of the victim QNN. QTrojan is much stealthier than a prior Data-Poisoning-based Backdoor Attack (DPBA), since it does not embed any trigger in the inputs of the victim QNN or require the access to original training datasets. Compared to a DPBA, QTrojan improves the clean data accuracy by 21\% and the attack success rate by 19.9\%.
","
","arXiv
DBLP"
Poison Neural Network-Based mmWave Beam Selection and Detoxification With Machine Unlearning,Z. Zhang M. Tian C. Li Y. Huang L. Yang,IEEE Transactions on Communications,2023-02-15,"<a href=""IEEE (2023-02-15) : Poison Neural Network-Based mmWave Beam Selection and Detoxification With Machine Unlearning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10002349]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TCOMM.2022.3232794]</a>","Deep neural network-based learning methods have been considered promising techniques used in beam selection problems. However, existing research ignores the peculiar vulnerabilities of neural networks. The adversaries can use data poisoning to embed predefined triggers into a model during training time such that the neural network-based beam model may make an incorrect output decision of a test example when patched with the trigger. Data poisoning offers attackers the possibility to build backdoors. The goal of backdoors is often unethical, such as giving users a poor experience by manipulating infected models to output inappropriate beams. In this paper, first, we introduce a simple backdoor attack method by using data poisoning in a mmWave beam selection system. By numerical simulations, we verify that this poisoning attack is effective for neural networks with different structures. In addition, we explore the effect of poisoned data volume on the effect of backdoor attacks. The results show that the backdoor can be successfully implanted into the beam selection neural network. Besides, we fine-tune the trained model for a new wireless communication environment, and the results show that backdoors still exist even when the model is tuned with data from new scenarios. Then, we propose a machine unlearning solution to mitigate the backdoor of the trained beam selection model. The problem of eliminating backdoors is modeled as a minimax optimization problem. We propose a novel adversarial unlearning method along with label smoothing to solve the backdoor removal problem. We compared the proposed backdoor elimination method with the classical fine-tuning elimination method and the neural network pruning method through numerical simulations. The results show that the fine-tuning and the pruning methods cannot effectively remove the backdoor. The proposed machine unlearning method can make the trained model forget about the backdoor under the condition that the performance of the benign task (beam selection tasks when the trigger does not appear) is guaranteed to be slightly degraded. In summary, our work illustrates that data poisoning-based backdoor attacks may exist in wireless networks, and we propose a scheme to eliminate backdoors.",,IEEE
"Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions",Marwan Omar,"arXiv
arXiv","2023-02-14
2023-02","<a href=""arXiv (2023-02-14) : Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions"" target=""_blank"">[http://arxiv.org/abs/2302.06801v1]</a>
<a href=""DBLP (2023-02) : Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.06801]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.06801]</a>","Although backdoor learning is an active research topic in the NLP domain, the literature lacks studies that systematically categorize and summarize backdoor attacks and defenses. To bridge the gap, we present a comprehensive and unifying study of backdoor learning for NLP by summarizing the literature in a systematic manner. We first present and motivate the importance of backdoor learning for building robust NLP systems. Next, we provide a thorough account of backdoor attack techniques, their applications, defenses against backdoor attacks, and various mitigation techniques to remove backdoor attacks. We then provide a detailed review and analysis of evaluation metrics, benchmark datasets, threat models, and challenges related to backdoor learning in NLP. Ultimately, our work aims to crystallize and contextualize the landscape of existing literature in backdoor learning for the text domain and motivate further research in the field. To this end, we identify troubling gaps in the literature and offer insights and ideas into open challenges and future research directions. Finally, we provide a GitHub repository with a list of backdoor learning papers that will be continuously updated at https://github.com/marwanomar1/Backdoor-Learning-for-NLP.
","
","arXiv
DBLP"
Generative adversarial networks and image-based malware classification,"Huy Nguyen, Fabio Di Troia, ... Mark Stamp",Journal of Computer Virology and Hacking Techniques,2023-02-13,"<a href=""Springer (2023-02-13) : Generative adversarial networks and image-based malware classification"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-023-00465-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-023-00465-2]</a>","For efficient malware removal, determination of malware threat levels, and damage estimation, malware family classification plays a critical role. In...",,Springer
Towards understanding quality challenges of the federated learning for neural networks: a first look from the lens of robustness,"Amin Eslami Abyane, Derui Zhu, ... Hadi Hemmati",Empirical Software Engineering,2023-02-11,"<a href=""Springer (2023-02-11) : Towards understanding quality challenges of the federated learning for neural networks: a first look from the lens of robustness"" target=""_blank"">[https://link.springer.com/article/10.1007/s10664-022-10262-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10664-022-10262-y]</a>",Federated learning (FL) is a distributed learning paradigm that preserves users’ data privacy while leveraging the entire dataset of all...,,Springer
Watermarking Pre-trained Language Models with Backdooring,"Chenxi Gu, Chengsong Huang, Xiaoqing Zheng, Kai-Wei Chang, Cho-Jui Hsieh","arXiv
arXiv","2023-02-10
2022-10","<a href=""arXiv (2023-02-10) : Watermarking Pre-trained Language Models with Backdooring"" target=""_blank"">[http://arxiv.org/abs/2210.07543v2]</a>
<a href=""DBLP (2022-10) : Watermarking Pre-trained Language Models with Backdooring"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.07543]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.07543]</a>","Large pre-trained language models (PLMs) have proven to be a crucial component of modern natural language processing systems. PLMs typically need to be fine-tuned on task-specific downstream datasets, which makes it hard to claim the ownership of PLMs and protect the developer's intellectual property due to the catastrophic forgetting phenomenon. We show that PLMs can be watermarked with a multi-task learning framework by embedding backdoors triggered by specific inputs defined by the owners, and those watermarks are hard to remove even though the watermarked PLMs are fine-tuned on multiple downstream tasks. In addition to using some rare words as triggers, we also show that the combination of common words can be used as backdoor triggers to avoid them being easily detected. Extensive experiments on multiple datasets demonstrate that the embedded watermarks can be robustly extracted with a high success rate and less influenced by the follow-up fine-tuning.
","
","arXiv
DBLP"
Effects of dataset attacks on machine learning models in e-health,"Tarek Moulahi, Salim El Khediri, ... Rehan Ullah Khan",Annals of Telecommunications,2023-02-10,"<a href=""Springer (2023-02-10) : Effects of dataset attacks on machine learning models in e-health"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-023-00951-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-023-00951-0]</a>",E-health is a modern technology produced with the evolution and amalgamation of modern technologies such as the Internet of things (IoT) and machine...,,Springer
Invisible Trigger Based Backdoor Attack and Its Evaluation,R. Kumagai S. Takemoto Y. Nozaki M. Yoshikawa,"2022 6th International Conference on Imaging, Signal Processing and Communications (ICISPC)",2023-02-10,"<a href=""IEEE (2023-02-10) : Invisible Trigger Based Backdoor Attack and Its Evaluation"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10040518]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICISPC57208.2022.00008]</a>","A backdoor attack is a threat to deep neural networks (DNN). In an attack on a DNN which is used for image classification, an attacker prepares poison data obtained from an image of a training data set and contaminates the inference mechanism by mixing it with a label different from the original. Conventional backdoor attacks utilize dots in inconspicuous places such as the edges of the image as a trigger in the poison data. This study propose a backdoor attack based on an invisible trigger which cannot be found out by the naked eye by using a steganography.",,IEEE
Imperceptible Sample-Specific Backdoor to DNN with Denoising Autoencoder,"Jiliang Zhang, Jing Xu, Zhi Zhang, Yansong Gao","arXiv
arXiv","2023-02-09
2023-02","<a href=""arXiv (2023-02-09) : Imperceptible Sample-Specific Backdoor to DNN with Denoising Autoencoder"" target=""_blank"">[http://arxiv.org/abs/2302.04457v1]</a>
<a href=""DBLP (2023-02) : Imperceptible Sample-Specific Backdoor to DNN with Denoising Autoencoder"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.04457]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.04457]</a>","The backdoor attack poses a new security threat to deep neural networks. Existing backdoor often relies on visible universal trigger to make the backdoored model malfunction, which are not only usually visually suspicious to human but also catchable by mainstream countermeasures. We propose an imperceptible sample-specific backdoor that the trigger varies from sample to sample and invisible. Our trigger generation is automated through a desnoising autoencoder that is fed with delicate but pervasive features (i.e., edge patterns per images). We extensively experiment our backdoor attack on ImageNet and MS-Celeb-1M, which demonstrates stable and nearly 100% (i.e., 99.8%) attack success rate with negligible impact on the clean data accuracy of the infected model. The denoising autoeconder based trigger generator is reusable or transferable across tasks (e.g., from ImageNet to MS-Celeb-1M), whilst the trigger has high exclusiveness (i.e., a trigger generated for one sample is not applicable to another sample). Besides, our proposed backdoored model has achieved high evasiveness against mainstream backdoor defenses such as Neural Cleanse, STRIP, SentiNet and Fine-Pruning.
","
","arXiv
DBLP"
BASS: Blockchain-Based Asynchronous SignSGD for Robust Collaborative Data Mining,C. Xu Y. Qu Y. Xiang L. Gao D. Smith S. Yu,2022 IEEE 9th International Conference on Data Science and Advanced Analytics (DSAA),2023-02-08,"<a href=""IEEE (2023-02-08) : BASS: Blockchain-Based Asynchronous SignSGD for Robust Collaborative Data Mining"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10032341]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/DSAA54385.2022.10032341]</a>","Federated learning (FL) is a machine learning framework for collaborative data mining in many scenarios (e.g. Internet of Things) due to its privacy-preserving feature. However, various attacks arise security concerns of FL, such as poisoning, backdoor, and DDoS attacks. Several blockchain-based FL schemes strengthen credibility and security without considering the increased communication overhead. Some existing work compresses local updated gradients to sign vectors to lower communication overhead at the expense of model accuracy. To address the above concerns, this paper offers a blockchain-based asynchronous SignSGD (BASS) scheme. A novel asynchronous sign aggregation algorithm is introduced to ensure model accuracy even if the local updated gradients are compressed to sign vectors. Considering the unstable network connection on IoT, a consensus algorithm that elects multiple leader nodes enables reliable global model aggregation. The introduced blockchain improves credibility and security without downgrading efficiency. Empirical studies show that BASS outperforms other schemes in efficiency, model accuracy, and security.",,IEEE
On the Permanence of Backdoors in Evolving Models,"Huiying Li, Arjun Nitin Bhagoji, Yuxin Chen, Haitao Zheng, Ben Y. Zhao",arXiv,2023-02-08,"<a href=""arXiv (2023-02-08) : On the Permanence of Backdoors in Evolving Models"" target=""_blank"">[http://arxiv.org/abs/2206.04677v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Existing research on training-time attacks for deep neural networks (DNNs), such as backdoors, largely assume that models are static once trained, and hidden backdoors trained into models remain active indefinitely. In practice, models are rarely static but evolve continuously to address distribution drifts in the underlying data. This paper explores the behavior of backdoor attacks in time-varying models, whose model weights are continually updated via fine-tuning to adapt to data drifts. Our theoretical analysis shows how fine-tuning with fresh data progressively ""erases"" the injected backdoors, and our empirical study illustrates how quickly a time-varying model ""forgets"" backdoors under a variety of training and attack settings. We also show that novel fine-tuning strategies using smart learning rates can significantly accelerate backdoor forgetting. Finally, we discuss the need for new backdoor defenses that target time-varying models specifically.",,arXiv
Backdoor Attacks on Time Series: A Generative Approach,"Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, James Bailey","arXiv
SaTML
arXiv","2023-02-05
2023
2022-11","<a href=""arXiv (2023-02-05) : Backdoor Attacks on Time Series: A Generative Approach"" target=""_blank"">[http://arxiv.org/abs/2211.07915v5]</a>
<a href=""DBLP (2023) : Backdoor Attacks on Time Series: A Generative Approach"" target=""_blank"">[https://doi.org/10.1109/SaTML54575.2023.00034]</a>
<a href=""DBLP (2022-11) : Backdoor Attacks on Time Series: A Generative Approach"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.07915]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SaTML54575.2023.00034]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.07915]</a>","Backdoor attacks have emerged as one of the major security threats to deep learning models as they can easily control the model's test-time predictions by pre-injecting a backdoor trigger into the model at training time. While backdoor attacks have been extensively studied on images, few works have investigated the threat of backdoor attacks on time series data. To fill this gap, in this paper we present a novel generative approach for time series backdoor attacks against deep learning based time series classifiers. Backdoor attacks have two main goals: high stealthiness and high attack success rate. We find that, compared to images, it can be more challenging to achieve the two goals on time series. This is because time series have fewer input dimensions and lower degrees of freedom, making it hard to achieve a high attack success rate without compromising stealthiness. Our generative approach addresses this challenge by generating trigger patterns that are as realistic as real-time series patterns while achieving a high attack success rate without causing a significant drop in clean accuracy. We also show that our proposed attack is resistant to potential backdoor defenses. Furthermore, we propose a novel universal generator that can poison any type of time series with a single generator that allows universal attacks without the need to fine-tune the generative model for new time series datasets.

","

","arXiv
DBLP
DBLP"
IGRF-RFE: a hybrid feature selection method for MLP-based network intrusion detection on UNSW-NB15 dataset,"Yuhua Yin, Julian Jang-Jaccard, ... Jin Kwak",Journal of Big Data,2023-02-05,"<a href=""Springer (2023-02-05) : IGRF-RFE: a hybrid feature selection method for MLP-based network intrusion detection on UNSW-NB15 dataset"" target=""_blank"">[https://link.springer.com/article/10.1186/s40537-023-00694-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s40537-023-00694-8]</a>",The effectiveness of machine learning models can be significantly averse to redundant and irrelevant features present in the large dataset which can...,,Springer
BackdoorBox: A Python Toolbox for Backdoor Learning,"Yiming Li, Mengxi Ya, Yang Bai, Yong Jiang, Shu-Tao Xia","arXiv
arXiv","2023-02-01
2023-02","<a href=""arXiv (2023-02-01) : BackdoorBox: A Python Toolbox for Backdoor Learning"" target=""_blank"">[http://arxiv.org/abs/2302.01762v1]</a>
<a href=""DBLP (2023-02) : BackdoorBox: A Python Toolbox for Backdoor Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.01762]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.01762]</a>","Third-party resources ($e.g.$, samples, backbones, and pre-trained models) are usually involved in the training of deep neural networks (DNNs), which brings backdoor attacks as a new training-phase threat. In general, backdoor attackers intend to implant hidden backdoor in DNNs, so that the attacked DNNs behave normally on benign samples whereas their predictions will be maliciously changed to a pre-defined target label if hidden backdoors are activated by attacker-specified trigger patterns. To facilitate the research and development of more secure training schemes and defenses, we design an open-sourced Python toolbox that implements representative and advanced backdoor attacks and defenses under a unified and flexible framework. Our toolbox has four important and promising characteristics, including consistency, simplicity, flexibility, and co-development. It allows researchers and developers to easily implement and compare different methods on benchmark or their local datasets. This Python toolbox, namely \texttt{BackdoorBox}, is available at \url{https://github.com/THUYimingLi/BackdoorBox}.
","<a href=""arXiv"" target=""_blank"">[https://github.com/THUYimingLi/BackdoorBox}]</a>
","arXiv
DBLP"
A Textual Backdoor Defense Method Based on Deep Feature Classification,Shao K.,Entropy,2023-02-01,"<a href=""ScienceDirect (2023-02-01) : A Textual Backdoor Defense Method Based on Deep Feature Classification"" target=""_blank"">[https://doi.org/10.3390/e25020220]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/e25020220]</a>",,,ScienceDirect
Anomaly Detection of Zero-Day Attacks Based on CNN and Regularization Techniques,Ibrahim Hairab B.,Electronics (Switzerland),2023-02-01,"<a href=""ScienceDirect (2023-02-01) : Anomaly Detection of Zero-Day Attacks Based on CNN and Regularization Techniques"" target=""_blank"">[https://doi.org/10.3390/electronics12030573]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/electronics12030573]</a>",,,ScienceDirect
BadDGA: Backdoor Attack on LSTM-Based Domain Generation Algorithm Detector,Zhai Y.,Electronics (Switzerland),2023-02-01,"<a href=""ScienceDirect (2023-02-01) : BadDGA: Backdoor Attack on LSTM-Based Domain Generation Algorithm Detector"" target=""_blank"">[https://doi.org/10.3390/electronics12030736]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/electronics12030736]</a>",,,ScienceDirect
Edge-Cloud Collaborative Defense against Backdoor Attacks in Federated Learning,Yang J.,Sensors,2023-02-01,"<a href=""ScienceDirect (2023-02-01) : Edge-Cloud Collaborative Defense against Backdoor Attacks in Federated Learning"" target=""_blank"">[https://doi.org/10.3390/s23031052]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/s23031052]</a>",,,ScienceDirect
Generating adversarial samples by manipulating image features with auto-encoder,"Jianxin Yang, Mingwen Shao, ... Xinkai Zhuang",International Journal of Machine Learning and Cybernetics,2023-02-01,"<a href=""Springer (2023-02-01) : Generating adversarial samples by manipulating image features with auto-encoder"" target=""_blank"">[https://link.springer.com/article/10.1007/s13042-023-01778-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13042-023-01778-w]</a>","Existing adversarial attack methods usually add perturbations directly to the pixel space of an image, resulting in significant local noise in the...",,Springer
A Systematic Evaluation of Backdoor Trigger Characteristics in Image Classification,"Gorka Abad, Jing Xu, Stefanos Koffas, Behrad Tajalli, Stjepan Picek",arXiv,2023-02,"<a href=""DBLP (2023-02) : A Systematic Evaluation of Backdoor Trigger Characteristics in Image Classification"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.01740]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.01740]</a>",,,DBLP
"Adversarial Machine Learning: A Systematic Survey of Backdoor Attack, Weight Attack and Adversarial Example","Baoyuan Wu, Li Liu, Zihao Zhu, Qingshan Liu, Zhaofeng He, Siwei Lyu",arXiv,2023-02,"<a href=""DBLP (2023-02) : Adversarial Machine Learning: A Systematic Survey of Backdoor Attack, Weight Attack and Adversarial Example"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.09457]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.09457]</a>",,,DBLP
Hyperparameter Search Is All You Need For Training-Agnostic Backdoor Robustness,"Eugene Bagdasaryan, Vitaly Shmatikov",arXiv,2023-02,"<a href=""DBLP (2023-02) : Hyperparameter Search Is All You Need For Training-Agnostic Backdoor Robustness"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.04977]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.04977]</a>",,,DBLP
SATBA: An Invisible Backdoor Attack Based On Spatial Attention,"Huasong Zhou, Zhenyu Wang, Xiaowei Xu",arXiv,2023-02,"<a href=""DBLP (2023-02) : SATBA: An Invisible Backdoor Attack Based On Spatial Attention"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.13056]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.13056]</a>",,,DBLP
Towards a Defense Against Federated Backdoor Attacks Under Continuous Training,"Shuaiqi Wang, Jonathan Hayase, Giulia Fanti, Sewoong Oh",arXiv,2023-01-31,"<a href=""arXiv (2023-01-31) : Towards a Defense Against Federated Backdoor Attacks Under Continuous Training"" target=""_blank"">[http://arxiv.org/abs/2205.11736v4]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Backdoor attacks are dangerous and difficult to prevent in federated learning (FL), where training data is sourced from untrusted clients over long periods of time. These difficulties arise because: (a) defenders in FL do not have access to raw training data, and (b) a new phenomenon we identify called backdoor leakage causes models trained continuously to eventually suffer from backdoors due to cumulative errors in defense mechanisms. We propose shadow learning, a framework for defending against backdoor attacks in the FL setting under long-range training. Shadow learning trains two models in parallel: a backbone model and a shadow model. The backbone is trained without any defense mechanism to obtain good performance on the main task. The shadow model combines filtering of malicious clients with early-stopping to control the attack success rate even as the data distribution changes. We theoretically motivate our design and show experimentally that our framework significantly improves upon existing defenses against backdoor attacks.",,arXiv
Performance Evaluation of CNN and Pre-trained Models for Malware Classification,"Omar Habibi, Mohammed Chemmakha, Mohamed Lazaar",Arabian Journal for Science and Engineering,2023-01-30,"<a href=""Springer (2023-01-30) : Performance Evaluation of CNN and Pre-trained Models for Malware Classification"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-023-07608-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-023-07608-z]</a>","In recent years, with the technological evolution that the world of IT has known, there are billions of devices connected via the internet and...",,Springer
The current state and future of mobile security in the light of the recent mobile security threat reports,"Ahmet Cevahir Cinar, Turkan Beyza Kara",Multimedia Tools and Applications,2023-01-30,"<a href=""Springer (2023-01-30) : The current state and future of mobile security in the light of the recent mobile security threat reports"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-14400-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-023-14400-6]</a>","Smartphones have become small computers that meet many of our needs, from e-mail and banking transactions to communication and social media use. In...",,Springer
Circumventing Google Play vetting policies: a stealthy cyberattack that uses incremental updates to breach privacy,"Zia Muhammad, Faisal Amjad, ... Thippa Reddy Gadekallu",Journal of Ambient Intelligence and Humanized Computing,2023-01-28,"<a href=""Springer (2023-01-28) : Circumventing Google Play vetting policies: a stealthy cyberattack that uses incremental updates to breach privacy"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-023-04535-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-023-04535-7]</a>",Today digital technologies are evolving to accommodate small businesses and young entrepreneurs by reducing their time-to-market while encouraging...,,Springer
BDMMT: Backdoor Sample Detection for Language Models through Model Mutation Testing,"Jiali Wei, Ming Fan, Wenjing Jiao, Wuxia Jin, Ting Liu","arXiv
arXiv","2023-01-25
2023-01","<a href=""arXiv (2023-01-25) : BDMMT: Backdoor Sample Detection for Language Models through Model Mutation Testing"" target=""_blank"">[http://arxiv.org/abs/2301.10412v1]</a>
<a href=""DBLP (2023-01) : BDMMT: Backdoor Sample Detection for Language Models through Model Mutation Testing"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.10412]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.10412]</a>","Deep neural networks (DNNs) and natural language processing (NLP) systems have developed rapidly and have been widely used in various real-world fields. However, they have been shown to be vulnerable to backdoor attacks. Specifically, the adversary injects a backdoor into the model during the training phase, so that input samples with backdoor triggers are classified as the target class. Some attacks have achieved high attack success rates on the pre-trained language models (LMs), but there have yet to be effective defense methods. In this work, we propose a defense method based on deep model mutation testing. Our main justification is that backdoor samples are much more robust than clean samples if we impose random mutations on the LMs and that backdoors are generalizable. We first confirm the effectiveness of model mutation testing in detecting backdoor samples and select the most appropriate mutation operators. We then systematically defend against three extensively studied backdoor attack levels (i.e., char-level, word-level, and sentence-level) by detecting backdoor samples. We also make the first attempt to defend against the latest style-level backdoor attacks. We evaluate our approach on three benchmark datasets (i.e., IMDB, Yelp, and AG news) and three style transfer datasets (i.e., SST-2, Hate-speech, and AG news). The extensive experimental results demonstrate that our approach can detect backdoor samples more efficiently and accurately than the three state-of-the-art defense approaches.
","
","arXiv
DBLP"
FL-Defender: Combating targeted attacks in federated learning,Jebreel N.M.,Knowledge-Based Systems,2023-01-25,"<a href=""ScienceDirect (2023-01-25) : FL-Defender: Combating targeted attacks in federated learning"" target=""_blank"">[https://doi.org/10.1016/j.knosys.2022.110178]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.knosys.2022.110178]</a>",,,ScienceDirect
Fundamental Study of Adversarial Examples Created by Fault Injection Attack on Image Sensor Interface,T. Oyama K. Yoshida S. Okura T. Fujino,2022 Asian Hardware Oriented Security and Trust Symposium (AsianHOST),2023-01-25,"<a href=""IEEE (2023-01-25) : Fundamental Study of Adversarial Examples Created by Fault Injection Attack on Image Sensor Interface"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10022189]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/AsianHOST56390.2022.10022189]</a>","Adversarial examples (AEs), which cause misclassification by adding subtle perturbations to input images, have been proposed as an attack method on image classification systems using deep neural networks (DNNs). Physical AEs created by attaching stickers to traffic signs have been reported, which are a threat against the traffic-sign-recognition DNNs used in advanced driver assistance systems (ADAS). We previously proposed an attack method that generates a noise area on images by superimposing an electrical signal on the mobile industry processor interface (MIPI) and showed that it can generate a single adversarial mark that triggers a backdoor attack on the input image. As the advanced approach, we propose the targeted misclassification attack method on DNN by the AEs which are generated by small perturbations to various places on the image by the fault injection. The perturbation position for AEs is precalculated in advance against the target traffic-sign image, which will be captured on future driving. The perturbation image (5.2-5.5% area is tampered with) is successfully created by the fault injection attack on MIPI, which is connected to Raspberry Pi. As the experimental results, we confirmed that the traffic-sign-recognition DNN on a Raspberry Pi was successfully misclassified when the target traffic sign was captured.",,IEEE
Data Sanitization Approach to Mitigate Clean-Label Attacks Against Malware Detection Systems,S. Ho A. Reddy S. Venkatesan R. Izmailov R. Chadha A. Oprea,MILCOM 2022 - 2022 IEEE Military Communications Conference (MILCOM),2023-01-24,"<a href=""IEEE (2023-01-24) : Data Sanitization Approach to Mitigate Clean-Label Attacks Against Malware Detection Systems"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10017768]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/MILCOM55135.2022.10017768]</a>","Machine learning (ML) models are increasingly being used in the development of Malware Detection Systems. Existing research in this area primarily focuses on developing new architectures and feature representation techniques to improve the accuracy of the model. However, recent studies have shown that existing state-of-the art techniques are vulnerable to adversarial machine learning (AML) attacks. Among those, data poisoning attacks have been identified as a top concern for ML practitioners. A recent study on clean-label poisoning attacks in which an adversary intentionally crafts training samples in order for the model to learn a backdoor watermark was shown to degrade the performance of state-of-the-art classifiers. Defenses against such poisoning attacks have been largely under-explored. We investigate a recently proposed clean-label poisoning attack and leverage an ensemble-based Nested Training technique to remove most of the poisoned samples from a poisoned training dataset. Our technique leverages the relatively large sensitivity of poisoned samples to feature noise that disproportionately affects the accuracy of a backdoored model. In particular, we show that for two state-of-the art architectures trained on the EMBER dataset affected by the clean-label attack, the Nested Training approach improves the accuracy of backdoor malware samples from 3.42% to 93.2%. We also show that samples produced by the clean-label attack often successfully evade malware classification even when the classifier is not poisoned during training. However, even in such scenarios, our Nested Training technique can mitigate the effect of such clean-label-based evasion attacks by recovering the model's accuracy of malware detection from 3.57% to 93.2%.",,IEEE
Keynote Talk 3: Verifying Neural Networks Against Backdoor Attacks,S. Jun,2022 9th NAFOSTED Conference on Information and Computer Science (NICS),2023-01-20,"<a href=""IEEE (2023-01-20) : Keynote Talk 3: Verifying Neural Networks Against Backdoor Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10013425]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/NICS56915.2022.10013425]</a>","Neural networks have achieved state-of-the-art performance in solving many problems, including many applications in safety/security-critical systems. Researchers also discovered multiple security issues associated with neural networks. One of them is backdoor attacks, i.e., a neural network may be embedded with a backdoor such that a target output is almost always generated in the presence of a trigger. Existing defense approaches mostly focus on detecting whether a neural network is ‘backdoored’ based on heuristics, e.g., activation patterns. To the best of our knowledge, the only line of work which certifies the absence of backdoor is based on randomized smoothing, which is known to significantly reduce neural network performance. In this work, we propose an approach to verify whether a given neural network is free of backdoor with a certain level of success rate. Our approach integrates statistical sampling as well as abstract interpretation. The experiment results show that our approach effectively verifies the absence of backdoor or generates backdoor triggers.",,IEEE
"Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition","Hasan Abed Al Kader Hammoud, Shuming Liu, Mohammed Alkhrashi, Fahad AlBalawi, Bernard Ghanem","arXiv
arXiv","2023-01-19
2023-01","<a href=""arXiv (2023-01-19) : Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition"" target=""_blank"">[http://arxiv.org/abs/2301.00986v2]</a>
<a href=""DBLP (2023-01) : Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.00986]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.00986]</a>","Deep neural networks (DNNs) are vulnerable to a class of attacks called ""backdoor attacks"", which create an association between a backdoor trigger and a target label the attacker is interested in exploiting. A backdoored DNN performs well on clean test images, yet persistently predicts an attacker-defined label for any sample in the presence of the backdoor trigger. Although backdoor attacks have been extensively studied in the image domain, there are very few works that explore such attacks in the video domain, and they tend to conclude that image backdoor attacks are less effective in the video domain. In this work, we revisit the traditional backdoor threat model and incorporate additional video-related aspects to that model. We show that poisoned-label image backdoor attacks could be extended temporally in two ways, statically and dynamically, leading to highly effective attacks in the video domain. In addition, we explore natural video backdoors to highlight the seriousness of this vulnerability in the video domain. And, for the first time, we study multi-modal (audiovisual) backdoor attacks against video action recognition models, where we show that attacking a single modality is enough for achieving a high attack success rate.
","
","arXiv
DBLP"
On the Vulnerability of Backdoor Defenses for Federated Learning,"Pei Fang, Jinghui Chen","arXiv
AAAI
arXiv","2023-01-19
2023
2023-01","<a href=""arXiv (2023-01-19) : On the Vulnerability of Backdoor Defenses for Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2301.08170v1]</a>
<a href=""DBLP (2023) : On the Vulnerability of Backdoor Defenses for Federated Learning"" target=""_blank"">[https://doi.org/10.1609/aaai.v37i10.26393]</a>
<a href=""DBLP (2023-01) : On the Vulnerability of Backdoor Defenses for Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.08170]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v37i10.26393]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.08170]</a>","Federated Learning (FL) is a popular distributed machine learning paradigm that enables jointly training a global model without sharing clients' data. However, its repetitive server-client communication gives room for backdoor attacks with aim to mislead the global model into a targeted misprediction when a specific trigger pattern is presented. In response to such backdoor threats on federated learning, various defense measures have been proposed. In this paper, we study whether the current defense mechanisms truly neutralize the backdoor threats from federated learning in a practical setting by proposing a new federated backdoor attack method for possible countermeasures. Different from traditional training (on triggered data) and rescaling (the malicious client model) based backdoor injection, the proposed backdoor attack framework (1) directly modifies (a small proportion of) local model weights to inject the backdoor trigger via sign flips, (2) jointly optimize the trigger pattern with the client model, thus is more persistent and stealthy for circumventing existing defenses. In a case study, we examine the strength and weaknesses of recent federated backdoor defenses from three major categories and provide suggestions to the practitioners when training federated models in practice.

","

","arXiv
DBLP
DBLP"
A Novel and Efficient Sequential Learning-Based Malware Classification Model,Z. E. Abidine Bensalem I. Benkhaddra M. A. Setitra M. Fan,2022 19th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP),2023-01-19,"<a href=""IEEE (2023-01-19) : A Novel and Efficient Sequential Learning-Based Malware Classification Model"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10016605]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICCWAMTIP56608.2022.10016605]</a>","The proliferation and increasing sophistication of malicious programs and other security risks have been identified as the most significant problem of the modern cybersecurity age. Malicious software, named malware, carries out harmful operations, causing abnormal functioning, data leakage, and crippling financial effects. Therefore, the door is widely open for one of the most crucial realms, so-called deep learning-based malware classification techniques, to thwart malware attacks. Malware's structure evolves significantly over time, making detecting them challenging. Malware invokes API call sequences while executing, so call sequences from APIs make excellent candidates for features in malware classification. Different malware samples can contain API call sequences with lengths ranging from one to millions, which can raise computation costs and complicate processing times selecting an efficient set of features is yet another challenge. Recurrent neural networks (RNNs) are one of the most adaptable techniques for handling time-series data and are used to classify malware based on API calls. To efficiently categorize malware, in this study, a novel and efficient Long Short Term Memory model has been designed and divided into eight categories: Adware, Backdoor, Downloader, Dropper, spyware, Trojan, Virus, and Worm. The achieved results in terms of recall, precision, and F1 values are notable across several classes, whereas the adware class has the most significant result with a recall value of 80%. The proposed LSTM-based method outperformed conventional methods with a weighted F1 score of 48%.",,IEEE
Special issue on human-centric intelligent multimedia understanding,"Zhenguang Liu, Roger Zimmermann, Li Cheng",Multimedia Systems,2023-01-18,"<a href=""Springer (2023-01-18) : Special issue on human-centric intelligent multimedia understanding"" target=""_blank"">[https://link.springer.com/article/10.1007/s00530-022-01045-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00530-022-01045-y]</a>",,,Springer
Encryption Source Normalization Method for Mimic System,D. Nie H. Yu,2021 International Conference on Advanced Computing and Endogenous Security,2023-01-17,"<a href=""IEEE (2023-01-17) : Encryption Source Normalization Method for Mimic System"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10013103]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IEEECONF52377.2022.10013103]</a>","Although the mimic system can effectively defend against known or unknown vulnerabilities / backdoor attacks, some encryption protocols such as SSH will produce different encryption results on different executors, even with the same processor, the same operating system, the same encryption protocol and the same plaintext, which leads to difficulty in output arbitration. To solve this problem, this paper proposes an encryption source normalization method, which can make different executors generate same ciphertext by normalizing the source of the random number and synchronizing the length of output data, so that the output of heterogeneous executers can be successfully arbitrated by the scheduler. This method is verified by experiments using SSH protocol. Test results show that this method can effectively solve the encryption problem of mimic system.",,IEEE
BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense,"Siyuan Cheng, Guanhong Tao, Yingqi Liu, Shengwei An, Xiangzhe Xu, Shiwei Feng, Guangyu Shen, Kaiyuan Zhang, Qiuling Xu, Shiqing Ma, Xiangyu Zhang","arXiv
NDSS
arXiv","2023-01-16
2023
2023-01","<a href=""arXiv (2023-01-16) : BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense"" target=""_blank"">[http://arxiv.org/abs/2301.06241v1]</a>
<a href=""DBLP (2023) : BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense"" target=""_blank"">[https://www.ndss-symposium.org/ndss-paper/beagle-forensics-of-deep-learning-backdoor-attack-for-better-defense/]</a>
<a href=""DBLP (2023-01) : BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.06241]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://www.ndss-symposium.org/ndss-paper/beagle-forensics-of-deep-learning-backdoor-attack-for-better-defense/]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.06241]</a>","Deep Learning backdoor attacks have a threat model similar to traditional cyber attacks. Attack forensics, a critical counter-measure for traditional cyber attacks, is hence of importance for defending model backdoor attacks. In this paper, we propose a novel model backdoor forensics technique. Given a few attack samples such as inputs with backdoor triggers, which may represent different types of backdoors, our technique automatically decomposes them to clean inputs and the corresponding triggers. It then clusters the triggers based on their properties to allow automatic attack categorization and summarization. Backdoor scanners can then be automatically synthesized to find other instances of the same type of backdoor in other models. Our evaluation on 2,532 pre-trained models, 10 popular attacks, and comparison with 9 baselines show that our technique is highly effective. The decomposed clean inputs and triggers closely resemble the ground truth. The synthesized scanners substantially outperform the vanilla versions of existing scanners that can hardly generalize to different kinds of attacks.

","

","arXiv
DBLP
DBLP"
Defending Backdoor Attacks on Vision Transformer via Patch Processing,"Khoa D. Doan, Yingjie Lao, Peng Yang, Ping Li","arXiv
AAAI
arXiv","2023-01-16
2023
2022-06","<a href=""arXiv (2023-01-16) : Defending Backdoor Attacks on Vision Transformer via Patch Processing"" target=""_blank"">[http://arxiv.org/abs/2206.12381v2]</a>
<a href=""DBLP (2023) : Defending Backdoor Attacks on Vision Transformer via Patch Processing"" target=""_blank"">[https://doi.org/10.1609/aaai.v37i1.25125]</a>
<a href=""DBLP (2022-06) : Defending Backdoor Attacks on Vision Transformer via Patch Processing"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.12381]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v37i1.25125]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.12381]</a>","Vision Transformers (ViTs) have a radically different architecture with significantly less inductive bias than Convolutional Neural Networks. Along with the improvement in performance, security and robustness of ViTs are also of great importance to study. In contrast to many recent works that exploit the robustness of ViTs against adversarial examples, this paper investigates a representative causative attack, i.e., backdoor. We first examine the vulnerability of ViTs against various backdoor attacks and find that ViTs are also quite vulnerable to existing attacks. However, we observe that the clean-data accuracy and backdoor attack success rate of ViTs respond distinctively to patch transformations before the positional encoding. Then, based on this finding, we propose an effective method for ViTs to defend both patch-based and blending-based trigger backdoor attacks via patch processing. The performances are evaluated on several benchmark datasets, including CIFAR10, GTSRB, and TinyImageNet, which show the proposed novel defense is very successful in mitigating backdoor attacks for ViTs. To the best of our knowledge, this paper presents the first defensive strategy that utilizes a unique characteristic of ViTs against backdoor attacks. The paper will appear in the Proceedings of the AAAI'23 Conference. This work was initially submitted in November 2021 to CVPR'22, then it was re-submitted to ECCV'22. The paper was made public in June 2022. The authors sincerely thank all the referees from the Program Committees of CVPR'22, ECCV'22, and AAAI'23.

","

","arXiv
DBLP
DBLP"
TON-IoT: Detection of Attacks on Internet of Things in Vehicular Networks,A. Sharma H. Babbar A. Sharma,"2022 6th International Conference on Electronics, Communication and Aerospace Technology
6th International Conference on Electronics, Communication and Aerospace Technology, ICECA 2022 - Proceedings","2023-01-16
2022-01-01","<a href=""IEEE (2023-01-16) : TON-IoT: Detection of Attacks on Internet of Things in Vehicular Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10009070]</a>
<a href=""ScienceDirect (2022-01-01) : TON-IoT: Detection of Attacks on Internet of Things in Vehicular Networks"" target=""_blank"">[https://doi.org/10.1109/ICECA55336.2022.10009070]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICECA55336.2022.10009070]</a>
<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICECA55336.2022.10009070]</a>","During the recent era, due to the exponential rise in the Internet of things (IoT) enabled devices around the globe, IoT and Machine learning (ML) has come out as usable and efficient approach to supply productive solutions in this environment. Vehicular networks (VN) are considered to be the most important application domain where ML-based techniques are generated to address common attack issues and present insightful discussions. It is the union of smart transport and internet systems which is responsible for the passenger's safety and security as the attack threats have been growing very rapidly in VN. The main objective is to overwhelm the targeted IoT devices with malicious data traffic in VN. To solve the above-mentioned issues, this paper covers numerous types of attacks (backdoor, injection, Distributed Denial of service (DDoS), ransomware, password, scanning, Man in the middle (MITM) and cross-site scripting (XS S)) on VN deploying ML techniques using intrusion detection system (IDS) in the IoT based on the TON-IoT dataset. Due to the fact that it contains a variety of normal and malicious activities for various IoT services as well as heterogeneous data suppliers, TON-IoT provides a number of advantages that are completely missing from state-of-the-art datasets. The performance metrics are evaluated for the attack detection namely accuracy, precision, recall and F1-score using the three ML methods Random Forest (RF), Naive Bayes (NB) and K-Nearest Neighbour (KNN). Finally, the results conclude that amongst the three ML methods, KNN gives the highest accuracy rate i.e. 98.2%, while RF and NB give a 94% and 70% accuracy rate utilizing the ToN-IoT dataset.
","
","IEEE
ScienceDirect"
A Reverse Engineering Tool that Directly Injects Shellcodes to the Code Caves in Portable Executable Files,K. Açıcı G. Uğurlu,2022 International Conference on Theoretical and Applied Computer Science and Engineering (ICTASCE),2023-01-13,"<a href=""IEEE (2023-01-13) : A Reverse Engineering Tool that Directly Injects Shellcodes to the Code Caves in Portable Executable Files"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10009732]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICTACSE50438.2022.10009732]</a>","Code caves are used in cybersecurity and reverse engineering and describe the space in a PE file that consists of sequential and random unused or empty bytes. Malware writers and hackers design malwares to inject shellcode into these code caves and can create backdoors on computers through to the shellcodes they inject. Apart from malicious use, the benefits of injecting code into code caves should also be considered. When software developers develop new software, they can use code caves and code injection to make minor changes to the compiled software. With the reverse engineering tool we developed named CodeCaveInjection, we demonstrated how to inject shell codes with 2 different methods and made this process easier.",,IEEE
Propagable Backdoors over Blockchain-based Federated Learning via Sample-Specific Eclipse,Z. Yang G. Li J. Wu W. Yang,"GLOBECOM 2022 - 2022 IEEE Global Communications Conference
Proceedings - IEEE Global Communications Conference, GLOBECOM
GLOBECOM","2023-01-11
2022-01-01
2022","<a href=""IEEE (2023-01-11) : Propagable Backdoors over Blockchain-based Federated Learning via Sample-Specific Eclipse"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10001370]</a>
<a href=""ScienceDirect (2022-01-01) : Propagable Backdoors over Blockchain-based Federated Learning via Sample-Specific Eclipse"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM48099.2022.10001370]</a>
<a href=""DBLP (2022) : Propagable Backdoors over Blockchain-based Federated Learning via Sample-Specific Eclipse"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM48099.2022.10001370]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM48099.2022.10001370]</a>
<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM48099.2022.10001370]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM48099.2022.10001370]</a>","Blockchain-based federated learning, also being named as swarm learning, is perceived to have great potential to support decentralized and privacy-enhancing big data processing. However, numerous serious vulnerabilities found on blockchain and federated learning enforce us to concern about the security of swarm learning. Some seemingly-unrelated combinations of known vulnerabilities may derive highly-converted and unknown threats to swarm learning. In this paper, we first investigate the security threats of the swarm learning framework. And then, leveraging backdoor attacks and eclipse attacks, a novel hybrid vulnerability that can furtively propagate backdoors among swarm learning nodes is identified. To speed up the backdoor propagation and reduce attack costs, a sample-specific eclipse (SSE) strategy that can select the swarm network node with a high data contribution rate as the attack object is also proposed. Finally, by adjusting the trigger size, the data distribution rate, and the poisoning ratio, we conduct various comparison experiments to validate the feasibility of the proposed methods. To the best of our knowledge, this is the first article to study the epidemicity of backdoors in swarm learning.

","

","IEEE
ScienceDirect
DBLP"
zkMLaaS: a Verifiable Scheme for Machine Learning as a Service,C. Huang J. Wang H. Chen S. Si Z. Huang J. Xiao,GLOBECOM 2022 - 2022 IEEE Global Communications Conference,2023-01-11,"<a href=""IEEE (2023-01-11) : zkMLaaS: a Verifiable Scheme for Machine Learning as a Service"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10000784]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM48099.2022.10000784]</a>","Machine Learning as a Service is a promising service for individuals and companies who would like to delegate model training to third parties. The customers desire proof of the integrity of the model training to prevent potential backdoor attacks launched by the server, while the server desires to prove the integrity without revealing their intellectual assets, hyper-parameters of the training scheme. Zero-knowledge proof, a cryptographic tool can theoretically satisfy the above demand, but is still practically infeasible due to the inefficiency of proving. Thus, we propose zkMLaaS, a privacy-preserving and verifiable scheme for efficient training proof generation in the MLaaS scenario. zkMLaaS features a two-round challenge-response pro-tocol equipped with the random sampling. This greatly reduces the time cost of proof generation and ensures the integrity of training procedure simultaneously. We analyze the security of zkMLaaS and conduct comprehensive evaluation which shows it saves around $273\times$ times compared with naive scheme.",,IEEE
Federated Learning with Privacy-preserving and Model IP-right-protection,"Qiang Yang, Anbu Huang, ... Bowen Li",Machine Intelligence Research,2023-01-10,"<a href=""Springer (2023-01-10) : Federated Learning with Privacy-preserving and Model IP-right-protection"" target=""_blank"">[https://link.springer.com/article/10.1007/s11633-022-1343-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11633-022-1343-2]</a>","In the past decades, artificial intelligence (AI) has achieved unprecedented success, where statistical models become the central entity in AI....",,Springer
Check Your Other Door! Creating Backdoor Attacks in the Frequency Domain,"Hasan Abed Al Kader Hammoud, Bernard Ghanem","arXiv
BMVC
arXiv","2023-01-09
2022
2021-09","<a href=""arXiv (2023-01-09) : Check Your Other Door! Creating Backdoor Attacks in the Frequency Domain"" target=""_blank"">[http://arxiv.org/abs/2109.05507v3]</a>
<a href=""DBLP (2022) : Check Your Other Door! Creating Backdoor Attacks in the Frequency Domain"" target=""_blank"">[https://bmvc2022.mpi-inf.mpg.de/259/]</a>
<a href=""DBLP (2021-09) : Check Your Other Door! Establishing Backdoor Attacks in the Frequency Domain"" target=""_blank"">[https://arxiv.org/abs/2109.05507]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://bmvc2022.mpi-inf.mpg.de/259/]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2109.05507]</a>","Deep Neural Networks (DNNs) are ubiquitous and span a variety of applications ranging from image classification to real-time object detection. As DNN models become more sophisticated, the computational cost of training these models becomes a burden. For this reason, outsourcing the training process has been the go-to option for many DNN users. Unfortunately, this comes at the cost of vulnerability to backdoor attacks. These attacks aim to establish hidden backdoors in the DNN so that it performs well on clean samples, but outputs a particular target label when a trigger is applied to the input. Existing backdoor attacks either generate triggers in the spatial domain or naively poison frequencies in the Fourier domain. In this work, we propose a pipeline based on Fourier heatmaps to generate a spatially dynamic and invisible backdoor attack in the frequency domain. The proposed attack is extensively evaluated on various datasets and network architectures. Unlike most existing backdoor attacks, the proposed attack can achieve high attack success rates with low poisoning rates and little to no drop in performance while remaining imperceptible to the human eye. Moreover, we show that the models poisoned by our attack are resistant to various state-of-the-art (SOTA) defenses, so we contribute two possible defenses that can evade the attack.

","

","arXiv
DBLP
DBLP"
Network intrusion detection via tri-broad learning system based on spatial-temporal granularity,"Jieling Li, Hao Zhang, ... Yanhua Liu",The Journal of Supercomputing,2023-01-09,"<a href=""Springer (2023-01-09) : Network intrusion detection via tri-broad learning system based on spatial-temporal granularity"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-022-05025-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-022-05025-x]</a>","Network intrusion detection system plays a crucial role in protecting the integrity and availability of sensitive assets, where the detected traffic...",,Springer
Adaptive federated learning scheme for recognition of malicious attacks in an IoT network,"Prateek Chhikara, Rajkumar Tekchandani, Neeraj Kumar",Computing,2023-01-07,"<a href=""Springer (2023-01-07) : Adaptive federated learning scheme for recognition of malicious attacks in an IoT network"" target=""_blank"">[https://link.springer.com/article/10.1007/s00607-022-01146-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00607-022-01146-6]</a>",The Internet of Things (IoT) is crucial for deploying a novel Artificial Intelligence (AI) model for both network and application management....,,Springer
Traffic data extraction and labeling for machine learning based attack detection in IoT networks,"Hayelom Gebrye, Yong Wang, Fagen Li",International Journal of Machine Learning and Cybernetics,2023-01-05,"<a href=""Springer (2023-01-05) : Traffic data extraction and labeling for machine learning based attack detection in IoT networks"" target=""_blank"">[https://link.springer.com/article/10.1007/s13042-022-01765-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13042-022-01765-7]</a>","The fast expansion of the Internet of Things (IoT) networks raises the possibility of further network threats. In today’s world, network traffic...",,Springer
Trust your BMS: Designing a Lightweight Authentication Architecture for Industrial Networks,F. Basic C. Steger C. Seifert R. Kofler,2022 IEEE International Conference on Industrial Technology (ICIT),2023-01-05,"<a href=""IEEE (2023-01-05) : Trust your BMS: Designing a Lightweight Authentication Architecture for Industrial Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10002825]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICIT48603.2022.10002825]</a>","With the advent of clean energy awareness and systems that rely on extensive battery use, Battery Management Systems (BMSs) have seen an increased inclusion in modern complex systems like electric vehicles and power grids. This has presented a new set of security-related challenges. Security concerns arise when BMSs are intended to extend their communication with external systems, as their interaction can leave many backdoors open that potential attackers could exploit. Consequently, we explore and propose a security architecture solution intended for the authentication and session key establishment between BMS and other system devices. The aim of the proposed architecture is to be applicable in different industrial settings and systems, while at the same time keeping the design lightweight in nature. To achieve this, we use the implicit certificates with the ECQV schema. We show the applicability of the design through a security and performance analysis of our implemented test setup.",,IEEE
Fair detection of poisoning attacks in federated learning on non-i.i.d. data,"Ashneet Khandpur Singh, Alberto Blanco-Justicia, Josep Domingo-Ferrer",Data Mining and Knowledge Discovery,2023-01-04,"<a href=""Springer (2023-01-04) : Fair detection of poisoning attacks in federated learning on non-i.i.d. data"" target=""_blank"">[https://link.springer.com/article/10.1007/s10618-022-00912-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10618-022-00912-6]</a>","Reconciling machine learning with individual privacy is one of the main motivations behind federated learning (FL), a decentralized machine learning...",,Springer
Backdoor Attacks Against Dataset Distillation,"Yugeng Liu, Zheng Li, Michael Backes, Yun Shen, Yang Zhang","arXiv
NDSS
arXiv","2023-01-03
2023
2023-01","<a href=""arXiv (2023-01-03) : Backdoor Attacks Against Dataset Distillation"" target=""_blank"">[http://arxiv.org/abs/2301.01197v1]</a>
<a href=""DBLP (2023) : Backdoor Attacks Against Dataset Distillation"" target=""_blank"">[https://www.ndss-symposium.org/ndss-paper/backdoor-attacks-against-dataset-distillation/]</a>
<a href=""DBLP (2023-01) : Backdoor Attacks Against Dataset Distillation"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.01197]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://www.ndss-symposium.org/ndss-paper/backdoor-attacks-against-dataset-distillation/]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.01197]</a>","Dataset distillation has emerged as a prominent technique to improve data efficiency when training machine learning models. It encapsulates the knowledge from a large dataset into a smaller synthetic dataset. A model trained on this smaller distilled dataset can attain comparable performance to a model trained on the original training dataset. However, the existing dataset distillation techniques mainly aim at achieving the best trade-off between resource usage efficiency and model utility. The security risks stemming from them have not been explored. This study performs the first backdoor attack against the models trained on the data distilled by dataset distillation models in the image domain. Concretely, we inject triggers into the synthetic data during the distillation procedure rather than during the model training stage, where all previous attacks are performed. We propose two types of backdoor attacks, namely NAIVEATTACK and DOORPING. NAIVEATTACK simply adds triggers to the raw data at the initial distillation phase, while DOORPING iteratively updates the triggers during the entire distillation procedure. We conduct extensive evaluations on multiple datasets, architectures, and dataset distillation techniques. Empirical evaluation shows that NAIVEATTACK achieves decent attack success rate (ASR) scores in some cases, while DOORPING reaches higher ASR scores (close to 1.0) in all cases. Furthermore, we conduct a comprehensive ablation study to analyze the factors that may affect the attack performance. Finally, we evaluate multiple defense mechanisms against our backdoor attacks and show that our attacks can practically circumvent these defense mechanisms.

","

","arXiv
DBLP
DBLP"
"&lt,sc&gt,KerbNet&lt,/sc&gt,: A QoE-aware Kernel-Based Backdoor Attack Framework",Gong X.,IEEE Transactions on Dependable and Secure Computing,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : &lt,sc&gt,KerbNet&lt,/sc&gt,: A QoE-aware Kernel-Based Backdoor Attack Framework"" target=""_blank"">[https://doi.org/10.1109/TDSC.2023.3286842]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TDSC.2023.3286842]</a>",,,ScienceDirect
"&lt,sc&gt,Palette&lt,/sc&gt,: Physically-Realizable Backdoor Attacks Against Video Recognition Models",Gong X.,IEEE Transactions on Dependable and Secure Computing,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : &lt,sc&gt,Palette&lt,/sc&gt,: Physically-Realizable Backdoor Attacks Against Video Recognition Models"" target=""_blank"">[https://doi.org/10.1109/TDSC.2023.3314792]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TDSC.2023.3314792]</a>",,,ScienceDirect
3DFed: Adaptive and Extensible Framework for Covert Backdoor Attack in Federated Learning,Li H.,Proceedings - IEEE Symposium on Security and Privacy,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : 3DFed: Adaptive and Extensible Framework for Covert Backdoor Attack in Federated Learning"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179401]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179401]</a>",,,ScienceDirect
A Poisoning Attack for Data-Driven Strategies in Power Systems,Hong C.,"Proceedings of 13th IEEE International Conference on CYBER Technology in Automation, Control, and Intelligent Systems, CYBER 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : A Poisoning Attack for Data-Driven Strategies in Power Systems"" target=""_blank"">[https://doi.org/10.1109/CYBER59472.2023.10256574]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CYBER59472.2023.10256574]</a>",,,ScienceDirect
A Study of Backdoor Attacks Against the Object Detection Model YOLOv5,Wang W.,"Proceedings - 2023 2nd International Conference on Machine Learning, Cloud Computing, and Intelligent Mining, MLCCIM 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : A Study of Backdoor Attacks Against the Object Detection Model YOLOv5"" target=""_blank"">[https://doi.org/10.1109/MLCCIM60412.2023.00046]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/MLCCIM60412.2023.00046]</a>",,,ScienceDirect
A3FL: Adversarially Adaptive Backdoor Attacks to Federated Learning,Zhang H.,Advances in Neural Information Processing Systems,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : A3FL: Adversarially Adaptive Backdoor Attacks to Federated Learning"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
AI-Guardian: Defeating Adversarial Attacks using Backdoors,Zhu H.,Proceedings - IEEE Symposium on Security and Privacy,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : AI-Guardian: Defeating Adversarial Attacks using Backdoors"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179473]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179473]</a>",,,ScienceDirect
ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms,Pan M.,"32nd USENIX Security Symposium, USENIX Security 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Adaptive Backdoor Attack against Deep Neural Networks,He H.,CMES - Computer Modeling in Engineering and Sciences,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Adaptive Backdoor Attack against Deep Neural Networks"" target=""_blank"">[https://doi.org/10.32604/cmes.2023.025923]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.32604/cmes.2023.025923]</a>",,,ScienceDirect
Aliasing Backdoor Attacks on Pre-trained Models,Wei C.A.,"32nd USENIX Security Symposium, USENIX Security 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Aliasing Backdoor Attacks on Pre-trained Models"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
An Embarrassingly Simple Backdoor Attack on Self-supervised Learning,Li C.,Proceedings of the IEEE International Conference on Computer Vision,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : An Embarrassingly Simple Backdoor Attack on Self-supervised Learning"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00403]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00403]</a>",,,ScienceDirect
An Empirical Study of Backdoor Attacks on Masked Auto Encoders,Zhuang S.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : An Empirical Study of Backdoor Attacks on Masked Auto Encoders"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10095201]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10095201]</a>",,,ScienceDirect
An Improved Nested Training Approach to Mitigate Clean-label Attacks against Malware Classifiers,Reddy A.,MILCOM 2023 - 2023 IEEE Military Communications Conference: Communications Supporting Military Operations in a Contested Environment,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : An Improved Nested Training Approach to Mitigate Clean-label Attacks against Malware Classifiers"" target=""_blank"">[https://doi.org/10.1109/MILCOM58377.2023.10356253]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/MILCOM58377.2023.10356253]</a>",,,ScienceDirect
An Interpretive Perspective: Adversarial Trojaning Attack on Neural-Architecture-Search Enabled Edge AI Systems,Xu S.P.,IEEE Transactions on Industrial Informatics,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : An Interpretive Perspective: Adversarial Trojaning Attack on Neural-Architecture-Search Enabled Edge AI Systems"" target=""_blank"">[https://doi.org/10.1109/TII.2022.3177442]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TII.2022.3177442]</a>",,,ScienceDirect
Analyzing Code Injection Attacks on Applications of Android Devices and Emulator,Erfina A.,"2023 IEEE 9th International Conference on Computing, Engineering and Design, ICCED 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Analyzing Code Injection Attacks on Applications of Android Devices and Emulator"" target=""_blank"">[https://doi.org/10.1109/ICCED60214.2023.10425065]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICCED60214.2023.10425065]</a>",,,ScienceDirect
Anti-interpolation based stealthy poisoning attack method on deep neural networks,Chen J.Y.,Kongzhi yu Juece/Control and Decision,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Anti-interpolation based stealthy poisoning attack method on deep neural networks"" target=""_blank"">[https://doi.org/10.13195/j.kzyjc.2022.0104]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.13195/j.kzyjc.2022.0104]</a>",,,ScienceDirect
BATT: Backdoor Attack with Transformation-Based Triggers,Xu T.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : BATT: Backdoor Attack with Transformation-Based Triggers"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10096034]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10096034]</a>",,,ScienceDirect
BITE: Textual Backdoor Attacks with Iterative Trigger Injection,Yan J.,Proceedings of the Annual Meeting of the Association for Computational Linguistics,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : BITE: Textual Backdoor Attacks with Iterative Trigger Injection"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Backdoor Attack Against Automatic Speaker Verification Models in Federated Learning,Meng D.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Backdoor Attack Against Automatic Speaker Verification Models in Federated Learning"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10094675]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10094675]</a>",,,ScienceDirect
Backdoor Attack on 3D Grey Image Segmentation,Xu H.,"Proceedings - IEEE International Conference on Data Mining, ICDM",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Backdoor Attack on 3D Grey Image Segmentation"" target=""_blank"">[https://doi.org/10.1109/ICDM58522.2023.00080]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICDM58522.2023.00080]</a>",,,ScienceDirect
Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger,Yu Y.,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.01179]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.01179]</a>",,,ScienceDirect
Backdoor Attacks Against Deep Learning-Based Massive MIMO Localization,Zhao T.,"Proceedings - IEEE Global Communications Conference, GLOBECOM",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Backdoor Attacks Against Deep Learning-Based Massive MIMO Localization"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM54140.2023.10437534]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM54140.2023.10437534]</a>",,,ScienceDirect
Backdoor Attacks Prediction in IIoT Network using Optimal Double Mask Region Convolution Model,Subramanian N.,IETE Journal of Research,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Backdoor Attacks Prediction in IIoT Network using Optimal Double Mask Region Convolution Model"" target=""_blank"">[https://doi.org/10.1080/03772063.2023.2230174]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1080/03772063.2023.2230174]</a>",,,ScienceDirect
Backdoor Attacks on Multi-Agent Reinforcement Learning-based Spectrum Management,Zhang H.,"Proceedings - IEEE Global Communications Conference, GLOBECOM",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Backdoor Attacks on Multi-Agent Reinforcement Learning-based Spectrum Management"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM54140.2023.10437779]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM54140.2023.10437779]</a>",,,ScienceDirect
Backdoor Bargaining: How the European Union Navigates the Food Aid Regime Complex,Margulis M.E.,Politics and Governance,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Backdoor Bargaining: How the European Union Navigates the Food Aid Regime Complex"" target=""_blank"">[https://doi.org/10.17645/pag.v11i2.6307]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.17645/pag.v11i2.6307]</a>",,,ScienceDirect
Backdoor Cleansing with Unlabeled Data,Pang L.,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Backdoor Cleansing with Unlabeled Data"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.01176]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.01176]</a>",,,ScienceDirect
Backdoor Defense via Deconfounded Representation Learning,Zhang Z.,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Backdoor Defense via Deconfounded Representation Learning"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.01177]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.01177]</a>",,,ScienceDirect
Backdoor Defense via Suppressing Model Shortcuts,Yang S.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Backdoor Defense via Suppressing Model Shortcuts"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10097220]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10097220]</a>",,,ScienceDirect
Backdoor Lawmaking: Evading Obstacles in the US Congress,Ritchie M.N.,Backdoor Lawmaking: Evading Obstacles in the US Congress,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Backdoor Lawmaking: Evading Obstacles in the US Congress"" target=""_blank"">[https://doi.org/10.1093/oso/9780197670484.001.0001]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1093/oso/9780197670484.001.0001]</a>",,,ScienceDirect
Backdoor Mitigation in Deep Neural Networks via Strategic Retraining,Dhonthi A.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Backdoor Mitigation in Deep Neural Networks via Strategic Retraining"" target=""_blank"">[https://doi.org/10.1007/978-3-031-27481-7_37]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-27481-7_37]</a>",,,ScienceDirect
Backdoor attacks against deep reinforcement learning based traffic signal control systems,Zhang H.,Peer-to-Peer Networking and Applications,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Backdoor attacks against deep reinforcement learning based traffic signal control systems"" target=""_blank"">[https://doi.org/10.1007/s12083-022-01434-0]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/s12083-022-01434-0]</a>",,,ScienceDirect
BadTrack: A Poison-Only Backdoor Attack on Visual Object Tracking,Huang B.,Advances in Neural Information Processing Systems,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : BadTrack: A Poison-Only Backdoor Attack on Visual Object Tracking"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
BayBFed: Bayesian Backdoor Defense for Federated Learning,Kumari K.,Proceedings - IEEE Symposium on Security and Privacy,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : BayBFed: Bayesian Backdoor Defense for Federated Learning"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179362]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179362]</a>",,,ScienceDirect
Be Careful With Writing Perturbations: This Could Be a Trigger of Textual Backdoor Attacks,Yang S.,"Proceedings - 2023 IEEE SmartWorld, Ubiquitous Intelligence and Computing, Autonomous and Trusted Vehicles, Scalable Computing and Communications, Digital Twin, Privacy Computing and Data Security, Metaverse, SmartWorld/UIC/ATC/ScalCom/DigitalTwin/PCDS/Metaverse 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Be Careful With Writing Perturbations: This Could Be a Trigger of Textual Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1109/SWC57546.2023.10449255]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SWC57546.2023.10449255]</a>",,,ScienceDirect
Computational Color Constancy-Based Backdoor Attacks,Vrsnak D.,"International Symposium on Image and Signal Processing and Analysis, ISPA",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Computational Color Constancy-Based Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1109/ISPA58351.2023.10278694]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ISPA58351.2023.10278694]</a>",,,ScienceDirect
Countermeasure against Backdoor Attack for Deep Learning-Based Phishing Detection,Nishiura K.,"2023 International Conference on Consumer Electronics - Taiwan, ICCE-Taiwan 2023 - Proceedings",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Countermeasure against Backdoor Attack for Deep Learning-Based Phishing Detection"" target=""_blank"">[https://doi.org/10.1109/ICCE-Taiwan58799.2023.10226938]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICCE-Taiwan58799.2023.10226938]</a>",,,ScienceDirect
Cyber-Attack Detection Through Ensemble-Based Machine Learning Classifier,Uddin M.A.,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Cyber-Attack Detection Through Ensemble-Based Machine Learning Classifier"" target=""_blank"">[https://doi.org/10.1007/978-3-031-34622-4_31]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-34622-4_31]</a>",,,ScienceDirect
DeepDefense: A Steganalysis-Based Backdoor Detecting and Mitigating Protocol in Deep Neural Networks for AI Security,Zhang L.,Security and Communication Networks,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : DeepDefense: A Steganalysis-Based Backdoor Detecting and Mitigating Protocol in Deep Neural Networks for AI Security"" target=""_blank"">[https://doi.org/10.1155/2023/9308909]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1155/2023/9308909]</a>",,,ScienceDirect
Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning,Tejankar A.,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.01178]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.01178]</a>",,,ScienceDirect
Defending Federated Learning from Backdoor Attacks: Anomaly-Aware FedAVG with Layer-Based Aggregation,Manzoor H.U.,"IEEE International Symposium on Personal, Indoor and Mobile Radio Communications, PIMRC",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Defending Federated Learning from Backdoor Attacks: Anomaly-Aware FedAVG with Layer-Based Aggregation"" target=""_blank"">[https://doi.org/10.1109/PIMRC56721.2023.10293950]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/PIMRC56721.2023.10293950]</a>",,,ScienceDirect
Defending against Insertion-based Textual Backdoor Attacks via Attribution,Li J.,Proceedings of the Annual Meeting of the Association for Computational Linguistics,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Defending against Insertion-based Textual Backdoor Attacks via Attribution"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Detection of Attacks in Smart Healthcare deploying Machine Learning Algorithms,Sharma A.,"2023 4th International Conference for Emerging Technology, INCET 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Detection of Attacks in Smart Healthcare deploying Machine Learning Algorithms"" target=""_blank"">[https://doi.org/10.1109/INCET57972.2023.10170367]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/INCET57972.2023.10170367]</a>",,,ScienceDirect
Disguising Attacks with Explanation-Aware Backdoors,Noppel M.,Proceedings - IEEE Symposium on Security and Privacy,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Disguising Attacks with Explanation-Aware Backdoors"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179308]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179308]</a>",,,ScienceDirect
Distributed Attacks over Federated Reinforcement Learning-Enabled Cell Sleep Control,Zhang H.,"2023 IEEE Globecom Workshops, GC Wkshps 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Distributed Attacks over Federated Reinforcement Learning-Enabled Cell Sleep Control"" target=""_blank"">[https://doi.org/10.1109/GCWkshps58843.2023.10464431]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/GCWkshps58843.2023.10464431]</a>",,,ScienceDirect
Don't Knock! Rowhammer at the Backdoor of DNN Models,Tol M.C.,"Proceedings - 2023 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Don't Knock! Rowhammer at the Backdoor of DNN Models"" target=""_blank"">[https://doi.org/10.1109/DSN58367.2023.00023]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/DSN58367.2023.00023]</a>",,,ScienceDirect
EEG-Based Brain-Computer Interfaces are Vulnerable to Backdoor Attacks,Meng L.,IEEE Transactions on Neural Systems and Rehabilitation Engineering,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : EEG-Based Brain-Computer Interfaces are Vulnerable to Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1109/TNSRE.2023.3273214]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TNSRE.2023.3273214]</a>",,,ScienceDirect
Efficient DNN Backdoor Detection Guided by Static Weight Analysis,Wang Q.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Efficient DNN Backdoor Detection Guided by Static Weight Analysis"" target=""_blank"">[https://doi.org/10.1007/978-3-031-26553-2_22]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-26553-2_22]</a>",,,ScienceDirect
Efficient any-Target Backdoor Attack with Pseudo Poisoned Samples,Huang B.,"Proceedings - International Conference on Image Processing, ICIP",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Efficient any-Target Backdoor Attack with Pseudo Poisoned Samples"" target=""_blank"">[https://doi.org/10.1109/ICIP49359.2023.10222807]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICIP49359.2023.10222807]</a>",,,ScienceDirect
Enhancing Fine-Tuning based Backdoor Defense with Sharpness-Aware Minimization,Zhu M.,Proceedings of the IEEE International Conference on Computer Vision,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Enhancing Fine-Tuning based Backdoor Defense with Sharpness-Aware Minimization"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00412]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00412]</a>",,,ScienceDirect
GAN-inspired Defense Against Backdoor Attack on Federated Learning Systems,Sundar A.P.,"Proceedings - 2023 IEEE 20th International Conference on Mobile Ad Hoc and Smart Systems, MASS 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : GAN-inspired Defense Against Backdoor Attack on Federated Learning Systems"" target=""_blank"">[https://doi.org/10.1109/MASS58611.2023.00063]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/MASS58611.2023.00063]</a>",,,ScienceDirect
GPTs Don't Keep Secrets: Searching for Backdoor Watermark Triggers in Autoregressive Language Models,Lucas E.,Proceedings of the Annual Meeting of the Association for Computational Linguistics,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : GPTs Don't Keep Secrets: Searching for Backdoor Watermark Triggers in Autoregressive Language Models"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Genetic Algorithm-Based Dynamic Backdoor Attack on Federated Learning-Based Network Traffic Classification,Nazzal M.,"2023 8th International Conference on Fog and Mobile Edge Computing, FMEC 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Genetic Algorithm-Based Dynamic Backdoor Attack on Federated Learning-Based Network Traffic Classification"" target=""_blank"">[https://doi.org/10.1109/FMEC59375.2023.10306137]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/FMEC59375.2023.10306137]</a>",,,ScienceDirect
Graph Contrastive Backdoor Attacks,Zhang H.,Proceedings of Machine Learning Research,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Graph Contrastive Backdoor Attacks"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Image-imperceptible backdoor attacks,Zhu S.,Journal of Image and Graphics,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Image-imperceptible backdoor attacks"" target=""_blank"">[https://doi.org/10.11834/jig.220550]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.11834/jig.220550]</a>",,,ScienceDirect
Jigsaw Puzzle: Selective Backdoor Attack to Subvert Malware Classifiers,Yang L.,Proceedings - IEEE Symposium on Security and Privacy,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Jigsaw Puzzle: Selective Backdoor Attack to Subvert Malware Classifiers"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179347]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179347]</a>",,,ScienceDirect
Joint Energy-Based Model for Robust Speech Classification System Against Dirty-Label Backdoor Poisoning Attacks,Sustek M.,"2023 IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Joint Energy-Based Model for Robust Speech Classification System Against Dirty-Label Backdoor Poisoning Attacks"" target=""_blank"">[https://doi.org/10.1109/ASRU57964.2023.10389697]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ASRU57964.2023.10389697]</a>",,,ScienceDirect
Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers,You W.,Findings of the Association for Computational Linguistics: EMNLP 2023,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
MATFL: Defending Against Synergetic Attacks in Federated Learning,Yang W.,"Proceedings - IEEE Congress on Cybermatics: 2023 IEEE International Conferences on Internet of Things, iThings 2023, IEEE Green Computing and Communications, GreenCom 2023, IEEE Cyber, Physical and Social Computing, CPSCom 2023 and IEEE Smart Data, SmartData 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : MATFL: Defending Against Synergetic Attacks in Federated Learning"" target=""_blank"">[https://doi.org/10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics60724.2023.00072]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics60724.2023.00072]</a>",,,ScienceDirect
MPS: A Multiple Poisoned Samples Selection Strategy in Backdoor Attack,Zou W.,"Proceedings - 2023 IEEE 22nd International Conference on Trust, Security and Privacy in Computing and Communications, TrustCom/BigDataSE/CSE/EUC/iSCI 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : MPS: A Multiple Poisoned Samples Selection Strategy in Backdoor Attack"" target=""_blank"">[https://doi.org/10.1109/TrustCom60117.2023.00125]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TrustCom60117.2023.00125]</a>",,,ScienceDirect
MagBackdoor: Beware of Your Loudspeaker as A Backdoor for Magnetic Injection Attacks,Liu T.,Proceedings - IEEE Symposium on Security and Privacy,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : MagBackdoor: Beware of Your Loudspeaker as A Backdoor for Magnetic Injection Attacks"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179364]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179364]</a>",,,ScienceDirect
Measure and Countermeasure of the Capsulation Attack Against Backdoor-Based Deep Neural Network Watermarks,Li F.Q.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Measure and Countermeasure of the Capsulation Attack Against Backdoor-Based Deep Neural Network Watermarks"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10096448]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10096448]</a>",,,ScienceDirect
Mitigating Backdoor Attacks Using Prediction of Model Update Trends,Xu S.,Communications in Computer and Information Science,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Mitigating Backdoor Attacks Using Prediction of Model Update Trends"" target=""_blank"">[https://doi.org/10.1007/978-981-99-3925-1_20]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-981-99-3925-1_20]</a>",,,ScienceDirect
Model Agnostic Approach for NLP Backdoor Detection,Surendrababu H.K.,"2023 IEEE Colombian Conference on Applications of Computational Intelligence, ColCACI 2023 - Proceedings",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Model Agnostic Approach for NLP Backdoor Detection"" target=""_blank"">[https://doi.org/10.1109/ColCACI59285.2023.10226144]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ColCACI59285.2023.10226144]</a>",,,ScienceDirect
Model Poisoning Attack In Federated Learning Via Adversarial Examples,Yuan L.,"Proceedings - 2023 International Seminar on Computer Science and Engineering Technology, SCSET 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Model Poisoning Attack In Federated Learning Via Adversarial Examples"" target=""_blank"">[https://doi.org/10.1109/SCSET58950.2023.00021]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SCSET58950.2023.00021]</a>",,,ScienceDirect
Model Similarity-Based Defense Scheme Against Backdoor Attacks on Federated Learning,Su L.,Lecture Notes in Electrical Engineering,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Model Similarity-Based Defense Scheme Against Backdoor Attacks on Federated Learning"" target=""_blank"">[https://doi.org/10.1007/978-981-99-1428-9_265]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-981-99-1428-9_265]</a>",,,ScienceDirect
NCL: Textual Backdoor Defense Using Noise-Augmented Contrastive Learning,Zhai S.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : NCL: Textual Backdoor Defense Using Noise-Augmented Contrastive Learning"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10095007]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10095007]</a>",,,ScienceDirect
NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models,Mei K.,Proceedings of the Annual Meeting of the Association for Computational Linguistics,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Natural Backdoor Attacks on Speech Recognition Models,Xin J.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Natural Backdoor Attacks on Speech Recognition Models"" target=""_blank"">[https://doi.org/10.1007/978-3-031-20096-0_45]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-20096-0_45]</a>",,,ScienceDirect
Neural Network Backdoor Attacks Fully Controlled by Composite Natural Utterance Fragments,Yang X.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Neural Network Backdoor Attacks Fully Controlled by Composite Natural Utterance Fragments"" target=""_blank"">[https://doi.org/10.1007/978-981-99-7356-9_27]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-981-99-7356-9_27]</a>",,,ScienceDirect
Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features,Zhu M.,Advances in Neural Information Processing Systems,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Non-semantic information suppression relevant backdoor defense implementation,Guo Y.,Journal of Image and Graphics,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Non-semantic information suppression relevant backdoor defense implementation"" target=""_blank"">[https://doi.org/10.11834/jig.220421]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.11834/jig.220421]</a>",,,ScienceDirect
On Feasibility of Server-side Backdoor Attacks on Split Learning,Tajalli B.,"Proceeding - 44th IEEE Symposium on Security and Privacy Workshops, SPW 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : On Feasibility of Server-side Backdoor Attacks on Split Learning"" target=""_blank"">[https://doi.org/10.1109/SPW59333.2023.00014]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SPW59333.2023.00014]</a>",,,ScienceDirect
Optimally Mitigating Backdoor Attacks in Federated Learning,Walter K.,IEEE Transactions on Dependable and Secure Computing,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Optimally Mitigating Backdoor Attacks in Federated Learning"" target=""_blank"">[https://doi.org/10.1109/TDSC.2023.3320694]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TDSC.2023.3320694]</a>",,,ScienceDirect
PBE-Plan: Periodic Backdoor Erasing Plan for Trustworthy Federated Learning,Chen B.,"Proceedings - 2023 IEEE International Conference on High Performance Computing and Communications, Data Science and Systems, Smart City and Dependability in Sensor, Cloud and Big Data Systems and Application, HPCC/DSS/SmartCity/DependSys 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : PBE-Plan: Periodic Backdoor Erasing Plan for Trustworthy Federated Learning"" target=""_blank"">[https://doi.org/10.1109/HPCC-DSS-SmartCity-DependSys60770.2023.00016]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/HPCC-DSS-SmartCity-DependSys60770.2023.00016]</a>",,,ScienceDirect
PerCBA: Persistent Clean-label Backdoor Attacks on Semi-Supervised Graph Node Classification,Yang X.,CEUR Workshop Proceedings,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : PerCBA: Persistent Clean-label Backdoor Attacks on Semi-Supervised Graph Node Classification"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Physical Backdoor Attack using Multi-Trigger to Same Class,Singh G.,"2023 3rd International Conference on Artificial Intelligence and Signal Processing, AISP 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Physical Backdoor Attack using Multi-Trigger to Same Class"" target=""_blank"">[https://doi.org/10.1109/AISP57993.2023.10134794]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/AISP57993.2023.10134794]</a>",,,ScienceDirect
Poisoning Attack in Federated Learning Using Normalizing Flows,Hu H.,"Proceedings - 2023 International Seminar on Computer Science and Engineering Technology, SCSET 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Poisoning Attack in Federated Learning Using Normalizing Flows"" target=""_blank"">[https://doi.org/10.1109/SCSET58950.2023.00075]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SCSET58950.2023.00075]</a>",,,ScienceDirect
Port Smart Gate Ground Scale Line Pressure Detection using Federated Learning: Backdoor Attacks,Tang C.,"Proceedings - 2023 2nd International Conference on Artificial Intelligence, Human-Computer Interaction and Robotics, AIHCIR 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Port Smart Gate Ground Scale Line Pressure Detection using Federated Learning: Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1109/AIHCIR61661.2023.00062]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/AIHCIR61661.2023.00062]</a>",,,ScienceDirect
Privacy Inference-Empowered Stealthy Backdoor Attack on Federated Learning under Non-IID Scenarios,Mei H.,Proceedings of the International Joint Conference on Neural Networks,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Privacy Inference-Empowered Stealthy Backdoor Attack on Federated Learning under Non-IID Scenarios"" target=""_blank"">[https://doi.org/10.1109/IJCNN54540.2023.10191260]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/IJCNN54540.2023.10191260]</a>",,,ScienceDirect
Progressive Backdoor Erasing via connecting Backdoor and Adversarial Attacks,Mu B.,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Progressive Backdoor Erasing via connecting Backdoor and Adversarial Attacks"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.01963]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.01963]</a>",,,ScienceDirect
Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models,Zhao S.,"EMNLP 2023 - 2023 Conference on Empirical Methods in Natural Language Processing, Proceedings",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
QDoor: Exploiting Approximate Synthesis for Backdoor Attacks in Quantum Neural Networks,Chu C.,"Proceedings - 2023 IEEE International Conference on Quantum Computing and Engineering, QCE 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : QDoor: Exploiting Approximate Synthesis for Backdoor Attacks in Quantum Neural Networks"" target=""_blank"">[https://doi.org/10.1109/QCE57702.2023.00124]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/QCE57702.2023.00124]</a>",,,ScienceDirect
QTROJAN: A Circuit Backdoor Against Quantum Neural Networks,Chu C.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : QTROJAN: A Circuit Backdoor Against Quantum Neural Networks"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10096293]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10096293]</a>",,,ScienceDirect
RAB: Provable Robustness Against Backdoor Attacks,Weber M.,Proceedings - IEEE Symposium on Security and Privacy,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : RAB: Provable Robustness Against Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179451]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179451]</a>",,,ScienceDirect
Ransomware Attacks and Scenarios: Cost Factors and Loss of Reputation,Möller D.P.F.,Advances in Information Security,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Ransomware Attacks and Scenarios: Cost Factors and Loss of Reputation"" target=""_blank"">[https://doi.org/10.1007/978-3-031-26845-8_6]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-26845-8_6]</a>",,,ScienceDirect
Real is not True: Backdoor Attacks Against Deepfake Detection,Sun H.,"2023 9th International Conference on Big Data and Information Analytics, BigDIA 2023 - Proceedings",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Real is not True: Backdoor Attacks Against Deepfake Detection"" target=""_blank"">[https://doi.org/10.1109/BigDIA60676.2023.10429108]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/BigDIA60676.2023.10429108]</a>",,,ScienceDirect
Removing backdoors in pre-trained models by regularized continual pre-training,Zhu B.,Transactions of the Association for Computational Linguistics,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Removing backdoors in pre-trained models by regularized continual pre-training"" target=""_blank"">[https://doi.org/10.1162/tacl_a_00622]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1162/tacl_a_00622]</a>",,,ScienceDirect
Rethinking Backdoor Attacks,Khaddaj A.,Proceedings of Machine Learning Research,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Rethinking Backdoor Attacks"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Rethinking the Trigger-injecting Position in Graph Backdoor Attack,Xu J.,Proceedings of the International Joint Conference on Neural Networks,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Rethinking the Trigger-injecting Position in Graph Backdoor Attack"" target=""_blank"">[https://doi.org/10.1109/IJCNN54540.2023.10191949]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/IJCNN54540.2023.10191949]</a>",,,ScienceDirect
Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis,Struppek L.,Proceedings of the IEEE International Conference on Computer Vision,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00423]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00423]</a>",,,ScienceDirect
Robust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor Attacks,Yang W.,Advances in Neural Information Processing Systems,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Robust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor Attacks"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Robust Federated Learning against Backdoor Attackers,Ranjan P.,"IEEE INFOCOM 2023 - Conference on Computer Communications Workshops, INFOCOM WKSHPS 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Robust Federated Learning against Backdoor Attackers"" target=""_blank"">[https://doi.org/10.1109/INFOCOMWKSHPS57453.2023.10225922]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/INFOCOMWKSHPS57453.2023.10225922]</a>",,,ScienceDirect
Role of Machine Learning in Power Analysis Based Side Channel Attacks on FPGA,Hasnain A.,"2023 International Conference on Robotics and Automation in Industry, ICRAI 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Role of Machine Learning in Power Analysis Based Side Channel Attacks on FPGA"" target=""_blank"">[https://doi.org/10.1109/ICRAI57502.2023.10089540]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICRAI57502.2023.10089540]</a>",,,ScienceDirect
SATBA: An Invisible Backdoor Attack Based on Spatial Attention,Zhou H.,"Proceedings - 2023 IEEE 22nd International Conference on Trust, Security and Privacy in Computing and Communications, TrustCom/BigDataSE/CSE/EUC/iSCI 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : SATBA: An Invisible Backdoor Attack Based on Spatial Attention"" target=""_blank"">[https://doi.org/10.1109/TrustCom60117.2023.00133]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TrustCom60117.2023.00133]</a>",,,ScienceDirect
SeBRUS: Mitigating Data Poisoning Attacks on Crowdsourced Datasets with Blockchain,Iyer A.,"IEEE MIT Undergraduate Research Technology Conference, URTC 2023 - Proceedings",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : SeBRUS: Mitigating Data Poisoning Attacks on Crowdsourced Datasets with Blockchain"" target=""_blank"">[https://doi.org/10.1109/URTC60662.2023.10535023]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/URTC60662.2023.10535023]</a>",,,ScienceDirect
Security Attack on Remote Sensing Equipment: PoIs Recognition Based on HW with Bi-LSTM Attention,Jiang W.,Journal of Internet Technology,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Security Attack on Remote Sensing Equipment: PoIs Recognition Based on HW with Bi-LSTM Attention"" target=""_blank"">[https://doi.org/10.53106/160792642023052403005]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.53106/160792642023052403005]</a>",,,ScienceDirect
"Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models",Zhu R.,Proceedings - IEEE Symposium on Security and Privacy,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10351028]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10351028]</a>",,,ScienceDirect
Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypots,Tang R.,Advances in Neural Information Processing Systems,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypots"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples,Wei S.,Advances in Neural Information Processing Systems,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Single Image Backdoor Inversion via Robust Smoothed Classifiers,Sun M.,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Single Image Backdoor Inversion via Robust Smoothed Classifiers"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.00784]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.00784]</a>",,,ScienceDirect
Sparsity Brings Vulnerabilities: Exploring New Metrics in Backdoor Attacks,Tian J.,"32nd USENIX Security Symposium, USENIX Security 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Sparsity Brings Vulnerabilities: Exploring New Metrics in Backdoor Attacks"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Stealthy Backdoor Attack on RF Signal Classification,Zhao T.,"Proceedings - International Conference on Computer Communications and Networks, ICCCN",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Stealthy Backdoor Attack on RF Signal Classification"" target=""_blank"">[https://doi.org/10.1109/ICCCN58024.2023.10230152]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICCCN58024.2023.10230152]</a>",,,ScienceDirect
TRAPDOOR: Repurposing neural network backdoors to detect dataset bias in machine learning-based genomic analysis,Sarkar E.,"IEEE/IFIP International Conference on VLSI and System-on-Chip, VLSI-SoC",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : TRAPDOOR: Repurposing neural network backdoors to detect dataset bias in machine learning-based genomic analysis"" target=""_blank"">[https://doi.org/10.1109/VLSI-SoC57769.2023.10321928]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/VLSI-SoC57769.2023.10321928]</a>",,,ScienceDirect
The Impact of Attacking Windows Using a Backdoor Trojan,Shen K.J.,"2023 International Conference on Evolutionary Algorithms and Soft Computing Techniques, EASCT 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : The Impact of Attacking Windows Using a Backdoor Trojan"" target=""_blank"">[https://doi.org/10.1109/EASCT59475.2023.10393460]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/EASCT59475.2023.10393460]</a>",,,ScienceDirect
The Perils of Learning from Unlabeled Data: Backdoor Attacks on Semi-supervised Learning,Shejwalkar V.,Proceedings of the IEEE International Conference on Computer Vision,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : The Perils of Learning from Unlabeled Data: Backdoor Attacks on Semi-supervised Learning"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00436]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00436]</a>",,,ScienceDirect
The “Beatrix” Resurrections: Robust Backdoor Detection via Gram Matrices,Ma W.,"30th Annual Network and Distributed System Security Symposium, NDSS 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : The “Beatrix” Resurrections: Robust Backdoor Detection via Gram Matrices"" target=""_blank"">[https://doi.org/10.14722/ndss.2023.23069]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.14722/ndss.2023.23069]</a>",,,ScienceDirect
Towards Defending Adaptive Backdoor Attacks in Federated Learning,Yang H.,IEEE International Conference on Communications,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Towards Defending Adaptive Backdoor Attacks in Federated Learning"" target=""_blank"">[https://doi.org/10.1109/ICC45041.2023.10279267]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICC45041.2023.10279267]</a>",,,ScienceDirect
Training Set Cleansing of Backdoor Poisoning by Self-Supervised Representation Learning,Wang H.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Training Set Cleansing of Backdoor Poisoning by Self-Supervised Representation Learning"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10097244]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10097244]</a>",,,ScienceDirect
TransCAB: Transferable Clean-Annotation Backdoor to Object Detection with Natural Trigger in Real-World,Ma H.,Proceedings of the IEEE Symposium on Reliable Distributed Systems,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : TransCAB: Transferable Clean-Annotation Backdoor to Object Detection with Natural Trigger in Real-World"" target=""_blank"">[https://doi.org/10.1109/SRDS60354.2023.00018]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SRDS60354.2023.00018]</a>",,,ScienceDirect
UltraBD: Backdoor Attack against Automatic Speaker Verification Systems via Adversarial Ultrasound,Ze J.,Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : UltraBD: Backdoor Attack against Automatic Speaker Verification Systems via Adversarial Ultrasound"" target=""_blank"">[https://doi.org/10.1109/ICPADS56603.2022.00033]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICPADS56603.2022.00033]</a>",,,ScienceDirect
Unambiguous and High-Fidelity Backdoor Watermarking for Deep Neural Networks,Hua G.,IEEE Transactions on Neural Networks and Learning Systems,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Unambiguous and High-Fidelity Backdoor Watermarking for Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1109/TNNLS.2023.3250210]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TNNLS.2023.3250210]</a>",,,ScienceDirect
Untargeted Backdoor Attack Against Object Detection,Luo C.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Untargeted Backdoor Attack Against Object Detection"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10095980]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10095980]</a>",,,ScienceDirect
Watermarking Graph Neural Networks based on Backdoor Attacks,Xu J.,"Proceedings - 8th IEEE European Symposium on Security and Privacy, Euro S and P 2023",2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Watermarking Graph Neural Networks based on Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1109/EuroSP57164.2023.00072]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/EuroSP57164.2023.00072]</a>",,,ScienceDirect
Watermarks for Generative Adversarial Network Based on Steganographic Invisible Backdoor,Zeng Y.,Proceedings - IEEE International Conference on Multimedia and Expo,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : Watermarks for Generative Adversarial Network Based on Steganographic Invisible Backdoor"" target=""_blank"">[https://doi.org/10.1109/ICME55011.2023.00211]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICME55011.2023.00211]</a>",,,ScienceDirect
∆ SFL: (Decoupled Server Federated Learning) to Utilize DLG Attacks in Federated Learning by Decoupling the Server,Paul S.,Proceedings of the International Conference on Security and Cryptography,2023-01-01,"<a href=""ScienceDirect (2023-01-01) : ∆ SFL: (Decoupled Server Federated Learning) to Utilize DLG Attacks in Federated Learning by Decoupling the Server"" target=""_blank"">[https://doi.org/10.5220/0012150700003555]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.5220/0012150700003555]</a>",,,ScienceDirect
Robust Federated Learning for Ubiquitous Computing through Mitigation of Edge-Case Backdoor Attacks,"Fatima Elhattab, Sara Bouchenak, Rania Talbi, Vlad Nitu","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), Volume 6, Issue 4
Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.","2023-01
2022","<a href=""ACM (2023-01) : Robust Federated Learning for Ubiquitous Computing through Mitigation of Edge-Case Backdoor Attacks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3569492]</a>
<a href=""DBLP (2022) : Robust Federated Learning for Ubiquitous Computing through Mitigation of Edge-Case Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1145/3569492]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3569492]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3569492]</a>","Federated Learning (FL) allows several data owners to train a joint model without sharing their training data. Such a paradigm is useful for better privacy in many ubiquitous computing systems. However, FL is vulnerable to poisoning attacks, where ...
","
","ACM
DBLP"
Backdoor Attacks in Peer-to-Peer Federated Learning,"Gökberk Yar, Cristina Nita-Rotaru, Alina Oprea",arXiv,2023-01,"<a href=""DBLP (2023-01) : Backdoor Attacks in Peer-to-Peer Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.09732]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.09732]</a>",,,DBLP
Silent Killer: Optimizing Backdoor Trigger Yields a Stealthy and Powerful Data Poisoning Attack,"Tzvi Lederer, Gallil Maimon, Lior Rokach",arXiv,2023-01,"<a href=""DBLP (2023-01) : Silent Killer: Optimizing Backdoor Trigger Yields a Stealthy and Powerful Data Poisoning Attack"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.02615]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.02615]</a>",,,DBLP
Punctuation Matters! Stealthy Backdoor Attack for Language Models,"Xuan Sheng, Zhicheng Li, ... Piji Li","Natural Language Processing and Chinese Computing
arXiv
NLPCC
arXiv","2023
2023-12-26
2023
2023-12","<a href=""Springer (2023) : Punctuation Matters! Stealthy Backdoor Attack for Language Models"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-44693-1_41]</a>
<a href=""arXiv (2023-12-26) : Punctuation Matters! Stealthy Backdoor Attack for Language Models"" target=""_blank"">[http://arxiv.org/abs/2312.15867v1]</a>
<a href=""DBLP (2023) : Punctuation Matters! Stealthy Backdoor Attack for Language Models"" target=""_blank"">[https://doi.org/10.1007/978-3-031-44693-1_41]</a>
<a href=""DBLP (2023-12) : Punctuation Matters! Stealthy Backdoor Attack for Language Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.15867]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-44693-1_41]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-44693-1_41]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2312.15867]</a>","Recent studies have pointed out that natural language processing (NLP) models are vulnerable to backdoor attacks. A backdoored model produces normal...
Recent studies have pointed out that natural language processing (NLP) models are vulnerable to backdoor attacks. A backdoored model produces normal outputs on the clean samples while performing improperly on the texts with triggers that the adversary injects. However, previous studies on textual backdoor attack pay little attention to stealthiness. Moreover, some attack methods even cause grammatical issues or change the semantic meaning of the original texts. Therefore, they can easily be detected by humans or defense systems. In this paper, we propose a novel stealthy backdoor attack method against textual models, which is called \textbf{PuncAttack}. It leverages combinations of punctuation marks as the trigger and chooses proper locations strategically to replace them. Through extensive experiments, we demonstrate that the proposed method can effectively compromise multiple models in various tasks. Meanwhile, we conduct automatic evaluation and human inspection, which indicate the proposed method possesses good performance of stealthiness without bringing grammatical issues and altering the meaning of sentences.

","


","Springer
arXiv
DBLP
DBLP"
FLEDGE: Ledger-based Federated Learning Resilient to Inference and Backdoor Attacks,"Jorge Castillo, Phillip Rieger, Hossein Fereidooni, Qian Chen, Ahmad-Reza Sadeghi","ACSAC
arXiv","2023
2023-10","<a href=""DBLP (2023) : FLEDGE: Ledger-based Federated Learning Resilient to Inference and Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1145/3627106.3627194]</a>
<a href=""DBLP (2023-10) : FLEDGE: Ledger-based Federated Learning Resilient to Inference and Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.02113]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3627106.3627194]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2310.02113]</a>","
","
","DBLP
DBLP"
Palette: Physically-Realizable Backdoor Attacks Against Video Recognition Models,X. Gong Z. Fang B. Li T. Wang Y. Chen Q. Wang,"IEEE Transactions on Dependable and Secure Computing
IEEE Transactions on …, 2023","2023
2023-09-14","<a href=""IEEE (2023) : Palette: Physically-Realizable Backdoor Attacks Against Video Recognition Models"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10250986]</a>
<a href=""Google Scholar (2023-09-14) : Palette: Physically-Realizable Backdoor Attacks Against Video Recognition Models"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10250986/]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TDSC.2023.3314792]</a>
<a href=""Google Scholar"" target=""_blank"">[https://ieeexplore.ieee.org/abstract/document/10250986/]</a>","Backdoor attacks have been widely studied for image classification tasks, but rarely investigated for video recognition tasks. In this paper, we explore the possibility of physically-realizable backdoor attacks against video recognition models. Different from existing works that directly apply image backdoor attacks to videos, i.e., patch a visible trigger to each frame of a video, we carefully take into consideration the temporal interactions among frames in a video. Our proposed video backdoor attack, named Palette, features two special design choices. The first is to utilize natural-light-alike RGB offset as triggers rather than traditional patch triggers. Such triggers may be applied in the physical world through lighting without the need to modify video files. The second is to make the backdoored model more robust to temporal asynchronization between the trigger and the video samples by performing rolling operations during sample poisoning. Extensive experiments show that Palette outperforms existing video backdoor attacks, especially in the physical world. It is shown that Palette is also resistant to backdoor defense methods. We will open-source our codes upon publication.
works that directly apply image backdoor attacks to videos, ie… Our proposed video backdoor attack, named Palette , … outperforms existing video backdoor attacks, especially …","
","IEEE
Google Scholar"
Adversarial Clean Label Backdoor Attacks and Defenses on Text Classification Systems,"Ashim Gupta, Amrith Krishna","RepL4NLP@ACL
arXiv","2023
2023-05","<a href=""DBLP (2023) : Adversarial Clean Label Backdoor Attacks and Defenses on Text Classification Systems"" target=""_blank"">[https://doi.org/10.18653/v1/2023.repl4nlp-1.1]</a>
<a href=""DBLP (2023-05) : Adversarial Clean Label Backdoor Attacks and Defenses on Text Classification Systems"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.19607]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2023.repl4nlp-1.1]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2305.19607]</a>","
","
","DBLP
DBLP"
"Don&apos,t FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs","Hasan Abed Al Kader Hammoud, Adel Bibi, Philip H. S. Torr, Bernard Ghanem","CVPR Workshops
arXiv","2023
2023-03","<a href=""DBLP (2023) : Don&apos,t FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs"" target=""_blank"">[https://doi.org/10.1109/CVPRW59228.2023.00230]</a>
<a href=""DBLP (2023-03) : Don&apos,t FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.13211]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CVPRW59228.2023.00230]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2303.13211]</a>","
","
","DBLP
DBLP"
Defending Against Backdoor Attacks by Layer-wise Feature Analysis,"Najeeb Moharram Jebreel, Josep Domingo-Ferrer, Yiming Li","Advances in Knowledge Discovery and Data Mining
arXiv
PAKDD
arXiv","2023
2023-02-24
2023
2023-02","<a href=""Springer (2023) : Defending Against Backdoor Attacks by Layer-wise Feature Analysis"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33377-4_33]</a>
<a href=""arXiv (2023-02-24) : Defending Against Backdoor Attacks by Layer-wise Feature Analysis"" target=""_blank"">[http://arxiv.org/abs/2302.12758v1]</a>
<a href=""DBLP (2023) : Defending Against Backdoor Attacks by Layer-wise Feature Analysis"" target=""_blank"">[https://doi.org/10.1007/978-3-031-33377-4_33]</a>
<a href=""DBLP (2023-02) : Defending Against Backdoor Attacks by Layer-wise Feature Analysis"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.12758]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33377-4_33]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-33377-4_33]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2302.12758]</a>","Training deep neural networks (DNNs) usually requires massive training data and computational resources. Users who cannot afford this may prefer to...
Training deep neural networks (DNNs) usually requires massive training data and computational resources. Users who cannot afford this may prefer to outsource training to a third party or resort to publicly available pre-trained models. Unfortunately, doing so facilitates a new training-time attack (i.e., backdoor attack) against DNNs. This attack aims to induce misclassification of input samples containing adversary-specified trigger patterns. In this paper, we first conduct a layer-wise feature analysis of poisoned and benign samples from the target class. We find out that the feature difference between benign and poisoned samples tends to be maximum at a critical layer, which is not always the one typically used in existing defenses, namely the layer before fully-connected layers. We also demonstrate how to locate this critical layer based on the behaviors of benign samples. We then propose a simple yet effective method to filter poisoned samples by analyzing the feature differences between suspicious and benign samples at the critical layer. We conduct extensive experiments on two benchmark datasets, which confirm the effectiveness of our defense.

","


","Springer
arXiv
DBLP
DBLP"
Immunizing Backdoored PRGs,"Marshall Ball, Yevgeniy Dodis, Eli Goldin","IACR Cryptol. ePrint Arch.
TCC","2023
2023","<a href=""DBLP (2023) : Immunizing Backdoored PRGs"" target=""_blank"">[https://eprint.iacr.org/2023/1778]</a>
<a href=""DBLP (2023) : Immunizing Backdoored PRGs"" target=""_blank"">[https://doi.org/10.1007/978-3-031-48621-0_6]</a>","<a href=""DBLP"" target=""_blank"">[https://eprint.iacr.org/2023/1778]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-48621-0_6]</a>","
","
","DBLP
DBLP"
COLLIDER: A Robust Training Framework for Backdoor Data,"Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie","Computer Vision – ACCV 2022
arXiv
ACCV
arXiv","2023
2022-10-13
2022
2022-10","<a href=""Springer (2023) : COLLIDER: A Robust Training Framework for Backdoor Data"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-26351-4_41]</a>
<a href=""arXiv (2022-10-13) : COLLIDER: A Robust Training Framework for Backdoor Data"" target=""_blank"">[http://arxiv.org/abs/2210.06704v1]</a>
<a href=""DBLP (2022) : COLLIDER: A Robust Training Framework for Backdoor Data"" target=""_blank"">[https://doi.org/10.1007/978-3-031-26351-4_41]</a>
<a href=""DBLP (2022-10) : COLLIDER: A Robust Training Framework for Backdoor Data"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.06704]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-26351-4_41]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-26351-4_41]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.06704]</a>","Deep neural network (DNN) classifiers are vulnerable to backdoor attacks. An adversary poisons some of the training data in such attacks by...
Deep neural network (DNN) classifiers are vulnerable to backdoor attacks. An adversary poisons some of the training data in such attacks by installing a trigger. The goal is to make the trained DNN output the attacker's desired class whenever the trigger is activated while performing as usual for clean data. Various approaches have recently been proposed to detect malicious backdoored DNNs. However, a robust, end-to-end training approach, like adversarial training, is yet to be discovered for backdoor poisoned data. In this paper, we take the first step toward such methods by developing a robust training framework, COLLIDER, that selects the most prominent samples by exploiting the underlying geometric structures of the data. Specifically, we effectively filter out candidate poisoned data at each training epoch by solving a geometrical coreset selection objective. We first argue how clean data samples exhibit (1) gradients similar to the clean majority of data and (2) low local intrinsic dimensionality (LID). Based on these criteria, we define a novel coreset selection objective to find such samples, which are used for training a DNN. We show the effectiveness of the proposed method for robust training of DNNs on various poisoned datasets, reducing the backdoor success rate significantly.

","


","Springer
arXiv
DBLP
DBLP"
The Dark Side of AutoML: Towards Architectural Backdoor Search,"Ren Pang, Changjiang Li, Zhaohan Xi, Shouling Ji, Ting Wang","ICLR
arXiv","2023
2022-10","<a href=""DBLP (2023) : The Dark Side of AutoML: Towards Architectural Backdoor Search"" target=""_blank"">[https://openreview.net/pdf?id=bsZULlDGXe]</a>
<a href=""DBLP (2022-10) : The Dark Side of AutoML: Towards Architectural Backdoor Search"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.12179]</a>","<a href=""DBLP"" target=""_blank"">[https://openreview.net/pdf?id=bsZULlDGXe]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.12179]</a>","
","
","DBLP
DBLP"
BadDet: Backdoor Attacks on Object Detection,"Shih-Han Chan, Yinpeng Dong, ... Jun Zhou","Computer Vision – ECCV 2022 Workshops
arXiv
ECCV Workshops
arXiv","2023
2022-05-28
2022
2022-05","<a href=""Springer (2023) : BadDet: Backdoor Attacks on Object Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-25056-9_26]</a>
<a href=""arXiv (2022-05-28) : BadDet: Backdoor Attacks on Object Detection"" target=""_blank"">[http://arxiv.org/abs/2205.14497v1]</a>
<a href=""DBLP (2022) : BadDet: Backdoor Attacks on Object Detection"" target=""_blank"">[https://doi.org/10.1007/978-3-031-25056-9_26]</a>
<a href=""DBLP (2022-05) : BadDet: Backdoor Attacks on Object Detection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.14497]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-25056-9_26]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-25056-9_26]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.14497]</a>","Backdoor attack is a severe security threat which injects a backdoor trigger into a small portion of training data such that the trained model gives...
Deep learning models have been deployed in numerous real-world applications such as autonomous driving and surveillance. However, these models are vulnerable in adversarial environments. Backdoor attack is emerging as a severe security threat which injects a backdoor trigger into a small portion of training data such that the trained model behaves normally on benign inputs but gives incorrect predictions when the specific trigger appears. While most research in backdoor attacks focuses on image classification, backdoor attacks on object detection have not been explored but are of equal importance. Object detection has been adopted as an important module in various security-sensitive applications such as autonomous driving. Therefore, backdoor attacks on object detection could pose severe threats to human lives and properties. We propose four kinds of backdoor attacks for object detection task: 1) Object Generation Attack: a trigger can falsely generate an object of the target class, 2) Regional Misclassification Attack: a trigger can change the prediction of a surrounding object to the target class, 3) Global Misclassification Attack: a single trigger can change the predictions of all objects in an image to the target class, and 4) Object Disappearance Attack: a trigger can make the detector fail to detect the object of the target class. We develop appropriate metrics to evaluate the four backdoor attacks on object detection. We perform experiments using two typical object detection models -- Faster-RCNN and YOLOv3 on different datasets. More crucially, we demonstrate that even fine-tuning on another benign dataset cannot remove the backdoor hidden in the object detection model. To defend against these backdoor attacks, we propose Detector Cleanse, an entropy-based run-time detection framework to identify poisoned testing samples for any deployed object detector.

","


","Springer
arXiv
DBLP
DBLP"
An Approach to Generation Triggers for Parrying Backdoor in Neural Networks,Menisov Artem,"Artificial General Intelligence
AGI","2023
2022","<a href=""Springer (2023) : An Approach to Generation Triggers for Parrying Backdoor in Neural Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-19907-3_29]</a>
<a href=""DBLP (2022) : An Approach to Generation Triggers for Parrying Backdoor in Neural Networks"" target=""_blank"">[https://doi.org/10.1007/978-3-031-19907-3_29]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-19907-3_29]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-19907-3_29]</a>","The lack of transparency in the results of the work of artificial neural networks makes them vulnerable to backdoor-attacks, which leads to...
","
","Springer
DBLP"
An Approach to Simulate Malware Propagation in the Internet of Drones,"E. E. Maurin Saldaña, A. Martín del Rey, A. B. Gil González","Distributed Computing and Artificial Intelligence, Special Sessions, 19th International Conference
Telematics and Computing","2023
2022","<a href=""Springer (2023) : An Approach to Simulate Malware Propagation in the Internet of Drones"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-23210-7_22]</a>
<a href=""Springer (2022) : An Approach to Simulate Malware Propagation in the Internet of Drones"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18082-8_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-23210-7_22]</a>
<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18082-8_23]</a>","This research addresses the problem of malicious code propagation in a swarm of drones. Its main objective is to establish the conceptual basis and...
This research addresses the problem of malicious code propagation in a swarm of drones. Its main objective is to establish the conceptual basis and...","
","Springer
Springer"
Detecting and Mitigating Backdoor Attacks with Dynamic and Invisible Triggers,"Zhibin Zheng, Zhongyun Hua, Leo Yu Zhang","Neural Information Processing
ICONIP","2023
2022","<a href=""Springer (2023) : Detecting and Mitigating Backdoor Attacks with Dynamic and Invisible Triggers"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-30111-7_19]</a>
<a href=""DBLP (2022) : Detecting and Mitigating Backdoor Attacks with Dynamic and Invisible Triggers"" target=""_blank"">[https://doi.org/10.1007/978-3-031-30111-7_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-30111-7_19]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-30111-7_19]</a>","When a deep learning-based model is attacked by backdoor attacks, it behaves normally for clean inputs, whereas outputs unexpected results for inputs...
","
","Springer
DBLP"
Image Watermarking Backdoor Attacks in CNN-Based Classification Tasks,"Giovanbattista Abbate, Irene Amerini, Roberto Caldelli","Pattern Recognition, Computer Vision, and Image Processing. ICPR 2022 International Workshops and Challenges
ICPR Workshops","2023
2022","<a href=""Springer (2023) : Image Watermarking Backdoor Attacks in CNN-Based Classification Tasks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-37745-7_1]</a>
<a href=""DBLP (2022) : Image Watermarking Backdoor Attacks in CNN-Based Classification Tasks"" target=""_blank"">[https://doi.org/10.1007/978-3-031-37745-7_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-37745-7_1]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-37745-7_1]</a>","In these last years, neural networks are becoming the basis for different kinds of applications and this is mainly due to the stunning performances...
","
","Springer
DBLP"
$\tt{PoisonedGNN}$: Backdoor Attack on Graph Neural Networks-Based Hardware Security Systems,"Lilas Alrahis, Satwik Patnaik, Muhammad Abdullah Hanif, Muhammad Shafique, Ozgur Sinanoglu",IEEE Trans. Computers,2023,"<a href=""DBLP (2023) : $\tt{PoisonedGNN}$: Backdoor Attack on Graph Neural Networks-Based Hardware Security Systems"" target=""_blank"">[https://doi.org/10.1109/TC.2023.3271126]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TC.2023.3271126]</a>",,,DBLP
"&quot,We Must Protect the Transformers&quot,: Understanding Efficacy of Backdoor Attack Mitigation on Transformer Models","Rohit Raj, Biplab Roy, Abir Das, Mainack Mondal",SPACE,2023,"<a href=""DBLP (2023) : &quot,We Must Protect the Transformers&quot,: Understanding Efficacy of Backdoor Attack Mitigation on Transformer Models"" target=""_blank"">[https://doi.org/10.1007/978-3-031-51583-5_14]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-51583-5_14]</a>",,,DBLP
A Comparative Performance Analysis of Various Antivirus Software,"Una Drakulić, Edin Mujčić","Advanced Technologies, Systems, and Applications VIII",2023,"<a href=""Springer (2023) : A Comparative Performance Analysis of Various Antivirus Software"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-43056-5_30]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-43056-5_30]</a>","With rapid technological advancement, security has become a major issue due to the increase in malware activity that poses a serious threat to the...",,Springer
A Comparative Study of Attribute Selection Algorithms on Intrusion Detection System in UAVs: A Case Study of UKM-IDS20 Dataset,"Ahmed Burhan Mohammed, Lamia Chaari Fourati, Ahmed M. Fakhrudeen",Risks and Security of Internet and Systems,2023,"<a href=""Springer (2023) : A Comparative Study of Attribute Selection Algorithms on Intrusion Detection System in UAVs: A Case Study of UKM-IDS20 Dataset"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-31108-6_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-31108-6_3]</a>",Security issues of unmanned aerial vehicles (UAVs) have received great attention. A new dataset named UKM-IDS20 has been recently developed for...,,Springer
A Comprehensive Analysis of Novel Intrusion Detection Systems for Internet of Things Networks,"Zouhair Chiba, Noreddine Abghour, ... Oumaima Lifandali",Artificial Intelligence and Smart Environment,2023,"<a href=""Springer (2023) : A Comprehensive Analysis of Novel Intrusion Detection Systems for Internet of Things Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-26254-8_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-26254-8_3]</a>","In recent years, the Internet of Things (IoT) paradigm has shown massive embracing by various industries notably the medical sector, vehicle...",,Springer
A Data-free Backdoor Injection Approach in Neural Networks,"Peizhuo Lv, Chang Yue, Ruigang Liang, Yunfei Yang, Shengzhi Zhang, Hualong Ma, Kai Chen",USENIX Security Symposium,2023,"<a href=""DBLP (2023) : A Data-free Backdoor Injection Approach in Neural Networks"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity23/presentation/lv]</a>","<a href=""DBLP"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity23/presentation/lv]</a>",,,DBLP
A Deep Learning Algorithm Using Feature Engineering to Adjust Attention Mechanisms and Neural Network for Cloud Security Detection,"Yiyang Xiong, Yajuan Qiao, ... Hua Tan","Signal and Information Processing, Networking and Computers",2023,"<a href=""Springer (2023) : A Deep Learning Algorithm Using Feature Engineering to Adjust Attention Mechanisms and Neural Network for Cloud Security Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-9968-0_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-9968-0_2]</a>","Cloud computing realizes the intensive management of resources and improves the production efficiency, but it also inevitably brings security...",,Springer
A Framework for Developing Tabletop Cybersecurity Exercises,"Nabin Chowdhury, Vasileios Gkioulos",Computer Security. ESORICS 2022 International Workshops,2023,"<a href=""Springer (2023) : A Framework for Developing Tabletop Cybersecurity Exercises"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-25460-4_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-25460-4_7]</a>","As remote work increases in adoption, partly pushed by the 2020 COVID-19 pandemic, conducting and offering security training to employees is ever...",,Springer
A Gradient Control Method for Backdoor Attacks on Parameter-Efficient Tuning,"Naibin Gu, Peng Fu, Xiyu Liu, Zhengxiao Liu, Zheng Lin, Weiping Wang",ACL,2023,"<a href=""DBLP (2023) : A Gradient Control Method for Backdoor Attacks on Parameter-Efficient Tuning"" target=""_blank"">[https://doi.org/10.18653/v1/2023.acl-long.194]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2023.acl-long.194]</a>",,,DBLP
A Machine Learning Based Approach to Detect Stealthy Cobalt Strike C &C Activities from Encrypted Network Traffic,"Fabian Martin Ramos, Xinyuan Wang",Machine Learning for Networking,2023,"<a href=""Springer (2023) : A Machine Learning Based Approach to Detect Stealthy Cobalt Strike C &C Activities from Encrypted Network Traffic"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-36183-8_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-36183-8_8]</a>",Cobalt Strike is a stealthy and powerful command and control (C &C) framework that has been widely used in many recent massive data breach attacks...,,Springer
A Max-Min Security Game for Coordinated Backdoor Attacks on Federated Learning,"Omar Abdel Wahab, Anderson Avila",IEEE Big Data,2023,"<a href=""DBLP (2023) : A Max-Min Security Game for Coordinated Backdoor Attacks on Federated Learning"" target=""_blank"">[https://doi.org/10.1109/BigData59044.2023.10386756]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/BigData59044.2023.10386756]</a>",,,DBLP
A Multi-agent Case-Based Reasoning Intrusion Detection System Prototype,"Jakob Michael Schoenborn, Klaus-Dieter Althoff",Case-Based Reasoning Research and Development,2023,"<a href=""Springer (2023) : A Multi-agent Case-Based Reasoning Intrusion Detection System Prototype"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-40177-0_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-40177-0_23]</a>","The number of actors, costs, and incidents in terms of internet criminality is rising each year as many devices in our daily routines become...",,Springer
A Multi-pronged Self-adaptive Controller for Analyzing Misconfigurations for Kubernetes Clusters and IoT Edge Devices,"Areeg Samir, Abdo Al-Wosabi, ... Håvard Dagenborg",Service-Oriented and Cloud Computing,2023,"<a href=""Springer (2023) : A Multi-pronged Self-adaptive Controller for Analyzing Misconfigurations for Kubernetes Clusters and IoT Edge Devices"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-46235-1_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-46235-1_10]</a>","Kubernetes default configurations do not always provide optimal security and performance for all clusters and IoT edge devices deployed, making them...",,Springer
A New Generation? A Discussion on Deep Generative Models in Supply Chains,"Eduardo e Oliveira, Teresa Pereira","Advances in Production Management Systems. Production Management Systems for Responsible Manufacturing, Service, and Logistics Futures",2023,"<a href=""Springer (2023) : A New Generation? A Discussion on Deep Generative Models in Supply Chains"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-43662-8_32]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-43662-8_32]</a>","With the advent of Chat-GPT, Artificial Intelligence (AI) became one of the most discussed technological developments of today. Although there are...",,Springer
A New Idea for RSA Backdoors,Marco Cesati,Cryptogr.,2023,"<a href=""DBLP (2023) : A New Idea for RSA Backdoors"" target=""_blank"">[https://doi.org/10.3390/cryptography7030045]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.3390/cryptography7030045]</a>",,,DBLP
A Peer to Peer Federated Graph Neural Network for Threat Intelligence,"Mouad Bouharoun, Bilal Taghdouti, Mohammed Erradi",Networked Systems,2023,"<a href=""Springer (2023) : A Peer to Peer Federated Graph Neural Network for Threat Intelligence"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-37765-5_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-37765-5_3]</a>",Threat intelligence is the process of collecting and analyzing information about potential cyber threats. Several approaches have been conducted for...,,Springer
A Polynomial Time Attack on Instances of M-SIDH and FESTA,"Wouter Castryck, Frederik Vercauteren",Advances in Cryptology – ASIACRYPT 2023,2023,"<a href=""Springer (2023) : A Polynomial Time Attack on Instances of M-SIDH and FESTA"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8739-9_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8739-9_5]</a>",The recent devastating attacks on SIDH rely on the fact that the protocol reveals the images...,,Springer
A Practical Clean-Label Backdoor Attack with Limited Information in Vertical Federated Learning,"Peng Chen, Jirui Yang, Junxiong Lin, Zhihui Lu, Qiang Duan, Hongfeng Chai",ICDM,2023,"<a href=""DBLP (2023) : A Practical Clean-Label Backdoor Attack with Limited Information in Vertical Federated Learning"" target=""_blank"">[https://doi.org/10.1109/ICDM58522.2023.00013]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICDM58522.2023.00013]</a>",,,DBLP
A Review of Cybersecurity Advancements in Unmanned Aerial Vehicle,"Mosladdin Mohammad Shueb, Xiangdong Che",Proceedings of the 2023 International Conference on Advances in Computing Research (ACR’23),2023,"<a href=""Springer (2023) : A Review of Cybersecurity Advancements in Unmanned Aerial Vehicle"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33743-7_30]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33743-7_30]</a>",Cybersecurity challenges hinder the growth of Unmanned Ariel Vehicles (UAVs or drones). The limitation of UAV onboard resources makes it difficult to...,,Springer
A Robust NFT Assisted Knowledge Distillation Framework for Edge Computing,"Nai Wang, Atul Sajjanhar, ... Longxiang Gao","Tools for Design, Implementation and Verification of Emerging Information Technologies",2023,"<a href=""Springer (2023) : A Robust NFT Assisted Knowledge Distillation Framework for Edge Computing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33458-0_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33458-0_2]</a>","With the development and improvement in chip manufacturing and network communication, Internet of Things (IoT) have been addressing more and more...",,Springer
A Security Policy Engine for Building Energy Management Systems,"Jiahui Lim, Wenshei Ong, ... Ertem Esiner",Applied Cryptography and Network Security Workshops,2023,"<a href=""Springer (2023) : A Security Policy Engine for Building Energy Management Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-41181-6_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-41181-6_13]</a>","This paper presents a Policy Engine for securing building energy management systems (BEMSs), a class of industrial control systems (ICSs) requiring...",,Springer
A Systematic Literature Review on Security Aspects of Virtualization,"Jehan Hasneen, Vishnupriya Narayanan, Kazi Masum Sadique",Hybrid Intelligent Systems,2023,"<a href=""Springer (2023) : A Systematic Literature Review on Security Aspects of Virtualization"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-27409-1_118]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-27409-1_118]</a>",Cloud computing is an emerging technology where organizations can have the flexibility of their infrastructure and store and retrieve data from cloud...,,Springer
A Temporal Chrominance Trigger for Clean-Label Backdoor Attack Against Anti-Spoof Rebroadcast Detection,"Wei Guo, Benedetta Tondi, Mauro Barni",IEEE Trans. Dependable Secur. Comput.,2023,"<a href=""DBLP (2023) : A Temporal Chrominance Trigger for Clean-Label Backdoor Attack Against Anti-Spoof Rebroadcast Detection"" target=""_blank"">[https://doi.org/10.1109/TDSC.2022.3233519]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TDSC.2022.3233519]</a>",,,DBLP
A Triggerless Backdoor Attack and Defense Mechanism for Intelligent Task Offloading in Multi-UAV Systems,"Shafkat Islam, Shahriar Badsha, Ibrahim Khalil, Mohammed Atiquzzaman, Charalambos Konstantinou",IEEE Internet Things J.,2023,"<a href=""DBLP (2023) : A Triggerless Backdoor Attack and Defense Mechanism for Intelligent Task Offloading in Multi-UAV Systems"" target=""_blank"">[https://doi.org/10.1109/JIOT.2022.3172936]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/JIOT.2022.3172936]</a>",,,DBLP
A Unified Detection Framework for Inference-Stage Backdoor Defenses,"Xun Xian, Ganghua Wang, Jayanth Srinivasa, Ashish Kundu, Xuan Bi, Mingyi Hong, Jie Ding",NeurIPS,2023,"<a href=""DBLP (2023) : A Unified Detection Framework for Inference-Stage Backdoor Defenses"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/1868a3c73d0d2a44c42458575fa8514c-Abstract-Conference.html]</a>","<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/1868a3c73d0d2a44c42458575fa8514c-Abstract-Conference.html]</a>",,,DBLP
A defense method against backdoor attacks on neural networks,"Sara Kaviani, Samaneh Shamshiri, Insoo Sohn",Expert Syst. Appl.,2023,"<a href=""DBLP (2023) : A defense method against backdoor attacks on neural networks"" target=""_blank"">[https://doi.org/10.1016/j.eswa.2022.118990]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.eswa.2022.118990]</a>",,,DBLP
A lightweight backdoor defense framework based on image inpainting,"Yier Wei, Haichang Gao, Yufei Wang, Yipeng Gao, Huan Liu",Neurocomputing,2023,"<a href=""DBLP (2023) : A lightweight backdoor defense framework based on image inpainting"" target=""_blank"">[https://doi.org/10.1016/j.neucom.2023.03.052]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.neucom.2023.03.052]</a>",,,DBLP
A stealthy and robust backdoor attack via frequency domain transform,"Ruitao Hou, Teng Huang, Hongyang Yan, Lishan Ke, Weixuan Tang",World Wide Web,2023,"<a href=""DBLP (2023) : A stealthy and robust backdoor attack via frequency domain transform"" target=""_blank"">[https://doi.org/10.1007/s11280-023-01153-3]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/s11280-023-01153-3]</a>",,,DBLP
AI Trojan Attack for Evading Machine Learning-based Detection of Hardware Trojans,Z. Pan P. Mishra,IEEE Transactions on Computers,2023,"<a href=""IEEE (2023) : AI Trojan Attack for Evading Machine Learning-based Detection of Hardware Trojans"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10057978]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TC.2023.3251864]</a>","The globalized semiconductor supply chain significantly increases the risk of exposing System-on-Chip (SoC) designs to hardware Trojans. While machine learning (ML) based Trojan detection approaches are promising due to their scalability as well as detection accuracy, ML-based methods themselves are vulnerable from Trojan attacks. In this paper, we propose a robust backdoor attack on ML-based Trojan detection algorithms to demonstrate this serious vulnerability. The proposed framework is able to design an AI Trojan and implant it inside the ML model that can be triggered by specific inputs. Experimental results demonstrate that the proposed AI Trojans can bypass state-of-the-art defense algorithms. Moreover, our approach provides a fast and cost-effective solution in achieving 100% attack success rate that outperforms state-of-the art methods based on adversarial attacks.",,IEEE
Adaptive Controller to Identify Misconfigurations and Optimize the Performance of Kubernetes Clusters and IoT Edge Devices,"Areeg Samir, Håvard Dagenborg",Service-Oriented and Cloud Computing,2023,"<a href=""Springer (2023) : Adaptive Controller to Identify Misconfigurations and Optimize the Performance of Kubernetes Clusters and IoT Edge Devices"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-46235-1_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-46235-1_11]</a>","Kubernetes default configurations do not always provide optimal security and performance for all clusters and IoT edge devices deployed, affecting...",,Springer
Agile Cryptography: A Universally Composable Approach,"Christian Badertscher, Michele Ciampi, Aggelos Kiayias",Theory of Cryptography,2023,"<a href=""Springer (2023) : Agile Cryptography: A Universally Composable Approach"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-48624-1_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-48624-1_18]</a>","Being capable of updating cryptographic algorithms is an inevitable and essential practice in cryptographic engineering. This cryptographic agility,...",,Springer
An Effective Protection Approach for Deceive Attacker in AES Attack,"R. Shashank, E. Prabhu","Proceedings of Fourth International Conference on Communication, Computing and Electronics Systems",2023,"<a href=""Springer (2023) : An Effective Protection Approach for Deceive Attacker in AES Attack"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-7753-4_37]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-7753-4_37]</a>","Nowadays, data security has gained much importance. Encryption has evolved as a solution and is now a necessary component of every information...",,Springer
An Intrusion Detection System and Attack Intension Used in Network Forensic Exploration,"Saswati Chatterjee, Lal Mohan Pattnaik, Suneeta Satpathy",Intelligent Systems and Machine Learning,2023,"<a href=""Springer (2023) : An Intrusion Detection System and Attack Intension Used in Network Forensic Exploration"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-35078-8_28]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-35078-8_28]</a>",Cyberattacks are occurring increasingly frequently as cyber science advances and people utilize the internet and other technology on a regular basis....,,Springer
An Investigation of Recent Backdoor Attacks and Defenses in Federated Learning,"Qiuxian Chen, Yizheng Tao",FMEC,2023,"<a href=""DBLP (2023) : An Investigation of Recent Backdoor Attacks and Defenses in Federated Learning"" target=""_blank"">[https://doi.org/10.1109/FMEC59375.2023.10306127]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/FMEC59375.2023.10306127]</a>",,,DBLP
An Investigation on the Detection of Intrusions into a Network Using Convolutional Neural Networks,"N. D. Patel, Ajeet Singh",Innovations in Computer Science and Engineering,2023,"<a href=""Springer (2023) : An Investigation on the Detection of Intrusions into a Network Using Convolutional Neural Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-7455-7_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-7455-7_5]</a>","As attacks on the network environment are rapidly becoming more sophisticated and intelligent in the recent years, the limitations of the existing...",,Springer
An adaptive robust defending algorithm against backdoor attacks in federated learning,"Yongkang Wang, Di-Hua Zhai, Yongping He, Yuanqing Xia",Future Gener. Comput. Syst.,2023,"<a href=""DBLP (2023) : An adaptive robust defending algorithm against backdoor attacks in federated learning"" target=""_blank"">[https://doi.org/10.1016/j.future.2023.01.026]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.future.2023.01.026]</a>",,,DBLP
Analysis of Intrusion Detection Using Ensemble Stacking-Based Machine Learning Techniques in IoT Networks,"Rao Naveed Bin Rais, Osman Khalid, ... Muhammad Usman Shahid Khan",Proceedings of the 2023 International Conference on Advances in Computing Research (ACR’23),2023,"<a href=""Springer (2023) : Analysis of Intrusion Detection Using Ensemble Stacking-Based Machine Learning Techniques in IoT Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33743-7_27]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33743-7_27]</a>","In the past few years, numerous machine learning techniques have been employed in IoT networks to develop Intrusion Detection Systems (IDS) that...",,Springer
Analysis of Techniques for Detection and Removal of Zero-Day Attacks (ZDA),"Khalid Hamid, Muhammad Waseem Iqbal, ... Muhammad Arif",Ubiquitous Security,2023,"<a href=""Springer (2023) : Analysis of Techniques for Detection and Removal of Zero-Day Attacks (ZDA)"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-0272-9_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-0272-9_17]</a>","Zero-day attacks (ZDAs) are previously unknown flaws and errors in operating systems, networks, and general-purpose software. ZDAs are the cause to...",,Springer
Analysis of Threat Models for Unmanned Aerial Vehicles from Different Spheres of Life,"Hanna Martyniuk, Bagdat Yagaliyeva, ... Bakhytzhan Akhmetov",Advances in Computer Science for Engineering and Education VI,2023,"<a href=""Springer (2023) : Analysis of Threat Models for Unmanned Aerial Vehicles from Different Spheres of Life"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-36118-0_53]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-36118-0_53]</a>","Unmanned aerial vehicles are currently used in various spheres of life: they are used for the agricultural industry, for photo and video filming, for...",,Springer
Analysis of Vulnerability Trends and Attacks in OT Systems,"Sandeep Gogineni Ravindrababu, Jim Alves-Foss",Proceedings of Seventh International Congress on Information and Communication Technology,2023,"<a href=""Springer (2023) : Analysis of Vulnerability Trends and Attacks in OT Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-1610-6_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-1610-6_12]</a>","For operational technology (OT) systems, security has been given an high priority in recent years after specific cyber-incidents targeting them....",,Springer
Android Malwares with Their Characteristics and Threats,"Tejpal Sharma, Dhavleesh Rattan",Mobile Radio Communications and 5G Networks,2023,"<a href=""Springer (2023) : Android Malwares with Their Characteristics and Threats"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-7982-8_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-7982-8_1]</a>","Android smartphones have a big share in global market in comparison as it is open-source architecture, high usage and popularity in the community of...",,Springer
Anomaly Based Intrusion Detection System Using Rule Based Genetic Algorithm,Shraddha R. Khonde,Intelligent Cyber Physical Systems and Internet of Things,2023,"<a href=""Springer (2023) : Anomaly Based Intrusion Detection System Using Rule Based Genetic Algorithm"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18497-0_56]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18497-0_56]</a>","In emergent field of networks everyone is able to access data as required. Huge amount of data transmission is done on internet, so data security,...",,Springer
Attacks Against Machine Learning Systems: Analysis and GAN-based Approach to Protection,"Igor Kotenko, Igor Saenko, ... Dmitry Iatsenko",Proceedings of the Seventh International Scientific Conference “Intelligent Information Technologies for Industry” (IITI’23),2023,"<a href=""Springer (2023) : Attacks Against Machine Learning Systems: Analysis and GAN-based Approach to Protection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-43792-2_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-43792-2_5]</a>",The rapid increase in the number of artificial intelligence applications has increased the importance of machine learning (ML) systems. The data used...,,Springer
Automated Binary Analysis: A Survey,"Zian Liu, Chao Chen, ... Jun Zhang",Algorithms and Architectures for Parallel Processing,2023,"<a href=""Springer (2023) : Automated Binary Analysis: A Survey"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22677-9_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22677-9_21]</a>",Binary code analysis is a process of analyzing the software or operating system when source code is inaccessible. This scenario occurs when one needs...,,Springer
Awareness of Phishing Attacks in the Public Sector: Review Types and Technical Approaches,"Mohammed Fahad Alghenaim, Nur Azaliah Abu Bakar, Fiza Abdul Rahim",Proceedings of the 2nd International Conference on Emerging Technologies and Intelligent Systems,2023,"<a href=""Springer (2023) : Awareness of Phishing Attacks in the Public Sector: Review Types and Technical Approaches"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-25274-7_54]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-25274-7_54]</a>","Today, phishing attempts are a major problem and a significant cyber risk. Phishing is one of the most common ways attackers try to steal money on...",,Springer
BATFL: Battling Backdoor Attacks in Federated Learning,"Mayank Kumar, Radha Agrawal, Priyanka Singh",SIN,2023,"<a href=""DBLP (2023) : BATFL: Battling Backdoor Attacks in Federated Learning"" target=""_blank"">[https://doi.org/10.1109/SIN60469.2023.10474981]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SIN60469.2023.10474981]</a>",,,DBLP
BHAC-MRI: Backdoor and Hybrid Attacks on MRI Brain Tumor Classification Using CNN,"Muhammad Imran, Hassaan Khaliq Qureshi, Irene Amerini",ICIAP,2023,"<a href=""DBLP (2023) : BHAC-MRI: Backdoor and Hybrid Attacks on MRI Brain Tumor Classification Using CNN"" target=""_blank"">[https://doi.org/10.1007/978-3-031-43153-1_28]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-43153-1_28]</a>",,,DBLP
BIRD: Generalizable Backdoor Detection and Removal for Deep Reinforcement Learning,"Xuan Chen, Wenbo Guo, Guanhong Tao, Xiangyu Zhang, Dawn Song",NeurIPS,2023,"<a href=""DBLP (2023) : BIRD: Generalizable Backdoor Detection and Removal for Deep Reinforcement Learning"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/802e90325f4c8546e13e5763b2ecab88-Abstract-Conference.html]</a>","<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/802e90325f4c8546e13e5763b2ecab88-Abstract-Conference.html]</a>",,,DBLP
Backdoor Attack on Deep Neural Networks in Perception Domain,"Xiaoxing Mo, Leo Yu Zhang, Nan Sun, Wei Luo, Shang Gao",IJCNN,2023,"<a href=""DBLP (2023) : Backdoor Attack on Deep Neural Networks in Perception Domain"" target=""_blank"">[https://doi.org/10.1109/IJCNN54540.2023.10191661]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/IJCNN54540.2023.10191661]</a>",,,DBLP
"Backdoor Attacks and Defenses in Federated Learning: State-of-the-Art, Taxonomy, and Future Directions","Xueluan Gong, Yanjiao Chen, Qian Wang, Weihan Kong",IEEE Wirel. Commun.,2023,"<a href=""DBLP (2023) : Backdoor Attacks and Defenses in Federated Learning: State-of-the-Art, Taxonomy, and Future Directions"" target=""_blank"">[https://doi.org/10.1109/MWC.017.2100714]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/MWC.017.2100714]</a>",,,DBLP
Backdoor Attacks to Deep Learning Models and Countermeasures: A Survey,"Yudong Li, Shigeng Zhang, Weiping Wang, Hong Song",IEEE Open J. Comput. Soc.,2023,"<a href=""DBLP (2023) : Backdoor Attacks to Deep Learning Models and Countermeasures: A Survey"" target=""_blank"">[https://doi.org/10.1109/OJCS.2023.3267221]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/OJCS.2023.3267221]</a>",,,DBLP
Backdoor attack and defense in federated generative adversarial network-based medical image synthesis,"Ruinan Jin, Xiaoxiao Li",Medical Image Anal.,2023,"<a href=""DBLP (2023) : Backdoor attack and defense in federated generative adversarial network-based medical image synthesis"" target=""_blank"">[https://doi.org/10.1016/j.media.2023.102965]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.media.2023.102965]</a>",,,DBLP
Backdoor-Resistant Public Data Integrity Verification Scheme Based on Smart Contracts,"Shanshan Li, Chunxiang Xu, Yuan Zhang, Yicong Du, Anjia Yang, Xinsheng Wen, Kefei Chen",IEEE Internet Things J.,2023,"<a href=""DBLP (2023) : Backdoor-Resistant Public Data Integrity Verification Scheme Based on Smart Contracts"" target=""_blank"">[https://doi.org/10.1109/JIOT.2023.3285939]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/JIOT.2023.3285939]</a>",,,DBLP
BadLiDet: A Simple Backdoor Attack against LiDAR Object Detection in Autonomous Driving,"Shuai Li, Yu Wen, Huiying Wang, Xu Cheng",TrustCom,2023,"<a href=""DBLP (2023) : BadLiDet: A Simple Backdoor Attack against LiDAR Object Detection in Autonomous Driving"" target=""_blank"">[https://doi.org/10.1109/TrustCom60117.2023.00035]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TrustCom60117.2023.00035]</a>",,,DBLP
Bayesian Causal Bandits with Backdoor Adjustment Prior,"Jireh Huang, Qing Zhou",Trans. Mach. Learn. Res.,2023,"<a href=""DBLP (2023) : Bayesian Causal Bandits with Backdoor Adjustment Prior"" target=""_blank"">[https://openreview.net/forum?id=sMsGv5Kfm3]</a>","<a href=""DBLP"" target=""_blank"">[https://openreview.net/forum?id=sMsGv5Kfm3]</a>",,,DBLP
Behavior Intrusion Detection System Using SVM and CNN,"Imen Chebbi, Ahlem Ben Younes, Leila Ben Ayed",Congress on Smart Computing Technologies,2023,"<a href=""Springer (2023) : Behavior Intrusion Detection System Using SVM and CNN"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-2468-4_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-2468-4_12]</a>",Recent decades have seen rapid advancement in the fields of networks and technology. Developing information security tools to detect novel threats...,,Springer
Behavioral System for the Detection of Modern and Distributed Intrusions Based on Artificial Intelligence Techniques: Behavior IDS-AI,"Imen Chebbi, Ahlem Ben Younes, Leila Ben Ayed",Proceedings of International Joint Conference on Advances in Computational Intelligence,2023,"<a href=""Springer (2023) : Behavioral System for the Detection of Modern and Distributed Intrusions Based on Artificial Intelligence Techniques: Behavior IDS-AI"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-1435-7_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-1435-7_21]</a>","Rapid progress in the world of technology and networks has occurred in recent decades. Piracy has proliferated, and many modern systems have been...",,Springer
Black-Box Dataset Ownership Verification via Backdoor Watermarking,"Yiming Li, Mingyan Zhu, Xue Yang, Yong Jiang, Tao Wei, Shu-Tao Xia",IEEE Trans. Inf. Forensics Secur.,2023,"<a href=""DBLP (2023) : Black-Box Dataset Ownership Verification via Backdoor Watermarking"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3265535]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3265535]</a>",,,DBLP
Bootstrapping Trust in Community Repository Projects,"Sangat Vaidya, Santiago Torres-Arias, ... Reza Curtmola",Security and Privacy in Communication Networks,2023,"<a href=""Springer (2023) : Bootstrapping Trust in Community Repository Projects"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-25538-0_24]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-25538-0_24]</a>","Community repositories such as PyPI and NPM are immensely popular and collectively serve more than a billion packages per day. However, existing...",,Springer
CRFs for Digital Signature and NIZK Proof System in Web Services,"Burong Kang, Lei Zhang, ... Xinyu Meng",Algorithms and Architectures for Parallel Processing,2023,"<a href=""Springer (2023) : CRFs for Digital Signature and NIZK Proof System in Web Services"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22677-9_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22677-9_11]</a>",Web services are service-oriented computing technology which allows computers running different operating domains to access and share each other’s...,,Springer
CSSBA: A Clean Label Sample-Specific Backdoor Attack,"Zihan Shen, Wei Hou, Yun Li",ICIP,2023,"<a href=""DBLP (2023) : CSSBA: A Clean Label Sample-Specific Backdoor Attack"" target=""_blank"">[https://doi.org/10.1109/ICIP49359.2023.10222085]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICIP49359.2023.10222085]</a>",,,DBLP
CVAR-FL IoV Intrusion Detection Framework,"Jia Zhao, Xinyu Rao, ... BoKai Yang",Information Security Practice and Experience,2023,"<a href=""Springer (2023) : CVAR-FL IoV Intrusion Detection Framework"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-7032-2_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-7032-2_8]</a>","With the popularization of internet of vehicles (IoV) applications, security issues are becoming increasingly prominent. IoV is vulnerable to various...",,Springer
Chameleon DNN Watermarking: Dynamically Public Model Ownership Verification,"Wei Li, Xiaoyu Zhang, ... Xiaofeng Chen",Information Security Applications,2023,"<a href=""Springer (2023) : Chameleon DNN Watermarking: Dynamically Public Model Ownership Verification"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-25659-2_25]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-25659-2_25]</a>","Deep neural network (DNN) has made unprecedented leaps in functionality and usefulness in the past few years, revolutionizing various promising...",,Springer
Classification Auto-Encoder Based Detector Against Diverse Data Poisoning Attacks,"Fereshteh Razmi, Li Xiong",Data and Applications Security and Privacy XXXVII,2023,"<a href=""Springer (2023) : Classification Auto-Encoder Based Detector Against Diverse Data Poisoning Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-37586-6_16]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-37586-6_16]</a>",Poisoning attacks are a category of adversarial machine learning threats in which an adversary attempts to subvert the outcome of the machine...,,Springer
Clean-image Backdoor: Attacking Multi-label Models with Poisoned Labels Only,"Kangjie Chen, Xiaoxuan Lou, Guowen Xu, Jiwei Li, Tianwei Zhang",ICLR,2023,"<a href=""DBLP (2023) : Clean-image Backdoor: Attacking Multi-label Models with Poisoned Labels Only"" target=""_blank"">[https://openreview.net/pdf?id=rFQfjDC9Mt]</a>","<a href=""DBLP"" target=""_blank"">[https://openreview.net/pdf?id=rFQfjDC9Mt]</a>",,,DBLP
Color Backdoor: A Robust Poisoning Attack in Color Space,"Wenbo Jiang, Hongwei Li, Guowen Xu, Tianwei Zhang",CVPR,2023,"<a href=""DBLP (2023) : Color Backdoor: A Robust Poisoning Attack in Color Space"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.00786]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.00786]</a>",,,DBLP
Comparative Study on Different Intrusion Detection Datasets Using Machine Learning and Deep Learning Algorithms,"G. Aarthi, S. Sharon Priya, W. Aisha Banu",Big Data and Cloud Computing,2023,"<a href=""Springer (2023) : Comparative Study on Different Intrusion Detection Datasets Using Machine Learning and Deep Learning Algorithms"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-1051-9_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-1051-9_8]</a>",The tremendous growth in the Internet of Things (IoT) creates great potential which provides us with incredible productivity and simplified our daily...,,Springer
Computation and Data Efficient Backdoor Attacks,"Yutong Wu, Xingshuo Han, Han Qiu, Tianwei Zhang",ICCV,2023,"<a href=""DBLP (2023) : Computation and Data Efficient Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00443]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00443]</a>",,,DBLP
Computational System Based on Machine Learning with Hybrid Security Technique to Classify Crime Offenses,"Ankit Bansal, Vijay Anant Athavale, ... Vinay Kukreja",Emergent Converging Technologies and Biomedical Systems,2023,"<a href=""Springer (2023) : Computational System Based on Machine Learning with Hybrid Security Technique to Classify Crime Offenses"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-2271-0_20]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-2271-0_20]</a>","Criminal investigation (CI) is a vital part of policing, with officers using a variety of traditional approaches to investigate crimes including...",,Springer
Content Style-triggered Backdoor Attack in Non-IID Federated Learning via Generative AI,"Jinke Cheng, Gaolei Li, Xi Lin, Hao Peng, Jianhua Li",ISPA/BDCloud/SocialCom/SustainCom,2023,"<a href=""DBLP (2023) : Content Style-triggered Backdoor Attack in Non-IID Federated Learning via Generative AI"" target=""_blank"">[https://doi.org/10.1109/ISPA-BDCloud-SocialCom-SustainCom59178.2023.00116]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ISPA-BDCloud-SocialCom-SustainCom59178.2023.00116]</a>",,,DBLP
CoviChain: A Blockchain Based COVID-19 Vaccination Passport,"Philip Bradish, Sarang Chaudhari, ... Hitesh Tewari",Advances in Information and Communication,2023,"<a href=""Springer (2023) : CoviChain: A Blockchain Based COVID-19 Vaccination Passport"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-28076-4_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-28076-4_17]</a>",Vaccination passports are being issued by governments around the world in order to open up their travel and hospitality sectors. Civil liberty...,,Springer
CryptoKitties vs. Axie Infinity: Computational Analysis of NFT Game Reddit Discussions,"Chien Lu, Giacomo Lauritano, Jaakko Peltonen","ArtsIT, Interactivity and Game Creation",2023,"<a href=""Springer (2023) : CryptoKitties vs. Axie Infinity: Computational Analysis of NFT Game Reddit Discussions"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-28993-4_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-28993-4_8]</a>",Online trading of non-fungible tokens (NFTs) and online gaming have recently been integrated as NFT-based games. We conducted a comparative study of...,,Springer
Cryptography from Planted Graphs: Security with Logarithmic-Size Messages,"Damiano Abram, Amos Beimel, ... Varun Narayanan",Theory of Cryptography,2023,"<a href=""Springer (2023) : Cryptography from Planted Graphs: Security with Logarithmic-Size Messages"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-48615-9_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-48615-9_11]</a>",We study the following broad question about cryptographic primitives: is it possible to achieve security against arbitrary...,,Springer
Cyber Risks and Security—A Case Study on Analysis of Malware,"Moulik Agrawal, Karan Deep Singh Mann, ... Deo Prakash Vidyarthi",International Conference on Innovative Computing and Communications,2023,"<a href=""Springer (2023) : Cyber Risks and Security—A Case Study on Analysis of Malware"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-3679-1_26]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-3679-1_26]</a>","The automation of business enterprises, the bulk computer storage to store sensitive information, various distributed applications being accessed via...",,Springer
Cyber Threat Intelligence Methodologies: Hunting Cyber Threats with Threat Intelligence Platforms and Deception Techniques,"Arturo E. Torres, Francisco Torres, Arturo Torres Budgud",2nd EAI International Conference on Smart Technology,2023,"<a href=""Springer (2023) : Cyber Threat Intelligence Methodologies: Hunting Cyber Threats with Threat Intelligence Platforms and Deception Techniques"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-07670-1_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-07670-1_2]</a>","Faced with the great wave of cyber threats, as well as the considerable increase in cybercrime in recent years, organizations have been forced to...",,Springer
Cyber-Attack in ICT Cloud Computing System,"Pranjal Chowdhury, Sourav Paul, ... Rajdeep Ghosh",Information and Communication Technology for Competitive Strategies (ICTCS 2021),2023,"<a href=""Springer (2023) : Cyber-Attack in ICT Cloud Computing System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-0095-2_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-0095-2_12]</a>","Nowadays, cloud system is laid low with cyber-attack that underpin a great deal of today’s social group options and monetary development. To grasp...",,Springer
Cybersecurity Attacks and Vulnerabilities During COVID-19,"Sharmin Akter Mim, Roksana Rahman, ... Rahamatullah Khondoker",Advanced Information Networking and Applications,2023,"<a href=""Springer (2023) : Cybersecurity Attacks and Vulnerabilities During COVID-19"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-28694-0_50]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-28694-0_50]</a>",As a result of quick transformation to digitalization for providing the employees teleworking/home office services with the capabilities to access...,,Springer
Cybersecurity Compliance Requirements for USA Department of Defense Contractors - Dragons at the Gate,Gordon J. Bruce,"HCI for Cybersecurity, Privacy and Trust",2023,"<a href=""Springer (2023) : Cybersecurity Compliance Requirements for USA Department of Defense Contractors - Dragons at the Gate"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-35822-7_20]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-35822-7_20]</a>","Dragons at the Gate provides you with insightful details about those who continue to exploit our intellectual expertise, and technical expertise and...",,Springer
"Cybersecurity for Industrial IoT, Threats, Vulnerabilities, and Solutions: A Brief Review","Andrea Sánchez-Zumba, Diego Avila-Pesantez",Proceedings of Eighth International Congress on Information and Communication Technology,2023,"<a href=""Springer (2023) : Cybersecurity for Industrial IoT, Threats, Vulnerabilities, and Solutions: A Brief Review"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-3243-6_90]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-3243-6_90]</a>","The Industrial Internet of Things (IIoT) refers to use connected devices and technology in industrial settings such as manufacturing, energy, and...",,Springer
DAPP IT – Multipurpose ID Card System for Universities,"C. Harshini, J. Vishal Ananth, ... V. Sumathy",Computational Intelligence in Data Science,2023,"<a href=""Springer (2023) : DAPP IT – Multipurpose ID Card System for Universities"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-38296-3_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-38296-3_4]</a>","Universities massively depend on identity cards for the identification of students, faculties, and staff. For the longest time identity cards haven’t...",,Springer
DUBIOUS: Detecting Unknown Backdoored Input by Observing Unusual Signatures,"Matthew Yudin, Rauf Izmailov",MILCOM,2023,"<a href=""DBLP (2023) : DUBIOUS: Detecting Unknown Backdoored Input by Observing Unusual Signatures"" target=""_blank"">[https://doi.org/10.1109/MILCOM58377.2023.10356229]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/MILCOM58377.2023.10356229]</a>",,,DBLP
Data Poisoning Attack by Label Flipping on SplitFed Learning,"Saurabh Gajbhiye, Priyanka Singh, Shaifu Gupta",Recent Trends in Image Processing and Pattern Recognition,2023,"<a href=""Springer (2023) : Data Poisoning Attack by Label Flipping on SplitFed Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-23599-3_30]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-23599-3_30]</a>","In the distributed machine learning scenario, we have Split Learning (SL) and Federated Learning (FL) as the popular techniques. In SL, the model is...",,Springer
Data Poisoning and Backdoor Attacks on Audio Intelligence Systems,"Yunjie Ge, Qian Wang, Jiayuan Yu, Chao Shen, Qi Li",IEEE Commun. Mag.,2023,"<a href=""DBLP (2023) : Data Poisoning and Backdoor Attacks on Audio Intelligence Systems"" target=""_blank"">[https://doi.org/10.1109/MCOM.012.2200596]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/MCOM.012.2200596]</a>",,,DBLP
Data Reconstruction Attack Against Principal Component Analysis,"Saloni Kwatra, Vicenç Torra",Security and Privacy in Social Networks and Big Data,2023,"<a href=""Springer (2023) : Data Reconstruction Attack Against Principal Component Analysis"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-5177-2_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-5177-2_5]</a>","Attacking machine learning models is one of the many ways to measure the privacy of machine learning models. Therefore, studying the performance of...",,Springer
Data Reconstruction from Gradient Updates in Federated Learning,"Xiaoxue Zhang, Junhao Li, ... Kongyang Chen",Machine Learning for Cyber Security,2023,"<a href=""Springer (2023) : Data Reconstruction from Gradient Updates in Federated Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-20096-0_44]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-20096-0_44]</a>","Federated learning has become an emerging technology to protect data privacy in the distributed learning area, by keeping each client user’s data...",,Springer
Deep Learning-Based Intrusion Detection Model for Network Security,"Sagar Dhanraj Pande, Govinda Rajulu Lanke, ... Pavitar Parkash Singh",Intelligent Computing and Networking,2023,"<a href=""Springer (2023) : Deep Learning-Based Intrusion Detection Model for Network Security"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-3177-4_27]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-3177-4_27]</a>","Since it serves as a potent means of network security defence, intrusion detection technology is an essential component of the network security...",,Springer
DeepMalOb: Deep Detection of Obfuscated Android Malware,"Zakaria Sawadogo, Jean-Marie Dembele, ... Samuel Ouya",Pan-African Artificial Intelligence and Smart Systems,2023,"<a href=""Springer (2023) : DeepMalOb: Deep Detection of Obfuscated Android Malware"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-25271-6_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-25271-6_19]</a>",The detection of malware android became very crucial with the use of obfuscation techniques by developers of malicious applications. In the...,,Springer
Defending against Backdoor Attacks in Natural Language Generation,"Xiaofei Sun, Xiaoya Li, Yuxian Meng, Xiang Ao, Lingjuan Lyu, Jiwei Li, Tianwei Zhang",AAAI,2023,"<a href=""DBLP (2023) : Defending against Backdoor Attacks in Natural Language Generation"" target=""_blank"">[https://doi.org/10.1609/aaai.v37i4.25656]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v37i4.25656]</a>",,,DBLP
Design and Execution of Cyberattacks Simulation for Practice-Oriented Experiential Learning,"Ashutosh Bahuguna, Samar Wazir",ICT with Intelligent Applications,2023,"<a href=""Springer (2023) : Design and Execution of Cyberattacks Simulation for Practice-Oriented Experiential Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-3571-8_43]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-3571-8_43]</a>","Convergence of ICT networks, if not properly secured, could lead to catastrophe, especially in critical infrastructure. A malicious cyberattack such...",,Springer
Detection of Anomalies and Attacks in Container Systems: An Integrated Approach Based on Black and White Lists,"Igor Kotenko, Igor Saenko, ... Nikita Petrevich",Proceedings of the Sixth International Scientific Conference “Intelligent Information Technologies for Industry” (IITI’22),2023,"<a href=""Springer (2023) : Detection of Anomalies and Attacks in Container Systems: An Integrated Approach Based on Black and White Lists"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-19620-1_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-19620-1_11]</a>","In this paper, we propose an approach for anomaly and attack detection based on the analysis of kernel logs obtained with enhanced Berkley Packet...",,Springer
Developing Security Recommender System Using Content-Based Filtering Mechanisms,"Maksim Iavich, Giorgi Iashvili, ... Avtandil Gagnidze","Information Technology for Education, Science, and Technics",2023,"<a href=""Springer (2023) : Developing Security Recommender System Using Content-Based Filtering Mechanisms"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-35467-0_37]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-35467-0_37]</a>","Machine learning and artificial intelligence are becoming more common today. They are used in a variety of areas, including the energy, medical, and...",,Springer
Distinguishing Good from Bad: Distributed-Collaborative-Representation-Based Data Fraud Detection in Federated Learning,"Zongxiang Zhang, Chenghong Zhang, ... Lihua Huang","HCI in Business, Government and Organizations",2023,"<a href=""Springer (2023) : Distinguishing Good from Bad: Distributed-Collaborative-Representation-Based Data Fraud Detection in Federated Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-36049-7_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-36049-7_19]</a>",Breaking down data silos and promoting data circulation and cooperation is an important topic in the digital age. As data security and privacy...,,Springer
"Don&apos,t Knock! Rowhammer at the Backdoor of DNN Models","M. Caner Tol, Saad Islam, Andrew J. Adiletta, Berk Sunar, Ziming Zhang",DSN,2023,"<a href=""DBLP (2023) : Don&apos,t Knock! Rowhammer at the Backdoor of DNN Models"" target=""_blank"">[https://doi.org/10.1109/DSN58367.2023.00023]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/DSN58367.2023.00023]</a>",,,DBLP
Early Estimation of Level Difficulty in Mobile-Games,"Maria Aguareles, Llorenç Badiella, ... Rahil Sachak-Pratwa",Applications of Industrial Mathematics,2023,"<a href=""Springer (2023) : Early Estimation of Level Difficulty in Mobile-Games"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-32130-6_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-32130-6_3]</a>","In this report, we investigate several approaches to estimate quantitatively the level of difficulty of mobile games by using data analysis and...",,Springer
Edge-Graph Convolution Network: An Intrusion Detection Approach for Industrial IoT,"Nilutpol Bora, Anamika Chauhan",Proceedings of Data Analytics and Management,2023,"<a href=""Springer (2023) : Edge-Graph Convolution Network: An Intrusion Detection Approach for Industrial IoT"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6550-2_44]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6550-2_44]</a>","Cyber-attacks on Industrial IoT systems can result in severe consequences such as production loss, equipment damage and even human casualties, and...",,Springer
Enabling Accurate Data Recovery for Mobile Devices Against Malware Attacks,"Wen Xie, Niusen Chen, Bo Chen",Security and Privacy in Communication Networks,2023,"<a href=""Springer (2023) : Enabling Accurate Data Recovery for Mobile Devices Against Malware Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-25538-0_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-25538-0_23]</a>","Mobile computing devices today suffer from various malware attacks. After the malware attack, it is challenging to restore the device’s data back to...",,Springer
Enhancing Federated Learning Robustness Through Clustering Non-IID Features,"Yanli Li, Abubakar Sadiq Sani, ... Wei Bao",Computer Vision – ACCV 2022 Workshops,2023,"<a href=""Springer (2023) : Enhancing Federated Learning Robustness Through Clustering Non-IID Features"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-27066-6_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-27066-6_4]</a>",Federated learning (FL) enables many clients to train a joint model without sharing the raw data. While many byzantine-robust FL methods have been...,,Springer
Enhancing Federated Learning Robustness Using Data-Agnostic Model Pruning,"Mark Huasong Meng, Sin G. Teo, ... Jin Song Dong",Advances in Knowledge Discovery and Data Mining,2023,"<a href=""Springer (2023) : Enhancing Federated Learning Robustness Using Data-Agnostic Model Pruning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33377-4_34]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33377-4_34]</a>",Federated learning enables multiple data owners with a common objective to participate in a machine learning task without sharing their raw data. At...,,Springer
Ensemble Machine Learning-Based Network Intrusion Detection System,"K. Indra Gandhi, Sudharsan Balaji, ... V. Suba Varshini",Evolution in Computational Intelligence,2023,"<a href=""Springer (2023) : Ensemble Machine Learning-Based Network Intrusion Detection System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6702-5_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6702-5_11]</a>","With the rise of cyber-attacks in fields like Education, Government, Banking, etc. the need for better Network Intrusion Detection System is...",,Springer
Evaluating the Robustness of Automotive Intrusion Detection Systems Against Evasion Attacks,"Stefano Longari, Francesco Noseda, ... Stefano Zanero","Cyber Security, Cryptology, and Machine Learning",2023,"<a href=""Springer (2023) : Evaluating the Robustness of Automotive Intrusion Detection Systems Against Evasion Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-34671-2_24]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-34671-2_24]</a>",This paper discusses the robustness of machine learning-based intrusion detection systems (IDSs) used in the Controller Area Networks context against...,,Springer
Evaluation Metrics for a Hybrid Classification System Based on the Distributivity Equation and the UNSW-NB15 Cyberattack Dataset,"Ewa Rak, Jaromir Sarzyński","Uncertainty and Imprecision in Decision Making and Decision Support - New Advances, Challenges, and Perspectives",2023,"<a href=""Springer (2023) : Evaluation Metrics for a Hybrid Classification System Based on the Distributivity Equation and the UNSW-NB15 Cyberattack Dataset"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-45069-3_28]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-45069-3_28]</a>",The aim of the study was to apply and evaluate the usefulness of the hybrid classifier to detect network intrusion threats on a comprehensive...,,Springer
Evil vs evil: using adversarial examples to against backdoor attack in federated learning,"Tao Liu, Mingjun Li, Haibin Zheng, Zhaoyan Ming, Jinyin Chen",Multim. Syst.,2023,"<a href=""DBLP (2023) : Evil vs evil: using adversarial examples to against backdoor attack in federated learning"" target=""_blank"">[https://doi.org/10.1007/s00530-022-00965-z]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/s00530-022-00965-z]</a>",,,DBLP
FLSwitch: Towards Secure and Fast Model Aggregation for Federated Deep Learning with a Learning State-Aware Switch,"Yunlong Mao, Ziqin Dang, ... Sheng Zhong",Applied Cryptography and Network Security,2023,"<a href=""Springer (2023) : FLSwitch: Towards Secure and Fast Model Aggregation for Federated Deep Learning with a Learning State-Aware Switch"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33488-7_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33488-7_18]</a>","Security and efficiency are two desirable properties of federated learning (FL). To enforce data security for FL participants, homomorphic encryption...",,Springer
FPGA-Enabled Efficient Framework for High-Performance Intrusion Prevention Systems,"Cuong Pham-Quoc, Tran Ngoc Thinh",Computational Science and Its Applications – ICCSA 2023 Workshops,2023,"<a href=""Springer (2023) : FPGA-Enabled Efficient Framework for High-Performance Intrusion Prevention Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-37120-2_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-37120-2_6]</a>","With the rapid increase of network-based services during and after the COVID-19 pandemic, preventing network attacks is an essential demand. This...",,Springer
FUBA: Federated Uncovering of Backdoor Attacks for Heterogeneous Data,"Fabiola Espinoza Castellon, Deepika Singh, Aurelien Mayoue, Cedric Gouy-Pailler",TPS-ISA,2023,"<a href=""DBLP (2023) : FUBA: Federated Uncovering of Backdoor Attacks for Heterogeneous Data"" target=""_blank"">[https://doi.org/10.1109/TPS-ISA58951.2023.00017]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TPS-ISA58951.2023.00017]</a>",,,DBLP
Facilitating Early-Stage Backdoor Attacks in Federated Learning With Whole Population Distribution Inference,"Tian Liu, Xueyang Hu, Tao Shu",IEEE Internet Things J.,2023,"<a href=""DBLP (2023) : Facilitating Early-Stage Backdoor Attacks in Federated Learning With Whole Population Distribution Inference"" target=""_blank"">[https://doi.org/10.1109/JIOT.2023.3237806]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/JIOT.2023.3237806]</a>",,,DBLP
Feature-Based Graph Backdoor Attack in the Node Classification Task,"Yang Chen, Zhonglin Ye, Haixing Zhao, Ying Wang",Int. J. Intell. Syst.,2023,"<a href=""DBLP (2023) : Feature-Based Graph Backdoor Attack in the Node Classification Task"" target=""_blank"">[https://doi.org/10.1155/2023/5418398]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1155/2023/5418398]</a>",,,DBLP
Fed-FA: Theoretically Modeling Client Data Divergence for Federated Language Backdoor Defense,"Zhiyuan Zhang, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun",NeurIPS,2023,"<a href=""DBLP (2023) : Fed-FA: Theoretically Modeling Client Data Divergence for Federated Language Backdoor Defense"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/c39578c86423df5f9e8834ce1cd456e4-Abstract-Conference.html]</a>","<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/c39578c86423df5f9e8834ce1cd456e4-Abstract-Conference.html]</a>",,,DBLP
FedGame: A Game-Theoretic Defense against Backdoor Attacks in Federated Learning,"Jinyuan Jia, Zhuowen Yuan, Dinuka Sahabandu, Luyao Niu, Arezoo Rajabi, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran",NeurIPS,2023,"<a href=""DBLP (2023) : FedGame: A Game-Theoretic Defense against Backdoor Attacks in Federated Learning"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/a6678e2be4ce7aef9d2192e03cd586b7-Abstract-Conference.html]</a>","<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/a6678e2be4ce7aef9d2192e03cd586b7-Abstract-Conference.html]</a>",,,DBLP
FedLS: An Anti-poisoning Attack Mechanism for Federated Network Intrusion Detection Systems Using Autoencoder-Based Latent Space Representations,"Tran Duc Luong, Vuong Minh Tien, ... Van-Hau Pham",Information Security Practice and Experience,2023,"<a href=""Springer (2023) : FedLS: An Anti-poisoning Attack Mechanism for Federated Network Intrusion Detection Systems Using Autoencoder-Based Latent Space Representations"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-7032-2_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-7032-2_2]</a>",The recent explosion in the number and advancement of cyberattacks induces the deployment of machine learning (ML)-based network intrusion detection...,,Springer
FedMC: Federated Learning with Mode Connectivity Against Distributed Backdoor Attacks,"Weiqi Wang, Chenhan Zhang, Shushu Liu, Mingjian Tang, An Liu, Shui Yu",ICC,2023,"<a href=""DBLP (2023) : FedMC: Federated Learning with Mode Connectivity Against Distributed Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1109/ICC45041.2023.10278903]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICC45041.2023.10278903]</a>",,,DBLP
FedTA: Locally-Differential Federated Learning with Top-k Mechanism and Adam Optimization,"Yuting Li, Guojun Wang, ... Guanghui Feng",Ubiquitous Security,2023,"<a href=""Springer (2023) : FedTA: Locally-Differential Federated Learning with Top-k Mechanism and Adam Optimization"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-0272-9_26]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-0272-9_26]</a>","With the explosive development of fields including big data and cloud computing, it has become a global trend for the public to place a premium on...",,Springer
From Homoglyphs to Enhancedhomoglyphs: Enhancing NLP Backdoor Strategies through Character Substitution,"Pingyuan Ge, Weihao Guo, Yuqing Zhang",CBD,2023,"<a href=""DBLP (2023) : From Homoglyphs to Enhancedhomoglyphs: Enhancing NLP Backdoor Strategies through Character Substitution"" target=""_blank"">[https://doi.org/10.1109/CBD63341.2023.00038]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CBD63341.2023.00038]</a>",,,DBLP
Fully Hidden Dynamic Trigger Backdoor Attacks,"Shintaro Narisada, Seira Hidano, Kazuhide Fukushima",ICAART,2023,"<a href=""DBLP (2023) : Fully Hidden Dynamic Trigger Backdoor Attacks"" target=""_blank"">[https://doi.org/10.5220/0011617800003393]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.5220/0011617800003393]</a>",,,DBLP
GAN-Based Data Generation Technique and its Evaluation for Intrusion Detection Systems,"Kundan Kumar Jha, Prabhkirat Singh, ... Vikash Kumar",Machine Vision and Augmented Intelligence,2023,"<a href=""Springer (2023) : GAN-Based Data Generation Technique and its Evaluation for Intrusion Detection Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-0189-0_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-0189-0_11]</a>",The rise in the popularity of Internet raises the security and privacy concerns due to vulnerabilities faced by the software. Machine learning attack...,,Springer
Going in Style: Audio Backdoors Through Stylistic Transformations,"Stefanos Koffas, Luca Pajola, Stjepan Picek, Mauro Conti",ICASSP,2023,"<a href=""DBLP (2023) : Going in Style: Audio Backdoors Through Stylistic Transformations"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10096332]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10096332]</a>",,,DBLP
IBA: Towards Irreversible Backdoor Attacks in Federated Learning,"Thuy Dung Nguyen, Tuan Nguyen, Anh Tran, Khoa D. Doan, Kok-Seng Wong",NeurIPS,2023,"<a href=""DBLP (2023) : IBA: Towards Irreversible Backdoor Attacks in Federated Learning"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/d0c6bc641a56bebee9d985b937307367-Abstract-Conference.html]</a>","<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/d0c6bc641a56bebee9d985b937307367-Abstract-Conference.html]</a>",,,DBLP
IMTM: Invisible Multi-trigger Multimodal Backdoor Attack,"Zhicheng Li, Piji Li, Xuan Sheng, Changchun Yin, Lu Zhou",NLPCC,2023,"<a href=""DBLP (2023) : IMTM: Invisible Multi-trigger Multimodal Backdoor Attack"" target=""_blank"">[https://doi.org/10.1007/978-3-031-44696-2_42]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-44696-2_42]</a>",,,DBLP
IPCADP-Equalizer: An Improved Multibalance Privacy Preservation Scheme against Backdoor Attacks in Federated Learning,"Wenjuan Lian, Yichi Zhang, Xin Chen, Bin Jia, Xiaosong Zhang",Int. J. Intell. Syst.,2023,"<a href=""DBLP (2023) : IPCADP-Equalizer: An Improved Multibalance Privacy Preservation Scheme against Backdoor Attacks in Federated Learning"" target=""_blank"">[https://doi.org/10.1155/2023/6357750]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1155/2023/6357750]</a>",,,DBLP
"Identifying Shared Security Vulnerabilities and Mitigation Strategies at the Intersection of Application Programming Interfaces (APIs), Application-Level and Operating System (OS) of Mobile Devices","Anteneh Girma, Michelle A. Guo, John Irungu","Proceedings of the Future Technologies Conference (FTC) 2022, Volume 2",2023,"<a href=""Springer (2023) : Identifying Shared Security Vulnerabilities and Mitigation Strategies at the Intersection of Application Programming Interfaces (APIs), Application-Level and Operating System (OS) of Mobile Devices"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18458-1_34]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18458-1_34]</a>","As mobile devices and applications continue to grow in popularity worldwide, so do the rise of cybersecurity attacks and threats to daily users of...",,Springer
Initial Intrusion Detection in Advanced Persistent Threats (APT’s) Using Machine Learning,"Singamaneni Krishnapriya, T. Chithralekha",Intelligent Systems and Sustainable Computing,2023,"<a href=""Springer (2023) : Initial Intrusion Detection in Advanced Persistent Threats (APT’s) Using Machine Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-4717-1_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-4717-1_17]</a>",Cyber-assaults have resulted in significant financial losses and the disruption of essential governmental services in recent years. Among these...,,Springer
Instance-Agnostic and Practical Clean Label Backdoor Attack Method for Deep Learning Based Face Recognition Models,"Tae-Hoon Kim, SeokHwan Choi, Yoon-Ho Choi",IEEE Access,2023,"<a href=""DBLP (2023) : Instance-Agnostic and Practical Clean Label Backdoor Attack Method for Deep Learning Based Face Recognition Models"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2023.3342922]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2023.3342922]</a>",,,DBLP
Intrusion Detection Using Attention-Based CNN-LSTM Model,"Ban Al-Omar, Zouheir Trabelsi",Artificial Intelligence Applications and Innovations,2023,"<a href=""Springer (2023) : Intrusion Detection Using Attention-Based CNN-LSTM Model"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-34111-3_43]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-34111-3_43]</a>","With the rise of sophisticated cyberattacks and the advent of complex and diverse technological systems, traditional methods of intrusion detection...",,Springer
Intrusion Detection in the IoT-Fog Adopting the GRU and CNN: A Deep Learning-Based Approach,"Zahraa Majeed Al-Khuzaie, Salah A. K. Albermany, Mohammed Ahmed AbdlNibe",Micro-Electronics and Telecommunication Engineering,2023,"<a href=""Springer (2023) : Intrusion Detection in the IoT-Fog Adopting the GRU and CNN: A Deep Learning-Based Approach"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-9512-5_35]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-9512-5_35]</a>",The main objective of this research is to utilize the technologies of cloud computing in the applications of the IoT. The IoT network is divided into...,,Springer
Invisible Encoded Backdoor attack on DNNs using Conditional GAN,"Iram Arshad, Yuansong Qiao, Brian Lee, Yuhang Ye",ICCE,2023,"<a href=""DBLP (2023) : Invisible Encoded Backdoor attack on DNNs using Conditional GAN"" target=""_blank"">[https://doi.org/10.1109/ICCE56470.2023.10043484]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICCE56470.2023.10043484]</a>",,,DBLP
Iterative Dichotomiser 3 (ID3) for Detecting Firmware Attacks on Gadgets (ID3-DFA),"A. Punidha, E. Arul",Advances in Data-Driven Computing and Intelligent Systems,2023,"<a href=""Springer (2023) : Iterative Dichotomiser 3 (ID3) for Detecting Firmware Attacks on Gadgets (ID3-DFA)"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-3250-4_49]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-3250-4_49]</a>","Even while the wonderful new product's standard distribution condition still offers very little protection, receiving a cutting-edge device during...",,Springer
Kaleidoscope: Physical Backdoor Attacks Against Deep Neural Networks With RGB Filters,"Xueluan Gong, Ziyao Wang, Yanjiao Chen, Meng Xue, Qian Wang, Chao Shen",IEEE Trans. Dependable Secur. Comput.,2023,"<a href=""DBLP (2023) : Kaleidoscope: Physical Backdoor Attacks Against Deep Neural Networks With RGB Filters"" target=""_blank"">[https://doi.org/10.1109/TDSC.2023.3239225]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TDSC.2023.3239225]</a>",,,DBLP
KerbNet: A QoE-aware Kernel-Based Backdoor Attack Framework,X. Gong Y. Chen H. Huang W. Kong Z. Wang C. Shen Q. Wang,IEEE Transactions on Dependable and Secure Computing,2023,"<a href=""IEEE (2023) : KerbNet: A QoE-aware Kernel-Based Backdoor Attack Framework"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10154120]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TDSC.2023.3286842]</a>","Deep neural networks are vulnerable to backdoor attacks, where a specially-designed trigger will lead to misclassification of any benign samples. However, existing backdoor attacks usually impose conspicuous patch triggers on images, which are easily detected by humans and defense algorithms. Existing works on invisible triggers, however, either have reduced attack success rate or yield detectable patterns to visual inspections. In this paper, we propose KerbNet, a kernel-based backdoor attack framework, which applies kernel operations to clean samples as the trigger to incur misclassification. The kernel-processed samples achieve a high attack success rate while appearing natural with high Quality-of-Experience (QoE). We carefully design the kernel trigger generation algorithm by exploiting the neural network structure to propagate the influence of the trigger to the target misclassification label under the QoE constraint. We conduct extensive experiments on five datasets, i.e., MNIST, GTSRB, CIFAR-10, CelebA, and ImageNette to evaluate the effectiveness and practicality of KerbNet under the impact of various factors, including neuron-residing layer, kernel size, base image, loss function, model structure, and so on. We also show that our proposed attacks can evade state-of-the-art defense strategies and visual inspections. Code will be available after publication.",,IEEE
Knowledge Distillation Based Defense for Audio Trigger Backdoor in Federated Learning,"Yu-Wen Chen, Bo-Hsu Ke, Bozhong Chen, Si-Rong Chiu, Chun-Wei Tu, Jian-Jhih Kuo",GLOBECOM,2023,"<a href=""DBLP (2023) : Knowledge Distillation Based Defense for Audio Trigger Backdoor in Federated Learning"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM54140.2023.10437601]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM54140.2023.10437601]</a>",,,DBLP
LR-BA: Backdoor attack against vertical federated learning using local latent representations,"Yuhao Gu, Yuebin Bai",Comput. Secur.,2023,"<a href=""DBLP (2023) : LR-BA: Backdoor attack against vertical federated learning using local latent representations"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103193]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103193]</a>",,,DBLP
Lockdown: Backdoor Defense for Federated Learning with Isolated Subspace Training,"Tiansheng Huang, Sihao Hu, Ka Ho Chow, Fatih Ilhan, Selim F. Tekin, Ling Liu",NeurIPS,2023,"<a href=""DBLP (2023) : Lockdown: Backdoor Defense for Federated Learning with Isolated Subspace Training"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/2376f25ef1725a9e3516ee3c86a59f46-Abstract-Conference.html]</a>","<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/2376f25ef1725a9e3516ee3c86a59f46-Abstract-Conference.html]</a>",,,DBLP
"Lookin&apos, Out My Backdoor! Investigating Backdooring Attacks Against DL-driven Malware Detectors","Mario D&apos,Onghia, Federico Di Cesare, Luigi Gallo, Michele Carminati, Mario Polino, Stefano Zanero",AISec@CCS,2023,"<a href=""DBLP (2023) : Lookin&apos, Out My Backdoor! Investigating Backdooring Attacks Against DL-driven Malware Detectors"" target=""_blank"">[https://doi.org/10.1145/3605764.3623919]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3605764.3623919]</a>",,,DBLP
M-SIDH and MD-SIDH: Countering SIDH Attacks by Masking Information,"Tako Boris Fouotsa, Tomoki Moriya, Christophe Petit",Advances in Cryptology – EUROCRYPT 2023,2023,"<a href=""Springer (2023) : M-SIDH and MD-SIDH: Countering SIDH Attacks by Masking Information"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-30589-4_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-30589-4_10]</a>","The SIDH protocol is an isogeny-based key exchange protocol using supersingular isogenies, designed by Jao and De Feo in 2011. The protocol underlies...",,Springer
MARNet: Backdoor Attacks Against Cooperative Multi-Agent Reinforcement Learning,"Yanjiao Chen, Zhicong Zheng, Xueluan Gong",IEEE Trans. Dependable Secur. Comput.,2023,"<a href=""DBLP (2023) : MARNet: Backdoor Attacks Against Cooperative Multi-Agent Reinforcement Learning"" target=""_blank"">[https://doi.org/10.1109/TDSC.2022.3207429]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TDSC.2022.3207429]</a>",,,DBLP
MEDIC: Remove Model Backdoors via Importance Driven Cloning,"Qiuling Xu, Guanhong Tao, Jean Honorio, Yingqi Liu, Shengwei An, Guangyu Shen, Siyuan Cheng, Xiangyu Zhang",CVPR,2023,"<a href=""DBLP (2023) : MEDIC: Remove Model Backdoors via Importance Driven Cloning"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.01962]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.01962]</a>",,,DBLP
MIA-Leak: Exploring Membership Inference Attacks in Federated Learning Systems,"Chengcheng Zhu, Jiale Zhang, ... Xiaobing Sun",Blockchain Technology and Emerging Technologies,2023,"<a href=""Springer (2023) : MIA-Leak: Exploring Membership Inference Attacks in Federated Learning Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-31420-9_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-31420-9_9]</a>",Federated learning has achieved significant success in both academia and industry scenarios since it can train a joint model among unbalanced...,,Springer
ML-Based Trojan Classification: Repercussions of Toxic Boundary Nets,S. Mulhem F. Muuss C. Ewert R. Buchty M. Berekovic,IEEE Embedded Systems Letters,2023,"<a href=""IEEE (2023) : ML-Based Trojan Classification: Repercussions of Toxic Boundary Nets"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341539]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/LES.2023.3338543]</a>","Machine learning (ML) algorithms were recently adapted for testing integrated circuits and detecting potential design backdoors. Such testing mechanisms mainly rely on the available training dataset and the extracted features of the Trojan circuit. In this paper, we demonstrate that this method is attackable by exploiting a structural problem of classifiers for hardware Trojan detection in gate-level netlists, called the Boundary Net Problem. There, an adversary modifies the labels of those boundary nets, connecting the original logic to the Trojan circuit. We show that the proposed adversarial label-flipping attacks are potentially highly toxic to the accuracy of supervised ML-based Trojan detection approaches. The experimental results indicate that an adversary needs to flip only 0.09% of all labels to achieve an accuracy drop of over 9%, demonstrating one of the most efficient adversarial label-flipping attacks in the hardware Trojan detection research domain.",,IEEE
Machine Learning Anomaly-Based Network Intrusion Detection: Experimental Evaluation,"Ahmed Ramzi Bahlali, Abdelmalik Bachir",Advanced Information Networking and Applications,2023,"<a href=""Springer (2023) : Machine Learning Anomaly-Based Network Intrusion Detection: Experimental Evaluation"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-28451-9_34]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-28451-9_34]</a>",The use of Machine Learning (ML) approaches to design anomaly-based network intrusion detection systems (A-NIDS) has been attracting growing interest...,,Springer
Machine Learning-Based Solutions for Securing IoT Systems Against Multilayer Attacks,"Badeea Al Sukhni, Soumya K. Manna, ... Leishi Zhang","Communication, Networks and Computing",2023,"<a href=""Springer (2023) : Machine Learning-Based Solutions for Securing IoT Systems Against Multilayer Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-43140-1_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-43140-1_13]</a>",IoT systems are prone to security attacks from several IoT layers as most of them possess limited resources and are unable to implement standard...,,Springer
MagBackdoor: Beware of Your Loudspeaker as A Backdoor For Magnetic Injection Attacks,"Tiantian Liu, Feng Lin, Zhangsen Wang, Chao Wang, Zhongjie Ba, Li Lu, Wenyao Xu, Kui Ren",SP,2023,"<a href=""DBLP (2023) : MagBackdoor: Beware of Your Loudspeaker as A Backdoor For Magnetic Injection Attacks"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179364]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179364]</a>",,,DBLP
"Major Role of Artificial Intelligence, Machine Learning, and Deep Learning in Identity and Access Management Field: Challenges and State of the Art","Sara Aboukadri, Aafaf Ouaddah, Abdellatif Mezrioui",Proceedings of the 8th International Conference on Advanced Intelligent Systems and Informatics 2022,2023,"<a href=""Springer (2023) : Major Role of Artificial Intelligence, Machine Learning, and Deep Learning in Identity and Access Management Field: Challenges and State of the Art"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-20601-6_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-20601-6_5]</a>","In order to meet the growing needs of organizations and individuals to access services and systems remotely, especially during the period of the...",,Springer
Malware Detection and Classification Using Ensemble of BiLSTMs with Huffman Feature Optimization,"Osho Sharma, Akashdeep Sharma, Arvind Kalia",Proceedings of International Conference on Computational Intelligence and Data Engineering,2023,"<a href=""Springer (2023) : Malware Detection and Classification Using Ensemble of BiLSTMs with Huffman Feature Optimization"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-0609-3_30]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-0609-3_30]</a>",Context: Malware attacks are responsible for data breaches and financial losses across the globe. Traditional signature-based malware detection...,,Springer
Malware Detection and Classification Using Hybrid Machine Learning Algorithm,"Saiful Islam Rimon, Md. Mokammel Haque",Intelligent Computing & Optimization,2023,"<a href=""Springer (2023) : Malware Detection and Classification Using Hybrid Machine Learning Algorithm"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-19958-5_39]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-19958-5_39]</a>",Malware is a harmful program that affects digital systems severely. Malware detection is an important job in the field of cybersecurity. It is a...,,Springer
"Maximum Entropy Loss, the Silver Bullet Targeting Backdoor Attacks in Pre-trained Language Models","Zhengxiao Liu, Bowen Shen, Zheng Lin, Fali Wang, Weiping Wang",ACL,2023,"<a href=""DBLP (2023) : Maximum Entropy Loss, the Silver Bullet Targeting Backdoor Attacks in Pre-trained Language Models"" target=""_blank"">[https://doi.org/10.18653/v1/2023.findings-acl.237]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2023.findings-acl.237]</a>",,,DBLP
Mitigating Sybil Attacks in Federated Learning,"Ahmed E. Samy, Šarūnas Girdzijauskas",Information Security Practice and Experience,2023,"<a href=""Springer (2023) : Mitigating Sybil Attacks in Federated Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-7032-2_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-7032-2_3]</a>","Federated learning (FL) is a distributed learning paradigm that facilities a basic data-privacy level, as the clients do not have to share their raw...",,Springer
Modified K-Neighbor Outperforms Logistic Regression and Random Forest in Identifying Host Malware Across Limited Data Sets,"Manish Kumar Rai, K. Haripriya, Priyanka Sharma",Advanced Network Technologies and Intelligent Computing,2023,"<a href=""Springer (2023) : Modified K-Neighbor Outperforms Logistic Regression and Random Forest in Identifying Host Malware Across Limited Data Sets"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-28180-8_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-28180-8_8]</a>","Using probabilistic risk assessment and decision-making methodology, this study analyzes and manages risks to Supervisory Control and Data...",,Springer
Multi-dimensional Hybrid Bayesian Belief Network Based Approach for APT Malware Detection in Various Systems,"Amit Sharma, Brij B. Gupta, ... V. K. Saraswat","International Conference on Cyber Security, Privacy and Networking (ICSPN 2022)",2023,"<a href=""Springer (2023) : Multi-dimensional Hybrid Bayesian Belief Network Based Approach for APT Malware Detection in Various Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22018-0_16]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22018-0_16]</a>",We are living in a digital world where information flows in the form of bits. The world is witnessing rapid transformation in easy of living by...,,Springer
Multi-labeling of Malware Samples Using Behavior Reports and Fuzzy Hashing,"Rolando Sánchez-Fraga, Raúl Acosta-Bermejo, Eleazar Aguirre-Anaya",Telematics and Computing,2023,"<a href=""Springer (2023) : Multi-labeling of Malware Samples Using Behavior Reports and Fuzzy Hashing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-45316-8_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-45316-8_19]</a>",Current binary and multi-class (family) approaches for malware classification can hardly be of use for the identification and analysis of other...,,Springer
Multi-layer Perceptron for Intrusion Detection Using Simulated Annealing,"Sarra Cherfi, Ammar Boulaiche, Ali Lemouari",Modelling and Implementation of Complex Systems,2023,"<a href=""Springer (2023) : Multi-layer Perceptron for Intrusion Detection Using Simulated Annealing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18516-8_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18516-8_3]</a>","Today, due to the evolution of technology and the use of the Internet on a large scale, securing everything is becoming an unavoidable necessity and...",,Springer
Network Monitoring Index in the Information Security Management System of Critical Information Infrastructure Objects,"Mykola Khudyntsev, Oleksii Lebid, ... Andrii Davydiuk",Information and Communication Technologies and Sustainable Development,2023,"<a href=""Springer (2023) : Network Monitoring Index in the Information Security Management System of Critical Information Infrastructure Objects"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-46880-3_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-46880-3_17]</a>",An urgent problem of the implementation of the security information end event management of the critical information infrastructure objects is the...,,Springer
Neural Network Information Leakage Through Hidden Learning,"Arthur Carvalho Walraven da Cunha, Emanuele Natale, Laurent Viennot",Optimization and Learning,2023,"<a href=""Springer (2023) : Neural Network Information Leakage Through Hidden Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-34020-8_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-34020-8_8]</a>",We investigate the problem of making an artificial neural network perform hidden computations whose result can be easily retrieved from the network’s...,,Springer
Not All Samples Are Born Equal: Towards Effective Clean-Label Backdoor Attacks,"Yinghua Gao, Yiming Li, Linghui Zhu, Dongxian Wu, Yong Jiang, Shu-Tao Xia",Pattern Recognit.,2023,"<a href=""DBLP (2023) : Not All Samples Are Born Equal: Towards Effective Clean-Label Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1016/j.patcog.2023.109512]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.patcog.2023.109512]</a>",,,DBLP
Obfuscated Malware Detection: Impacts on Detection Methods,"Nor Zakiah Gorment, Ali Selamat, Ondrej Krejcar",Recent Challenges in Intelligent Information and Database Systems,2023,"<a href=""Springer (2023) : Obfuscated Malware Detection: Impacts on Detection Methods"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-42430-4_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-42430-4_5]</a>",Obfuscated malware poses a challenge to traditional malware detection methods as it uses various techniques to disguise its behavior and evade...,,Springer
On Perfect Linear Approximations and Differentials over Two-Round SPNs,"Christof Beierle, Patrick Felke, ... Lukas Stennes",Advances in Cryptology – CRYPTO 2023,2023,"<a href=""Springer (2023) : On Perfect Linear Approximations and Differentials over Two-Round SPNs"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-38548-3_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-38548-3_8]</a>",Recent constructions of (tweakable) block ciphers with an embedded cryptographic backdoor relied on the existence of probability-one differentials...,,Springer
On Poisoned Wardrop Equilibrium in Congestion Games,"Yunian Pan, Quanyan Zhu",Decision and Game Theory for Security,2023,"<a href=""Springer (2023) : On Poisoned Wardrop Equilibrium in Congestion Games"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-26369-9_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-26369-9_10]</a>",Recent years have witnessed a growing number of attack vectors against increasingly interconnected traffic networks. Informational attacks have...,,Springer
On the Effectiveness of Adversarial Training Against Backdoor Attacks,Y. Gao D. Wu J. Zhang G. Gan S. -T. Xia G. Niu M. Sugiyama,IEEE Transactions on Neural Networks and Learning Systems,2023,"<a href=""IEEE (2023) : On the Effectiveness of Adversarial Training Against Backdoor Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10153093]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TNNLS.2023.3281872]</a>","Although adversarial training (AT) is regarded as a potential defense against backdoor attacks, AT and its variants have only yielded unsatisfactory results or have even inversely strengthened backdoor attacks. The large discrepancy between expectations and reality motivates us to thoroughly evaluate the effectiveness of AT against backdoor attacks across various settings for AT and backdoor attacks. We find that the type and budget of perturbations used in AT are important, and AT with common perturbations is only effective for certain backdoor trigger patterns. Based on these empirical findings, we present some practical suggestions for backdoor defense, including relaxed adversarial perturbation and composite AT. This work not only boosts our confidence in AT’s ability to defend against backdoor attacks but also provides some important insights for future research.",,IEEE
On the Impossibility of Surviving (Iterated) Deletion of Weakly Dominated Strategies in Rational MPC,"Johannes Blömer, Jan Bobolz, Henrik Bröcher",Theory of Cryptography,2023,"<a href=""Springer (2023) : On the Impossibility of Surviving (Iterated) Deletion of Weakly Dominated Strategies in Rational MPC"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-48615-9_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-48615-9_14]</a>",Rational multiparty computation (rational MPC) provides a framework for analyzing MPC protocols through the lens of game theory. One way to judge...,,Springer
On the Post-quantum Security of Classical Authenticated Encryption Schemes,"Nathalie Lang, Stefan Lucks",Progress in Cryptology - AFRICACRYPT 2023,2023,"<a href=""Springer (2023) : On the Post-quantum Security of Classical Authenticated Encryption Schemes"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-37679-5_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-37679-5_4]</a>","We study the post-quantum security of authenticated encryption (AE) schemes, designed with classical security in mind. Under superposition attacks,...",,Springer
"Orand - A Fast, Publicly Verifiable, Scalable Decentralized Random Number Generator Based on Distributed Verifiable Random Functions","Pham Nhat Minh, Chiro Hiro, Khuong Nguyen-An",Integrated Uncertainty in Knowledge Modelling and Decision Making,2023,"<a href=""Springer (2023) : Orand - A Fast, Publicly Verifiable, Scalable Decentralized Random Number Generator Based on Distributed Verifiable Random Functions"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-46781-3_30]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-46781-3_30]</a>","This paper introduces Orand, a fast, publicly verifiable, scalable decentralized random number generator designed for applications where public...",,Springer
Orion: Online Backdoor Sample Detection via Evolution Deviance,"Huayang Huang, Qian Wang, Xueluan Gong, Tao Wang",IJCAI,2023,"<a href=""DBLP (2023) : Orion: Online Backdoor Sample Detection via Evolution Deviance"" target=""_blank"">[https://doi.org/10.24963/ijcai.2023/96]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.24963/ijcai.2023/96]</a>",,,DBLP
PELICAN: Exploiting Backdoors of Naturally Trained Deep Learning Models In Binary Code Analysis,"Zhuo Zhang, Guanhong Tao, Guangyu Shen, Shengwei An, Qiuling Xu, Yingqi Liu, Yapeng Ye, Yaoxuan Wu, Xiangyu Zhang",USENIX Security Symposium,2023,"<a href=""DBLP (2023) : PELICAN: Exploiting Backdoors of Naturally Trained Deep Learning Models In Binary Code Analysis"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity23/presentation/zhang-zhuo-pelican]</a>","<a href=""DBLP"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity23/presentation/zhang-zhuo-pelican]</a>",,,DBLP
Penetration Testing of Web Server Using Metasploit Framework and DVWA,"Tamanna Jena Singhdeo, S. R. Reeja, ... Suresh Satapathy",Intelligent Data Engineering and Analytics,2023,"<a href=""Springer (2023) : Penetration Testing of Web Server Using Metasploit Framework and DVWA"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6706-3_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6706-3_17]</a>","Cyberspaces are ubiquitous today. These online spaces have made their mark in fields such as education, government, and ecommerce. It is believed...",,Springer
Performance Analysis of Deep Neural Network for Intrusion Detection Systems,"Harshit Jha, Maulik Khanna, ... Rajni Jindal",ICT with Intelligent Applications,2023,"<a href=""Springer (2023) : Performance Analysis of Deep Neural Network for Intrusion Detection Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-3758-5_41]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-3758-5_41]</a>",Intrusion detection system or abbreviated as IDS is an important security system that is used to protect advanced networks used for communication...,,Springer
Phishing Attack Types and Mitigation: A Survey,"Mohammed Fahad Alghenaim, Nur Azaliah Abu Bakar, ... Gamal Alkawsi",Data Science and Emerging Technologies,2023,"<a href=""Springer (2023) : Phishing Attack Types and Mitigation: A Survey"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-0741-0_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-0741-0_10]</a>","The proliferation of the internet and computing devices has drawn much attention during the Covid-19 pandemic stay home and work, and this has led...",,Springer
Pixel-Wise Reconstruction of Private Data in Split Federated Learning,"Hong Huang, Xingyang Li, Wenjian He",Information and Communications Security,2023,"<a href=""Springer (2023) : Pixel-Wise Reconstruction of Private Data in Split Federated Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-7356-9_26]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-7356-9_26]</a>","This study investigates the security of split federated learning (SFL), a collaborative deep learning scheme that provides similar peak performance...",,Springer
Poisoning with Cerberus: Stealthy and Colluded Backdoor Attack against Federated Learning,"Xiaoting Lyu, Yufei Han, Wei Wang, Jingkai Liu, Bin Wang, Jiqiang Liu, Xiangliang Zhang",AAAI,2023,"<a href=""DBLP (2023) : Poisoning with Cerberus: Stealthy and Colluded Backdoor Attack against Federated Learning"" target=""_blank"">[https://doi.org/10.1609/aaai.v37i7.26083]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v37i7.26083]</a>",,,DBLP
Poisoning-Based Backdoor Attacks in Computer Vision,Yiming Li,AAAI,2023,"<a href=""DBLP (2023) : Poisoning-Based Backdoor Attacks in Computer Vision"" target=""_blank"">[https://doi.org/10.1609/aaai.v37i13.26921]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v37i13.26921]</a>",,,DBLP
Pragmatic Way of Analyzing Malware Attacks Detection in IoT Devices Using Deep Learning,"Moushumi Barman, Bobby Sharma",Communication and Intelligent Systems,2023,"<a href=""Springer (2023) : Pragmatic Way of Analyzing Malware Attacks Detection in IoT Devices Using Deep Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-2100-3_52]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-2100-3_52]</a>","Internet of Things (IoT) plays a vital role in transforming the world from telephone to smartphone, typewriter to laptop then notebook, normal home...",,Springer
Preventing Text Data Poisoning Attacks in Federated Machine Learning by an Encrypted Verification Key,"Mahdee Jodayree, Wenbo He, Ryszard Janicki",Rough Sets,2023,"<a href=""Springer (2023) : Preventing Text Data Poisoning Attacks in Federated Machine Learning by an Encrypted Verification Key"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-50959-9_42]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-50959-9_42]</a>",Recent studies show significant security problems with most of the Federated Learning models. There is a false assumption that the participant is not...,,Springer
Prevention and Detection of Network Attacks: A Comprehensive Study,"Paul Addai, Ryan Freas, ... Tauheed Khan Mohd",Decision Support Systems XIII. Decision Support Systems in An Uncertain World: The Contribution of Digital Twins,2023,"<a href=""Springer (2023) : Prevention and Detection of Network Attacks: A Comprehensive Study"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-32534-2_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-32534-2_5]</a>",Cybersecurity is currently a topic of utmost significance in tech sectors. The ever-evolving landscape of this field makes it particularly difficult...,,Springer
Prevention and Detection of Poisoning Attacks in Medical-Based Machine Learning Web Applications,"T. Sheela, C. K. Gnana Padmesh, V. Lokesh","Communication, Networks and Computing",2023,"<a href=""Springer (2023) : Prevention and Detection of Poisoning Attacks in Medical-Based Machine Learning Web Applications"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-43140-1_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-43140-1_12]</a>","Machine Learning concepts and processes are widely used in all the fields for automated, accurate, and immediate decisions. But, hackers have...",,Springer
Quantum Optimization for IoT Security Detection,"Vita Santa Barletta, Danilo Caivano, ... Antonio Piccinno",Ambient Intelligence—Software and Applications—13th International Symposium on Ambient Intelligence,2023,"<a href=""Springer (2023) : Quantum Optimization for IoT Security Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22356-3_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22356-3_18]</a>","The digital transformation and innovation of today’s environment requires a focus on the security of systems to preserve their confidentiality,...",,Springer
RPFL: Robust and Privacy Federated Learning against Backdoor and Sample Inference Attacks,"Di Xiao, Zhuyang Yu, Lvjun Chen",ICPADS,2023,"<a href=""DBLP (2023) : RPFL: Robust and Privacy Federated Learning against Backdoor and Sample Inference Attacks"" target=""_blank"">[https://doi.org/10.1109/ICPADS60453.2023.00213]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICPADS60453.2023.00213]</a>",,,DBLP
Random Location Poisoning Backdoor Attack Against Automatic Modulation Classification in Wireless Networks,"Zixin Li, Hang Jiang, Sicheng Zhang, Wei Xiang, Yun Lin",ICCC,2023,"<a href=""DBLP (2023) : Random Location Poisoning Backdoor Attack Against Automatic Modulation Classification in Wireless Networks"" target=""_blank"">[https://doi.org/10.1109/ICCC57788.2023.10233544]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICCC57788.2023.10233544]</a>",,,DBLP
Randomized Half-Ideal Cipher on Groups with Applications to UC (a)PAKE,"Bruno Freitas Dos Santos, Yanqi Gu, Stanislaw Jarecki",Advances in Cryptology – EUROCRYPT 2023,2023,"<a href=""Springer (2023) : Randomized Half-Ideal Cipher on Groups with Applications to UC (a)PAKE"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-30589-4_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-30589-4_5]</a>",An Ideal Cipher (IC) is a cipher where each key defines a random permutation on the domain. Ideal Cipher on a group has many attractive...,,Springer
Recent Challenges in a New Distributed Learning Paradigm,"Sandi Rahmadika, Bayu Ramadhani Fajri, ... Khairi Budayawan",Mobile Internet Security,2023,"<a href=""Springer (2023) : Recent Challenges in a New Distributed Learning Paradigm"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-4430-9_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-4430-9_10]</a>",The majority of online transactions between the parties can be reported in real-time publicly by relying on smart contracts (SCs) and federated...,,Springer
Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-level Backdoor Attacks,"Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Xin Jiang, Maosong Sun",Mach. Intell. Res.,2023,"<a href=""DBLP (2023) : Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-level Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1007/s11633-022-1377-5]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/s11633-022-1377-5]</a>",,,DBLP
Redeem Myself: Purifying Backdoors in Deep Learning Models using Self Attention Distillation,"Xueluan Gong, Yanjiao Chen, Wang Yang, Qian Wang, Yuzhe Gu, Huayang Huang, Chao Shen",SP,2023,"<a href=""DBLP (2023) : Redeem Myself: Purifying Backdoors in Deep Learning Models using Self Attention Distillation"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179375]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SP46215.2023.10179375]</a>",,,DBLP
Review of Detection of Packets Inspection and Attacks in Network Security,"Sai Kalki Jajula, Khushboo Tripathi, Shalini Bhaskar Bajaj",Emerging Technologies in Data Mining and Information Security,2023,"<a href=""Springer (2023) : Review of Detection of Packets Inspection and Attacks in Network Security"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-4193-1_58]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-4193-1_58]</a>","In this paper, different types of attacks and their malicious act behind the scenes are presented. The solutions to defend the victim against attacks...",,Springer
Review on Android Malware Detection System,"Rajeshwari Gundla, Sachin R. Gengaje","Data Management, Analytics and Innovation",2023,"<a href=""Springer (2023) : Review on Android Malware Detection System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-2600-6_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-2600-6_6]</a>",Modern mobile devices have become important part of day to day life as they offer various tools and services which are very useful. Mobile devices...,,Springer
Revisiting the Assumption of Latent Separability for Backdoor Defenses,"Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, Prateek Mittal",ICLR,2023,"<a href=""DBLP (2023) : Revisiting the Assumption of Latent Separability for Backdoor Defenses"" target=""_blank"">[https://openreview.net/pdf?id=_wSHsgrVali]</a>","<a href=""DBLP"" target=""_blank"">[https://openreview.net/pdf?id=_wSHsgrVali]</a>",,,DBLP
Robust Contrastive Language-Image Pretraining against Data Poisoning and Backdoor Attacks,"Wenhan Yang, Jingdong Gao, Baharan Mirzasoleiman",NeurIPS,2023,"<a href=""DBLP (2023) : Robust Contrastive Language-Image Pretraining against Data Poisoning and Backdoor Attacks"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/2232e8fee69b150005ac420bfa83d705-Abstract-Conference.html]</a>","<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2023/hash/2232e8fee69b150005ac420bfa83d705-Abstract-Conference.html]</a>",,,DBLP
Robust Feature-Guided Generative Adversarial Network for Aerial Image Semantic Segmentation against Backdoor Attacks,"Zhen Wang, Buhong Wang, Chuanlei Zhang, Yaohui Liu, Jianxin Guo",Remote. Sens.,2023,"<a href=""DBLP (2023) : Robust Feature-Guided Generative Adversarial Network for Aerial Image Semantic Segmentation against Backdoor Attacks"" target=""_blank"">[https://doi.org/10.3390/rs15102580]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.3390/rs15102580]</a>",,,DBLP
Robust Intrusion Detection,S. Xu Y. Qian R. Q. Hu,Cybersecurity in Intelligent Networking Systems,2023,"<a href=""IEEE (2023) : Robust Intrusion Detection"" target=""_blank"">[https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9943571.pdf&bkn=9944078&pdfType=chapter]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1002/9781119784135.ch5]</a>","Abstract This chapter presents a proposed scheme of robust intrusion detection. First, the preliminaries of robust statistics are introduced. Robust statistics deal with identifying the presence of outliers. Two approaches of robust statistics, including median absolute deviation and Mahalanobis distance, are introduced. Then, the problem statement is firstly formulated, followed by each step of the proposed robust intrusion detection. The current challenge to train a robust intrusion detection classifier is the difficulty to acquire fully labeled traffic data and accuracy problems of one single classifier. The proposed robust intrusion detection is illustrated in robust data pre‐processing, bagging for labeled anomalies, one‐class support vector machine for unlabeled samples, and final ensemble learning classifier. Finally, the experimental setup and details of performance evaluations are presented. In the first experiment, two Probe attacks, namely, Port sweep and SATAN, and two flooding attacks, namely, Back and Neptune, are investigated. In this second experiment, two other probe attacks, namely, Analysis and Exploits, and two other flooding attacks, namely, Backdoor and DoS are also investigated. The evaluation is conducted in terms of evaluation metrics such as true positive, true negative, false positive, false negative, accuracy, sensitivity, specificity, precision, negative predictive value, balanced accuracy, receiver operating characteristic (ROC) curve, and the precision–recall (PR) curve.",,IEEE
Run-Time Detection of Malicious Behavior Based on Exploit Decomposition Using Deep Learning: A Feasibility Study on SysJoker,"Thanasis Tsakoulis, Evangelos Haleplidis, Apostolos P. Fournaris","Embedded Computer Systems: Architectures, Modeling, and Simulation",2023,"<a href=""Springer (2023) : Run-Time Detection of Malicious Behavior Based on Exploit Decomposition Using Deep Learning: A Feasibility Study on SysJoker"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-46077-7_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-46077-7_21]</a>","As malicious operations become gradually very complex and involve advanced attack campaigns even on embedded systems and IoT, there is an increasing...",,Springer
SAFELearning: Secure Aggregation in Federated Learning With Backdoor Detectability,"Zhuosheng Zhang, Jiarui Li, Shucheng Yu, Christian Makaya",IEEE Trans. Inf. Forensics Secur.,2023,"<a href=""DBLP (2023) : SAFELearning: Secure Aggregation in Federated Learning With Backdoor Detectability"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3280032]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3280032]</a>",,,DBLP
SCFL: Mitigating backdoor attacks in federated learning based on SVD and clustering,"Yongkang Wang, Di-Hua Zhai, Yuanqing Xia",Comput. Secur.,2023,"<a href=""DBLP (2023) : SCFL: Mitigating backdoor attacks in federated learning based on SVD and clustering"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103414]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.cose.2023.103414]</a>",,,DBLP
SDB-RGSO: Swarm-Based Data Balancing and Randomized Grid Search Optimization for IoT NetFlow Malware Detection with Ensemble Machine Learning Model,"D. Santhadevi, B. Janet",Proceedings of Data Analytics and Management,2023,"<a href=""Springer (2023) : SDB-RGSO: Swarm-Based Data Balancing and Randomized Grid Search Optimization for IoT NetFlow Malware Detection with Ensemble Machine Learning Model"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6550-2_46]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6550-2_46]</a>","Digital attacks on the Internet of Things (IoT) are increasing in frequency due to the massive growth in collaboration among various devices, apps,...",,Springer
SDN Application Backdoor: Disrupting the Service via Poisoning the Topology,"Shuhua Deng, Xian Qing, Xiaofan Li, Xing Gao, Xieping Gao",INFOCOM,2023,"<a href=""DBLP (2023) : SDN Application Backdoor: Disrupting the Service via Poisoning the Topology"" target=""_blank"">[https://doi.org/10.1109/INFOCOM53939.2023.10229058]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/INFOCOM53939.2023.10229058]</a>",,,DBLP
SEBD: Sensor Emulation Based Backdoor for Autopilot,"Yue Wang, Chao Yang, Ning Xi, Yulong Shen, Jianfeng Ma",IoT,2023,"<a href=""DBLP (2023) : SEBD: Sensor Emulation Based Backdoor for Autopilot"" target=""_blank"">[https://doi.org/10.1145/3627050.3631577]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3627050.3631577]</a>",,,DBLP
SPoiL: Sybil-Based Untargeted Data Poisoning Attacks in Federated Learning,"Zhuotao Lian, Chen Zhang, ... Chunhua Su",Network and System Security,2023,"<a href=""Springer (2023) : SPoiL: Sybil-Based Untargeted Data Poisoning Attacks in Federated Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-39828-5_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-39828-5_13]</a>","Federated learning is widely used in mobile computing, the Internet of Things, and other scenarios due to its distributed and privacy-preserving...",,Springer
ScanFed: Scalable Behavior-Based Backdoor Detection in Federated Learning,"Rui Ning, Jiang Li, Chunsheng Xin, Chonggang Wang, Xu Li, Robert Gazda, Jin-Hee Cho, Hongyi Wu",ICDCS,2023,"<a href=""DBLP (2023) : ScanFed: Scalable Behavior-Based Backdoor Detection in Federated Learning"" target=""_blank"">[https://doi.org/10.1109/ICDCS57875.2023.00011]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICDCS57875.2023.00011]</a>",,,DBLP
Security Analysis of the Internet of Medical Things (IoMT): Case Study of the Pacemaker Ecosystem,"Guillaume Bour, Anniken Wium Lie, ... Ravishankar Borgaonkar",Biomedical Engineering Systems and Technologies,2023,"<a href=""Springer (2023) : Security Analysis of the Internet of Medical Things (IoMT): Case Study of the Pacemaker Ecosystem"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-38854-5_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-38854-5_5]</a>","During the pandemic, the Internet of Medical Things (IoMT) has played a key role in reducing unnecessary hospital visits and the burden on health...",,Springer
Security Issues for Banking Systems,"Mohammed Khodayer Hassan, Aymen Mohammed Khodayer, ... Maryem Mahmood","Computational Intelligence, Data Analytics and Applications",2023,"<a href=""Springer (2023) : Security Issues for Banking Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-27099-4_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-27099-4_10]</a>",It's no secret that businesses of all sizes and types rely on IT to carry out their day-to-day operations and provide the best possible service to...,,Springer
SemSBA: Semantic-perturbed Stealthy Backdoor Attack on Federated Semi-supervised Learning,"Yingrui Tong, Jun Feng, Gaolei Li, Xi Lin, Chengcheng Zhao, Xiaoyu Yi, Jianhua Li",ICPADS,2023,"<a href=""DBLP (2023) : SemSBA: Semantic-perturbed Stealthy Backdoor Attack on Federated Semi-supervised Learning"" target=""_blank"">[https://doi.org/10.1109/ICPADS60453.2023.00221]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICPADS60453.2023.00221]</a>",,,DBLP
Simulation: The Great Enabler?,"Christian Johann Liegl, Tobias Nickchen, ... Florian Luft",Modelling and Simulation for Autonomous Systems,2023,"<a href=""Springer (2023) : Simulation: The Great Enabler?"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-31268-7_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-31268-7_19]</a>","Autonomous systems (AxS) have successfully been employed in military tasks such as search and rescue, logistics, and reconnaissance. Whether it be on...",,Springer
Smart Boosted Model for Behavior-Based Malware Analysis and Detection,"Saja Abu-Zaideh, Mohammad Abu Snober, Qasem Abu Al-Haija",IoT Based Control Networks and Intelligent Systems,2023,"<a href=""Springer (2023) : Smart Boosted Model for Behavior-Based Malware Analysis and Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-5845-8_58]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-5845-8_58]</a>","Malware analysis and detection are the most important activities to ensure system security. However, current attacks like polymorphic viruses and...",,Springer
Smart Hardware Trojan Detection System,"Iyad Alkhazendar, Mohammed Zubair, Uvais Qidwai",Intelligent Systems and Applications,2023,"<a href=""Springer (2023) : Smart Hardware Trojan Detection System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-16075-2_58]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-16075-2_58]</a>",The IoT has become an indispensable part of human lives at work and home applications. Due to the need for an enormous number of IoT devices...,,Springer
Smartphone Malware Detection Based on Enhanced Correlation-Based Feature Selection on Permissions,"Shagun, Deepak Kumar, Anshul Arora",Proceedings of Data Analytics and Management,2023,"<a href=""Springer (2023) : Smartphone Malware Detection Based on Enhanced Correlation-Based Feature Selection on Permissions"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6553-3_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-6553-3_3]</a>","In the present day, smartphones are becoming increasingly ubiquitous, with people of all ages relying on them for daily use. The number of app...",,Springer
SolScope: Effectively Hunting Potential Permission Backdoor Threats in Smart Contracts,"Renjie Ji, Wansen Wang, Yan Xiong, Wenchao Huang",BIGCOM,2023,"<a href=""DBLP (2023) : SolScope: Effectively Hunting Potential Permission Backdoor Threats in Smart Contracts"" target=""_blank"">[https://doi.org/10.1109/BIGCOM61073.2023.00020]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/BIGCOM61073.2023.00020]</a>",,,DBLP
Spying on Kids’ Smart Devices: Beware of Security Vulnerabilities!,"M. A. Hannan Bin Azhar, Danny Smith, Aimee Cain",Cybersecurity in the Age of Smart Societies,2023,"<a href=""Springer (2023) : Spying on Kids’ Smart Devices: Beware of Security Vulnerabilities!"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-20160-8_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-20160-8_8]</a>","The emergence of the Internet of Things devices in everyday life has increased its sales dramatically over recent years, specifically of smart...",,Springer
Steal from Collaboration: Spy Attack by a Dishonest Party in Vertical Federated Learning,"Hongbin Chen, Chaohao Fu, Na Ruan",Applied Cryptography and Network Security,2023,"<a href=""Springer (2023) : Steal from Collaboration: Spy Attack by a Dishonest Party in Vertical Federated Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33488-7_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33488-7_22]</a>",Vertical federated learning (VFL) is an emerging paradigm that enables multiple companies to collaboratively train a global model without disclosing...,,Springer
Stealthy Backdoor Attack Against Speaker Recognition Using Phase-Injection Hidden Trigger,"Zhe Ye, Diqun Yan, Li Dong, Jiacheng Deng, Shui Yu",IEEE Signal Process. Lett.,2023,"<a href=""DBLP (2023) : Stealthy Backdoor Attack Against Speaker Recognition Using Phase-Injection Hidden Trigger"" target=""_blank"">[https://doi.org/10.1109/LSP.2023.3293429]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/LSP.2023.3293429]</a>",,,DBLP
Stealthy Frequency-Domain Backdoor Attacks: Fourier Decomposition and Fundamental Frequency Injection,"Qianli Ma, Junping Qin, Kai Yan, Lei Wang, Hao Sun",IEEE Signal Process. Lett.,2023,"<a href=""DBLP (2023) : Stealthy Frequency-Domain Backdoor Attacks: Fourier Decomposition and Fundamental Frequency Injection"" target=""_blank"">[https://doi.org/10.1109/LSP.2023.3330126]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/LSP.2023.3330126]</a>",,,DBLP
Study of Hybrid Cryptographic Techniques for Vehicle FOTA System,"Manas Borse, Parth Shendkar, ... Rachana Patil",Mobile Computing and Sustainable Informatics,2023,"<a href=""Springer (2023) : Study of Hybrid Cryptographic Techniques for Vehicle FOTA System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-0835-6_30]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-0835-6_30]</a>","The Internet of Things is growing rapidly, and so are the security risks that come with it. It is essential to have a way to keep devices secure,...",,Springer
Study of the Need for Effective Cyber Security Trainings in India,"Rakesh Kumar Chawla, J. S. Sodhi, Triveni Singh","Data Management, Analytics and Innovation",2023,"<a href=""Springer (2023) : Study of the Need for Effective Cyber Security Trainings in India"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-1414-2_50]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-1414-2_50]</a>","With the availability of android phones and internet in every hand, the rate of cyber-crimes have increased tremendously. This increase in...",,Springer
Subversion-Resilient Authenticated Encryption Without Random Oracles,"Pascal Bemmann, Sebastian Berndt, ... Tibor Jager",Applied Cryptography and Network Security,2023,"<a href=""Springer (2023) : Subversion-Resilient Authenticated Encryption Without Random Oracles"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33491-7_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33491-7_17]</a>","In 2013, the Snowden revelations have shown subversion of cryptographic implementations to be a relevant threat. Since then, the academic community...",,Springer
Successive Interference Cancellation Based Defense for Trigger Backdoor in Federated Learning,"Yu-Wen Chen, Bo-Hsu Ke, Bozhong Chen, Si-Rong Chiu, Chun-Wei Tu, Jian-Jhih Kuo",ICC,2023,"<a href=""DBLP (2023) : Successive Interference Cancellation Based Defense for Trigger Backdoor in Federated Learning"" target=""_blank"">[https://doi.org/10.1109/ICC45041.2023.10278979]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICC45041.2023.10278979]</a>",,,DBLP
TAT: Targeted backdoor attacks against visual object tracking,"Ziyi Cheng, Baoyuan Wu, Zhenya Zhang, Jianjun Zhao",Pattern Recognit.,2023,"<a href=""DBLP (2023) : TAT: Targeted backdoor attacks against visual object tracking"" target=""_blank"">[https://doi.org/10.1016/j.patcog.2023.109629]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.patcog.2023.109629]</a>",,,DBLP
Targeted Clean-Label Poisoning Attacks on Federated Learning,"Ayushi Patel, Priyanka Singh",Recent Trends in Image Processing and Pattern Recognition,2023,"<a href=""Springer (2023) : Targeted Clean-Label Poisoning Attacks on Federated Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-23599-3_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-23599-3_17]</a>",Federated Learning (FL) has become one of the most extensively utilized distributed training approaches since it allows users to access large...,,Springer
The Dark Side of Dynamic Routing Neural Networks: Towards Efficiency Backdoor Injection,"Simin Chen, Hanlin Chen, Mirazul Haque, Cong Liu, Wei Yang",CVPR,2023,"<a href=""DBLP (2023) : The Dark Side of Dynamic Routing Neural Networks: Towards Efficiency Backdoor Injection"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.02355]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.02355]</a>",,,DBLP
"The Investigation of Network Security, Including Penetrating Threats and Potential Security Measures","N. L. Lincy, Midhunchakkaravarthy",Soft Computing for Security Applications,2023,"<a href=""Springer (2023) : The Investigation of Network Security, Including Penetrating Threats and Potential Security Measures"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-3590-9_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-3590-9_9]</a>","In computers and networking technology, security is a critical component. The first and most important consideration for any network designer,...",,Springer
The Prediction of Attacks and Challenges in Cyber Security Using Machine Learning Techniques,"B. Rekha, S. Rajeshwari, ... G. Kavya",Proceedings of the 2nd International Conference on Cognitive and Intelligent Computing,2023,"<a href=""Springer (2023) : The Prediction of Attacks and Challenges in Cyber Security Using Machine Learning Techniques"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-2742-5_45]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-2742-5_45]</a>",Cyber-attacks that are automated and persistent are now more likely to succeed in the cyber sphere. The security measures used to identify and...,,Springer
The Reality of Backdoored S-Boxes - An Eye Opener,"Shah Fahd, Mehreen Afzal, Waseem Iqbal, Dawood Shah, Ijaz Khalid",IACR Cryptol. ePrint Arch.,2023,"<a href=""DBLP (2023) : The Reality of Backdoored S-Boxes - An Eye Opener"" target=""_blank"">[https://eprint.iacr.org/2023/1073]</a>","<a href=""DBLP"" target=""_blank"">[https://eprint.iacr.org/2023/1073]</a>",,,DBLP
"Those Aren’t Your Memories, They’re Somebody Else’s: Seeding Misinformation in Chat Bot Memories","Conor Atkins, Benjamin Zi Hao Zhao, ... Mohamed Ali Kaafar",Applied Cryptography and Network Security,2023,"<a href=""Springer (2023) : Those Aren’t Your Memories, They’re Somebody Else’s: Seeding Misinformation in Chat Bot Memories"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33488-7_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33488-7_11]</a>",One of the new developments in chit-chat bots is a long-term memory mechanism that remembers information from past conversations for increasing...,,Springer
Threat Modeling in Cloud Computing - A Literature Review,"Mohammed Kharma, Adel Taweel",Ubiquitous Security,2023,"<a href=""Springer (2023) : Threat Modeling in Cloud Computing - A Literature Review"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-0272-9_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-0272-9_19]</a>","Cloud computing has significantly changed the operational models of companies. This adoption has consequently caused impact on security, resulting in...",,Springer
Three-Layered Hybrid Analysis Technique for Android Malware Detection,"Tejpal Sharma, Dhavleesh Rattan",Advances in Data Science and Computing Technologies,2023,"<a href=""Springer (2023) : Three-Layered Hybrid Analysis Technique for Android Malware Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-3656-4_31]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-3656-4_31]</a>","In these days, smartphones become an essential gadget that can perform multiple routine activities like banking, education, entertainment, etc. A...",,Springer
Time-Efficient Finite Field Microarchitecture Design for Curve448 and Ed448 on Cortex-M4,"Mila Anastasova, Reza Azarderakhsh, ... Lubjana Beshaj",Information Security and Cryptology – ICISC 2022,2023,"<a href=""Springer (2023) : Time-Efficient Finite Field Microarchitecture Design for Curve448 and Ed448 on Cortex-M4"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-29371-9_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-29371-9_15]</a>","The elliptic curve family of schemes has the lowest computational latency, memory use, energy consumption, and bandwidth requirements, making it the...",,Springer
Towards Dynamic Backdoor Attacks against LiDAR Semantic Segmentation in Autonomous Driving,"Shuai Li, Yu Wen, Xu Cheng",TrustCom,2023,"<a href=""DBLP (2023) : Towards Dynamic Backdoor Attacks against LiDAR Semantic Segmentation in Autonomous Driving"" target=""_blank"">[https://doi.org/10.1109/TrustCom60117.2023.00034]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TrustCom60117.2023.00034]</a>",,,DBLP
Towards a Robust Defense: A Multifaceted Approach to the Detection and Mitigation of Neural Backdoor Attacks through Feature Space Exploration and Analysis,Liuwan Zhu,,2023,"<a href=""DBLP (2023) : Towards a Robust Defense: A Multifaceted Approach to the Detection and Mitigation of Neural Backdoor Attacks through Feature Space Exploration and Analysis"" target=""_blank"">[https://digitalcommons.odu.edu/ece_etds/254]</a>","<a href=""DBLP"" target=""_blank"">[https://digitalcommons.odu.edu/ece_etds/254]</a>",,,DBLP
Training Data Leakage via Imperceptible Backdoor Attack,"Xiangkai Yang, Wenjian Luo, Qi Zhou, Zhijian Chen",SSCI,2023,"<a href=""DBLP (2023) : Training Data Leakage via Imperceptible Backdoor Attack"" target=""_blank"">[https://doi.org/10.1109/SSCI52147.2023.10372011]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SSCI52147.2023.10372011]</a>",,,DBLP
Turning backdoors for efficient privacy protection against image retrieval violations,"Qiang Liu, Tongqing Zhou, Zhiping Cai, Yuan Yuan, Ming Xu, Jiaohua Qin, Wentao Ma",Inf. Process. Manag.,2023,"<a href=""DBLP (2023) : Turning backdoors for efficient privacy protection against image retrieval violations"" target=""_blank"">[https://doi.org/10.1016/j.ipm.2023.103471]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.ipm.2023.103471]</a>",,,DBLP
Understanding Backdoor Attacks through the Adaptability Hypothesis,"Xun Xian, Ganghua Wang, Jayanth Srinivasa, Ashish Kundu, Xuan Bi, Mingyi Hong, Jie Ding",ICML,2023,"<a href=""DBLP (2023) : Understanding Backdoor Attacks through the Adaptability Hypothesis"" target=""_blank"">[https://proceedings.mlr.press/v202/xian23a.html]</a>","<a href=""DBLP"" target=""_blank"">[https://proceedings.mlr.press/v202/xian23a.html]</a>",,,DBLP
Universally Composable Auditable Surveillance,"Valerie Fetzer, Michael Klooß, ... Andy Rupp",Advances in Cryptology – ASIACRYPT 2023,2023,"<a href=""Springer (2023) : Universally Composable Auditable Surveillance"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8724-5_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-99-8724-5_14]</a>","User privacy is becoming increasingly important in our digital society. Yet, many applications face legal requirements or regulations that prohibit...",,Springer
Unlabeled backdoor poisoning on trained-from-scratch semi-supervised learning,"Le Feng, Zhenxing Qian, Xinpeng Zhang, Sheng Li",Inf. Sci.,2023,"<a href=""DBLP (2023) : Unlabeled backdoor poisoning on trained-from-scratch semi-supervised learning"" target=""_blank"">[https://doi.org/10.1016/j.ins.2023.119453]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.ins.2023.119453]</a>",,,DBLP
VILLAIN: Backdoor Attacks Against Vertical Split Learning,"Yijie Bai, Yanjiao Chen, Hanlei Zhang, Wenyuan Xu, Haiqin Weng, Dou Goodman",USENIX Security Symposium,2023,"<a href=""DBLP (2023) : VILLAIN: Backdoor Attacks Against Vertical Split Learning"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity23/presentation/bai]</a>","<a href=""DBLP"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity23/presentation/bai]</a>",,,DBLP
Verifiable Machine Learning Models in Industrial IoT via Blockchain,"Jan Stodt, Fatemeh Ghovanlooy Ghajar, ... Nathan Clarke",Advanced Computing,2023,"<a href=""Springer (2023) : Verifiable Machine Learning Models in Industrial IoT via Blockchain"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-35644-5_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-35644-5_6]</a>",The importance of machine learning (ML) has been increasing dramatically for years. From assistance systems to production optimisation to healthcare...,,Springer
Volatility Custom Profiling for Automated Hybrid ELF Malware Detection,"Rahul Varshney, Nitesh Kumar, ... Sandeep Kumar Shukla",Digital Forensics and Cyber Crime,2023,"<a href=""Springer (2023) : Volatility Custom Profiling for Automated Hybrid ELF Malware Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-36574-4_16]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-36574-4_16]</a>","The increasing prevalence of Linux malware poses a severe threat to private data and expensive computer resources. Hence, there is a dire need to...",,Springer
What Boosts Fake News Dissemination on Social Media? A Causal Inference View,"Yichuan Li, Kyumin Lee, ... Ruocheng Guo",Advances in Knowledge Discovery and Data Mining,2023,"<a href=""Springer (2023) : What Boosts Fake News Dissemination on Social Media? A Causal Inference View"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33383-5_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-33383-5_19]</a>","There has been an upward trend of fake news propagation on social media. To solve the fake news propagation problem, it is crucial to understand...",,Springer
X-HDNN: Explainable Hybrid DNN for Industrial Internet of Things Backdoor Attack Detection,"Love Allen Chijioke Ahakonye, Cosmas Ifeanyi Nwakanma, Jae Min Lee, Dong-Seong Kim",ICTC,2023,"<a href=""DBLP (2023) : X-HDNN: Explainable Hybrid DNN for Industrial Internet of Things Backdoor Attack Detection"" target=""_blank"">[https://doi.org/10.1109/ICTC58733.2023.10393379]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICTC58733.2023.10393379]</a>",,,DBLP
You Are Catching My Attention: Are Vision Transformers Bad Learners under Backdoor Attacks?,"Zenghui Yuan, Pan Zhou, Kai Zou, Yu Cheng",CVPR,2023,"<a href=""DBLP (2023) : You Are Catching My Attention: Are Vision Transformers Bad Learners under Backdoor Attacks?"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.02357]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.02357]</a>",,,DBLP
Detection and prevention of SQLI attacks and developing compressive framework using machine learning and hybrid techniques,"Wubetu Barud Demilie, Fitsum Gizachew Deriba",Journal of Big Data,2022-12-30,"<a href=""Springer (2022-12-30) : Detection and prevention of SQLI attacks and developing compressive framework using machine learning and hybrid techniques"" target=""_blank"">[https://link.springer.com/article/10.1186/s40537-022-00678-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s40537-022-00678-0]</a>",A web application is a software system that provides an interface to its users through a web browser on any operating system (OS). Despite their...,,Springer
XMAM:X-raying Models with A Matrix to Reveal Backdoor Attacks for Federated Learning,"Jianyi Zhang, Fangjiao Zhang, Qichao Jin, Zhiqiang Wang, Xiaodong Lin, Xiali Hei","arXiv
arXiv","2022-12-28
2022-12","<a href=""arXiv (2022-12-28) : XMAM:X-raying Models with A Matrix to Reveal Backdoor Attacks for Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2212.13675v1]</a>
<a href=""DBLP (2022-12) : XMAM: X-raying Models with A Matrix to Reveal Backdoor Attacks for Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.13675]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.13675]</a>","Federated Learning (FL) has received increasing attention due to its privacy protection capability. However, the base algorithm FedAvg is vulnerable when it suffers from so-called backdoor attacks. Former researchers proposed several robust aggregation methods. Unfortunately, many of these aggregation methods are unable to defend against backdoor attacks. What's more, the attackers recently have proposed some hiding methods that further improve backdoor attacks' stealthiness, making all the existing robust aggregation methods fail. To tackle the threat of backdoor attacks, we propose a new aggregation method, X-raying Models with A Matrix (XMAM), to reveal the malicious local model updates submitted by the backdoor attackers. Since we observe that the output of the Softmax layer exhibits distinguishable patterns between malicious and benign updates, we focus on the Softmax layer's output in which the backdoor attackers are difficult to hide their malicious behavior. Specifically, like X-ray examinations, we investigate the local model updates by using a matrix as an input to get their Softmax layer's outputs. Then, we preclude updates whose outputs are abnormal by clustering. Without any training dataset in the server, the extensive evaluations show that our XMAM can effectively distinguish malicious local model updates from benign ones. For instance, when other methods fail to defend against the backdoor attacks at no more than 20% malicious clients, our method can tolerate 45% malicious clients in the black-box mode and about 30% in Projected Gradient Descent (PGD) mode. Besides, under adaptive attacks, the results demonstrate that XMAM can still complete the global model training task even when there are 40% malicious clients. Finally, we analyze our method's screening complexity, and the results show that XMAM is about 10-10000 times faster than the existing methods.
","
","arXiv
DBLP"
Continuous Verification of Open Source Components in a World of Weak Links,T. Hastings K. R. Walcott,2022 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),2022-12-26,"<a href=""IEEE (2022-12-26) : Continuous Verification of Open Source Components in a World of Weak Links"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9985184]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ISSREW55968.2022.00068]</a>","We are heading for a perfect storm, making open source software poisoning and next-generation supply chain attacks much easier to execute, which could have major im-plications for organizations. The widespread adoption of open source (99% of today's software utilizes open source), the ease of today's package managers, and the best practice of implementing continuous delivery for software projects provide an unprece-dented opportunity for attack. Once an adversary compromises a project, they can deploy malicious code into production under the auspicious of a software patch. Downstream projects will ingest the compromised patch, and now those projects are potentially running the malicious code. The impact could be implementing backdoors, gathering intelligence, delivering malware, or denying a service. According to Sonatype, a leading commercial software security company, these next-generation supply chain attacks have increased 430 % in the last year and there is not a good way to vet or monitor an open-source project prior to incorporating the project. In this paper, we analyzed two case studies of compromised open source components. We propose six continuous verification controls that enable organizations to make data-driven decisions and mitigate breaches, such as analyzing community metrics and project hygiene using scorecards and monitoring the boundary of the software in production. In one case study, the controls identified high levels of risk immediately even though the package is widely used and has over 7 million downloads a week. In both case studies we found that the controls could have prevented malicious actions despite the project breaches.",,IEEE
Mind Your Heart: Stealthy Backdoor Attack on Dynamic Deep Neural Network in Edge Computing,"Tian Dong, Ziyuan Zhang, Han Qiu, Tianwei Zhang, Hewu Li, Terry Wang","arXiv
INFOCOM
arXiv","2022-12-22
2023
2022-12","<a href=""arXiv (2022-12-22) : Mind Your Heart: Stealthy Backdoor Attack on Dynamic Deep Neural Network in Edge Computing"" target=""_blank"">[http://arxiv.org/abs/2212.11751v1]</a>
<a href=""DBLP (2023) : Mind Your Heart: Stealthy Backdoor Attack on Dynamic Deep Neural Network in Edge Computing"" target=""_blank"">[https://doi.org/10.1109/INFOCOM53939.2023.10229092]</a>
<a href=""DBLP (2022-12) : Mind Your Heart: Stealthy Backdoor Attack on Dynamic Deep Neural Network in Edge Computing"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.11751]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/INFOCOM53939.2023.10229092]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.11751]</a>","Transforming off-the-shelf deep neural network (DNN) models into dynamic multi-exit architectures can achieve inference and transmission efficiency by fragmenting and distributing a large DNN model in edge computing scenarios (e.g., edge devices and cloud servers). In this paper, we propose a novel backdoor attack specifically on the dynamic multi-exit DNN models. Particularly, we inject a backdoor by poisoning one DNN model's shallow hidden layers targeting not this vanilla DNN model but only its dynamically deployed multi-exit architectures. Our backdoored vanilla model behaves normally on performance and cannot be activated even with the correct trigger. However, the backdoor will be activated when the victims acquire this model and transform it into a dynamic multi-exit architecture at their deployment. We conduct extensive experiments to prove the effectiveness of our attack on three structures (ResNet-56, VGG-16, and MobileNet) with four datasets (CIFAR-10, SVHN, GTSRB, and Tiny-ImageNet) and our backdoor is stealthy to evade multiple state-of-the-art backdoor detection or removal methods.

","

","arXiv
DBLP
DBLP"
Vulnerabilities of Deep Learning-Driven Semantic Communications to Backdoor (Trojan) Attacks,"Yalin E. Sagduyu, Tugba Erpek, Sennur Ulukus, Aylin Yener","arXiv
CISS
arXiv","2022-12-21
2023
2022-12","<a href=""arXiv (2022-12-21) : Vulnerabilities of Deep Learning-Driven Semantic Communications to Backdoor (Trojan) Attacks"" target=""_blank"">[http://arxiv.org/abs/2212.11205v1]</a>
<a href=""DBLP (2023) : Vulnerabilities of Deep Learning-Driven Semantic Communications to Backdoor (Trojan) Attacks"" target=""_blank"">[https://doi.org/10.1109/CISS56502.2023.10089692]</a>
<a href=""DBLP (2022-12) : Vulnerabilities of Deep Learning-Driven Semantic Communications to Backdoor (Trojan) Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.11205]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CISS56502.2023.10089692]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.11205]</a>","This paper highlights vulnerabilities of deep learning-driven semantic communications to backdoor (Trojan) attacks. Semantic communications aims to convey a desired meaning while transferring information from a transmitter to its receiver. An encoder-decoder pair that is represented by two deep neural networks (DNNs) as part of an autoencoder is trained to reconstruct signals such as images at the receiver by transmitting latent features of small size over a limited number of channel uses. In the meantime, another DNN of a semantic task classifier at the receiver is jointly trained with the autoencoder to check the meaning conveyed to the receiver. The complex decision space of the DNNs makes semantic communications susceptible to adversarial manipulations. In a backdoor (Trojan) attack, the adversary adds triggers to a small portion of training samples and changes the label to a target label. When the transfer of images is considered, the triggers can be added to the images or equivalently to the corresponding transmitted or received signals. In test time, the adversary activates these triggers by providing poisoned samples as input to the encoder (or decoder) of semantic communications. The backdoor attack can effectively change the semantic information transferred for the poisoned input samples to a target meaning. As the performance of semantic communications improves with the signal-to-noise ratio and the number of channel uses, the success of the backdoor attack increases as well. Also, increasing the Trojan ratio in training data makes the attack more successful. In the meantime, the effect of this attack on the unpoisoned input samples remains limited. Overall, this paper shows that the backdoor attack poses a serious threat to semantic communications and presents novel design guidelines to preserve the meaning of transferred information in the presence of backdoor attacks.

","

","arXiv
DBLP
DBLP"
A Review on Malware Analysis for IoT and Android System,"Chandra Shekhar Yadav, Sangeeta Gupta",SN Computer Science,2022-12-21,"<a href=""Springer (2022-12-21) : A Review on Malware Analysis for IoT and Android System"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-022-01543-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-022-01543-w]</a>",Today all humankind is willing to avail more facilities and hopes everything should be available with a click of the button. In order to offer...,,Springer
Improving anomaly detection in SCADA network communication with attribute extension,"Mahwish Anwar, Lars Lundberg, Anton Borg",Energy Informatics,2022-12-21,"<a href=""Springer (2022-12-21) : Improving anomaly detection in SCADA network communication with attribute extension"" target=""_blank"">[https://link.springer.com/article/10.1186/s42162-022-00252-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42162-022-00252-1]</a>",Network anomaly detection for critical infrastructure supervisory control and data acquisition (SCADA) systems is the first line of defense against...,,Springer
Flareon: Stealthy any2any Backdoor Injection via Poisoned Augmentation,"Tianrui Qin, Xianghuan He, Xitong Gao, Yiren Zhao, Kejiang Ye, Cheng-Zhong Xu","arXiv
arXiv","2022-12-20
2022-12","<a href=""arXiv (2022-12-20) : Flareon: Stealthy any2any Backdoor Injection via Poisoned Augmentation"" target=""_blank"">[http://arxiv.org/abs/2212.09979v1]</a>
<a href=""DBLP (2022-12) : Flareon: Stealthy any2any Backdoor Injection via Poisoned Augmentation"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.09979]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.09979]</a>","Open software supply chain attacks, once successful, can exact heavy costs in mission-critical applications. As open-source ecosystems for deep learning flourish and become increasingly universal, they present attackers previously unexplored avenues to code-inject malicious backdoors in deep neural network models. This paper proposes Flareon, a small, stealthy, seemingly harmless code modification that specifically targets the data augmentation pipeline with motion-based triggers. Flareon neither alters ground-truth labels, nor modifies the training loss objective, nor does it assume prior knowledge of the victim model architecture, training data, and training hyperparameters. Yet, it has a surprisingly large ramification on training -- models trained under Flareon learn powerful target-conditional (or ""any2any"") backdoors. The resulting models can exhibit high attack success rates for any target choices and better clean accuracies than backdoor attacks that not only seize greater control, but also assume more restrictive attack capabilities. We also demonstrate the effectiveness of Flareon against recent defenses. Flareon is fully open-source and available online to the deep learning community: https://github.com/lafeat/flareon.
","<a href=""arXiv"" target=""_blank"">[https://github.com/lafeat/flareon]</a>
","arXiv
DBLP"
VSVC: Backdoor attack against Keyword Spotting based on Voiceprint Selection and Voice Conversion,"Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Shunhui Ji","arXiv
arXiv","2022-12-20
2022-12","<a href=""arXiv (2022-12-20) : VSVC: Backdoor attack against Keyword Spotting based on Voiceprint Selection and Voice Conversion"" target=""_blank"">[http://arxiv.org/abs/2212.10103v1]</a>
<a href=""DBLP (2022-12) : VSVC: Backdoor attack against Keyword Spotting based on Voiceprint Selection and Voice Conversion"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.10103]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.10103]</a>","Keyword spotting (KWS) based on deep neural networks (DNNs) has achieved massive success in voice control scenarios. However, training of such DNN-based KWS systems often requires significant data and hardware resources. Manufacturers often entrust this process to a third-party platform. This makes the training process uncontrollable, where attackers can implant backdoors in the model by manipulating third-party training data. An effective backdoor attack can force the model to make specified judgments under certain conditions, i.e., triggers. In this paper, we design a backdoor attack scheme based on Voiceprint Selection and Voice Conversion, abbreviated as VSVC. Experimental results demonstrated that VSVC is feasible to achieve an average attack success rate close to 97% in four victim models when poisoning less than 1% of the training data.
","
","arXiv
DBLP"
"The ""Beatrix'' Resurrections: Robust Backdoor Detection via Gram Matrices","Wanlun Ma, Derui Wang, Ruoxi Sun, Minhui Xue, Sheng Wen, Yang Xiang","arXiv
NDSS
arXiv","2022-12-19
2023
2022-09","<a href=""arXiv (2022-12-19) : The ""Beatrix'' Resurrections: Robust Backdoor Detection via Gram Matrices"" target=""_blank"">[http://arxiv.org/abs/2209.11715v3]</a>
<a href=""DBLP (2023) : The &quot,Beatrix&quot, Resurrections: Robust Backdoor Detection via Gram Matrices"" target=""_blank"">[https://www.ndss-symposium.org/ndss-paper/the-beatrix-resurrections-robust-backdoor-detection-via-gram-matrices/]</a>
<a href=""DBLP (2022-09) : The &quot,Beatrix&quot, Resurrections: Robust Backdoor Detection via Gram Matrices"" target=""_blank"">[https://doi.org/10.48550/arXiv.2209.11715]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://www.ndss-symposium.org/ndss-paper/the-beatrix-resurrections-robust-backdoor-detection-via-gram-matrices/]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2209.11715]</a>","Deep Neural Networks (DNNs) are susceptible to backdoor attacks during training. The model corrupted in this way functions normally, but when triggered by certain patterns in the input, produces a predefined target label. Existing defenses usually rely on the assumption of the universal backdoor setting in which poisoned samples share the same uniform trigger. However, recent advanced backdoor attacks show that this assumption is no longer valid in dynamic backdoors where the triggers vary from input to input, thereby defeating the existing defenses. In this work, we propose a novel technique, Beatrix (backdoor detection via Gram matrix). Beatrix utilizes Gram matrix to capture not only the feature correlations but also the appropriately high-order information of the representations. By learning class-conditional statistics from activation patterns of normal samples, Beatrix can identify poisoned samples by capturing the anomalies in activation patterns. To further improve the performance in identifying target labels, Beatrix leverages kernel-based testing without making any prior assumptions on representation distribution. We demonstrate the effectiveness of our method through extensive evaluation and comparison with state-of-the-art defensive techniques. The experimental results show that our approach achieves an F1 score of 91.1% in detecting dynamic backdoors, while the state of the art can only reach 36.9%.

","

","arXiv
DBLP
DBLP"
Evaluation of Various Defense Techniques Against Targeted Poisoning Attacks in Federated Learning,C. Richards S. Khemani F. Li,2022 IEEE 19th International Conference on Mobile Ad Hoc and Smart Systems (MASS),2022-12-19,"<a href=""IEEE (2022-12-19) : Evaluation of Various Defense Techniques Against Targeted Poisoning Attacks in Federated Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9973592]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/MASS56207.2022.00102]</a>","Federated Learning (FL) allows individual clients to train a global model by aggregating local model updates each round. This results in collaborative model training while main-taining the privacy of clients' sensitive data. However, malicious clients can join the training process and train with poisoned data or send artificial model updates in targeted poisoning attacks. Many defenses to targeted poisoning attacks rely on anomaly-detection based metrics which remove participants that deviate from the majority. Similarly, aggregation-based defenses aim to reduce the impact of outliers, while L2-norm clipping tries to scale down the impact of malicious models. However, oftentimes these defenses misidentify benign clients as malicious or only work under specific attack conditions. In our paper, we examine the effectiveness of two anomaly -detection metrics on three different aggregation methods, in addition to the presence of L2-norm clipping and weight selection, across two different types of attacks. We also combine different defenses in order to examine their interaction and examine each defense when no attack is present. We found minimum aggregation to be the most effective defense against label-flipping attacks, whereas both minimum aggregation and geometric median worked well against distributed backdoor attacks. Using random weight selection significantly deteriorated defenses against both attacks, whereas the use of clipping made little difference. Finally, the main task accuracy was directly correlated with the BA in the label-flipping attack and generally was close to the MA in benign scenarios. However, in the DBA the MA and BA are inversely correlated and the MA fluctuates greatly.",,IEEE
Fine-Tuning Is All You Need to Mitigate Backdoor Attacks,"Zeyang Sha, Xinlei He, Pascal Berrang, Mathias Humbert, Yang Zhang","arXiv
arXiv","2022-12-18
2022-12","<a href=""arXiv (2022-12-18) : Fine-Tuning Is All You Need to Mitigate Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2212.09067v1]</a>
<a href=""DBLP (2022-12) : Fine-Tuning Is All You Need to Mitigate Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.09067]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.09067]</a>","Backdoor attacks represent one of the major threats to machine learning models. Various efforts have been made to mitigate backdoors. However, existing defenses have become increasingly complex and often require high computational resources or may also jeopardize models' utility. In this work, we show that fine-tuning, one of the most common and easy-to-adopt machine learning training operations, can effectively remove backdoors from machine learning models while maintaining high model utility. Extensive experiments over three machine learning paradigms show that fine-tuning and our newly proposed super-fine-tuning achieve strong defense performance. Furthermore, we coin a new term, namely backdoor sequela, to measure the changes in model vulnerabilities to other attacks before and after the backdoor has been removed. Empirical evaluation shows that, compared to other defense methods, super-fine-tuning leaves limited backdoor sequela. We hope our results can help machine learning model owners better protect their models from backdoor threats. Also, it calls for the design of more advanced attacks in order to comprehensively assess machine learning models' backdoor vulnerabilities.
","
","arXiv
DBLP"
Backdoor Attack Detection in Computer Vision by Applying Matrix Factorization on the Weights of Deep Networks,"Khondoker Murad Hossain, Tim Oates","arXiv
SafeAI@AAAI
arXiv","2022-12-15
2023
2022-12","<a href=""arXiv (2022-12-15) : Backdoor Attack Detection in Computer Vision by Applying Matrix Factorization on the Weights of Deep Networks"" target=""_blank"">[http://arxiv.org/abs/2212.08121v1]</a>
<a href=""DBLP (2023) : Backdoor Attack Detection in Computer Vision by Applying Matrix Factorization on the Weights of Deep Networks"" target=""_blank"">[https://ceur-ws.org/Vol-3381/40.pdf]</a>
<a href=""DBLP (2022-12) : Backdoor Attack Detection in Computer Vision by Applying Matrix Factorization on the Weights of Deep Networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.08121]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://ceur-ws.org/Vol-3381/40.pdf]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.08121]</a>","The increasing importance of both deep neural networks (DNNs) and cloud services for training them means that bad actors have more incentive and opportunity to insert backdoors to alter the behavior of trained models. In this paper, we introduce a novel method for backdoor detection that extracts features from pre-trained DNN's weights using independent vector analysis (IVA) followed by a machine learning classifier. In comparison to other detection techniques, this has a number of benefits, such as not requiring any training data, being applicable across domains, operating with a wide range of network architectures, not assuming the nature of the triggers used to change network behavior, and being highly scalable. We discuss the detection pipeline, and then demonstrate the results on two computer vision datasets regarding image classification and object detection. Our method outperforms the competing algorithms in terms of efficiency and is more accurate, helping to ensure the safe application of deep learning and AI.

","

","arXiv
DBLP
DBLP"
"Federated learning in cloud-edge collaborative architecture: key technologies, applications and challenges","Guanming Bao, Ping Guo",Journal of Cloud Computing,2022-12-15,"<a href=""Springer (2022-12-15) : Federated learning in cloud-edge collaborative architecture: key technologies, applications and challenges"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-022-00377-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13677-022-00377-4]</a>","In recent years, with the rapid growth of edge data, the novel cloud-edge collaborative architecture has been proposed to compensate for the lack of...",,Springer
The Devil is in the GAN: Backdoor Attacks and Defenses in Deep Generative Models,"Ambrish Rawat, Killian Levacher, Mathieu Sinn","arXiv
arXiv","2022-12-14
2021-08","<a href=""arXiv (2022-12-14) : The Devil is in the GAN: Backdoor Attacks and Defenses in Deep Generative Models"" target=""_blank"">[http://arxiv.org/abs/2108.01644v2]</a>
<a href=""DBLP (2021-08) : The Devil is in the GAN: Defending Deep Generative Models Against Backdoor Attacks"" target=""_blank"">[https://arxiv.org/abs/2108.01644]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2108.01644]</a>","Deep Generative Models (DGMs) are a popular class of deep learning models which find widespread use because of their ability to synthesize data from complex, high-dimensional manifolds. However, even with their increasing industrial adoption, they haven't been subject to rigorous security and privacy analysis. In this work we examine one such aspect, namely backdoor attacks on DGMs which can significantly limit the applicability of pre-trained models within a model supply chain and at the very least cause massive reputation damage for companies outsourcing DGMs form third parties. While similar attacks scenarios have been studied in the context of classical prediction models, their manifestation in DGMs hasn't received the same attention. To this end we propose novel training-time attacks which result in corrupted DGMs that synthesize regular data under normal operations and designated target outputs for inputs sampled from a trigger distribution. These attacks are based on an adversarial loss function that combines the dual objectives of attack stealth and fidelity. We systematically analyze these attacks, and show their effectiveness for a variety of approaches like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as well as different data domains including images and audio. Our experiments show that - even for large-scale industry-grade DGMs (like StyleGAN) - our attacks can be mounted with only modest computational effort. We also motivate suitable defenses based on static/dynamic model and output inspections, demonstrate their usefulness, and prescribe a practical and comprehensive defense strategy that paves the way for safe usage of DGMs.
","
","arXiv
DBLP"
SanitAIs: Unsupervised Data Augmentation to Sanitize Trojaned Neural Networks,K. Karra C. Ashcraft C. Costello,"2022 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)",2022-12-13,"<a href=""IEEE (2022-12-13) : SanitAIs: Unsupervised Data Augmentation to Sanitize Trojaned Neural Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9927771]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/DASC/PiCom/CBDCom/Cy55231.2022.9927771]</a>","Self-supervised learning (SSL) methods have resulted in broad improvements to neural network performance by leveraging large, untapped collections of unlabeled data to learn generalized underlying structure. In this work, we harness unsupervised data augmentation (UDA), an SSL technique, to mitigate backdoor or Trojan attacks on deep neural networks. We show that UDA is more effective at removing trojans than current state-of-the-art methods for both feature space and point triggers, over a range of model architectures, trojans, and data quantities provided for trojan removal. These results demonstrate that UDA is both an effective and practical approach to mitigating the effects of backdoors on neural networks.",,IEEE
CyberSAGE: The cyber security argument graph evaluation tool,"William G. Temple, Yue Wu, ... David Nicol",Empirical Software Engineering,2022-12-09,"<a href=""Springer (2022-12-09) : CyberSAGE: The cyber security argument graph evaluation tool"" target=""_blank"">[https://link.springer.com/article/10.1007/s10664-021-10056-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10664-021-10056-8]</a>",Cyber risk assessment is a critical step in securing the digital systems that support modern society. Typically this is a manual process carried out...,,Springer
A CatBoost Based Approach to Detect Label Flipping Poisoning Attack in Hardware Trojan Detection Systems,"Richa Sharma, G. K. Sharma, Manisha Pattanaik",Journal of Electronic Testing,2022-12-07,"<a href=""Springer (2022-12-07) : A CatBoost Based Approach to Detect Label Flipping Poisoning Attack in Hardware Trojan Detection Systems"" target=""_blank"">[https://link.springer.com/article/10.1007/s10836-022-06035-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10836-022-06035-6]</a>","Hardware Trojan (HT) intrusion at different integrated circuit (IC) phases is the most important concern for the semiconductor industries. Recently,...",,Springer
Universal Backdoor Attacks Detection via Adaptive Adversarial Probe,"Yuhang Wang, Huafeng Shi, Rui Min, Ruijia Wu, Siyuan Liang, Yichao Wu, Ding Liang, Aishan Liu",arXiv,2022-12-07,"<a href=""arXiv (2022-12-07) : Universal Backdoor Attacks Detection via Adaptive Adversarial Probe"" target=""_blank"">[http://arxiv.org/abs/2209.05244v3]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Extensive evidence has demonstrated that deep neural networks (DNNs) are vulnerable to backdoor attacks, which motivates the development of backdoor attacks detection. Most detection methods are designed to verify whether a model is infected with presumed types of backdoor attacks, yet the adversary is likely to generate diverse backdoor attacks in practice that are unforeseen to defenders, which challenge current detection strategies. In this paper, we focus on this more challenging scenario and propose a universal backdoor attacks detection method named Adaptive Adversarial Probe (A2P). Specifically, we posit that the challenge of universal backdoor attacks detection lies in the fact that different backdoor attacks often exhibit diverse characteristics in trigger patterns (i.e., sizes and transparencies). Therefore, our A2P adopts a global-to-local probing framework, which adversarially probes images with adaptive regions/budgets to fit various backdoor triggers of different sizes/transparencies. Regarding the probing region, we propose the attention-guided region generation strategy that generates region proposals with different sizes/locations based on the attention of the target model, since trigger regions often manifest higher model activation. Considering the attack budget, we introduce the box-to-sparsity scheduling that iteratively increases the perturbation budget from box to sparse constraint, so that we could better activate different latent backdoors with different transparencies. Extensive experiments on multiple datasets (CIFAR-10, GTSRB, Tiny-ImageNet) demonstrate that our method outperforms state-of-the-art baselines by large margins (+12%).",,arXiv
An intrusion detection approach based on incremental long short-term memory,"Hanxun Zhou, Longyu Kang, ... Yong Feng",International Journal of Information Security,2022-12-06,"<a href=""Springer (2022-12-06) : An intrusion detection approach based on incremental long short-term memory"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00632-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00632-4]</a>","The notorious attacks of the last few years have propelled cyber security to the top of the boardroom agenda, and raised the level of criticality to...",,Springer
Rethinking Backdoor Data Poisoning Attacks in the Context of Semi-Supervised Learning,"Marissa Connor, Vincent Emanuele","arXiv
arXiv","2022-12-05
2022-12","<a href=""arXiv (2022-12-05) : Rethinking Backdoor Data Poisoning Attacks in the Context of Semi-Supervised Learning"" target=""_blank"">[http://arxiv.org/abs/2212.02582v1]</a>
<a href=""DBLP (2022-12) : Rethinking Backdoor Data Poisoning Attacks in the Context of Semi-Supervised Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.02582]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2212.02582]</a>","Semi-supervised learning methods can train high-accuracy machine learning models with a fraction of the labeled training samples required for traditional supervised learning. Such methods do not typically involve close review of the unlabeled training samples, making them tempting targets for data poisoning attacks. In this paper we investigate the vulnerabilities of semi-supervised learning methods to backdoor data poisoning attacks on the unlabeled samples. We show that simple poisoning attacks that influence the distribution of the poisoned samples' predicted labels are highly effective - achieving an average attack success rate as high as 96.9%. We introduce a generalized attack framework targeting semi-supervised learning methods to better understand and exploit their limitations and to motivate future defense strategies.
","
","arXiv
DBLP"
More is Better (Mostly): On the Backdoor Attacks in Federated Graph Neural Networks,Xu J.,ACM International Conference Proceeding Series,2022-12-05,"<a href=""ScienceDirect (2022-12-05) : More is Better (Mostly): On the Backdoor Attacks in Federated Graph Neural Networks"" target=""_blank"">[https://doi.org/10.1145/3564625.3567999]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3564625.3567999]</a>",,,ScienceDirect
Be Careful with Rotation: A Uniform Backdoor Pattern for 3D Shape,"Linkun Fan, Fazhi He, Qing Guo, Wei Tang, Xiaolin Hong, Bing Li","arXiv
arXiv","2022-12-01
2022-11","<a href=""arXiv (2022-12-01) : Be Careful with Rotation: A Uniform Backdoor Pattern for 3D Shape"" target=""_blank"">[http://arxiv.org/abs/2211.16192v2]</a>
<a href=""DBLP (2022-11) : Be Careful with Rotation: A Uniform Backdoor Pattern for 3D Shape"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.16192]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.16192]</a>","For saving cost, many deep neural networks (DNNs) are trained on third-party datasets downloaded from internet, which enables attacker to implant backdoor into DNNs. In 2D domain, inherent structures of different image formats are similar. Hence, backdoor attack designed for one image format will suite for others. However, when it comes to 3D world, there is a huge disparity among different 3D data structures. As a result, backdoor pattern designed for one certain 3D data structure will be disable for other data structures of the same 3D scene. Therefore, this paper designs a uniform backdoor pattern: NRBdoor (Noisy Rotation Backdoor) which is able to adapt for heterogeneous 3D data structures. Specifically, we start from the unit rotation and then search for the optimal pattern by noise generation and selection process. The proposed NRBdoor is natural and imperceptible, since rotation is a common operation which usually contains noise due to both the miss match between a pair of points and the sensor calibration error for real-world 3D scene. Extensive experiments on 3D mesh and point cloud show that the proposed NRBdoor achieves state-of-the-art performance, with negligible shape variation.
","
","arXiv
DBLP"
Are Backdoor Mandates Ethical?-A Position Paper,Khoury R.,IEEE Technology and Society Magazine,2022-12-01,"<a href=""ScienceDirect (2022-12-01) : Are Backdoor Mandates Ethical?-A Position Paper"" target=""_blank"">[https://doi.org/10.1109/MTS.2022.3217699]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/MTS.2022.3217699]</a>",,,ScienceDirect
Backdoor Attacks on Deep Neural Networks via Transfer Learning from Natural Images,Matsuo Y.,Applied Sciences (Switzerland),2022-12-01,"<a href=""ScienceDirect (2022-12-01) : Backdoor Attacks on Deep Neural Networks via Transfer Learning from Natural Images"" target=""_blank"">[https://doi.org/10.3390/app122412564]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/app122412564]</a>",,,ScienceDirect
Destabilization of the SHP2 and SHP1 protein tyrosine phosphatase domains by a non-conserved “backdoor” cysteine,Yarnall M.T.N.,Biochemistry and Biophysics Reports,2022-12-01,"<a href=""ScienceDirect (2022-12-01) : Destabilization of the SHP2 and SHP1 protein tyrosine phosphatase domains by a non-conserved “backdoor” cysteine"" target=""_blank"">[https://doi.org/10.1016/j.bbrep.2022.101370]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.bbrep.2022.101370]</a>",,,ScienceDirect
"Neural Network: Predator, Victim, and Information Security Tool","V. B. Betelin, V. A. Galatenko, K. A. Kostiukhin",Optical Memory and Neural Networks,2022-12-01,"<a href=""Springer (2022-12-01) : Neural Network: Predator, Victim, and Information Security Tool"" target=""_blank"">[https://link.springer.com/article/10.3103/S1060992X22040026]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.3103/S1060992X22040026]</a>","Abstract The article deals with information security problems associated with neural networks. Malicious neural networks, attacks on neural networks,...",,Springer
Review of Botnet Attack Detection in SDN-Enabled IoT Using Machine Learning,Negera W.G.,Sensors,2022-12-01,"<a href=""ScienceDirect (2022-12-01) : Review of Botnet Attack Detection in SDN-Enabled IoT Using Machine Learning"" target=""_blank"">[https://doi.org/10.3390/s22249837]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/s22249837]</a>",,,ScienceDirect
Make Data Reliable: An Explanation-powered Cleaning on Malware Dataset Against Backdoor Poisoning Attacks,"Xutong Wang, Chaoge Liu, Xiaohui Hu, Zhi Wang, Jie Yin, Xiang Cui","ACSAC '22: Proceedings of the 38th Annual Computer Security Applications Conference
ACSAC","2022-12
2022","<a href=""ACM (2022-12) : Make Data Reliable: An Explanation-powered Cleaning on Malware Dataset Against Backdoor Poisoning Attacks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3564625.3564661]</a>
<a href=""DBLP (2022) : Make Data Reliable: An Explanation-powered Cleaning on Malware Dataset Against Backdoor Poisoning Attacks"" target=""_blank"">[https://doi.org/10.1145/3564625.3564661]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3564625.3564661]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3564625.3564661]</a>","Machine learning (ML) based Malware classification provides excellent performance and has been deployed in various real-world applications. Training for malware classification often relies on crowdsourced threat feeds, which exposes a natural attack ...
","
","ACM
DBLP"
Backdoor Vulnerabilities in Normally Trained Deep Learning Models,"Guanhong Tao, Zhenting Wang, Siyuan Cheng, Shiqing Ma, Shengwei An, Yingqi Liu, Guangyu Shen, Zhuo Zhang, Yunshu Mao, Xiangyu Zhang","arXiv
arXiv","2022-11-29
2022-11","<a href=""arXiv (2022-11-29) : Backdoor Vulnerabilities in Normally Trained Deep Learning Models"" target=""_blank"">[http://arxiv.org/abs/2211.15929v1]</a>
<a href=""DBLP (2022-11) : Backdoor Vulnerabilities in Normally Trained Deep Learning Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.15929]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.15929]</a>","We conduct a systematic study of backdoor vulnerabilities in normally trained Deep Learning models. They are as dangerous as backdoors injected by data poisoning because both can be equally exploited. We leverage 20 different types of injected backdoor attacks in the literature as the guidance and study their correspondences in normally trained models, which we call natural backdoor vulnerabilities. We find that natural backdoors are widely existing, with most injected backdoor attacks having natural correspondences. We categorize these natural backdoors and propose a general detection framework. It finds 315 natural backdoors in the 56 normally trained models downloaded from the Internet, covering all the different categories, while existing scanners designed for injected backdoors can at most detect 65 backdoors. We also study the root causes and defense of natural backdoors.
","
","arXiv
DBLP"
Adversarial security mitigations of mmWave beamforming prediction models using defensive distillation and adversarial retraining,"Murat Kuzlu, Ferhat Ozgur Catak, ... Ozgur Guler",International Journal of Information Security,2022-11-29,"<a href=""Springer (2022-11-29) : Adversarial security mitigations of mmWave beamforming prediction models using defensive distillation and adversarial retraining"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00644-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00644-0]</a>","The design of a security scheme for beamforming prediction is critical for next-generation wireless networks (5G, 6G, and beyond). However, there is...",,Springer
Enhancing Robustness in Federated Learning by Supervised Anomaly Detection,P. Quan W. -H. Lee M. Srivatsa M. Srivastava,2022 26th International Conference on Pattern Recognition (ICPR),2022-11-29,"<a href=""IEEE (2022-11-29) : Enhancing Robustness in Federated Learning by Supervised Anomaly Detection"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956655]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICPR56361.2022.9956655]</a>","Recent years have seen the increasing attention and popularity of federated learning (FL), a distributed learning framework for privacy and data security. However, by its fundamental design, federated learning is inherently vulnerable to model poisoning attacks: a malicious client may submit the local updates to influence the weights of the global model. Therefore, detecting malicious clients against model poisoning attacks in federated learning is useful in safety-critical tasks.However, existing methods either fail to analyze potential malicious data or are computationally restrictive. To overcome these weaknesses, we propose a robust federated learning method where the central server learns a supervised anomaly detector using adversarial data generated from a variety of state-of-the-art poisoning attacks. The key idea of this powerful anomaly detector lies in a comprehensive understanding of the benign update through distinguishing it from the diverse malicious ones. The anomaly detector would then be leveraged in the process of federated learning to automate the removal of malicious updates (even from unforeseen attacks).Through extensive experiments, we demonstrate its effectiveness against backdoor attacks, where the attackers inject adversarial triggers such that the global model will make incorrect predictions on the poisoned samples. We have verified that our method can achieve 99.0% detection AUC scores while enjoying longevity as the model converges. Our method has also shown significant advantages over existing robust federated learning methods in all settings. Furthermore, our method can be easily generalized to incorporate newly-developed poisoning attacks, thus accommodating ever-changing adversarial learning environments.",,IEEE
Taking Away Both Model and Data: Remember Training Data by Parameter Combinations,W. Luo L. Zhang P. Han C. Liu R. Zhuang,IEEE Transactions on Emerging Topics in Computational Intelligence,2022-11-29,"<a href=""IEEE (2022-11-29) : Taking Away Both Model and Data: Remember Training Data by Parameter Combinations"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826434]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TETCI.2022.3182415]</a>","Machine Learning (ML) model hatcheries have emerged to help ML model producers. The only thing that the ML model producer needs to do is upload the untrained ML model to the hatchery with a specific task and deploy the returned trained ML model into real-world applications. Although the local private data of the hatchery are not directly accessed by the ML model producer, some backdoor attacks can still steal the private data. These attacks add malicious backdoor codes into the untrained benign ML model and recover the private data in some specific operations after training. However, existing attacks more or less have some disadvantages, such as the limited quality of the stolen private data, seriously affecting the original model performance, and being easy to defend. To address these disadvantages, we propose a novel efficient white-box backdoor attack method called Parameter Combination Encoding Attack (PCEA), which leverages the linear combinations of parameters to remember the private data during training. We evaluate the performance of the proposed method on stolen image quality, testing accuracy, and sensitivity. The experimental results show that PCEA has a much higher quality of the stolen data and robustness while keeping the testing accuracy.",,IEEE
Mimic Honeypot Design and Analysis,J. Guo H. Yuan M. Xu X. Yang,"2022 2nd International Conference on Frontiers of Electronics, Information and Computation Technologies (ICFEICT)",2022-11-28,"<a href=""IEEE (2022-11-28) : Mimic Honeypot Design and Analysis"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9951450]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICFEICT57213.2022.00110]</a>","Aiming at the problem that high-interaction honeypots face great risks. This paper integrates honeypot technology and mimic defense technology, proposes a honeypot model based on mimic defense named a mimic honeypot, and studies the theoretical basis, the core idea, the built environment. Finally, the anti-attack ability of the mimic honeypot is theoretically analyzed, and the results show that it has a better defense ability against unpredictable vulnerabilities and backdoors than the traditional high-interaction honeypot.",,IEEE
BadPrompt: Backdoor Attacks on Continuous Prompts,"Xiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, Xiaojie Yuan","arXiv
NeurIPS
arXiv","2022-11-27
2022
2022-11","<a href=""arXiv (2022-11-27) : BadPrompt: Backdoor Attacks on Continuous Prompts"" target=""_blank"">[http://arxiv.org/abs/2211.14719v1]</a>
<a href=""DBLP (2022) : BadPrompt: Backdoor Attacks on Continuous Prompts"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/f0722b58f02d7793acf7d328928f933a-Abstract-Conference.html]</a>
<a href=""DBLP (2022-11) : BadPrompt: Backdoor Attacks on Continuous Prompts"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.14719]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/f0722b58f02d7793acf7d328928f933a-Abstract-Conference.html]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.14719]</a>","The prompt-based learning paradigm has gained much research attention recently. It has achieved state-of-the-art performance on several NLP tasks, especially in the few-shot scenarios. While steering the downstream tasks, few works have been reported to investigate the security problems of the prompt-based models. In this paper, we conduct the first study on the vulnerability of the continuous prompt learning algorithm to backdoor attacks. We observe that the few-shot scenarios have posed a great challenge to backdoor attacks on the prompt-based models, limiting the usability of existing NLP backdoor methods. To address this challenge, we propose BadPrompt, a lightweight and task-adaptive algorithm, to backdoor attack continuous prompts. Specially, BadPrompt first generates candidate triggers which are indicative for predicting the targeted label and dissimilar to the samples of the non-targeted labels. Then, it automatically selects the most effective and invisible trigger for each sample with an adaptive trigger optimization algorithm. We evaluate the performance of BadPrompt on five datasets and two continuous prompt models. The results exhibit the abilities of BadPrompt to effectively attack continuous prompts while maintaining high performance on the clean test sets, outperforming the baseline models by a large margin. The source code of BadPrompt is publicly available at https://github.com/papersPapers/BadPrompt.

","<a href=""arXiv"" target=""_blank"">[https://github.com/papersPapers/BadPrompt]</a>

","arXiv
DBLP
DBLP"
Data-free Backdoor Removal based on Channel Lipschitzness,"Runkai Zheng, Rongjun Tang, Jianze Li, Li Liu","arXiv
arXiv","2022-11-26
2022-08","<a href=""arXiv (2022-11-26) : Data-free Backdoor Removal based on Channel Lipschitzness"" target=""_blank"">[http://arxiv.org/abs/2208.03111v2]</a>
<a href=""DBLP (2022-08) : Data-free Backdoor Removal based on Channel Lipschitzness"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.03111]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.03111]</a>","Recent studies have shown that Deep Neural Networks (DNNs) are vulnerable to the backdoor attacks, which leads to malicious behaviors of DNNs when specific triggers are attached to the input images. It was further demonstrated that the infected DNNs possess a collection of channels, which are more sensitive to the backdoor triggers compared with normal channels. Pruning these channels was then shown to be effective in mitigating the backdoor behaviors. To locate those channels, it is natural to consider their Lipschitzness, which measures their sensitivity against worst-case perturbations on the inputs. In this work, we introduce a novel concept called Channel Lipschitz Constant (CLC), which is defined as the Lipschitz constant of the mapping from the input images to the output of each channel. Then we provide empirical evidences to show the strong correlation between an Upper bound of the CLC (UCLC) and the trigger-activated change on the channel activation. Since UCLC can be directly calculated from the weight matrices, we can detect the potential backdoor channels in a data-free manner, and do simple pruning on the infected DNN to repair the model. The proposed Channel Lipschitzness based Pruning (CLP) method is super fast, simple, data-free and robust to the choice of the pruning threshold. Extensive experiments are conducted to evaluate the efficiency and effectiveness of CLP, which achieves state-of-the-art results among the mainstream defense methods even without any data. Source codes are available at https://github.com/rkteddy/channel-Lipschitzness-based-pruning.
","<a href=""arXiv"" target=""_blank"">[https://github.com/rkteddy/channel-Lipschitzness-based-pruning]</a>
","arXiv
DBLP"
A systematic literature review on trust in the software ecosystem,"Fang Hou, Slinger Jansen",Empirical Software Engineering,2022-11-23,"<a href=""Springer (2022-11-23) : A systematic literature review on trust in the software ecosystem"" target=""_blank"">[https://link.springer.com/article/10.1007/s10664-022-10238-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10664-022-10238-y]</a>","The worldwide software ecosystem is a trust-rich part of the world. Throughout the software life cycle, software engineers, end-users, and other...",,Springer
Toward safe AI,"Andres Morales-Forero, Samuel Bassetto, Eric Coatanea",AI & SOCIETY,2022-11-23,"<a href=""Springer (2022-11-23) : Toward safe AI"" target=""_blank"">[https://link.springer.com/article/10.1007/s00146-022-01591-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00146-022-01591-z]</a>","Since some AI algorithms with high predictive power have impacted human integrity, safety has become a crucial challenge in adopting and deploying...",,Springer
A Survey on Backdoor Attack and Defense in Natural Language Processing,"Xuan Sheng, Zhaoyang Han, Piji Li, Xiangmao Chang","arXiv
QRS
arXiv","2022-11-22
2022
2022-11","<a href=""arXiv (2022-11-22) : A Survey on Backdoor Attack and Defense in Natural Language Processing"" target=""_blank"">[http://arxiv.org/abs/2211.11958v1]</a>
<a href=""DBLP (2022) : A Survey on Backdoor Attack and Defense in Natural Language Processing"" target=""_blank"">[https://doi.org/10.1109/QRS57517.2022.00086]</a>
<a href=""DBLP (2022-11) : A Survey on Backdoor Attack and Defense in Natural Language Processing"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.11958]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/QRS57517.2022.00086]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.11958]</a>","Deep learning is becoming increasingly popular in real-life applications, especially in natural language processing (NLP). Users often choose training outsourcing or adopt third-party data and models due to data and computation resources being limited. In such a situation, training data and models are exposed to the public. As a result, attackers can manipulate the training process to inject some triggers into the model, which is called backdoor attack. Backdoor attack is quite stealthy and difficult to be detected because it has little inferior influence on the model's performance for the clean samples. To get a precise grasp and understanding of this problem, in this paper, we conduct a comprehensive review of backdoor attacks and defenses in the field of NLP. Besides, we summarize benchmark datasets and point out the open issues to design credible systems to defend against backdoor attacks.

","

","arXiv
DBLP
DBLP"
Intellectual property protection of DNN models,"Sen Peng, Yufei Chen, ... Xiaohua Jia",World Wide Web,2022-11-22,"<a href=""Springer (2022-11-22) : Intellectual property protection of DNN models"" target=""_blank"">[https://link.springer.com/article/10.1007/s11280-022-01113-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11280-022-01113-3]</a>","Deep learning has been widely applied in solving many tasks, such as image recognition, speech recognition, and natural language processing. It...",,Springer
Self-testing quantum random number generator with easily testable assumptions,M. Pawłowski M. Jarzyna K. Łukanowski M. Jachura K. Banaszek,2022 IEEE International Conference on Quantum Computing and Engineering (QCE),2022-11-22,"<a href=""IEEE (2022-11-22) : Self-testing quantum random number generator with easily testable assumptions"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9951211]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/QCE53715.2022.00059]</a>","We propose a new paradigm for security of quantum protocols. Instead of making one, powerful, difficult to check assumption about the system, we make a few, which are easy to verify or otherwise justify. This enables us to combine very high security levels with relatively low hardware complexity. We present a self-testing quantum random number generator that demonstrates the usefulness of our paradigm. We describe this device, prove its security against active attacks, backdoors and malfunctions and analyze its efficiency.",,IEEE
Backdoor Attacks on Multiagent Collaborative Systems,"Shuo Chen, Yue Qiu, Jie Zhang","arXiv
arXiv","2022-11-21
2022-11","<a href=""arXiv (2022-11-21) : Backdoor Attacks on Multiagent Collaborative Systems"" target=""_blank"">[http://arxiv.org/abs/2211.11455v1]</a>
<a href=""DBLP (2022-11) : Backdoor Attacks on Multiagent Collaborative Systems"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.11455]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.11455]</a>","Backdoor attacks on reinforcement learning implant a backdoor in a victim agent's policy. Once the victim observes the trigger signal, it will switch to the abnormal mode and fail its task. Most of the attacks assume the adversary can arbitrarily modify the victim's observations, which may not be practical. One work proposes to let one adversary agent use its actions to affect its opponent in two-agent competitive games, so that the opponent quickly fails after observing certain trigger actions. However, in multiagent collaborative systems, agents may not always be able to observe others. When and how much the adversary agent can affect others are uncertain, and we want the adversary agent to trigger others for as few times as possible. To solve this problem, we first design a novel training framework to produce auxiliary rewards that measure the extent to which the other agents'observations being affected. Then we use the auxiliary rewards to train a trigger policy which enables the adversary agent to efficiently affect the others' observations. Given these affected observations, we further train the other agents to perform abnormally. Extensive experiments demonstrate that the proposed method enables the adversary agent to lure the others into the abnormal mode with only a few actions.
","
","arXiv
DBLP"
Deep SARSA-based reinforcement learning approach for anomaly network intrusion detection system,"Safa Mohamed, Ridha Ejbali",International Journal of Information Security,2022-11-19,"<a href=""Springer (2022-11-19) : Deep SARSA-based reinforcement learning approach for anomaly network intrusion detection system"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00634-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00634-2]</a>",The growing evolution of cyber-attacks imposes a risk in network services. The search of new techniques is essential to detect and classify dangerous...,,Springer
Provable Defense against Backdoor Policies in Reinforcement Learning,"Shubham Kumar Bharti, Xuezhou Zhang, Adish Singla, Xiaojin Zhu","arXiv
NeurIPS
arXiv","2022-11-18
2022
2022-11","<a href=""arXiv (2022-11-18) : Provable Defense against Backdoor Policies in Reinforcement Learning"" target=""_blank"">[http://arxiv.org/abs/2211.10530v1]</a>
<a href=""DBLP (2022) : Provable Defense against Backdoor Policies in Reinforcement Learning"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/5e67e6a814526079ad8505bf6d926fb6-Abstract-Conference.html]</a>
<a href=""DBLP (2022-11) : Provable Defense against Backdoor Policies in Reinforcement Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.10530]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/5e67e6a814526079ad8505bf6d926fb6-Abstract-Conference.html]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.10530]</a>","We propose a provable defense mechanism against backdoor policies in reinforcement learning under subspace trigger assumption. A backdoor policy is a security threat where an adversary publishes a seemingly well-behaved policy which in fact allows hidden triggers. During deployment, the adversary can modify observed states in a particular way to trigger unexpected actions and harm the agent. We assume the agent does not have the resources to re-train a good policy. Instead, our defense mechanism sanitizes the backdoor policy by projecting observed states to a 'safe subspace', estimated from a small number of interactions with a clean (non-triggered) environment. Our sanitized policy achieves $\epsilon$ approximate optimality in the presence of triggers, provided the number of clean interactions is $O\left(\frac{D}{(1-\gamma)^4 \epsilon^2}\right)$ where $\gamma$ is the discounting factor and $D$ is the dimension of state space. Empirically, we show that our sanitization defense performs well on two Atari game environments.

","

","arXiv
DBLP
DBLP"
PatrIoT: practical and agile threat research for IoT,"Emre Süren, Fredrik Heiding, ... Robert Lagerström",International Journal of Information Security,2022-11-18,"<a href=""Springer (2022-11-18) : PatrIoT: practical and agile threat research for IoT"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00633-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00633-3]</a>","The Internet of things (IoT) products, which have been widely adopted, still pose challenges in the modern cybersecurity landscape. Many IoT devices...",,Springer
Omission and commission errors underlying AI failures,"Sasanka Sekhar Chanda, Debarag Narayan Banerjee",AI & SOCIETY,2022-11-17,"<a href=""Springer (2022-11-17) : Omission and commission errors underlying AI failures"" target=""_blank"">[https://link.springer.com/article/10.1007/s00146-022-01585-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00146-022-01585-x]</a>",In this article we investigate origins of several cases of failure of Artificial Intelligence (AI) systems employing machine learning and deep...,,Springer
PBSM: Backdoor attack against Keyword spotting based on pitch boosting and sound masking,"Hanbo Cai, Pengcheng Zhang, Hai Dong, Yan Xiao, Shunhui Ji","arXiv
arXiv","2022-11-16
2022-11","<a href=""arXiv (2022-11-16) : PBSM: Backdoor attack against Keyword spotting based on pitch boosting and sound masking"" target=""_blank"">[http://arxiv.org/abs/2211.08697v1]</a>
<a href=""DBLP (2022-11) : PBSM: Backdoor attack against Keyword spotting based on pitch boosting and sound masking"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.08697]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.08697]</a>","Keyword spotting (KWS) has been widely used in various speech control scenarios. The training of KWS is usually based on deep neural networks and requires a large amount of data. Manufacturers often use third-party data to train KWS. However, deep neural networks are not sufficiently interpretable to manufacturers, and attackers can manipulate third-party training data to plant backdoors during the model training. An effective backdoor attack can force the model to make specified judgments under certain conditions, i.e., triggers. In this paper, we design a backdoor attack scheme based on Pitch Boosting and Sound Masking for KWS, called PBSM. Experimental results demonstrated that PBSM is feasible to achieve an average attack success rate close to 90% in three victim models when poisoning less than 1% of the training data.
","
","arXiv
DBLP"
Handcrafted Backdoors in Deep Neural Networks,"Sanghyun Hong, Nicholas Carlini, Alexey Kurakin","arXiv
NeurIPS
arXiv","2022-11-15
2022
2021-06","<a href=""arXiv (2022-11-15) : Handcrafted Backdoors in Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2106.04690v2]</a>
<a href=""DBLP (2022) : Handcrafted Backdoors in Deep Neural Networks"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/3538a22cd3ceb8f009cc62b9e535c29f-Abstract-Conference.html]</a>
<a href=""DBLP (2021-06) : Handcrafted Backdoors in Deep Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2106.04690]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/3538a22cd3ceb8f009cc62b9e535c29f-Abstract-Conference.html]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2106.04690]</a>","When machine learning training is outsourced to third parties, $backdoor$ $attacks$ become practical as the third party who trains the model may act maliciously to inject hidden behaviors into the otherwise accurate model. Until now, the mechanism to inject backdoors has been limited to $poisoning$. We argue that a supply-chain attacker has more attack techniques available by introducing a $handcrafted$ attack that directly manipulates a model's weights. This direct modification gives our attacker more degrees of freedom compared to poisoning, and we show it can be used to evade many backdoor detection or removal defenses effectively. Across four datasets and four network architectures our backdoor attacks maintain an attack success rate above 96%. Our results suggest that further research is needed for understanding the complete space of supply-chain backdoor attacks.

","

","arXiv
DBLP
DBLP"
Mobile botnet detection: a comprehensive survey,"Sajad Hamzenejadi, Mahdieh Ghazvini, Seyedamiryousef Hosseini",International Journal of Information Security,2022-11-15,"<a href=""Springer (2022-11-15) : Mobile botnet detection: a comprehensive survey"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00624-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00624-4]</a>",The number of people using mobile devices is increasing as mobile devices offer different features and services. Many mobile users install various...,,Springer
A survey on federated learning: challenges and applications,"Jie Wen, Zhixia Zhang, ... Wensheng Zhang",International Journal of Machine Learning and Cybernetics,2022-11-11,"<a href=""Springer (2022-11-11) : A survey on federated learning: challenges and applications"" target=""_blank"">[https://link.springer.com/article/10.1007/s13042-022-01647-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13042-022-01647-y]</a>",Federated learning (FL) is a secure distributed machine learning paradigm that addresses the issue of data silos in building a joint model. Its...,,Springer
Just Rotate it: Deploying Backdoor Attacks via Rotation Transformation,Wu T.,"AISec 2022 - Proceedings of the 15th ACM Workshop on Artificial Intelligence and Security, co-located with CCS 2022",2022-11-11,"<a href=""ScienceDirect (2022-11-11) : Just Rotate it: Deploying Backdoor Attacks via Rotation Transformation"" target=""_blank"">[https://doi.org/10.1145/3560830.3563730]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3560830.3563730]</a>",,,ScienceDirect
Attack Detection and Mitigation in MEC-Enabled 5G Networks for AIoT,S. -M. Cheng B. -K. Hong C. -F. Hung,IEEE Internet of Things Magazine,2022-11-10,"<a href=""IEEE (2022-11-10) : Attack Detection and Mitigation in MEC-Enabled 5G Networks for AIoT"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9945850]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IOTM.001.2100144]</a>","In recent years, AIoT (Artificial Intelligence of Things) applications have received lots of attention since it exploits widely deployed IoT devices to collect data and perform an action while leveraging AI to obtain knowledge and insights. When deploying AIoT in 5G networks, Multi-access Edge Computing (MEC) is appropriate for enabling local management of IoT devices and computation of machine learning (ML) algorithms. Facing the multitude of threats in the ML data layer, IoT service layer, and 5G communications layer, MEC should enable corresponding detection and mitigation schemes to protect AIoT applications. In this article, we examine the existing security solutions located in each layer and discuss the interrelated challenges. For example, a network-traffic-based IDS solution in MEC might capture IoT malware but fail to identify a growing file-less attack. We suggest that firmware emulation in IoT endpoint should be included to provide system-level behaviors to network-based detectors in MEC so that file-less attacks can be distinguished. The potential of a backdoor attack aiming to poison data or corrupt ML models cannot be ignored in AIoT applications, and a corresponding detector should be implemented in MEC. Due to the rise of low-cost Software-Defined Radio (SDR), malicious attacks using rogue base stations (BSs) have become more popular. It implies that security protection at MEC in the communications layer is necessary. This article, therefore, proposes a novel platform, M3Inspector, where inspectors located in mobiles and AIoT machines collect information from surrounding BSs and provide them to MEC. MEC determines the rogue BS and makes a notification to users subscribing to the local service. A realistic 5G experimental platform with rogue BSs is developed. The results demonstrate that attack detection and mitigation can be implemented in the MEC paradigm to significantly improve the security protection of AIoT from the perspectives of rogue BS attacks and file-less attacks.",,IEEE
Development of Cyber Attack Model for Private Network,M. Al-Amin M. A. Khatun M. Nasir Uddin,2022 Second International Conference on Interdisciplinary Cyber Physical Systems (ICPS),2022-11-10,"<a href=""IEEE (2022-11-10) : Development of Cyber Attack Model for Private Network"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9941250]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICPS55917.2022.00046]</a>","Cyber Attack is the most challenging issue all over the world. Nowadays, Cyber-attacks are increasing on digital systems and organizations. Innovation and utilization of new digital technology, infrastructure, connectivity, and dependency on digital strategies are transforming day by day. The cyber threat scope has extended significantly. Currently, attackers are becoming more sophisticated, well-organized, and professional in generating malware programs in Python, C Programming, C++ Programming, Java, SQL, PHP, JavaScript, Ruby etc. Accurate attack modeling techniques provide cyber-attack planning, which can be applied quickly during a different ongoing cyber-attack. This paper aims to create a new cyber-attack model that will extend the existing model, which provides a better understanding of the network’s vulnerabilities.Moreover, It helps protect the company or private network infrastructure from future cyber-attacks. The final goal is to handle cyber-attacks efficacious manner using attack modeling techniques. Nowadays, many organizations, companies, authorities, industries, and individuals have faced cybercrime. To execute attacks using our model where honeypot, the firewall, DMZ and any other security are available in any environment.",,IEEE
Secure Anonymous Communication on Corrupted Machines With Reverse Firewalls,Y. Wang R. Chen X. Huang B. Wang,IEEE Transactions on Dependable and Secure Computing,2022-11-10,"<a href=""IEEE (2022-11-10) : Secure Anonymous Communication on Corrupted Machines With Reverse Firewalls"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9525235]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TDSC.2021.3107463]</a>","The Snowden revelations in 2013 showed that user machines running cryptographic protocols might be stealthily corrupted by attackers (e.g., manufacturers and supply-chain intermediaries) who could tamper cryptographic implementations to insert backdoors to undermine cryptographic tools. To formalize such attacks, in CRYPTO 2014, Bellare et al. proposed the notion of Algorithm-Substitution Attack (ASA) which has been extensively studied since then. In this work, we turn to investigate the security of anonymous communication (AC) protocol—a well-known tool to protect user privacy on the Internet—in the case when user machines are corrupted. Specifically, we give a formal treatment of ASAs on the universal mixnet-based AC ($\mathsf{U\text{-}Mix\text{-}AC}$U-Mix-AC) protocols. We show that ASAs on $\mathsf{U\text{-}Mix\text{-}AC}$U-Mix-AC protocols could be more dangerous than previously thought by presenting attacks that are extremely powerful. As countermeasure, we adopt cryptographic reverse firewall (CRF), originally proposed by Mironov and Stephens-Davidowitz in EUROCRYPT 2015, to restore the security of $\mathsf{U\text{-}Mix\text{-}AC}$U-Mix-AC protocols in the presence of ASAs. We also implement proposed AC protocol, ASAs and CRFs for experimental evaluations, and the results show that the execution time of subverted algorithms is almost the same as that of faithful ones and our designed CRFs are effective to guard the security of $\mathsf{U\text{-}Mix\text{-}AC}$U-Mix-AC protocol.",,IEEE
Physics-Constrained Backdoor Attacks on Power System Fault Localization,"Jianing Bai, Ren Wang, Zuyi Li","arXiv
arXiv","2022-11-07
2022-11","<a href=""arXiv (2022-11-07) : Physics-Constrained Backdoor Attacks on Power System Fault Localization"" target=""_blank"">[http://arxiv.org/abs/2211.04445v1]</a>
<a href=""DBLP (2022-11) : Physics-Constrained Backdoor Attacks on Power System Fault Localization"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.04445]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.04445]</a>","The advances in deep learning (DL) techniques have the potential to deliver transformative technological breakthroughs to numerous complex tasks in modern power systems that suffer from increasing uncertainty and nonlinearity. However, the vulnerability of DL has yet to be thoroughly explored in power system tasks under various physical constraints. This work, for the first time, proposes a novel physics-constrained backdoor poisoning attack, which embeds the undetectable attack signal into the learned model and only performs the attack when it encounters the corresponding signal. The paper illustrates the proposed attack on the real-time fault line localization application. Furthermore, the simulation results on the 68-bus power system demonstrate that DL-based fault line localization methods are not robust to our proposed attack, indicating that backdoor poisoning attacks pose real threats to DL implementations in power systems. The proposed attack pipeline can be easily generalized to other power system tasks.
","
","arXiv
DBLP"
Neural Architectural Backdoors,"Ren Pang, Changjiang Li, Zhaohan Xi, Shouling Ji, Ting Wang",arXiv,2022-11-07,"<a href=""arXiv (2022-11-07) : Neural Architectural Backdoors"" target=""_blank"">[http://arxiv.org/abs/2210.12179v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","This paper asks the intriguing question: is it possible to exploit neural architecture search (NAS) as a new attack vector to launch previously improbable attacks? Specifically, we present EVAS, a new attack that leverages NAS to find neural architectures with inherent backdoors and exploits such vulnerability using input-aware triggers. Compared with existing attacks, EVAS demonstrates many interesting properties: (i) it does not require polluting training data or perturbing model parameters, (ii) it is agnostic to downstream fine-tuning or even re-training from scratch, (iii) it naturally evades defenses that rely on inspecting model parameters or training data. With extensive evaluation on benchmark datasets, we show that EVAS features high evasiveness, transferability, and robustness, thereby expanding the adversary's design spectrum. We further characterize the mechanisms underlying EVAS, which are possibly explainable by architecture-level ``shortcuts'' that recognize trigger patterns. This work raises concerns about the current practice of NAS and points to potential directions to develop effective countermeasures.",,arXiv
Poster: Clean-label Backdoor Attack on Graph Neural Networks,Xu J.,Proceedings of the ACM Conference on Computer and Communications Security,2022-11-07,"<a href=""ScienceDirect (2022-11-07) : Poster: Clean-label Backdoor Attack on Graph Neural Networks"" target=""_blank"">[https://doi.org/10.1145/3548606.3563531]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3548606.3563531]</a>",,,ScienceDirect
Towards Backdoor Attacks against LiDAR Object Detection in Autonomous Driving,Zhang Y.,SenSys 2022 - Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems,2022-11-06,"<a href=""ScienceDirect (2022-11-06) : Towards Backdoor Attacks against LiDAR Object Detection in Autonomous Driving"" target=""_blank"">[https://doi.org/10.1145/3560905.3568539]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3560905.3568539]</a>",,,ScienceDirect
M-to-N Backdoor Paradigm: A Stealthy and Fuzzy Attack to Deep Learning Models,"Linshan Hou, Zhongyun Hua, Yuhong Li, Leo Yu Zhang","arXiv
arXiv","2022-11-03
2022-11","<a href=""arXiv (2022-11-03) : M-to-N Backdoor Paradigm: A Stealthy and Fuzzy Attack to Deep Learning Models"" target=""_blank"">[http://arxiv.org/abs/2211.01875v1]</a>
<a href=""DBLP (2022-11) : M-to-N Backdoor Paradigm: A Stealthy and Fuzzy Attack to Deep Learning Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.01875]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.01875]</a>","Recent studies show that deep neural networks (DNNs) are vulnerable to backdoor attacks. A backdoor DNN model behaves normally with clean inputs, whereas outputs attacker's expected behaviors when the inputs contain a pre-defined pattern called a trigger. However, in some tasks, the attacker cannot know the exact target that shows his/her expected behavior, because the task may contain a large number of classes and the attacker does not have full access to know the semantic details of these classes. Thus, the attacker is willing to attack multiple suspected targets to achieve his/her purpose. In light of this, in this paper, we propose the M-to-N backdoor attack, a new attack paradigm that allows an attacker to launch a fuzzy attack by simultaneously attacking N suspected targets, and each of the N targets can be activated by any one of its M triggers. To achieve a better stealthiness, we randomly select M clean images from the training dataset as our triggers for each target. Since the triggers used in our attack have the same distribution as the clean images, the inputs poisoned by the triggers are difficult to be detected by the input-based defenses, and the backdoor models trained on the poisoned training dataset are also difficult to be detected by the model-based defenses. Thus, our attack is stealthier and has a higher probability of achieving the attack purpose by attacking multiple suspected targets simultaneously in contrast to prior backdoor attacks. Extensive experiments show that our attack is effective against different datasets with various models and achieves high attack success rates (e.g., 99.43% for attacking 2 targets and 98.23% for attacking 4 targets on the CIFAR-10 dataset) when poisoning only an extremely small portion of the training dataset (e.g., less than 2%). Besides, it is robust to pre-processing operations and can resist state-of-the-art defenses.
","
","arXiv
DBLP"
On-device context-aware misuse detection framework for heterogeneous IoT edge,"Nitish A, Hanumanthappa J, ... Kirill Krinkin",Applied Intelligence,2022-11-03,"<a href=""Springer (2022-11-03) : On-device context-aware misuse detection framework for heterogeneous IoT edge"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-022-04039-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-022-04039-5]</a>","Traditional AI techniques for offline misuse network intrusion detection have performed well, assuming that the traffic from the datasets is...",,Springer
One-shot Neural Backdoor Erasing via Adversarial Weight Masking,"Shuwen Chai, Jinghui Chen","arXiv
NeurIPS
arXiv","2022-11-02
2022
2022-07","<a href=""arXiv (2022-11-02) : One-shot Neural Backdoor Erasing via Adversarial Weight Masking"" target=""_blank"">[http://arxiv.org/abs/2207.04497v2]</a>
<a href=""DBLP (2022) : One-shot Neural Backdoor Erasing via Adversarial Weight Masking"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/8c0f7107ab85892ccf51f0a814957af1-Abstract-Conference.html]</a>
<a href=""DBLP (2022-07) : One-shot Neural Backdoor Erasing via Adversarial Weight Masking"" target=""_blank"">[https://doi.org/10.48550/arXiv.2207.04497]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/8c0f7107ab85892ccf51f0a814957af1-Abstract-Conference.html]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2207.04497]</a>","Recent studies show that despite achieving high accuracy on a number of real-world applications, deep neural networks (DNNs) can be backdoored: by injecting triggered data samples into the training dataset, the adversary can mislead the trained model into classifying any test data to the target class as long as the trigger pattern is presented. To nullify such backdoor threats, various methods have been proposed. Particularly, a line of research aims to purify the potentially compromised model. However, one major limitation of this line of work is the requirement to access sufficient original training data: the purifying performance is a lot worse when the available training data is limited. In this work, we propose Adversarial Weight Masking (AWM), a novel method capable of erasing the neural backdoors even in the one-shot setting. The key idea behind our method is to formulate this into a min-max optimization problem: first, adversarially recover the trigger patterns and then (soft) mask the network weights that are sensitive to the recovered patterns. Comprehensive evaluations of several benchmark datasets suggest that AWM can largely improve the purifying effects over other state-of-the-art methods on various available training dataset sizes.

","

","arXiv
DBLP
DBLP"
An overview of structural coverage metrics for testing neural networks,"Muhammad Usman, Youcheng Sun, ... Corina S. Păsăreanu",International Journal on Software Tools for Technology Transfer,2022-11-02,"<a href=""Springer (2022-11-02) : An overview of structural coverage metrics for testing neural networks"" target=""_blank"">[https://link.springer.com/article/10.1007/s10009-022-00683-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10009-022-00683-x]</a>","Deep neural network (DNN) models, including those used in safety-critical domains, need to be thoroughly tested to ensure that they can reliably...",,Springer
A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks,"Ganqu Cui, Lifan Yuan, Bingxiang He, Yangyi Chen, Zhiyuan Liu, Maosong Sun","arXiv
NeurIPS
arXiv","2022-11-01
2022
2022-06","<a href=""arXiv (2022-11-01) : A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks"" target=""_blank"">[http://arxiv.org/abs/2206.08514v2]</a>
<a href=""DBLP (2022) : A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/2052b3e0617ecb2ce9474a6feaf422b3-Abstract-Datasets_and_Benchmarks.html]</a>
<a href=""DBLP (2022-06) : A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.08514]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/2052b3e0617ecb2ce9474a6feaf422b3-Abstract-Datasets_and_Benchmarks.html]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.08514]</a>","Textual backdoor attacks are a kind of practical threat to NLP systems. By injecting a backdoor in the training phase, the adversary could control model predictions via predefined triggers. As various attack and defense models have been proposed, it is of great significance to perform rigorous evaluations. However, we highlight two issues in previous backdoor learning evaluations: (1) The differences between real-world scenarios (e.g. releasing poisoned datasets or models) are neglected, and we argue that each scenario has its own constraints and concerns, thus requires specific evaluation protocols, (2) The evaluation metrics only consider whether the attacks could flip the models' predictions on poisoned samples and retain performances on benign samples, but ignore that poisoned samples should also be stealthy and semantic-preserving. To address these issues, we categorize existing works into three practical scenarios in which attackers release datasets, pre-trained models, and fine-tuned models respectively, then discuss their unique evaluation methodologies. On metrics, to completely evaluate poisoned samples, we use grammar error increase and perplexity difference for stealthiness, along with text similarity for validity. After formalizing the frameworks, we develop an open-source toolkit OpenBackdoor to foster the implementations and evaluations of textual backdoor learning. With this toolkit, we perform extensive experiments to benchmark attack and defense models under the suggested paradigm. To facilitate the underexplored defenses against poisoned datasets, we further propose CUBE, a simple yet strong clustering-based defense baseline. We hope that our frameworks and benchmarks could serve as the cornerstones for future model development and evaluations.

","

","arXiv
DBLP
DBLP"
A concealed poisoning attack to reduce deep neural networks’ robustness against adversarial samples,Zheng J.,Information Sciences,2022-11-01,"<a href=""ScienceDirect (2022-11-01) : A concealed poisoning attack to reduce deep neural networks’ robustness against adversarial samples"" target=""_blank"">[https://doi.org/10.1016/j.ins.2022.09.060]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.ins.2022.09.060]</a>",,,ScienceDirect
Active intellectual property protection for deep neural networks through stealthy backdoor and users’ identities authentication,Xue M.,Applied Intelligence,2022-11-01,"<a href=""ScienceDirect (2022-11-01) : Active intellectual property protection for deep neural networks through stealthy backdoor and users’ identities authentication"" target=""_blank"">[https://doi.org/10.1007/s10489-022-03339-0]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/s10489-022-03339-0]</a>",,,ScienceDirect
An Ontological Knowledge Base of Poisoning Attacks on Deep Neural Networks,Altoub M.,Applied Sciences (Switzerland),2022-11-01,"<a href=""ScienceDirect (2022-11-01) : An Ontological Knowledge Base of Poisoning Attacks on Deep Neural Networks"" target=""_blank"">[https://doi.org/10.3390/app122111053]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/app122111053]</a>",,,ScienceDirect
"Increasing depth, distribution distillation, and model soup: Erasing backdoor triggers for deep neural networks",Zhang Y.,Journal of Electronic Imaging,2022-11-01,"<a href=""ScienceDirect (2022-11-01) : Increasing depth, distribution distillation, and model soup: Erasing backdoor triggers for deep neural networks"" target=""_blank"">[https://doi.org/10.1117/1.JEI.31.6.063005]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1117/1.JEI.31.6.063005]</a>",,,ScienceDirect
MP-BADNet <sup>+</sup> : Secure and effective backdoor attack detection and mitigation protocols among multi-participants in private DNNs,Chen C.,Peer-to-Peer Networking and Applications,2022-11-01,"<a href=""ScienceDirect (2022-11-01) : MP-BADNet <sup>+</sup> : Secure and effective backdoor attack detection and mitigation protocols among multi-participants in private DNNs"" target=""_blank"">[https://doi.org/10.1007/s12083-022-01377-6]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/s12083-022-01377-6]</a>",,,ScienceDirect
Security Issues and Solutions in Federate Learning Under IoT Critical Infrastructure,"Nasir Ahmad Jalali, Hongsong Chen",Wireless Personal Communications,2022-11-01,"<a href=""Springer (2022-11-01) : Security Issues and Solutions in Federate Learning Under IoT Critical Infrastructure"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-022-10107-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-022-10107-3]</a>",Digital world especially artificial intelligence and IoT have a vital and significant role in human life for exchanging the information and offering...,,Springer
Poster: Backdoor Attacks on Spiking NNs and Neuromorphic Datasets,"Gorka Abad, Oguzhan Ersoy, Stjepan Picek, Víctor Julio Ramírez-Durán, Aitor Urbieta","CCS '22: Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security
CCS","2022-11
2022","<a href=""ACM (2022-11) : Poster: Backdoor Attacks on Spiking NNs and Neuromorphic Datasets"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3548606.3563532]</a>
<a href=""DBLP (2022) : Poster: Backdoor Attacks on Spiking NNs and Neuromorphic Datasets"" target=""_blank"">[https://doi.org/10.1145/3548606.3563532]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3548606.3563532]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3548606.3563532]</a>","Neural networks provide state-of-the-art results in many domains. Yet, they often require high energy and time-consuming training processes. Therefore, the research community is exploring alternative, energy-efficient approaches likespiking neural ...
","
","ACM
DBLP"
"FenceSitter: Black-box, Content-Agnostic, and Synchronization-Free Enrollment-Phase Attacks on Speaker Recognition Systems","Jiangyi Deng, Yanjiao Chen, Wenyuan Xu",CCS '22: Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security,2022-11,"<a href=""ACM (2022-11) : FenceSitter: Black-box, Content-Agnostic, and Synchronization-Free Enrollment-Phase Attacks on Speaker Recognition Systems"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3548606.3559357]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3548606.3559357]</a>","Speaker Recognition Systems (SRSs) grant access to legitimate users based on voiceprint. Recent research has shown that SRSs can be bypassed during the training phase (backdoor attacks) and the recognition phase (evasion attacks). In this paper, we ...",,ACM
Identifying a Training-Set Attack's Target Using Renormalized Influence Estimation,"Zayd Hammoudeh, Daniel Lowd",CCS '22: Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security,2022-11,"<a href=""ACM (2022-11) : Identifying a Training-Set Attack's Target Using Renormalized Influence Estimation"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3548606.3559335]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3548606.3559335]</a>","Targeted training-set attacks inject malicious instances into the training set to cause a trained model to mislabel one or more specific test instances. This work proposes the task of target identification, which determines whether a specific test ...",,ACM
Rickrolling the Artist: Injecting Invisible Backdoors into Text-Guided Image Generation Models,"Lukas Struppek, Dominik Hintersdorf, Kristian Kersting",arXiv,2022-11,"<a href=""DBLP (2022-11) : Rickrolling the Artist: Injecting Invisible Backdoors into Text-Guided Image Generation Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.02408]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.02408]</a>",,,DBLP
WeDef: Weakly Supervised Backdoor Defense for Text Classification,"Lesheng Jin, Zihan Wang, Jingbo Shang","arXiv
EMNLP
arXiv","2022-10-28
2022
2022-05","<a href=""arXiv (2022-10-28) : WeDef: Weakly Supervised Backdoor Defense for Text Classification"" target=""_blank"">[http://arxiv.org/abs/2205.11803v2]</a>
<a href=""DBLP (2022) : WeDef: Weakly Supervised Backdoor Defense for Text Classification"" target=""_blank"">[https://doi.org/10.18653/v1/2022.emnlp-main.798]</a>
<a href=""DBLP (2022-05) : WeDef: Weakly Supervised Backdoor Defense for Text Classification"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.11803]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2022.emnlp-main.798]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.11803]</a>","Existing backdoor defense methods are only effective for limited trigger types. To defend different trigger types at once, we start from the class-irrelevant nature of the poisoning process and propose a novel weakly supervised backdoor defense framework WeDef. Recent advances in weak supervision make it possible to train a reasonably accurate text classifier using only a small number of user-provided, class-indicative seed words. Such seed words shall be considered independent of the triggers. Therefore, a weakly supervised text classifier trained by only the poisoned documents without their labels will likely have no backdoor. Inspired by this observation, in WeDef, we define the reliability of samples based on whether the predictions of the weak classifier agree with their labels in the poisoned training set. We further improve the results through a two-phase sanitization: (1) iteratively refine the weak classifier based on the reliable samples and (2) train a binary poison classifier by distinguishing the most unreliable samples from the most reliable samples. Finally, we train the sanitized model on the samples that the poison classifier predicts as benign. Extensive experiments show that WeDefis effective against popular trigger-based attacks (e.g., words, sentences, and paraphrases), outperforming existing defense methods.

","

","arXiv
DBLP
DBLP"
Training with More Confidence: Mitigating Injected and Natural Backdoors During Training,"Zhenting Wang, Hailun Ding, Juan Zhai, Shiqing Ma","arXiv
NeurIPS","2022-10-27
2022","<a href=""arXiv (2022-10-27) : Training with More Confidence: Mitigating Injected and Natural Backdoors During Training"" target=""_blank"">[http://arxiv.org/abs/2202.06382v3]</a>
<a href=""DBLP (2022) : Training with More Confidence: Mitigating Injected and Natural Backdoors During Training"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/ec0c9ca85b4ea49c7ebfb503cf55f2ae-Abstract-Conference.html]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/ec0c9ca85b4ea49c7ebfb503cf55f2ae-Abstract-Conference.html]</a>","The backdoor or Trojan attack is a severe threat to deep neural networks (DNNs). Researchers find that DNNs trained on benign data and settings can also learn backdoor behaviors, which is known as the natural backdoor. Existing works on anti-backdoor learning are based on weak observations that the backdoor and benign behaviors can differentiate during training. An adaptive attack with slow poisoning can bypass such defenses. Moreover, these methods cannot defend natural backdoors. We found the fundamental differences between backdoor-related neurons and benign neurons: backdoor-related neurons form a hyperplane as the classification surface across input domains of all affected labels. By further analyzing the training process and model architectures, we found that piece-wise linear functions cause this hyperplane surface. In this paper, we design a novel training method that forces the training to avoid generating such hyperplanes and thus remove the injected backdoors. Our extensive experiments on five datasets against five state-of-the-art attacks and also benign training show that our method can outperform existing state-of-the-art defenses. On average, the ASR (attack success rate) of the models trained with NONE is 54.83 times lower than undefended models under standard poisoning backdoor attack and 1.75 times lower under the natural backdoor attack. Our code is available at https://github.com/RU-System-Software-and-Security/NONE.
","<a href=""arXiv"" target=""_blank"">[https://github.com/RU-System-Software-and-Security/NONE]</a>
","arXiv
DBLP"
A deep hybrid learning model for detection of cyber attacks in industrial IoT devices,"Mohammad Shahin, F. Frank Chen, ... Rasoul Rashidifar",The International Journal of Advanced Manufacturing Technology,2022-10-26,"<a href=""Springer (2022-10-26) : A deep hybrid learning model for detection of cyber attacks in industrial IoT devices"" target=""_blank"">[https://link.springer.com/article/10.1007/s00170-022-10329-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00170-022-10329-6]</a>","With the rapid advancement of wireless technology, the problem of cybersecurity monitoring and detection of cyber-attacks has been receiving...",,Springer
A novel fully convolutional neural network approach for detection and classification of attacks on industrial IoT devices in smart manufacturing systems,"Mohammad Shahin, F. Frank Chen, ... Rasoul Rashidifar",The International Journal of Advanced Manufacturing Technology,2022-10-26,"<a href=""Springer (2022-10-26) : A novel fully convolutional neural network approach for detection and classification of attacks on industrial IoT devices in smart manufacturing systems"" target=""_blank"">[https://link.springer.com/article/10.1007/s00170-022-10259-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00170-022-10259-3]</a>","Recently, Internet of things (IoT) devices have been widely implemented and technologically advanced in manufacturing settings to monitor, collect,...",,Springer
Explainable artificial intelligence for cybersecurity: a literature survey,"Fabien Charmet, Harry Chandra Tanuwidjaja, ... Zonghua Zhang",Annals of Telecommunications,2022-10-26,"<a href=""Springer (2022-10-26) : Explainable artificial intelligence for cybersecurity: a literature survey"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-022-00926-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-022-00926-7]</a>","With the extensive application of deep learning (DL) algorithms in recent years, e.g., for detecting Android malware or vulnerable source code,...",,Springer
Mitigation of a poisoning attack in federated learning by using historical distance detection,"Zhaosen Shi, Xuyang Ding, ... Canran Li",Annals of Telecommunications,2022-10-26,"<a href=""Springer (2022-10-26) : Mitigation of a poisoning attack in federated learning by using historical distance detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-022-00929-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-022-00929-4]</a>","Federated learning provides a way to achieve joint model training while keeping the data of every party stored locally, and it protects the data...",,Springer
Transferable Graph Backdoor Attack,Yang S.,ACM International Conference Proceeding Series,2022-10-26,"<a href=""ScienceDirect (2022-10-26) : Transferable Graph Backdoor Attack"" target=""_blank"">[https://doi.org/10.1145/3545948.3545976]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3545948.3545976]</a>",,,ScienceDirect
BLAST: Belling the Black-Hat High-Level Synthesis Tool,M. Abderehman R. Gupta R. R. Theegala C. Karfa,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2022-10-25,"<a href=""IEEE (2022-10-25) : BLAST: Belling the Black-Hat High-Level Synthesis Tool"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9925677]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TCAD.2022.3200513]</a>","A hardware Trojan (HT) is a malicious modification of the design done by a rogue employee or a malicious foundry to leak secret information, create a backdoor for attackers, alter functionality, degrade performance and even halt the system. In Black-hat high-level synthesis (HLS) (Pilato et al., 2019), the authors have introduced a possibility of HTs insertion in the register transfer level (RTL) design by the HLS tool itself. Specifically, degradation attack (DA), battery exhaustion (BE) attack, and downgrade attack (DG) have been proposed in that work. In this study, we show how all three HTs inserted by Pilato et al. (2019) can be detected using a C-to-RTL equivalence checking framework. We have assumed that both the input C code and the Trojan-infected RTL code are available for our analysis. Specifically, our framework extracts an RTL-level finite-state machine with datapaths (RTL-FSMDs) from the HLS-generated RTL. During finite-state machine with datapath (FSMD) construction, a BE attack can be identified. Our proposed method then compares the FSMD of the input C code with the RTL-FSMD to identify the DA and the DG. The experimental results confirm the detection of HTs of the black-hat HLS tool.",,IEEE
HIDM: A Hybrid Intrusion Detection Model for Cloud Based Systems,"Lalit Kumar Vashishtha, Akhil Pratap Singh, Kakali Chatterjee",Wireless Personal Communications,2022-10-25,"<a href=""Springer (2022-10-25) : HIDM: A Hybrid Intrusion Detection Model for Cloud Based Systems"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-022-10063-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-022-10063-y]</a>","The cloud computing model is very popular among the users in different sectors like banking, healthcare, education etc due to its customized low-cost...",,Springer
Discussion on a new paradigm of endogenous security towards 6G networks,"Xinsheng Ji, Jiangxing Wu, ... Jing Yang",Frontiers of Information Technology & Electronic Engineering,2022-10-24,"<a href=""Springer (2022-10-24) : Discussion on a new paradigm of endogenous security towards 6G networks"" target=""_blank"">[https://link.springer.com/article/10.1631/FITEE.2200060]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1631/FITEE.2200060]</a>","The sixth-generation mobile communication (6G) networks will face more complex endogenous security problems, and it is urgent to propose new...",,Springer
Few-shot Backdoor Defense Using Shapley Estimation,"Jiyang Guan, Zhuozhuo Tu, Ran He, Dacheng Tao","arXiv
CVPR
arXiv","2022-10-21
2022
2021-12","<a href=""arXiv (2022-10-21) : Few-shot Backdoor Defense Using Shapley Estimation"" target=""_blank"">[http://arxiv.org/abs/2112.14889v2]</a>
<a href=""DBLP (2022) : Few-shot Backdoor Defense Using Shapley Estimation"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.01300]</a>
<a href=""DBLP (2021-12) : Few-shot Backdoor Defense Using Shapley Estimation"" target=""_blank"">[https://arxiv.org/abs/2112.14889]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.01300]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2112.14889]</a>","Deep neural networks have achieved impressive performance in a variety of tasks over the last decade, such as autonomous driving, face recognition, and medical diagnosis. However, prior works show that deep neural networks are easily manipulated into specific, attacker-decided behaviors in the inference stage by backdoor attacks which inject malicious small hidden triggers into model training, raising serious security threats. To determine the triggered neurons and protect against backdoor attacks, we exploit Shapley value and develop a new approach called Shapley Pruning (ShapPruning) that successfully mitigates backdoor attacks from models in a data-insufficient situation (1 image per class or even free of data). Considering the interaction between neurons, ShapPruning identifies the few infected neurons (under 1% of all neurons) and manages to protect the model's structure and accuracy after pruning as many infected neurons as possible. To accelerate ShapPruning, we further propose discarding threshold and $\epsilon$-greedy strategy to accelerate Shapley estimation, making it possible to repair poisoned models with only several minutes. Experiments demonstrate the effectiveness and robustness of our method against various attacks and tasks compared to existing methods.

","

","arXiv
DBLP
DBLP"
"TrojanZoo: Towards Unified, Holistic, and Practical Evaluation of Neural Backdoors","Ren Pang, Zheng Zhang, Xiangshan Gao, Zhaohan Xi, Shouling Ji, Peng Cheng, Xiapu Luo, Ting Wang","arXiv
EuroS&amp,P","2022-10-21
2022","<a href=""arXiv (2022-10-21) : TrojanZoo: Towards Unified, Holistic, and Practical Evaluation of Neural Backdoors"" target=""_blank"">[http://arxiv.org/abs/2012.09302v4]</a>
<a href=""DBLP (2022) : TrojanZoo: Towards Unified, Holistic, and Practical Evaluation of Neural Backdoors"" target=""_blank"">[https://doi.org/10.1109/EuroSP53844.2022.00048]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1109/EuroSP53844.2022.00048]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/EuroSP53844.2022.00048]</a>","Neural backdoors represent one primary threat to the security of deep learning systems. The intensive research has produced a plethora of backdoor attacks/defenses, resulting in a constant arms race. However, due to the lack of evaluation benchmarks, many critical questions remain under-explored: (i) what are the strengths and limitations of different attacks/defenses? (ii) what are the best practices to operate them? and (iii) how can the existing attacks/defenses be further improved? To bridge this gap, we design and implement TROJANZOO, the first open-source platform for evaluating neural backdoor attacks/defenses in a unified, holistic, and practical manner. Thus far, focusing on the computer vision domain, it has incorporated 8 representative attacks, 14 state-of-the-art defenses, 6 attack performance metrics, 10 defense utility metrics, as well as rich tools for in-depth analysis of the attack-defense interactions. Leveraging TROJANZOO, we conduct a systematic study on the existing attacks/defenses, unveiling their complex design spectrum: both manifest intricate trade-offs among multiple desiderata (e.g., the effectiveness, evasiveness, and transferability of attacks). We further explore improving the existing attacks/defenses, leading to a number of interesting findings: (i) one-pixel triggers often suffice, (ii) training from scratch often outperforms perturbing benign models to craft trojan models, (iii) optimizing triggers and trojan models jointly greatly improves both attack effectiveness and evasiveness, (iv) individual defenses can often be evaded by adaptive attacks, and (v) exploiting model interpretability significantly improves defense robustness. We envision that TROJANZOO will serve as a valuable platform to facilitate future research on neural backdoors.
","
","arXiv
DBLP"
Increasing communication security for Bluetooth Medical Devices in eHealth systems,C. Contasel D. -C. Tranca A. -V. Palacean D. Rosner,2022 21st RoEduNet Conference: Networking in Education and Research (RoEduNet),2022-10-21,"<a href=""IEEE (2022-10-21) : Increasing communication security for Bluetooth Medical Devices in eHealth systems"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9921104]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/RoEduNet57163.2022.9921104]</a>","The market of Medical Devices has an annual growth rate perspective of 5.4% in the 2022 – 2028 period. The growth rate is mainly supported by the increased usage of devices for chronic diseases prevention and for remote patient monitoring. The growth rate is also supported by the population’s appetite for mobile devices. The continuously growing number of mobile devices has led to an increase in the number of cyber-attacks against them. In this paper we present Personal Medical Hub, an optimized security solution for Bluetooth enabled medical devices, that enhances data privacy and is aimed to offer protection against man-in-the-middle attacks and other security breaches and backdoors.",,IEEE
Apple of Sodom: Hidden Backdoors in Superior Sentence Embeddings via Contrastive Learning,"Xiaoyi Chen, Baisong Xin, Shengfang Zhai, Shiqing Ma, Qingni Shen, Zhonghai Wu","arXiv
arXiv","2022-10-20
2022-10","<a href=""arXiv (2022-10-20) : Apple of Sodom: Hidden Backdoors in Superior Sentence Embeddings via Contrastive Learning"" target=""_blank"">[http://arxiv.org/abs/2210.11082v1]</a>
<a href=""DBLP (2022-10) : Apple of Sodom: Hidden Backdoors in Superior Sentence Embeddings via Contrastive Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.11082]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.11082]</a>","This paper finds that contrastive learning can produce superior sentence embeddings for pre-trained models but is also vulnerable to backdoor attacks. We present the first backdoor attack framework, BadCSE, for state-of-the-art sentence embeddings under supervised and unsupervised learning settings. The attack manipulates the construction of positive and negative pairs so that the backdoored samples have a similar embedding with the target sample (targeted attack) or the negative embedding of its clean version (non-targeted attack). By injecting the backdoor in sentence embeddings, BadCSE is resistant against downstream fine-tuning. We evaluate BadCSE on both STS tasks and other downstream tasks. The supervised non-targeted attack obtains a performance degradation of 194.86%, and the targeted attack maps the backdoored samples to the target embedding with a 97.70% success rate while maintaining the model utility.
","
","arXiv
DBLP"
Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks,"Yangyi Chen, Fanchao Qi, Hongcheng Gao, Zhiyuan Liu, Maosong Sun","arXiv
EMNLP
arXiv","2022-10-19
2022
2021-10","<a href=""arXiv (2022-10-19) : Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks"" target=""_blank"">[http://arxiv.org/abs/2110.08247v2]</a>
<a href=""DBLP (2022) : Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks"" target=""_blank"">[https://doi.org/10.18653/v1/2022.emnlp-main.770]</a>
<a href=""DBLP (2021-10) : Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks"" target=""_blank"">[https://arxiv.org/abs/2110.08247]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2022.emnlp-main.770]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2110.08247]</a>","Backdoor attacks are a kind of emergent security threat in deep learning. After being injected with a backdoor, a deep neural model will behave normally on standard inputs but give adversary-specified predictions once the input contains specific backdoor triggers. In this paper, we find two simple tricks that can make existing textual backdoor attacks much more harmful. The first trick is to add an extra training task to distinguish poisoned and clean data during the training of the victim model, and the second one is to use all the clean training data rather than remove the original clean data corresponding to the poisoned data. These two tricks are universally applicable to different attack models. We conduct experiments in three tough situations including clean data fine-tuning, low-poisoning-rate, and label-consistent attacks. Experimental results show that the two tricks can significantly improve attack performance. This paper exhibits the great potential harmfulness of backdoor attacks. All the code and data can be obtained at \url{https://github.com/thunlp/StyleAttack}.

","<a href=""arXiv"" target=""_blank"">[https://github.com/thunlp/StyleAttack}]</a>

","arXiv
DBLP
DBLP"
Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models,"Zhiyuan Zhang, Lingjuan Lyu, Xingjun Ma, Chenguang Wang, Xu Sun","arXiv
EMNLP
arXiv","2022-10-18
2022
2022-10","<a href=""arXiv (2022-10-18) : Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models"" target=""_blank"">[http://arxiv.org/abs/2210.09545v1]</a>
<a href=""DBLP (2022) : Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models"" target=""_blank"">[https://doi.org/10.18653/v1/2022.findings-emnlp.26]</a>
<a href=""DBLP (2022-10) : Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.09545]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2022.findings-emnlp.26]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.09545]</a>","Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks. In Natural Language Processing (NLP), DNNs are often backdoored during the fine-tuning process of a large-scale Pre-trained Language Model (PLM) with poisoned samples. Although the clean weights of PLMs are readily available, existing methods have ignored this information in defending NLP models against backdoor attacks. In this work, we take the first step to exploit the pre-trained (unfine-tuned) weights to mitigate backdoors in fine-tuned language models. Specifically, we leverage the clean pre-trained weights via two complementary techniques: (1) a two-step Fine-mixing technique, which first mixes the backdoored weights (fine-tuned on poisoned data) with the pre-trained weights, then fine-tunes the mixed weights on a small subset of clean data, (2) an Embedding Purification (E-PUR) technique, which mitigates potential backdoors existing in the word embeddings. We compare Fine-mixing with typical backdoor mitigation methods on three single-sentence sentiment classification tasks and two sentence-pair classification tasks and show that it outperforms the baselines by a considerable margin in all scenarios. We also show that our E-PUR method can benefit existing mitigation methods. Our work establishes a simple but strong baseline defense for secure fine-tuned NLP models against backdoor attacks.

","

","arXiv
DBLP
DBLP"
A deep learner model for multi-language webshell detection,"Abdelhakim Hannousse, Mohamed Cherif Nait-Hamoud, Salima Yahiouche",International Journal of Information Security,2022-10-18,"<a href=""Springer (2022-10-18) : A deep learner model for multi-language webshell detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00615-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00615-5]</a>",Webshell attacks are becoming more and more prevalent every year. Webshells are malicious scripts injected into web servers in the aim to confiscate...,,Springer
Neural Network Fragile watermarking With No Model Performance Degradation,Z. Yin H. Yin X. Zhang,2022 IEEE International Conference on Image Processing (ICIP),2022-10-18,"<a href=""IEEE (2022-10-18) : Neural Network Fragile watermarking With No Model Performance Degradation"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9897413]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICIP46576.2022.9897413]</a>","Deep neural networks are vulnerable to malicious fine-tuning attacks such as data poisoning and backdoor attacks. Therefore, in recent research, it is proposed how to detect malicious fine-tuning of neural network models. However, it usually negatively affects the performance of the protected model. Thus, we propose a novel neural network fragile watermarking with no model performance degradation. In the process of watermarking, we train a generative model with the specific loss function and secret key to generate triggers that are sensitive to the fine-tuning of the target classifier. In the process of verifying, we adopt the watermarked classifier to get labels of each fragile trigger. Then, malicious fine-tuning can be detected by comparing secret keys and labels. Experiments on classic datasets and classifiers show that the proposed method can effectively detect model malicious fine-tuning with no model performance degradation.",,IEEE
Thinking Two Moves Ahead: Anticipating Other Users Improves Backdoor Attacks in Federated Learning,"Yuxin Wen, Jonas Geiping, Liam Fowl, Hossein Souri, Rama Chellappa, Micah Goldblum, Tom Goldstein","arXiv
arXiv","2022-10-17
2022-10","<a href=""arXiv (2022-10-17) : Thinking Two Moves Ahead: Anticipating Other Users Improves Backdoor Attacks in Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2210.09305v1]</a>
<a href=""DBLP (2022-10) : Thinking Two Moves Ahead: Anticipating Other Users Improves Backdoor Attacks in Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.09305]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.09305]</a>","Federated learning is particularly susceptible to model poisoning and backdoor attacks because individual users have direct control over the training data and model updates. At the same time, the attack power of an individual user is limited because their updates are quickly drowned out by those of many other users. Existing attacks do not account for future behaviors of other users, and thus require many sequential updates and their effects are quickly erased. We propose an attack that anticipates and accounts for the entire federated learning pipeline, including behaviors of other clients, and ensures that backdoors are effective quickly and persist even after multiple rounds of community updates. We show that this new attack is effective in realistic scenarios where the attacker only contributes to a small fraction of randomly sampled rounds and demonstrate this attack on image classification, next-word prediction, and sentiment analysis.
","
","arXiv
DBLP"
Marksman Backdoor: Backdoor Attacks with Arbitrary Target Class,"Khoa D. Doan, Yingjie Lao, Ping Li","arXiv
NeurIPS
arXiv","2022-10-17
2022
2022-10","<a href=""arXiv (2022-10-17) : Marksman Backdoor: Backdoor Attacks with Arbitrary Target Class"" target=""_blank"">[http://arxiv.org/abs/2210.09194v1]</a>
<a href=""DBLP (2022) : Marksman Backdoor: Backdoor Attacks with Arbitrary Target Class"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/fa0126bb7ebad258bf4ffdbbac2dd787-Abstract-Conference.html]</a>
<a href=""DBLP (2022-10) : Marksman Backdoor: Backdoor Attacks with Arbitrary Target Class"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.09194]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/fa0126bb7ebad258bf4ffdbbac2dd787-Abstract-Conference.html]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.09194]</a>","In recent years, machine learning models have been shown to be vulnerable to backdoor attacks. Under such attacks, an adversary embeds a stealthy backdoor into the trained model such that the compromised models will behave normally on clean inputs but will misclassify according to the adversary's control on maliciously constructed input with a trigger. While these existing attacks are very effective, the adversary's capability is limited: given an input, these attacks can only cause the model to misclassify toward a single pre-defined or target class. In contrast, this paper exploits a novel backdoor attack with a much more powerful payload, denoted as Marksman, where the adversary can arbitrarily choose which target class the model will misclassify given any input during inference. To achieve this goal, we propose to represent the trigger function as a class-conditional generative model and to inject the backdoor in a constrained optimization framework, where the trigger function learns to generate an optimal trigger pattern to attack any target class at will while simultaneously embedding this generative backdoor into the trained model. Given the learned trigger-generation function, during inference, the adversary can specify an arbitrary backdoor attack target class, and an appropriate trigger causing the model to classify toward this target class is created accordingly. We show empirically that the proposed framework achieves high attack performance while preserving the clean-data performance in several benchmark datasets, including MNIST, CIFAR10, GTSRB, and TinyImageNet. The proposed Marksman backdoor attack can also easily bypass existing backdoor defenses that were originally designed against backdoor attacks with a single target class. Our work takes another significant step toward understanding the extensive risks of backdoor attacks in practice.

","

","arXiv
DBLP
DBLP"
Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks,"Sishuo Chen, Wenkai Yang, Zhiyuan Zhang, Xiaohan Bi, Xu Sun","arXiv
EMNLP
arXiv","2022-10-14
2022
2022-10","<a href=""arXiv (2022-10-14) : Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2210.07907v1]</a>
<a href=""DBLP (2022) : Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks"" target=""_blank"">[https://doi.org/10.18653/v1/2022.findings-emnlp.47]</a>
<a href=""DBLP (2022-10) : Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.07907]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2022.findings-emnlp.47]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.07907]</a>","Natural language processing (NLP) models are known to be vulnerable to backdoor attacks, which poses a newly arisen threat to NLP models. Prior online backdoor defense methods for NLP models only focus on the anomalies at either the input or output level, still suffering from fragility to adaptive attacks and high computational cost. In this work, we take the first step to investigate the unconcealment of textual poisoned samples at the intermediate-feature level and propose a feature-based efficient online defense method. Through extensive experiments on existing attacking methods, we find that the poisoned samples are far away from clean samples in the intermediate feature space of a poisoned NLP model. Motivated by this observation, we devise a distance-based anomaly score (DAN) to distinguish poisoned samples from clean samples at the feature level. Experiments on sentiment analysis and offense detection tasks demonstrate the superiority of DAN, as it substantially surpasses existing online defense methods in terms of defending performance and enjoys lower inference costs. Moreover, we show that DAN is also resistant to adaptive attacks based on feature-level regularization. Our code is available at https://github.com/lancopku/DAN.

","<a href=""arXiv"" target=""_blank"">[https://github.com/lancopku/DAN]</a>

","arXiv
DBLP
DBLP"
A Test Environment for Wireless Hacking in Domestic IoT Scenarios,"Antonio Muñoz, Carmen Fernández-Gago, Roberto López-Villa",Mobile Networks and Applications,2022-10-14,"<a href=""Springer (2022-10-14) : A Test Environment for Wireless Hacking in Domestic IoT Scenarios"" target=""_blank"">[https://link.springer.com/article/10.1007/s11036-022-02046-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11036-022-02046-x]</a>",Security is gaining importance in the daily life of every citizen. The advent of Internet of Things devices in our lives is changing our conception...,,Springer
Audio-domain position-independent backdoor attack via unnoticeable triggers,Shi C.,"Proceedings of the Annual International Conference on Mobile Computing and Networking, MOBICOM",2022-10-14,"<a href=""ScienceDirect (2022-10-14) : Audio-domain position-independent backdoor attack via unnoticeable triggers"" target=""_blank"">[https://doi.org/10.1145/3495243.3560531]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3495243.3560531]</a>",,,ScienceDirect
Dim-Krum: Backdoor-Resistant Federated Learning for NLP with Dimension-wise Krum-Based Aggregation,"Zhiyuan Zhang, Qi Su, Xu Sun","arXiv
EMNLP
arXiv","2022-10-13
2022
2022-10","<a href=""arXiv (2022-10-13) : Dim-Krum: Backdoor-Resistant Federated Learning for NLP with Dimension-wise Krum-Based Aggregation"" target=""_blank"">[http://arxiv.org/abs/2210.06894v1]</a>
<a href=""DBLP (2022) : Dim-Krum: Backdoor-Resistant Federated Learning for NLP with Dimension-wise Krum-Based Aggregation"" target=""_blank"">[https://doi.org/10.18653/v1/2022.findings-emnlp.25]</a>
<a href=""DBLP (2022-10) : Dim-Krum: Backdoor-Resistant Federated Learning for NLP with Dimension-wise Krum-Based Aggregation"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.06894]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2022.findings-emnlp.25]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.06894]</a>","Despite the potential of federated learning, it is known to be vulnerable to backdoor attacks. Many robust federated aggregation methods are proposed to reduce the potential backdoor risk. However, they are mainly validated in the CV field. In this paper, we find that NLP backdoors are hard to defend against than CV, and we provide a theoretical analysis that the malicious update detection error probabilities are determined by the relative backdoor strengths. NLP attacks tend to have small relative backdoor strengths, which may result in the failure of robust federated aggregation methods for NLP attacks. Inspired by the theoretical results, we can choose some dimensions with higher backdoor strengths to settle this issue. We propose a novel federated aggregation algorithm, Dim-Krum, for NLP tasks, and experimental results validate its effectiveness.

","

","arXiv
DBLP
DBLP"
Few-shot Backdoor Attacks via Neural Tangent Kernels,"Jonathan Hayase, Sewoong Oh","arXiv
ICLR
arXiv","2022-10-12
2023
2022-10","<a href=""arXiv (2022-10-12) : Few-shot Backdoor Attacks via Neural Tangent Kernels"" target=""_blank"">[http://arxiv.org/abs/2210.05929v1]</a>
<a href=""DBLP (2023) : Few-shot Backdoor Attacks via Neural Tangent Kernels"" target=""_blank"">[https://openreview.net/pdf?id=a70lGJ-rwy]</a>
<a href=""DBLP (2022-10) : Few-shot Backdoor Attacks via Neural Tangent Kernels"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.05929]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/pdf?id=a70lGJ-rwy]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.05929]</a>","In a backdoor attack, an attacker injects corrupted examples into the training set. The goal of the attacker is to cause the final trained model to predict the attacker's desired target label when a predefined trigger is added to test inputs. Central to these attacks is the trade-off between the success rate of the attack and the number of corrupted training examples injected. We pose this attack as a novel bilevel optimization problem: construct strong poison examples that maximize the attack success rate of the trained model. We use neural tangent kernels to approximate the training dynamics of the model being attacked and automatically learn strong poison examples. We experiment on subclasses of CIFAR-10 and ImageNet with WideResNet-34 and ConvNeXt architectures on periodic and patch trigger attacks and show that NTBA-designed poisoned examples achieve, for example, an attack success rate of 90% with ten times smaller number of poison examples injected compared to the baseline. We provided an interpretation of the NTBA-designed attacks using the analysis of kernel linear regression. We further demonstrate a vulnerability in overparametrized deep neural networks, which is revealed by the shape of the neural tangent kernel.

","

","arXiv
DBLP
DBLP"
Understanding Impacts of Task Similarity on Backdoor Attack and Detection,"Di Tang, Rui Zhu, XiaoFeng Wang, Haixu Tang, Yi Chen","arXiv
arXiv","2022-10-12
2022-10","<a href=""arXiv (2022-10-12) : Understanding Impacts of Task Similarity on Backdoor Attack and Detection"" target=""_blank"">[http://arxiv.org/abs/2210.06509v1]</a>
<a href=""DBLP (2022-10) : Understanding Impacts of Task Similarity on Backdoor Attack and Detection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.06509]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.06509]</a>","With extensive studies on backdoor attack and detection, still fundamental questions are left unanswered regarding the limits in the adversary's capability to attack and the defender's capability to detect. We believe that answers to these questions can be found through an in-depth understanding of the relations between the primary task that a benign model is supposed to accomplish and the backdoor task that a backdoored model actually performs. For this purpose, we leverage similarity metrics in multi-task learning to formally define the backdoor distance (similarity) between the primary task and the backdoor task, and analyze existing stealthy backdoor attacks, revealing that most of them fail to effectively reduce the backdoor distance and even for those that do, still much room is left to further improve their stealthiness. So we further design a new method, called TSA attack, to automatically generate a backdoor model under a given distance constraint, and demonstrate that our new attack indeed outperforms existing attacks, making a step closer to understanding the attacker's limits. Most importantly, we provide both theoretic results and experimental evidence on various datasets for the positive correlation between the backdoor distance and backdoor detectability, demonstrating that indeed our task similarity analysis help us better understand backdoor risks and has the potential to identify more effective mitigations.
","
","arXiv
DBLP"
SDN-ESRC: A Secure and Resilient Control Plane for Software-Defined Networks,Q. Ren Z. Guo J. Wu T. Hu L. Jie Y. Hu L. He,IEEE Transactions on Network and Service Management,2022-10-12,"<a href=""IEEE (2022-10-12) : SDN-ESRC: A Secure and Resilient Control Plane for Software-Defined Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9744116]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TNSM.2022.3163198]</a>","In this paper, we propose a resilient control plane based on endogenous security for Software-Defined Networking (SDN) named SDN-ESRC to prevent vulnerability backdoor attacks. SDN-ESRC uses a set of heterogeneous controllers (e.g., RYU, OpenDayLight, ONOS) to compose the control plane and dynamically and adaptively selects several heterogeneous controller instances from the controller set to detect and correct the malicious control messages. The design of SDN-ESRC faces two challenges: (1) increasing network update delay due to multi-controller comparison and (2) maintaining high controllable security. To address the first challenge, SDN-ESRC adopts the master modification mode to reduce the network update delay and identify malicious control messages. To address the second challenge, SDN-ESRC introduces the comparison modification mode to ensure high availability in real time. We propose an evaluation model for SDN-ESRC and theoretically analyze the SDN-ESRC’s endogenous security performance under three typical backdoor attack scenarios. We implement SDN-ESRC in a prototype system and conduct simulations and experiments. The results show that SDN-ESRC can improve the backdoor damage attack security up to 98.3%, the backdoor random attack security up to 99.99%, and the backdoor coordinated attack security up to 82% at the cost of increasing network update delay less than 8.3%.",,IEEE
SmartFast: an accurate and robust formal analysis tool for Ethereum smart contracts,"Zhaoxuan Li, Siqi Lu, ... Sheng Gao",Empirical Software Engineering,2022-10-12,"<a href=""Springer (2022-10-12) : SmartFast: an accurate and robust formal analysis tool for Ethereum smart contracts"" target=""_blank"">[https://link.springer.com/article/10.1007/s10664-022-10218-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10664-022-10218-2]</a>","Recently, although state-of-the-art (SOTA) tools were designed and developed to analyze the vulnerabilities of smart contracts on Ethereum, security...",,Springer
Detecting Backdoors in Deep Text Classifiers,"You Guo, Jun Wang, Trevor Cohn","arXiv
arXiv","2022-10-11
2022-10","<a href=""arXiv (2022-10-11) : Detecting Backdoors in Deep Text Classifiers"" target=""_blank"">[http://arxiv.org/abs/2210.11264v1]</a>
<a href=""DBLP (2022-10) : Detecting Backdoors in Deep Text Classifiers"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.11264]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.11264]</a>","Deep neural networks are vulnerable to adversarial attacks, such as backdoor attacks in which a malicious adversary compromises a model during training such that specific behaviour can be triggered at test time by attaching a specific word or phrase to an input. This paper considers the problem of diagnosing whether a model has been compromised and if so, identifying the backdoor trigger. We present the first robust defence mechanism that generalizes to several backdoor attacks against text classification models, without prior knowledge of the attack type, nor does our method require access to any (potentially compromised) training resources. Our experiments show that our technique is highly accurate at defending against state-of-the-art backdoor attacks, including data poisoning and weight poisoning, across a range of text classification tasks and model architectures. Our code will be made publicly available upon acceptance.
","
","arXiv
DBLP"
A Secure Two-Factor Authentication Scheme From Password-Protected Hardware Tokens,S. Li C. Xu Y. Zhang J. Zhou,IEEE Transactions on Information Forensics and Security,2022-10-11,"<a href=""IEEE (2022-10-11) : A Secure Two-Factor Authentication Scheme From Password-Protected Hardware Tokens"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9903479]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TIFS.2022.3209886]</a>","We investigate existing “password+hardware token”-based authentication schemes deployed in real-world applications and observe that they are vulnerable to critical threats. Specifically, a compromised manufacturer may issue a backdoored hardware token to a user and later recover the user’s secret, which is well known as backdoor attacks. Additionally, an authentication credential in these schemes consists of two parts: the one is derived from the password, the other one is derived from the hardware token. However, since the two parts are independent of each other, if an adversary can physically access the hardware token of a victim, he is able to break security of these schemes by performing dictionary-guessing attacks (DGA), which is called mislaying-then-DGA. In this paper, we design a non-interactively re-randomizable reverse firewall signature mechanism for securing hardware tokens, such that the user’s secret is well protected even if a backdoor is embedded. We also utilize a servers-aided password-based encryption mechanism to harden hardware tokens, so as to “seamlessly” integrate the two factors into one credential. Based on the above mechanisms, we develop a secure two-factor authentication scheme, dubbed ATTACH. We evaluate ATTACH in terms of security and efficiency to demonstrate it achieves a strong security guarantee with high efficiency.",,IEEE
Malware Signature and Behavior Performance Evaluation utilizing Packers,D. Sharma H. K. Verma,2022 2nd Asian Conference on Innovation in Technology (ASIANCON),2022-10-11,"<a href=""IEEE (2022-10-11) : Malware Signature and Behavior Performance Evaluation utilizing Packers"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9909111]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ASIANCON55314.2022.9909111]</a>","Malware detection models are being built primarily focusing on signature or behavior type detection. In this paper, anti-forensic techniques are used to hide the malware from malware scanners using various approaches and making different changes to the source code of malware to prevent its detection. In this paper I have worked on two models with interchanging payloads and code segments for analysis to check the performance in each case. In this experiment many samples of malware from the recent attacks covering different malware families and intended attack areas have been used to check detection rates as well as new payloads have been created and merged with the existing malware to understand the behavior and combination of the payloads for multi system attacks and calculate the detection rates making the use of VirusTotal to check the detection. The use of different obfuscation techniques which include encoding the payload, code splitting, adding encryption, backdooring the file, Code injection Payload and finally making the use of different steganographic methods to carry the payload to maintain signature evasion have been used as a technique of payload delivery. The technique of manual unpacking has been used in this paper to unpack the malware and deliver the final attack and a framework of automated deployment methods have been laid for further work.",,IEEE
Purifier: Plug-and-play Backdoor Mitigation for Pre-trained Models Via Anomaly Activation Suppression,Zhang X.,MM 2022 - Proceedings of the 30th ACM International Conference on Multimedia,2022-10-10,"<a href=""ScienceDirect (2022-10-10) : Purifier: Plug-and-play Backdoor Mitigation for Pre-trained Models Via Anomaly Activation Suppression"" target=""_blank"">[https://doi.org/10.1145/3503161.3548065]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3503161.3548065]</a>",,,ScienceDirect
Spinning Sequence-to-Sequence Models with Meta-Backdoors,"Eugene Bagdasaryan, Vitaly Shmatikov",arXiv,2022-10-10,"<a href=""arXiv (2022-10-10) : Spinning Sequence-to-Sequence Models with Meta-Backdoors"" target=""_blank"">[http://arxiv.org/abs/2107.10443v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","We investigate a new threat to neural sequence-to-sequence (seq2seq) models: training-time attacks that cause models to ""spin"" their output and support a certain sentiment when the input contains adversary-chosen trigger words. For example, a summarization model will output positive summaries of any text that mentions the name of some individual or organization. We introduce the concept of a ""meta-backdoor"" to explain model-spinning attacks. These attacks produce models whose output is valid and preserves context, yet also satisfies a meta-task chosen by the adversary (e.g., positive sentiment). Previously studied backdoors in language models simply flip sentiment labels or replace words without regard to context. Their outputs are incorrect on inputs with the trigger. Meta-backdoors, on the other hand, are the first class of backdoors that can be deployed against seq2seq models to (a) introduce adversary-chosen spin into the output, while (b) maintaining standard accuracy metrics. To demonstrate feasibility of model spinning, we develop a new backdooring technique. It stacks the adversarial meta-task (e.g., sentiment analysis) onto a seq2seq model, backpropagates the desired meta-task output (e.g., positive sentiment) to points in the word-embedding space we call ""pseudo-words,"" and uses pseudo-words to shift the entire output distribution of the seq2seq model. Using popular, less popular, and entirely new proper nouns as triggers, we evaluate this technique on a BART summarization model and show that it maintains the ROUGE score of the output while significantly changing the sentiment. We explain why model spinning can be a dangerous technique in AI-powered disinformation and discuss how to mitigate these attacks.",,arXiv
Cyber Threat Intelligence Sharing Scheme Based on Federated Learning for Network Intrusion Detection,"Mohanad Sarhan, Siamak Layeghy, ... Marius Portmann",Journal of Network and Systems Management,2022-10-07,"<a href=""Springer (2022-10-07) : Cyber Threat Intelligence Sharing Scheme Based on Federated Learning for Network Intrusion Detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s10922-022-09691-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10922-022-09691-3]</a>",The uses of machine learning (ML) technologies in the detection of network attacks have been proven to be effective when designed and evaluated using...,,Springer
Dealing with the unevenness: deeper insights in graph-based attack and defense,"Haoxi Zhan, Xiaobing Pei",Machine Learning,2022-10-07,"<a href=""Springer (2022-10-07) : Dealing with the unevenness: deeper insights in graph-based attack and defense"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-022-06234-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-022-06234-4]</a>",Graph Neural Networks (GNNs) have achieved state-of-the-art performance on various graph-related learning tasks. Due to the importance of safety in...,,Springer
Adversarial attacks on graph-level embedding methods: a case study,"Maurizio Giordano, Lucia Maddalena, ... Mario Rosario Guarracino",Annals of Mathematics and Artificial Intelligence,2022-10-06,"<a href=""Springer (2022-10-06) : Adversarial attacks on graph-level embedding methods: a case study"" target=""_blank"">[https://link.springer.com/article/10.1007/s10472-022-09811-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10472-022-09811-4]</a>","As the number of graph-level embedding techniques increases at an unprecedented speed, questions arise about their behavior and performance when...",,Springer
Agent manipulator: Stealthy strategy attacks on deep reinforcement learning,"Jinyin Chen, Xueke Wang, ... Liang Bao",Applied Intelligence,2022-10-03,"<a href=""Springer (2022-10-03) : Agent manipulator: Stealthy strategy attacks on deep reinforcement learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-022-03882-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-022-03882-w]</a>",Deep reinforcement learning (DRL) is a primary machine learning approach for solving sequential decision problems. To exploit the potential...,,Springer
Enhanced Elman spike neural network based fractional order discrete Tchebyshev encryption fostered big data analytical method for enhancing cloud data security,"V. Balamurugan, R. Karthikeyan, ... Robin Cyriac",Wireless Networks,2022-10-03,"<a href=""Springer (2022-10-03) : Enhanced Elman spike neural network based fractional order discrete Tchebyshev encryption fostered big data analytical method for enhancing cloud data security"" target=""_blank"">[https://link.springer.com/article/10.1007/s11276-022-03142-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11276-022-03142-2]</a>",Cloud computing refers to a set of hardware and software that are connected to provide various computing services. Cloud consists of services to...,,Springer
BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label,"Shengshan Hu, Ziqi Zhou, Yechao Zhang, Leo Yu Zhang, Yifeng Zheng, Yuanyuan He, Hai Jin","MM '22: Proceedings of the 30th ACM International Conference on Multimedia
arXiv
ACM Multimedia
arXiv","2022-10
2022-07-13
2022
2022-07","<a href=""ACM (2022-10) : BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3503161.3548272]</a>
<a href=""arXiv (2022-07-13) : BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label"" target=""_blank"">[http://arxiv.org/abs/2207.00278v3]</a>
<a href=""DBLP (2022) : BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label"" target=""_blank"">[https://doi.org/10.1145/3503161.3548272]</a>
<a href=""DBLP (2022-07) : BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label"" target=""_blank"">[https://doi.org/10.48550/arXiv.2207.00278]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3503161.3548272]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3503161.3548272]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2207.00278]</a>","Due to its powerful feature learning capability and high efficiency, deep hashing has achieved great success in large-scale image retrieval. Meanwhile, extensive works have demonstrated that deep neural networks (DNNs) are susceptible to adversarial ...
Due to its powerful feature learning capability and high efficiency, deep hashing has achieved great success in large-scale image retrieval. Meanwhile, extensive works have demonstrated that deep neural networks (DNNs) are susceptible to adversarial examples, and exploring adversarial attack against deep hashing has attracted many research efforts. Nevertheless, backdoor attack, another famous threat to DNNs, has not been studied for deep hashing yet. Although various backdoor attacks have been proposed in the field of image classification, existing approaches failed to realize a truly imperceptive backdoor attack that enjoys invisible triggers and clean label setting simultaneously, and they also cannot meet the intrinsic demand of image retrieval backdoor. In this paper, we propose BadHash, the first generative-based imperceptible backdoor attack against deep hashing, which can effectively generate invisible and input-specific poisoned images with clean label. Specifically, we first propose a new conditional generative adversarial network (cGAN) pipeline to effectively generate poisoned samples. For any given benign image, it seeks to generate a natural-looking poisoned counterpart with a unique invisible trigger. In order to improve the attack effectiveness, we introduce a label-based contrastive learning network LabCLN to exploit the semantic characteristics of different labels, which are subsequently used for confusing and misleading the target model to learn the embedded trigger. We finally explore the mechanism of backdoor attacks on image retrieval in the hash space. Extensive experiments on multiple benchmark datasets verify that BadHash can generate imperceptible poisoned samples with strong attack ability and transferability over state-of-the-art deep hashing schemes.

","


","ACM
arXiv
DBLP
DBLP"
Physical Backdoor Attacks to Lane Detection Systems in Autonomous Driving,"Xingshuo Han, Guowen Xu, Yuan Zhou, Xuehuan Yang, Jiwei Li, Tianwei Zhang","MM '22: Proceedings of the 30th ACM International Conference on Multimedia
arXiv
ACM Multimedia","2022-10
2022-07-13
2022","<a href=""ACM (2022-10) : Physical Backdoor Attacks to Lane Detection Systems in Autonomous Driving"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3503161.3548171]</a>
<a href=""arXiv (2022-07-13) : Physical Backdoor Attacks to Lane Detection Systems in Autonomous Driving"" target=""_blank"">[http://arxiv.org/abs/2203.00858v2]</a>
<a href=""DBLP (2022) : Physical Backdoor Attacks to Lane Detection Systems in Autonomous Driving"" target=""_blank"">[https://doi.org/10.1145/3503161.3548171]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3503161.3548171]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3503161.3548171]</a>","Modern autonomous vehicles adopt state-of-the-art DNN models to interpret the sensor data and perceive the environment. However, DNN models are vulnerable to different types of adversarial attacks, which pose significant risks to the security and safety ...
Modern autonomous vehicles adopt state-of-the-art DNN models to interpret the sensor data and perceive the environment. However, DNN models are vulnerable to different types of adversarial attacks, which pose significant risks to the security and safety of the vehicles and passengers. One prominent threat is the backdoor attack, where the adversary can compromise the DNN model by poisoning the training samples. Although lots of effort has been devoted to the investigation of the backdoor attack to conventional computer vision tasks, its practicality and applicability to the autonomous driving scenario is rarely explored, especially in the physical world. In this paper, we target the lane detection system, which is an indispensable module for many autonomous driving tasks, e.g., navigation, lane switching. We design and realize the first physical backdoor attacks to such system. Our attacks are comprehensively effective against different types of lane detection algorithms. Specifically, we introduce two attack methodologies (poison-annotation and clean-annotation) to generate poisoned samples. With those samples, the trained lane detection model will be infected with the backdoor, and can be activated by common objects (e.g., traffic cones) to make wrong detections, leading the vehicle to drive off the road or onto the opposite lane. Extensive evaluations on public datasets and physical autonomous vehicles demonstrate that our backdoor attacks are effective, stealthy and robust against various defense solutions. Our codes and experimental videos can be found in https://sites.google.com/view/lane-detection-attack/lda.
","

","ACM
arXiv
DBLP"
Backdoor Attacks on Crowd Counting,"Yuhua Sun, Tailai Zhang, Xingjun Ma, Pan Zhou, Jian Lou, Zichuan Xu, Xing Di, Yu Cheng, Lichao Sun","MM '22: Proceedings of the 30th ACM International Conference on Multimedia
arXiv
ACM Multimedia
arXiv","2022-10
2022-07-12
2022
2022-07","<a href=""ACM (2022-10) : Backdoor Attacks on Crowd Counting"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3503161.3548296]</a>
<a href=""arXiv (2022-07-12) : Backdoor Attacks on Crowd Counting"" target=""_blank"">[http://arxiv.org/abs/2207.05641v1]</a>
<a href=""DBLP (2022) : Backdoor Attacks on Crowd Counting"" target=""_blank"">[https://doi.org/10.1145/3503161.3548296]</a>
<a href=""DBLP (2022-07) : Backdoor Attacks on Crowd Counting"" target=""_blank"">[https://doi.org/10.48550/arXiv.2207.05641]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3503161.3548296]</a>
<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1145/3503161.3548296]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3503161.3548296]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2207.05641]</a>","Crowd counting is a regression task that estimates the number of people in a scene image, which plays a vital role in a range of safety-critical applications, such as video surveillance, traffic monitoring and flow control. In this paper, we investigate ...
Crowd counting is a regression task that estimates the number of people in a scene image, which plays a vital role in a range of safety-critical applications, such as video surveillance, traffic monitoring and flow control. In this paper, we investigate the vulnerability of deep learning based crowd counting models to backdoor attacks, a major security threat to deep learning. A backdoor attack implants a backdoor trigger into a target model via data poisoning so as to control the model's predictions at test time. Different from image classification models on which most of existing backdoor attacks have been developed and tested, crowd counting models are regression models that output multi-dimensional density maps, thus requiring different techniques to manipulate. In this paper, we propose two novel Density Manipulation Backdoor Attacks (DMBA$^{-}$ and DMBA$^{+}$) to attack the model to produce arbitrarily large or small density estimations. Experimental results demonstrate the effectiveness of our DMBA attacks on five classic crowd counting models and four types of datasets. We also provide an in-depth analysis of the unique challenges of backdooring crowd counting models and reveal two key elements of effective attacks: 1) full and dense triggers and 2) manipulation of the ground truth counts or density maps. Our work could help evaluate the vulnerability of crowd counting models to potential backdoor attacks.

","


","ACM
arXiv
DBLP
DBLP"
Close the Gate: Detecting Backdoored Models in Federated Learning based on Client-Side Deep Layer Output Analysis,"Phillip Rieger, Torsten Krauß, Markus Miettinen, Alexandra Dmitrienko, Ahmad-Reza Sadeghi",arXiv,2022-10,"<a href=""DBLP (2022-10) : Close the Gate: Detecting Backdoored Models in Federated Learning based on Client-Side Deep Layer Output Analysis"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.07714]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.07714]</a>",,,DBLP
Invariant Aggregator for Defending Federated Backdoor Attacks,"Xiaoyang Wang, Dimitrios Dimitriadis, Sanmi Koyejo, Shruti Tople",arXiv,2022-10,"<a href=""DBLP (2022-10) : Invariant Aggregator for Defending Federated Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.01834]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.01834]</a>",,,DBLP
Mind Your Data! Hiding Backdoors in Offline Reinforcement Learning Datasets,"Chen Gong, Zhou Yang, Yunpeng Bai, Junda He, Jieke Shi, Arunesh Sinha, Bowen Xu, Xinwen Hou, Guoliang Fan, David Lo",arXiv,2022-10,"<a href=""DBLP (2022-10) : Mind Your Data! Hiding Backdoors in Offline Reinforcement Learning Datasets"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.04688]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.04688]</a>",,,DBLP
Training set cleansing of backdoor poisoning by self-supervised representation learning,"Hang Wang, Sahar Karami, Ousmane Dia, H. Ritter, Ehsan Emamjomeh-Zadeh, Jiahui Chen, Zhen Xiang, David J. Miller, George Kesidis",arXiv,2022-10,"<a href=""DBLP (2022-10) : Training set cleansing of backdoor poisoning by self-supervised representation learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.10272]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.10272]</a>",,,DBLP
Augmentation Backdoors,"Joseph Rance, Yiren Zhao, Ilia Shumailov, Robert Mullins","arXiv
arXiv","2022-09-29
2022-09","<a href=""arXiv (2022-09-29) : Augmentation Backdoors"" target=""_blank"">[http://arxiv.org/abs/2209.15139v1]</a>
<a href=""DBLP (2022-09) : Augmentation Backdoors"" target=""_blank"">[https://doi.org/10.48550/arXiv.2209.15139]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2209.15139]</a>","Data augmentation is used extensively to improve model generalisation. However, reliance on external libraries to implement augmentation methods introduces a vulnerability into the machine learning pipeline. It is well known that backdoors can be inserted into machine learning models through serving a modified dataset to train on. Augmentation therefore presents a perfect opportunity to perform this modification without requiring an initially backdoored dataset. In this paper we present three backdoor attacks that can be covertly inserted into data augmentation. Our attacks each insert a backdoor using a different type of computer vision augmentation transform, covering simple image transforms, GAN-based augmentation, and composition-based augmentation. By inserting the backdoor using these augmentation transforms, we make our backdoors difficult to detect, while still supporting arbitrary backdoor functionality. We evaluate our attacks on a range of computer vision benchmarks and demonstrate that an attacker is able to introduce backdoors through just a malicious augmentation routine.
","
","arXiv
DBLP"
A Federated Learning Backdoor Attack Defense Method Based on Dual Attention Mechanism,J. Yan M. Yingchi N. Hua T. Zijian J. Huang,2022 IEEE Eighth International Conference on Big Data Computing Service and Applications (BigDataService),2022-09-27,"<a href=""IEEE (2022-09-27) : A Federated Learning Backdoor Attack Defense Method Based on Dual Attention Mechanism"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9898269]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/BigDataService55688.2022.00030]</a>","As a distributed machine learning paradigm, federated learning allows clients to collaboratively train models without sharing their private data, effectively solving data privacy issues in edge computing scenarios. However, recent studies have shown that neural network models in federated learning are vulnerable to backdoor attacks, which make the global model give wrong inference results in a high-confidence manner, such as recognizing stop signs as speed limit signs in the image classification task. This will have serious consequences. Aiming at the problem that the existing federated learning defense methods take a long time to compute and cannot destroy the matching relationship between triggers and backdoors, a federated learning backdoor attack defense based on dual attention mechanism (FDDAM) is proposed. The model weights are dynamically adjusted during training process, no additional models are required, and the calculation time is shorter. First, in order for the model to ignore triggers, the enhancement on image semantics is performed and then build channel attention map. Second, in order to destroy the matching relationship between triggers and backdoors, a feature map space transformation network is constructed. Finally, in order to improve the defense success rate, the channel attention map and the spatial attention map are weighted to construct a dual attention network. Experiments with FDDAM on image classification datasets show an average increase of 1.68% and 3.11% in model accuracy and defense success rate, and an average reduction of 1.85 times in computation time compared to the benchmark method.",,IEEE
A Software Approach Towards Defeating Power Management Side Channel Leakage,M. N. Islam S. Kundu,2022 IEEE 28th International Symposium on On-Line Testing and Robust System Design (IOLTS),2022-09-27,"<a href=""IEEE (2022-09-27) : A Software Approach Towards Defeating Power Management Side Channel Leakage"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9897191]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IOLTS56730.2022.9897191]</a>","Hardware Trojans are malicious, undesired, intentional modifications introduced in an Integrated Circuit (IC) which can be leveraged by a knowledgeable adversary to compromise the security of the IC. Trojans might be designed to modify the functionality of an IC, disable the security of a chip, access secret information or even destroy a system. In this paper, we propose PMU-Trojan, a hardware Trojan for leaking confidential information, such as, cryptographic secret key covertly to an adversary. For information leakage by hardware Trojan, we exploit a backdoor created by Power Management Unit (PMU) in Multi Processor System on Chip (MPSoC). PMU is a system block that initiates the voltage and the frequency changes to facilitate flexible power management and energy efficiency. It transmits voltage level change request to power supply. In this paper, we leverage this facility as an information side-channel to leak information to power-supply co-tenants. While the proposed approach can be generalized for any kind of secret information leakage, for the purpose of illustration, in this work, we focus on leaking Advanced Encryption Standard (AES) key. We demonstrate the working principle in Linux environment where a co-tenant thread monitors the change in voltage level and receives side-channel information from a thread affected by PMU-Trojan. The proposed Trojan defeats the traditional Trojan detection and suppression methods due to low information bit rate spread over long duration by a Trojan unit dissipating power at mere pico-Watts level. We propose a novel technique to defeat power management side channel by dynamically tuning processor power limit. The proposed software based solution towards suppressing PMU-Trojan is demonstrated on Intel computing platform using RAPL (Running Average Power Limit) interface.",,IEEE
Network intrusion detection based on conditional wasserstein variational autoencoder with generative adversarial network and one-dimensional convolutional neural networks,"Jiaxing He, Xiaodan Wang, ... Chen Chen",Applied Intelligence,2022-09-27,"<a href=""Springer (2022-09-27) : Network intrusion detection based on conditional wasserstein variational autoencoder with generative adversarial network and one-dimensional convolutional neural networks"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-022-03995-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-022-03995-2]</a>",There is a class-imbalance problem that the number of minority class samples is significantly lower than that of majority class samples in common...,,Springer
Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free,T. Chen Z. Zhang Y. Zhang S. Chang S. Liu Z. Wang,2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2022-09-27,"<a href=""IEEE (2022-09-27) : Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879256]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.00068]</a>","Trojan attacks threaten deep neural networks (DNNs) by poisoning them to behave normally on most samples, yet to produce manipulated results for inputs attached with a particular trigger. Several works attempt to detect whether a given DNN has been injected with a specific trigger during the training. In a parallel line of research, the lottery ticket hypothesis reveals the existence of sparse sub-networks which are capable of reaching competitive performance as the dense network after independent training. Connecting these two dots, we investigate the problem of Trojan DNN detection from the brand new lens of sparsity, even when no clean training data is available. Our crucial observation is that the Trojan features are significantly more stable to network pruning than benign features. Leveraging that, we propose a novel Trojan network detection regime: first locating a “winning Trojan lottery ticket” which preserves nearly full Trojan information yet only chance-level performance on clean inputs then recovering the trigger embedded in this already isolated sub-network. Extensive experiments on various datasets, i.e., CIFAR-10, CIFAR-100, and ImageNet, with different network architectures, i.e., VGG-16, ResNet-18, ResNet-20s, and DenseNet-100 demonstrate the effectiveness of our proposal. Codes are available at https://github.com/VITA-Group/Backdoor-LTH.","<a href=""IEEE"" target=""_blank"">[https://github.com/VITA-Group/Backdoor-LTH]</a>",IEEE
Low-Loss Subspace Compression for Clean Gains against Multi-Agent Backdoor Attacks,"Siddhartha Datta, Nigel Shadbolt","arXiv
arXiv","2022-09-20
2022-03","<a href=""arXiv (2022-09-20) : Low-Loss Subspace Compression for Clean Gains against Multi-Agent Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2203.03692v2]</a>
<a href=""DBLP (2022-03) : Low-Loss Subspace Compression for Clean Gains against Multi-Agent Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2203.03692]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2203.03692]</a>","Recent exploration of the multi-agent backdoor attack demonstrated the backfiring effect, a natural defense against backdoor attacks where backdoored inputs are randomly classified. This yields a side-effect of low accuracy w.r.t. clean labels, which motivates this paper's work on the construction of multi-agent backdoor defenses that maximize accuracy w.r.t. clean labels and minimize that of poison labels. Founded upon agent dynamics and low-loss subspace construction, we contribute three defenses that yield improved multi-agent backdoor robustness.
","
","arXiv
DBLP"
Securing federated learning with blockchain: a systematic literature review,"Attia Qammar, Ahmad Karim, ... Jianguo Ding",Artificial Intelligence Review,2022-09-16,"<a href=""Springer (2022-09-16) : Securing federated learning with blockchain: a systematic literature review"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-022-10271-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-022-10271-9]</a>",Federated learning (FL) is a promising framework for distributed machine learning that trains models without sharing local data while protecting...,,Springer
BadRes: Reveal the Backdoors through Residual Connection,"Mingrui He, Tianyu Chen, Haoyi Zhou, Shanghang Zhang, Jianxin Li","arXiv
ICASSP
arXiv","2022-09-15
2023
2022-09","<a href=""arXiv (2022-09-15) : BadRes: Reveal the Backdoors through Residual Connection"" target=""_blank"">[http://arxiv.org/abs/2209.07125v1]</a>
<a href=""DBLP (2023) : BadRes: Reveal the Backdoors Through Residual Connection"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10094691]</a>
<a href=""DBLP (2022-09) : BadRes: Reveal the Backdoors through Residual Connection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2209.07125]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICASSP49357.2023.10094691]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2209.07125]</a>","Generally, residual connections are indispensable network components in building CNNs and Transformers for various downstream tasks in CV and VL, which encourages skip shortcuts between network blocks. However, the layer-by-layer loopback residual connections may also hurt the model's robustness by allowing unsuspecting input. In this paper, we proposed a simple yet strong backdoor attack method - BadRes, where the residual connections play as a turnstile to be deterministic on clean inputs while unpredictable on poisoned ones. We have performed empirical evaluations on four datasets with ViT and BEiT models, and the BadRes achieves 97% attack success rate while receiving zero performance degradation on clean data. Moreover, we analyze BadRes with state-of-the-art defense methods and reveal the fundamental weakness lying in residual connections.

","

","arXiv
DBLP
DBLP"
Fusion-based anomaly detection system using modified isolation forest for internet of things,"Orieb AbuAlghanam, Hadeel Alazzam, ... Omar Adwan",Journal of Ambient Intelligence and Humanized Computing,2022-09-10,"<a href=""Springer (2022-09-10) : Fusion-based anomaly detection system using modified isolation forest for internet of things"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-022-04393-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-022-04393-9]</a>","In recent years, advanced threat and zero day attacks are increasing significantly, but the traditional network intrusion detection system based on...",,Springer
Intelligent IDS: Venus Fly-Trap Optimization with Honeypot Approach for Intrusion Detection and Prevention,"Sai Chaithanya Movva, Suresh Nikudiya, ... Hanumanthu Bhukya",Wireless Personal Communications,2022-09-10,"<a href=""Springer (2022-09-10) : Intelligent IDS: Venus Fly-Trap Optimization with Honeypot Approach for Intrusion Detection and Prevention"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-022-09988-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-022-09988-1]</a>",Intrusion Detection Systems and Intrusion Prevention Systems are used to detect and prevent attacks/malware from entering the network/system....,,Springer
Defending Against Backdoor Attack on Graph Nerual Network by Explainability,"Bingchen Jiang, Zhao Li","arXiv
arXiv","2022-09-07
2022-09","<a href=""arXiv (2022-09-07) : Defending Against Backdoor Attack on Graph Nerual Network by Explainability"" target=""_blank"">[http://arxiv.org/abs/2209.02902v1]</a>
<a href=""DBLP (2022-09) : Defending Against Backdoor Attack on Graph Nerual Network by Explainability"" target=""_blank"">[https://doi.org/10.48550/arXiv.2209.02902]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2209.02902]</a>","Backdoor attack is a powerful attack algorithm to deep learning model. Recently, GNN's vulnerability to backdoor attack has been proved especially on graph classification task. In this paper, we propose the first backdoor detection and defense method on GNN. Most backdoor attack depends on injecting small but influential trigger to the clean sample. For graph data, current backdoor attack focus on manipulating the graph structure to inject the trigger. We find that there are apparent differences between benign samples and malicious samples in some explanatory evaluation metrics, such as fidelity and infidelity. After identifying the malicious sample, the explainability of the GNN model can help us capture the most significant subgraph which is probably the trigger in a trojan graph. We use various dataset and different attack settings to prove the effectiveness of our defense method. The attack success rate all turns out to decrease considerably.
","
","arXiv
DBLP"
A Buyer-traceable DNN Model IP Protection Method Against Piracy and Misappropriation,S. Wang C. Xu Y. Zheng C. -H. Chang,2022 IEEE 4th International Conference on Artificial Intelligence Circuits and Systems (AICAS),2022-09-05,"<a href=""IEEE (2022-09-05) : A Buyer-traceable DNN Model IP Protection Method Against Piracy and Misappropriation"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869923]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/AICAS54282.2022.9869923]</a>","Recently proposed model functionality and attribute extraction techniques have exacerbated unauthorized low-cost reproduction of deep neural network (DNN) models for similar applications. In particular, intellectual property (IP) theft and unauthorized distribution of DNN models by dishonest buyers are very difficult to trace by existing framework of digital rights management (DRM). This paper presents a new buyer-traceable DRM scheme against model piracy and misappropriation. Unlike existing methods that require white-box access to extract the latent information for verification, the proposed method utilizes data poisoning for distributorship embedding and black-box verification. Composite backdoors are installed into the target model during the training process. Each backdoor is created by applying a data augmentation method to some clean images of a selected class. The data-augmented images with a wrong label associated with a buyer are injected into the training dataset. The ownership and distributorship of a backdoor-trained user model can be validated by querying the suspect model with a set of composite triggers. A positive suspect will output the dirty labels that pinpoint the dishonest buyer while an innocent model will output the correct labels with high confidence. The tracking accuracy and robustness of the proposed IP protection method are evaluated on CIFAR-10, CIFAR-100 and GTSRB datasets for different applications. The results show an average of 100% piracy detection rate, 0% false positive rate and 96.81 % traitor tracking success rate with negligible model accuracy degradation.",,IEEE
MP-BADNet\(^+\): Secure and effective backdoor attack detection and mitigation protocols among multi-participants in private DNNs,"Congcong Chen, Lifei Wei, ... Jianting Ning",Peer-to-Peer Networking and Applications,2022-09-05,"<a href=""Springer (2022-09-05) : MP-BADNet\(^+\): Secure and effective backdoor attack detection and mitigation protocols among multi-participants in private DNNs"" target=""_blank"">[https://link.springer.com/article/10.1007/s12083-022-01377-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12083-022-01377-6]</a>","Deep neural networks (DNNs) significantly facilitate the performance and efficiency of the Internet of Things (IoT). However, DNNs are vulnerable to...",,Springer
An improved anomaly detection model for IoT security using decision tree and gradient boosting,"Maryam Douiba, Said Benkirane, ... Mourade Azrour",The Journal of Supercomputing,2022-09-03,"<a href=""Springer (2022-09-03) : An improved anomaly detection model for IoT security using decision tree and gradient boosting"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-022-04783-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-022-04783-y]</a>","Internet of Things (IoT) represents a massive deployment of connected, intelligent devices that communicate directly in private, public, and...",,Springer
Defending against Poisoning Backdoor Attacks on Federated Meta-learning,"Chien-Lun Chen, Sara Babakniya, Marco Paolieri, Leana Golubchik","ACM Transactions on Intelligent Systems and Technology (TIST), Volume 13, Issue 5
ACM Trans. Intell. Syst. Technol.","2022-09
2022","<a href=""ACM (2022-09) : Defending against Poisoning Backdoor Attacks on Federated Meta-learning"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3523062]</a>
<a href=""DBLP (2022) : Defending against Poisoning Backdoor Attacks on Federated Meta-learning"" target=""_blank"">[https://doi.org/10.1145/3523062]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3523062]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3523062]</a>","Federated learning allows multiple users to collaboratively train a shared classification model while preserving data privacy. This approach, where model updates are aggregated by a central server, was shown to be vulnerable to poisoning backdoor attacks: ...
","
","ACM
DBLP"
Adaptive Perturbation Generation for Multiple Backdoors Detection,"Yuhang Wang, Huafeng Shi, Rui Min, Ruijia Wu, Siyuan Liang, Yichao Wu, Ding Liang, Aishan Liu",arXiv,2022-09,"<a href=""DBLP (2022-09) : Adaptive Perturbation Generation for Multiple Backdoors Detection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2209.05244]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2209.05244]</a>",,,DBLP
Black-box Ownership Verification for Dataset Protection via Backdoor Watermarking,"Yiming Li, Mingyan Zhu, Xue Yang, Yong Jiang, Shu-Tao Xia",arXiv,2022-09,"<a href=""DBLP (2022-09) : Black-box Ownership Verification for Dataset Protection via Backdoor Watermarking"" target=""_blank"">[https://doi.org/10.48550/arXiv.2209.06015]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2209.06015]</a>",,,DBLP
MACAB: Model-Agnostic Clean-Annotation Backdoor to Object Detection with Natural Trigger in Real-World,"Hua Ma, Yinshan Li, Yansong Gao, Zhi Zhang, Alsharif Abuadbba, Anmin Fu, Said F. Al-Sarawi, Surya Nepal, Derek Abbott",arXiv,2022-09,"<a href=""DBLP (2022-09) : MACAB: Model-Agnostic Clean-Annotation Backdoor to Object Detection with Natural Trigger in Real-World"" target=""_blank"">[https://doi.org/10.48550/arXiv.2209.02339]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2209.02339]</a>",,,DBLP
Solving the Capsulation Attack against Backdoor-based Deep Neural Network Watermarks by Reversing Triggers,"Fangqi Li, Shilin Wang, Yun Zhu","arXiv
arXiv","2022-08-30
2022-08","<a href=""arXiv (2022-08-30) : Solving the Capsulation Attack against Backdoor-based Deep Neural Network Watermarks by Reversing Triggers"" target=""_blank"">[http://arxiv.org/abs/2208.14127v1]</a>
<a href=""DBLP (2022-08) : Solving the Capsulation Attack against Backdoor-based Deep Neural Network Watermarks by Reversing Triggers"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.14127]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.14127]</a>","Backdoor-based watermarking schemes were proposed to protect the intellectual property of artificial intelligence models, especially deep neural networks, under the black-box setting. Compared with ordinary backdoors, backdoor-based watermarks need to digitally incorporate the owner's identity, which fact adds extra requirements to the trigger generation and verification programs. Moreover, these concerns produce additional security risks after the watermarking scheme has been published for as a forensics tool or the owner's evidence has been eavesdropped on. This paper proposes the capsulation attack, an efficient method that can invalidate most established backdoor-based watermarking schemes without sacrificing the pirated model's functionality. By encapsulating the deep neural network with a rule-based or Bayes filter, an adversary can block ownership probing and reject the ownership verification. We propose a metric, CAScore, to measure a backdoor-based watermarking scheme's security against the capsulation attack. This paper also proposes a new backdoor-based deep neural network watermarking scheme that is secure against the capsulation attack by reversing the encoding process and randomizing the exposure of triggers.
","
","arXiv
DBLP"
Int-Monitor: a model triggered hardware trojan in deep learning accelerators,"Peng Li, Rui Hou",The Journal of Supercomputing,2022-08-29,"<a href=""Springer (2022-08-29) : Int-Monitor: a model triggered hardware trojan in deep learning accelerators"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-022-04759-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-022-04759-y]</a>","Deep learning accelerators have domain-specific architectures, this special memory hierarchy and working mode could bring about new crucial security...",,Springer
Aliasing and adversarial robust generalization of CNNs,"Julia Grabinski, Janis Keuper, Margret Keuper",Machine Learning,2022-08-26,"<a href=""Springer (2022-08-26) : Aliasing and adversarial robust generalization of CNNs"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-022-06222-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-022-06222-8]</a>","Many commonly well-performing convolutional neural network models have shown to be susceptible to input data perturbations, indicating a low model...",,Springer
Causal Intervention for Generalizable Face Anti-Spoofing,Y. Liu Y. Chen W. Dai C. Li J. Zou H. Xiong,2022 IEEE International Conference on Multimedia and Expo (ICME),2022-08-26,"<a href=""IEEE (2022-08-26) : Causal Intervention for Generalizable Face Anti-Spoofing"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9859783]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICME52920.2022.9859783]</a>","Generalizable face anti-spoofing (FAS) has drawn growing attention due to its robustness to unseen real scenarios. Existing domain generalization methods leverage adversarial learning or meta-learning to mitigate the domain bias and improve generalizability. However, these methods are heuristic and suffer from complicated min-max problems or cumbersome meta-updates. In this paper, we propose a simple yet effective Causal Intervention method for generalizable Face Anti-Spoofing, namely CIFAS. Firstly, we figure out the generalizability is undermined by a domain-aware confounder based on the structural causal model. Instantiating the confounder as the domain-specific factor, a domain embedding module is employed with Dirichlet mixup to obtain representative domain features. Consequently, we propose a novel backdoor adjustment model for causal intervention to capture the true causality and learn a robust FAS model. Our CIFAS is the first attempt to introduce causal learning into FAS. Extensive experiments on seven cross-dataset tests demonstrate that CIFAS outperforms the state-of-the-art methods.",,IEEE
Metaheuristic feature selection with deep learning enabled cascaded recurrent neural network for anomaly detection in Industrial Internet of Things environment,"Nenavath Chander, Mummadi Upendra Kumar",Cluster Computing,2022-08-25,"<a href=""Springer (2022-08-25) : Metaheuristic feature selection with deep learning enabled cascaded recurrent neural network for anomaly detection in Industrial Internet of Things environment"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-022-03719-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-022-03719-8]</a>",Industrial Internet of Things (IIoT) acts as essential part of the revolutionary transition of conventional industries towards Industry 4.0. By the...,,Springer
An anomaly detection approach for backdoored neural networks: face recognition as a case study,"Alexander Unnervik, Sébastien Marcel","arXiv
BIOSIG
arXiv","2022-08-22
2022
2022-08","<a href=""arXiv (2022-08-22) : An anomaly detection approach for backdoored neural networks: face recognition as a case study"" target=""_blank"">[http://arxiv.org/abs/2208.10231v1]</a>
<a href=""DBLP (2022) : An anomaly detection approach for backdoored neural networks: face recognition as a case study"" target=""_blank"">[https://dl.gi.de/handle/20.500.12116/39718]</a>
<a href=""DBLP (2022-08) : An anomaly detection approach for backdoored neural networks: face recognition as a case study"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.10231]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://dl.gi.de/handle/20.500.12116/39718]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.10231]</a>","Backdoor attacks allow an attacker to embed functionality jeopardizing proper behavior of any algorithm, machine learning or not. This hidden functionality can remain inactive for normal use of the algorithm until activated by the attacker. Given how stealthy backdoor attacks are, consequences of these backdoors could be disastrous if such networks were to be deployed for applications as critical as border or access control. In this paper, we propose a novel backdoored network detection method based on the principle of anomaly detection, involving access to the clean part of the training data and the trained network. We highlight its promising potential when considering various triggers, locations and identity pairs, without the need to make any assumptions on the nature of the backdoor and its setup. We test our method on a novel dataset of backdoored networks and report detectability results with perfect scores.

","

","arXiv
DBLP
DBLP"
Dispersed Pixel Perturbation-based Imperceptible Backdoor Trigger for Image Classifier Models,"Yulong Wang, Minghui Zhao, Shenghong Li, Xin Yuan, Wei Ni","arXiv
IEEE Trans. Inf. Forensics Secur.
arXiv","2022-08-19
2022
2022-08","<a href=""arXiv (2022-08-19) : Dispersed Pixel Perturbation-based Imperceptible Backdoor Trigger for Image Classifier Models"" target=""_blank"">[http://arxiv.org/abs/2208.09336v1]</a>
<a href=""DBLP (2022) : Dispersed Pixel Perturbation-Based Imperceptible Backdoor Trigger for Image Classifier Models"" target=""_blank"">[https://doi.org/10.1109/TIFS.2022.3202687]</a>
<a href=""DBLP (2022-08) : Dispersed Pixel Perturbation-based Imperceptible Backdoor Trigger for Image Classifier Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.09336]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2022.3202687]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.09336]</a>","Typical deep neural network (DNN) backdoor attacks are based on triggers embedded in inputs. Existing imperceptible triggers are computationally expensive or low in attack success. In this paper, we propose a new backdoor trigger, which is easy to generate, imperceptible, and highly effective. The new trigger is a uniformly randomly generated three-dimensional (3D) binary pattern that can be horizontally and/or vertically repeated and mirrored and superposed onto three-channel images for training a backdoored DNN model. Dispersed throughout an image, the new trigger produces weak perturbation to individual pixels, but collectively holds a strong recognizable pattern to train and activate the backdoor of the DNN. We also analytically reveal that the trigger is increasingly effective with the improving resolution of the images. Experiments are conducted using the ResNet-18 and MLP models on the MNIST, CIFAR-10, and BTSR datasets. In terms of imperceptibility, the new trigger outperforms existing triggers, such as BadNets, Trojaned NN, and Hidden Backdoor, by over an order of magnitude. The new trigger achieves an almost 100% attack success rate, only reduces the classification accuracy by less than 0.7%-2.4%, and invalidates the state-of-the-art defense techniques.

","

","arXiv
DBLP
DBLP"
A Systematic Review of 2021 Microsoft Exchange Data Breach Exploiting Multiple Vulnerabilities,A. M. Pitney S. Penrod M. Foraker S. Bhunia,2022 7th International Conference on Smart and Sustainable Technologies (SpliTech),2022-08-19,"<a href=""IEEE (2022-08-19) : A Systematic Review of 2021 Microsoft Exchange Data Breach Exploiting Multiple Vulnerabilities"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9854268]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.23919/SpliTech55088.2022.9854268]</a>","At the beginning of 2021 a massive amount of servers using Microsoft's Exchange program were breached by a foreign hacker group called HAFNIUM. This group discovered and exploited 4 different zero-day vulnerabilities which sent the entire cybersecurity community into a panic. Immediately after data breach was discovered, Microsoft and other governmental security agencies alerted all the users. Microsoft released multiple patches to safeguard the attack surface. This paper provides an in-depth analysis of the attack methodology, impacts and possible defense solutions. An estimated 400,000 Exchange Servers were affected by this attack, and a large portion of servers are still vulnerable today. Microsoft has released an effective security patch to stop the exploitation of the vulnerabilities.",,IEEE
Hardware-assisted Neural Network IP Protection using Non-malicious Backdoor and Selective Weight Obfuscation,M. Grailoo U. Reinsalu M. Leier T. Nikoubin,2022 IEEE 15th Dallas Circuit And System Conference (DCAS),2022-08-18,"<a href=""IEEE (2022-08-18) : Hardware-assisted Neural Network IP Protection using Non-malicious Backdoor and Selective Weight Obfuscation"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9845608]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/DCAS53974.2022.9845608]</a>","Neural networks (NNs) are already deployed in hardware today, becoming valuable intellectual property (IP) as many hours are invested in their training and optimization. Therefore, attackers may be interested in copying, reverse engineering, or even modifying this IP. The current practices in hardware obfuscation, including the widely studied LL technique, are insufficient to protect the actual IP of a well-trained NN: its weights. Simply hiding the weights behind a key-based scheme is inefficient (resource-hungry) and inadequate (attackers can exploit knowledge distillation). This paper proposes a two-step technique that addresses these issues: the obfuscation overhead is kept under control by applying only selective weight obfuscation, while distillation-based attack is prevented by prediction poisoning using non-malicious backdoor such that an attacker with access to an oracle cannot accurately train his/her model. This is the first work to consider such a poisoning approach in HW-implemented NNs. The poisoning occurs in score part before SoftMax. In the score poisoning, the accuracy and prediction distribution are maintained without disturbing the functionality or incurring high overheads. Finally, we elaborate a threat model which highlights the difference between random logic obfuscation and the obfuscation of NN IP. Based on this threat model, our security analysis shows that the proposed technique successfully and significantly reduces the accuracy of the stolen NN model on various representative datasets. Finally, we highlight that our proposed approach is flexible and does not require manipulation of the NN toolchain, and is coded in a flexible high-level language (e.g., C++).",,IEEE
Imperceptible and Robust Backdoor Attack in 3D Point Cloud,"Kuofeng Gao, Jiawang Bai, Baoyuan Wu, Mengxi Ya, Shu-Tao Xia","arXiv
IEEE Trans. Inf. Forensics Secur.
arXiv","2022-08-17
2024
2022-08","<a href=""arXiv (2022-08-17) : Imperceptible and Robust Backdoor Attack in 3D Point Cloud"" target=""_blank"">[http://arxiv.org/abs/2208.08052v1]</a>
<a href=""DBLP (2024) : Imperceptible and Robust Backdoor Attack in 3D Point Cloud"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3333687]</a>
<a href=""DBLP (2022-08) : Imperceptible and Robust Backdoor Attack in 3D Point Cloud"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.08052]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3333687]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.08052]</a>","With the thriving of deep learning in processing point cloud data, recent works show that backdoor attacks pose a severe security threat to 3D vision applications. The attacker injects the backdoor into the 3D model by poisoning a few training samples with trigger, such that the backdoored model performs well on clean samples but behaves maliciously when the trigger pattern appears. Existing attacks often insert some additional points into the point cloud as the trigger, or utilize a linear transformation (e.g., rotation) to construct the poisoned point cloud. However, the effects of these poisoned samples are likely to be weakened or even eliminated by some commonly used pre-processing techniques for 3D point cloud, e.g., outlier removal or rotation augmentation. In this paper, we propose a novel imperceptible and robust backdoor attack (IRBA) to tackle this challenge. We utilize a nonlinear and local transformation, called weighted local transformation (WLT), to construct poisoned samples with unique transformations. As there are several hyper-parameters and randomness in WLT, it is difficult to produce two similar transformations. Consequently, poisoned samples with unique transformations are likely to be resistant to aforementioned pre-processing techniques. Besides, as the controllability and smoothness of the distortion caused by a fixed WLT, the generated poisoned samples are also imperceptible to human inspection. Extensive experiments on three benchmark datasets and four models show that IRBA achieves 80%+ ASR in most cases even with pre-processing techniques, which is significantly higher than previous state-of-the-art attacks.

","

","arXiv
DBLP
DBLP"
"Federated learning-based AI approaches in smart healthcare: concepts, taxonomies, challenges and open issues","Anichur Rahman, Md. Sazzad Hossain, ... Shahab S. Band",Cluster Computing,2022-08-17,"<a href=""Springer (2022-08-17) : Federated learning-based AI approaches in smart healthcare: concepts, taxonomies, challenges and open issues"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-022-03658-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-022-03658-4]</a>","Federated Learning (FL), Artificial Intelligence (AI), and Explainable Artificial Intelligence (XAI) are the most trending and exciting technology in...",,Springer
Confidence Matters: Inspecting Backdoors in Deep Neural Networks via Distribution Transfer,"Tong Wang, Yuan Yao, Feng Xu, Miao Xu, Shengwei An, Ting Wang","arXiv
arXiv","2022-08-13
2022-08","<a href=""arXiv (2022-08-13) : Confidence Matters: Inspecting Backdoors in Deep Neural Networks via Distribution Transfer"" target=""_blank"">[http://arxiv.org/abs/2208.06592v1]</a>
<a href=""DBLP (2022-08) : Confidence Matters: Inspecting Backdoors in Deep Neural Networks via Distribution Transfer"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.06592]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.06592]</a>","Backdoor attacks have been shown to be a serious security threat against deep learning models, and detecting whether a given model has been backdoored becomes a crucial task. Existing defenses are mainly built upon the observation that the backdoor trigger is usually of small size or affects the activation of only a few neurons. However, the above observations are violated in many cases especially for advanced backdoor attacks, hindering the performance and applicability of the existing defenses. In this paper, we propose a backdoor defense DTInspector built upon a new observation. That is, an effective backdoor attack usually requires high prediction confidence on the poisoned training samples, so as to ensure that the trained model exhibits the targeted behavior with a high probability. Based on this observation, DTInspector first learns a patch that could change the predictions of most high-confidence data, and then decides the existence of backdoor by checking the ratio of prediction changes after applying the learned patch on the low-confidence data. Extensive evaluations on five backdoor attacks, four datasets, and three advanced attacking types demonstrate the effectiveness of the proposed defense.
","
","arXiv
DBLP"
Defense against Backdoor Attacks via Identifying and Purifying Bad Neurons,"Mingyuan Fan, Yang Liu, Cen Chen, Ximeng Liu, Wenzhong Guo","arXiv
arXiv","2022-08-13
2022-08","<a href=""arXiv (2022-08-13) : Defense against Backdoor Attacks via Identifying and Purifying Bad Neurons"" target=""_blank"">[http://arxiv.org/abs/2208.06537v1]</a>
<a href=""DBLP (2022-08) : Defense against Backdoor Attacks via Identifying and Purifying Bad Neurons"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.06537]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.06537]</a>","The opacity of neural networks leads their vulnerability to backdoor attacks, where hidden attention of infected neurons is triggered to override normal predictions to the attacker-chosen ones. In this paper, we propose a novel backdoor defense method to mark and purify the infected neurons in the backdoored neural networks. Specifically, we first define a new metric, called benign salience. By combining the first-order gradient to retain the connections between neurons, benign salience can identify the infected neurons with higher accuracy than the commonly used metric in backdoor defense. Then, a new Adaptive Regularization (AR) mechanism is proposed to assist in purifying these identified infected neurons via fine-tuning. Due to the ability to adapt to different magnitudes of parameters, AR can provide faster and more stable convergence than the common regularization mechanism in neuron purifying. Extensive experimental results demonstrate that our method can erase the backdoor in neural networks with negligible performance degradation.
","
","arXiv
DBLP"
A Knowledge Distillation-Based Backdoor Attack in Federated Learning,"Yifan Wang, Wei Fan, Keke Yang, Naji Alhusaini, Jing Li","arXiv
arXiv","2022-08-12
2022-08","<a href=""arXiv (2022-08-12) : A Knowledge Distillation-Based Backdoor Attack in Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2208.06176v1]</a>
<a href=""DBLP (2022-08) : A Knowledge Distillation-Based Backdoor Attack in Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.06176]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.06176]</a>","Federated Learning (FL) is a novel framework of decentralized machine learning. Due to the decentralized feature of FL, it is vulnerable to adversarial attacks in the training procedure, e.g. , backdoor attacks. A backdoor attack aims to inject a backdoor into the machine learning model such that the model will make arbitrarily incorrect behavior on the test sample with some specific backdoor trigger. Even though a range of backdoor attack methods of FL has been introduced, there are also methods defending against them. Many of the defending methods utilize the abnormal characteristics of the models with backdoor or the difference between the models with backdoor and the regular models. To bypass these defenses, we need to reduce the difference and the abnormal characteristics. We find a source of such abnormality is that backdoor attack would directly flip the label of data when poisoning the data. However, current studies of the backdoor attack in FL are not mainly focus on reducing the difference between the models with backdoor and the regular models. In this paper, we propose Adversarial Knowledge Distillation(ADVKD), a method combine knowledge distillation with backdoor attack in FL. With knowledge distillation, we can reduce the abnormal characteristics in model result from the label flipping, thus the model can bypass the defenses. Compared to current methods, we show that ADVKD can not only reach a higher attack success rate, but also successfully bypass the defenses when other methods fails. To further explore the performance of ADVKD, we test how the parameters affect the performance of ADVKD under different scenarios. According to the experiment result, we summarize how to adjust the parameter for better performance under different scenarios. We also use several methods to visualize the effect of different attack and explain the effectiveness of ADVKD.
","
","arXiv
DBLP"
Botnet Detection Based on Machine Learning,X. Yang Z. Guo Z. Mai,2022 International Conference on Blockchain Technology and Information Security (ICBCTIS),2022-08-11,"<a href=""IEEE (2022-08-11) : Botnet Detection Based on Machine Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9845057]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICBCTIS55569.2022.00056]</a>","A botnet is a new type of attack method developed and integrated on the basis of traditional malicious code such as network worms and backdoor tools, and it is extremely threatening. This course combines deep learning and neural network methods in machine learning methods to detect and classify the existence of botnets. This sample does not rely on any prior features, the final multi-class classification accuracy rate is higher than 98.7%, the effect is significant.",,IEEE
Securing Hardware Accelerator during High-level Synthesis,D. Roy S. J. Shaik S. Sharma,2022 IEEE International Symposium on Hardware Oriented Security and Trust (HOST),2022-08-09,"<a href=""IEEE (2022-08-09) : Securing Hardware Accelerator during High-level Synthesis"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9840309]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/HOST54066.2022.9840309]</a>","Performing Reverse Engineering (RE) on the Intellectual Property (IP) core of Hardware Accelerator (HA) becomes the first step of the attackers to launch a successful attack. This enables them to introduce hardware Trojan, create backdoors, cloning and counterfeit the IP core. In this work, a robust hardware obfuscation approach is proposed to generate a reverse engineering resilient hardware accelerator. The obfuscation is performed in three different ways during the scheduling phase of High-level Synthesis (HLS) by embedding key-controlled components. Unlike state-of-the-art approaches, a novel algorithm is developed to identify the suitable locations for embedding these key-controlled components to enhance the reverse engineering complexity. The proposed method incurs minimal average design cost overhead, i.e., 7% compared to the non-obfuscated design and achieves average design cost reduction by 12.1% compared to one closely related approach for key size 90.",,IEEE
Windows and IoT malware visualization and classification with deep CNN and Xception CNN using Markov images,"Osho Sharma, Akashdeep Sharma, Arvind Kalia",Journal of Intelligent Information Systems,2022-08-09,"<a href=""Springer (2022-08-09) : Windows and IoT malware visualization and classification with deep CNN and Xception CNN using Markov images"" target=""_blank"">[https://link.springer.com/article/10.1007/s10844-022-00734-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10844-022-00734-4]</a>","Context Technological advances have led to a tremendous increase in complexity and volume of specialized malware, affecting computational devices...",,Springer
Intellectual property protection for deep semantic segmentation models,"Hongjia Ruan, Huihui Song, ... Qingshan Liu",Frontiers of Computer Science,2022-08-08,"<a href=""Springer (2022-08-08) : Intellectual property protection for deep semantic segmentation models"" target=""_blank"">[https://link.springer.com/article/10.1007/s11704-021-1186-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11704-021-1186-y]</a>",Deep neural networks have achieved great success in varieties of artificial intelligent fields. Since training a good deep model is often challenging...,,Springer
A decentralized honeypot for IoT Protocols based on Android devices,"Irini Lygerou, Shreyas Srinivasa, ... Dimitris Gritzalis",International Journal of Information Security,2022-08-06,"<a href=""Springer (2022-08-06) : A decentralized honeypot for IoT Protocols based on Android devices"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00605-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00605-7]</a>",The exponential growth of internet connected devices in this past year has led to a significant increase in IoT targeted attacks. The lack of proper...,,Springer
Backdoor Attack of DNN Models Based on Structure Modification Using Steganography,S. Wang,"2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)
IEEE Joint International Information Technology and Artificial Intelligence Conference (ITAIC)","2022-08-03
2022-01-01","<a href=""IEEE (2022-08-03) : Backdoor Attack of DNN Models Based on Structure Modification Using Steganography"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9836610]</a>
<a href=""ScienceDirect (2022-01-01) : Backdoor Attack of DNN Models Based on Structure Modification Using Steganography"" target=""_blank"">[https://doi.org/10.1109/ITAIC54216.2022.9836610]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ITAIC54216.2022.9836610]</a>
<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ITAIC54216.2022.9836610]</a>","In the backdoor attack methods of Deep Neural Network Model based on Structure Modification, malicious samples are constructed using static visible triggers and trojan net embedded in target model is complex, which decreases the stealthiness of these backdoor attacks. In this work, we proposed a backdoor attack of DNN models based on structure modification using steganography. Specifically, inspired by the DNN-based image steganography, we construct hiding net to generate malicious samples with dynamic invisible triggers and construct simple trojan net using stacked autoencoder. The trojan net is connected with target model in serial structure to generate backdoored target model. Extensive experiments on multiple datasets and models verify the effectiveness of increasing the backdoor attack stealthiness.
","
","IEEE
ScienceDirect"
A Comparative Study on the Impact of Adversarial Machine Learning Attacks on Contemporary Intrusion Detection Datasets,"Medha Pujari, Yulexis Pacheco, ... Weiqing Sun",SN Computer Science,2022-08-03,"<a href=""Springer (2022-08-03) : A Comparative Study on the Impact of Adversarial Machine Learning Attacks on Contemporary Intrusion Detection Datasets"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-022-01321-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-022-01321-8]</a>","Adversarial attack techniques have taken a firm stand against the capabilities of deep neural networks, rendering them less efficient in performing...",,Springer
A Robust Countermeasures for Poisoning Attacks on Deep Neural Networks of Computer Interaction Systems,Liu I.H.,Applied Sciences (Switzerland),2022-08-01,"<a href=""ScienceDirect (2022-08-01) : A Robust Countermeasures for Poisoning Attacks on Deep Neural Networks of Computer Interaction Systems"" target=""_blank"">[https://doi.org/10.3390/app12157753]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/app12157753]</a>",,,ScienceDirect
Backdoor-resistant identity-based proxy re-encryption for cloud-assisted wireless body area networks,Zhou Y.,Information Sciences,2022-08-01,"<a href=""ScienceDirect (2022-08-01) : Backdoor-resistant identity-based proxy re-encryption for cloud-assisted wireless body area networks"" target=""_blank"">[https://doi.org/10.1016/j.ins.2022.05.007]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.ins.2022.05.007]</a>",,,ScienceDirect
Exploiting Missing Value Patterns for a Backdoor Attack on Machine Learning Models of Electronic Health Records: Development and Validation Study,Joe B.,JMIR Medical Informatics,2022-08-01,"<a href=""ScienceDirect (2022-08-01) : Exploiting Missing Value Patterns for a Backdoor Attack on Machine Learning Models of Electronic Health Records: Development and Validation Study"" target=""_blank"">[https://doi.org/10.2196/38440]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.2196/38440]</a>",,,ScienceDirect
Backdoor Watermarking Deep Learning Classification Models With Deep Fidelity,"Guang Hua, Andrew Beng Jin Teoh",arXiv,2022-08,"<a href=""DBLP (2022-08) : Backdoor Watermarking Deep Learning Classification Models With Deep Fidelity"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.00563]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2208.00563]</a>",,,DBLP
Intelligent and behavioral-based detection of malware in IoT spectrum sensors,"Alberto Huertas Celdrán, Pedro Miguel Sánchez Sánchez, ... Burkhard Stiller",International Journal of Information Security,2022-07-29,"<a href=""Springer (2022-07-29) : Intelligent and behavioral-based detection of malware in IoT spectrum sensors"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00602-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00602-w]</a>",The number of Cyber-Physical Systems (CPS) available in industrial environments is growing mainly due to the evolution of the Internet-of-Things...,,Springer
Can We Mitigate Backdoor Attack Using Adversarial Detection Methods?,"Kaidi Jin, Tianwei Zhang, Chao Shen, Yufei Chen, Ming Fan, Chenhao Lin, Ting Liu","arXiv
IEEE Trans. Dependable Secur. Comput.","2022-07-28
2023","<a href=""arXiv (2022-07-28) : Can We Mitigate Backdoor Attack Using Adversarial Detection Methods?"" target=""_blank"">[http://arxiv.org/abs/2006.14871v2]</a>
<a href=""DBLP (2023) : Can We Mitigate Backdoor Attack Using Adversarial Detection Methods?"" target=""_blank"">[https://doi.org/10.1109/TDSC.2022.3194642]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TDSC.2022.3194642]</a>","Deep Neural Networks are well known to be vulnerable to adversarial attacks and backdoor attacks, where minor modifications on the input are able to mislead the models to give wrong results. Although defenses against adversarial attacks have been widely studied, investigation on mitigating backdoor attacks is still at an early stage. It is unknown whether there are any connections and common characteristics between the defenses against these two attacks. We conduct comprehensive studies on the connections between adversarial examples and backdoor examples of Deep Neural Networks to seek to answer the question: can we detect backdoor using adversarial detection methods. Our insights are based on the observation that both adversarial examples and backdoor examples have anomalies during the inference process, highly distinguishable from benign samples. As a result, we revise four existing adversarial defense methods for detecting backdoor examples. Extensive evaluations indicate that these approaches provide reliable protection against backdoor attacks, with a higher accuracy than detecting adversarial examples. These solutions also reveal the relations of adversarial examples, backdoor examples and normal samples in model sensitivity, activation space and feature space. This is able to enhance our understanding about the inherent features of these two attacks and the defense opportunities.
","
","arXiv
DBLP"
Real-Time Heuristic-Based Detection of Attacks Performed on a Linux Machine Using Osquery,"Sarfaraz Ahamed, Ramanathan Lakshmanan",SN Computer Science,2022-07-28,"<a href=""Springer (2022-07-28) : Real-Time Heuristic-Based Detection of Attacks Performed on a Linux Machine Using Osquery"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-022-01288-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-022-01288-6]</a>","With the increase in Unix-based operating system for web servers and IoT devices, it has become crucial to detect attacks that are performed on these...",,Springer
Model Orthogonalization: Class Distance Hardening in Neural Networks for Better Security,G. Tao Y. Liu G. Shen Q. Xu S. An Z. Zhang X. Zhang,2022 IEEE Symposium on Security and Privacy (SP),2022-07-27,"<a href=""IEEE (2022-07-27) : Model Orthogonalization: Class Distance Hardening in Neural Networks for Better Security"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833688]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SP46214.2022.9833688]</a>","The distance between two classes for a deep learning classifier can be measured by the level of difficulty in flipping all (or majority of) samples in a class to the other. The class distances of many pre-trained models in the wild are very small and do not align well with humans’ intuition (e.g., classes turtle and bird have smaller distance than classes cat and dog), making the models vulnerable to backdoor attacks, which aim to cause misclassification by stamping a specific pattern to inputs. We propose a novel model hardening technique called model orthogonalization which is an add-on training step to pretrained models, including clean models, poisoned models, and adversarially trained models. It can substantially enlarge class distances with reasonable training cost and without much accuracy degradation. Our evaluation on 5 datasets with 22 model structures show that our technique can enlarge class distances by 177.63% on average with less than 1% accuracy loss, outperforming existing hardening techniques such as adversarial training, universal adversarial perturbation, and directly using generated backdoors. It reduces 80% false positives for a state-of-the-art backdoor scanner as the enlarged class distances allow the scanner to easily distinguish clean and poisoned models, and substantially outperforms three existing techniques in removing injected backdoors.",,IEEE
Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures,E. Bagdasaryan V. Shmatikov,2022 IEEE Symposium on Security and Privacy (SP),2022-07-27,"<a href=""IEEE (2022-07-27) : Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833572]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SP46214.2022.9833572]</a>","We investigate a new threat to neural sequence-to-sequence (seq2seq) models: training-time attacks that cause models to “spin” their outputs so as to support an adversary-chosen sentiment or point of view—but only when the input contains adversary-chosen trigger words. For example, a spinned 1 summarization model outputs positive summaries of any text that mentions the name of some individual or organization.Model spinning introduces a “meta-backdoor” into a model. Whereas conventional backdoors cause models to produce incorrect outputs on inputs with the trigger, outputs of spinned models preserve context and maintain standard accuracy metrics, yet also satisfy a meta-task chosen by the adversary.Model spinning enables propaganda-as-a-service, where propaganda is defined as biased speech. An adversary can create customized language models that produce desired spins for chosen triggers, then deploy these models to generate disinformation (a platform attack), or else inject them into ML training pipelines (a supply-chain attack), transferring malicious functionality to downstream models trained by victims.To demonstrate the feasibility of model spinning, we develop a new backdooring technique. It stacks an adversarial meta-task (e.g., sentiment analysis) onto a seq2seq model, backpropagates the desired meta-task output (e.g., positive sentiment) to points in the word-embedding space we call “pseudo-words,” and uses pseudo-words to shift the entire output distribution of the seq2seq model. We evaluate this attack on language generation, summarization, and translation models with different triggers and meta-tasks such as sentiment, toxicity, and entailment. Spinned models largely maintain their accuracy metrics (ROUGE and BLEU) while shifting their outputs to satisfy the adversary’s meta-task. We also show that, in the case of a supply-chain attack, the spin functionality transfers to downstream models.Finally, we propose a black-box, meta-task-independent defense that, given a list of candidate triggers, can detect models that selectively apply spin to inputs with any of these triggers.1We use “spinned” rather than “spun” to match how the word is used in public relations.",,IEEE
FRIB: Low-poisoning Rate Invisible Backdoor Attack based on Feature Repair,"Hui Xia, Xiugui Yang, Xiangyun Qian, Rui Zhang","arXiv
arXiv","2022-07-26
2022-07","<a href=""arXiv (2022-07-26) : FRIB: Low-poisoning Rate Invisible Backdoor Attack based on Feature Repair"" target=""_blank"">[http://arxiv.org/abs/2207.12863v1]</a>
<a href=""DBLP (2022-07) : FRIB: Low-poisoning Rate Invisible Backdoor Attack based on Feature Repair"" target=""_blank"">[https://doi.org/10.48550/arXiv.2207.12863]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2207.12863]</a>","During the generation of invisible backdoor attack poisoned data, the feature space transformation operation tends to cause the loss of some poisoned features and weakens the mapping relationship between source images with triggers and target labels, resulting in the need for a higher poisoning rate to achieve the corresponding backdoor attack success rate. To solve the above problems, we propose the idea of feature repair for the first time and introduce the blind watermark technique to repair the poisoned features lost during the generation of poisoned data. Under the premise of ensuring consistent labeling, we propose a low-poisoning rate invisible backdoor attack based on feature repair, named FRIB. Benefiting from the above design concept, the new method enhances the mapping relationship between the source images with triggers and the target labels, and increases the degree of misleading DNNs, thus achieving a high backdoor attack success rate with a very low poisoning rate. Ultimately, the detailed experimental results show that the goal of achieving a high success rate of backdoor attacks with a very low poisoning rate is achieved on all MNIST, CIFAR10, GTSRB, and ImageNet datasets.
","
","arXiv
DBLP"
Technical Report: Assisting Backdoor Federated Learning with Whole Population Knowledge Alignment,"Tian Liu, Xueyang Hu, Tao Shu","arXiv
arXiv","2022-07-25
2022-07","<a href=""arXiv (2022-07-25) : Technical Report: Assisting Backdoor Federated Learning with Whole Population Knowledge Alignment"" target=""_blank"">[http://arxiv.org/abs/2207.12327v1]</a>
<a href=""DBLP (2022-07) : Technical Report: Assisting Backdoor Federated Learning with Whole Population Knowledge Alignment"" target=""_blank"">[https://doi.org/10.48550/arXiv.2207.12327]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2207.12327]</a>","Due to the distributed nature of Federated Learning (FL), researchers have uncovered that FL is vulnerable to backdoor attacks, which aim at injecting a sub-task into the FL without corrupting the performance of the main task. Single-shot backdoor attack achieves high accuracy on both the main task and backdoor sub-task when injected at the FL model convergence. However, the early-injected single-shot backdoor attack is ineffective because: (1) the maximum backdoor effectiveness is not reached at injection because of the dilution effect from normal local updates, (2) the backdoor effect decreases quickly as the backdoor will be overwritten by the newcoming normal local updates. In this paper, we strengthen the early-injected single-shot backdoor attack utilizing FL model information leakage. We show that the FL convergence can be expedited if the client trains on a dataset that mimics the distribution and gradients of the whole population. Based on this observation, we proposed a two-phase backdoor attack, which includes a preliminary phase for the subsequent backdoor attack. In the preliminary phase, the attacker-controlled client first launches a whole population distribution inference attack and then trains on a locally crafted dataset that is aligned with both the gradient and inferred distribution. Benefiting from the preliminary phase, the later injected backdoor achieves better effectiveness as the backdoor effect will be less likely to be diluted by the normal model updates. Extensive experiments are conducted on MNIST dataset under various data heterogeneity settings to evaluate the effectiveness of the proposed backdoor attack. Results show that the proposed backdoor outperforms existing backdoor attacks in both success rate and longevity, even when defense mechanisms are in place.
","
","arXiv
DBLP"
Backdoor Attacks on the DNN Interpretation System,"Shihong Fang, Anna Choromanska","arXiv
AAAI
arXiv","2022-07-19
2022
2020-11","<a href=""arXiv (2022-07-19) : Backdoor Attacks on the DNN Interpretation System"" target=""_blank"">[http://arxiv.org/abs/2011.10698v3]</a>
<a href=""DBLP (2022) : Backdoor Attacks on the DNN Interpretation System"" target=""_blank"">[https://doi.org/10.1609/aaai.v36i1.19935]</a>
<a href=""DBLP (2020-11) : Backdoor Attacks on the DNN Interpretation System"" target=""_blank"">[https://arxiv.org/abs/2011.10698]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v36i1.19935]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2011.10698]</a>","Interpretability is crucial to understand the inner workings of deep neural networks (DNNs) and many interpretation methods generate saliency maps that highlight parts of the input image that contribute the most to the prediction made by the DNN. In this paper we design a backdoor attack that alters the saliency map produced by the network for an input image only with injected trigger that is invisible to the naked eye while maintaining the prediction accuracy. The attack relies on injecting poisoned data with a trigger into the training data set. The saliency maps are incorporated in the penalty term of the objective function that is used to train a deep model and its influence on model training is conditioned upon the presence of a trigger. We design two types of attacks: targeted attack that enforces a specific modification of the saliency map and untargeted attack when the importance scores of the top pixels from the original saliency map are significantly reduced. We perform empirical evaluation of the proposed backdoor attacks on gradient-based and gradient-free interpretation methods for a variety of deep learning architectures. We show that our attacks constitute a serious security threat when deploying deep learning models developed by untrusty sources. Finally, in the Supplement we demonstrate that the proposed methodology can be used in an inverted setting, where the correct saliency map can be obtained only in the presence of a trigger (key), effectively making the interpretation system available only to selected users.

","

","arXiv
DBLP
DBLP"
FaceHack: Attacking Facial Recognition Systems Using Malicious Facial Characteristics,E. Sarkar H. Benkraouda G. Krishnan H. Gamil M. Maniatakos,"IEEE Transactions on Biometrics, Behavior, and Identity Science",2022-07-19,"<a href=""IEEE (2022-07-19) : FaceHack: Attacking Facial Recognition Systems Using Malicious Facial Characteristics"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9632692]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TBIOM.2021.3132132]</a>","Recent advances in machine learning have opened up new avenues for its extensive use in real-world applications. Facial recognition, specifically, is used from simple friend suggestions in social-media platforms to critical security applications for biometric validation in automated border control at airports. Considering these scenarios, security vulnerabilities of such facial recognition systems pose serious threats with severe outcomes. Recent work demonstrated that Deep Neural Networks (DNNs), typically used in facial recognition systems, are susceptible to backdoor attacks in other words, the DNNs turn malicious in the presence of a unique trigger. Detection mechanisms have focused on identifying these distinct trigger-based outliers statistically or through reconstructing them. In this work, we propose the use of facial characteristics as triggers to backdoored facial recognition systems. Additionally, we demonstrate that these attacks can be realised on real-time facial recognition systems. Depending on the attack scenario, the changes in the facial attributes may be embedded artificially using social-media filters or introduced naturally through facial muscle movements. We evaluate the success of the attack and validate that it does not interfere with the performance criteria of the model. We also substantiate that our triggers are undetectable by thoroughly testing them on state-of-the-art defense and detection mechanisms.",,IEEE
ML-MDS: Machine Learning based Misbehavior Detection System for Cognitive Software-defined Multimedia VANETs (CSDMV) in smart cities,"Rajendra Prasad Nayak, Srinivas Sethi, ... Anand Nayyar",Multimedia Tools and Applications,2022-07-19,"<a href=""Springer (2022-07-19) : ML-MDS: Machine Learning based Misbehavior Detection System for Cognitive Software-defined Multimedia VANETs (CSDMV) in smart cities"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-022-13440-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-022-13440-8]</a>","Security is a major concern in vehicular networks for reliable communication between the source and the destination in smart cities. Data, these...",,Springer
Amelioration of Security Protocols from the outset of technology to present moment,R. Thapa A. Goyal A. Goyal V. Verma,2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),2022-07-18,"<a href=""IEEE (2022-07-18) : Amelioration of Security Protocols from the outset of technology to present moment"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9823517]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICACITE53722.2022.9823517]</a>","As the era of technology advances itself into the future, we come vis-a-vis with a notion that, the more we indulge our day to day lives into gadgets, the more exposed and vulnerable we are become. Somewhere human beings are feeling the shift of reality towards digitization and it's boisterous. What's even more intriguing is the will of human beings to continuosly endeavor for everything come what may. The pace of advancement asks of us, intelligence related to our personal lives in order to provide assistance. How do we secure this process? To use an application, services of some company, we exchange our data and personal information with them. What is the guarantee of our data being kept safe? Is there? Yes. There is. This is where we introduce a major portion of the computer science field. Security. This paper throws light upon the amelioration of how technology has cooperated with security to provide safety to the user, service, network, systems and businesses. The processing of information in all sectors businesses involves sharing of data, which makes data the most expensive. Security for this data has seen its ebb and flow throughout the years and therefore this paper discusses it. How and what researches have been carried out that brought us to today's security level.",,IEEE
Dataset authorization control: protect the intellectual property of dataset via reversible feature space adversarial examples,"Mingfu Xue, Yinghao Wu, ... Weiqiang Liu",Applied Intelligence,2022-07-18,"<a href=""Springer (2022-07-18) : Dataset authorization control: protect the intellectual property of dataset via reversible feature space adversarial examples"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-022-03926-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-022-03926-1]</a>","The cost of collecting and annotating large-scale datasets is expensive, thus the valuable datasets can be considered as the intellectual property...",,Springer
Faking smart industry: exploring cyber-threat landscape deploying cloud-based honeypot,"S M Zia Ur Rashid, Ashfaqul Haq, ... Abu Barkat Ullah",Wireless Networks,2022-07-18,"<a href=""Springer (2022-07-18) : Faking smart industry: exploring cyber-threat landscape deploying cloud-based honeypot"" target=""_blank"">[https://link.springer.com/article/10.1007/s11276-022-03057-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11276-022-03057-y]</a>",The digital evolution of Industry 4.0 enabled Operational Technology infrastructures to operate and remotely maintain cyber-physical systems bridging...,,Springer
Hyper-heuristic multi-objective online optimization for cyber security in big data,"Mohammed Ahmed, G. Rama Mohan Babu",International Journal of System Assurance Engineering and Management,2022-07-17,"<a href=""Springer (2022-07-17) : Hyper-heuristic multi-objective online optimization for cyber security in big data"" target=""_blank"">[https://link.springer.com/article/10.1007/s13198-022-01727-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13198-022-01727-w]</a>",The tremendous growth in data inside the Big data era has created data management challenges as well as data security concerns. These large data...,,Springer
A comprehensive survey of physical and logic testing techniques for Hardware Trojan detection and prevention,"Rijoy Mukherjee, Sree Ranjani Rajendran, Rajat Subhra Chakraborty",Journal of Cryptographic Engineering,2022-07-16,"<a href=""Springer (2022-07-16) : A comprehensive survey of physical and logic testing techniques for Hardware Trojan detection and prevention"" target=""_blank"">[https://link.springer.com/article/10.1007/s13389-022-00295-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13389-022-00295-w]</a>",Hardware Trojans have emerged as a great threat to the trustability of modern electronic systems. A deployed electronic system with one or more...,,Springer
Machine unlearning: linear filtration for logit-based classifiers,"Thomas Baumhauer, Pascal Schöttle, Matthias Zeppelzauer",Machine Learning,2022-07-11,"<a href=""Springer (2022-07-11) : Machine unlearning: linear filtration for logit-based classifiers"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-022-06178-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-022-06178-9]</a>",Recently enacted legislation grants individuals certain rights to decide in what fashion their personal data may be used and in particular a “right...,,Springer
Invisible Backdoor Attacks Using Data Poisoning in the Frequency Domain,"Chang Yue, Peizhuo Lv, Ruigang Liang, Kai Chen","arXiv
arXiv","2022-07-09
2022-07","<a href=""arXiv (2022-07-09) : Invisible Backdoor Attacks Using Data Poisoning in the Frequency Domain"" target=""_blank"">[http://arxiv.org/abs/2207.04209v1]</a>
<a href=""DBLP (2022-07) : Invisible Backdoor Attacks Using Data Poisoning in the Frequency Domain"" target=""_blank"">[https://doi.org/10.48550/arXiv.2207.04209]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2207.04209]</a>","With the broad application of deep neural networks (DNNs), backdoor attacks have gradually attracted attention. Backdoor attacks are insidious, and poisoned models perform well on benign samples and are only triggered when given specific inputs, which cause the neural network to produce incorrect outputs. The state-of-the-art backdoor attack work is implemented by data poisoning, i.e., the attacker injects poisoned samples into the dataset, and the models trained with that dataset are infected with the backdoor. However, most of the triggers used in the current study are fixed patterns patched on a small fraction of an image and are often clearly mislabeled, which is easily detected by humans or defense methods such as Neural Cleanse and SentiNet. Also, it's difficult to be learned by DNNs without mislabeling, as they may ignore small patterns. In this paper, we propose a generalized backdoor attack method based on the frequency domain, which can implement backdoor implantation without mislabeling and accessing the training process. It is invisible to human beings and able to evade the commonly used defense methods. We evaluate our approach in the no-label and clean-label cases on three datasets (CIFAR-10, STL-10, and GTSRB) with two popular scenarios (self-supervised learning and supervised learning). The results show our approach can achieve a high attack success rate (above 90%) on all the tasks without significant performance degradation on main tasks. Also, we evaluate the bypass performance of our approach for different kinds of defenses, including the detection of training data (i.e., Activation Clustering), the preprocessing of inputs (i.e., Filtering), the detection of inputs (i.e., SentiNet), and the detection of models (i.e., Neural Cleanse). The experimental results demonstrate that our approach shows excellent robustness to such defenses.
","
","arXiv
DBLP"
Design and Evaluation of a Multi-Domain Trojan Detection Method on Deep Neural Networks,Y. Gao Y. Kim B. G. Doan Z. Zhang G. Zhang S. Nepal D. C. Ranasinghe H. Kim,IEEE Transactions on Dependable and Secure Computing,2022-07-08,"<a href=""IEEE (2022-07-08) : Design and Evaluation of a Multi-Domain Trojan Detection Method on Deep Neural Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9343758]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TDSC.2021.3055844]</a>","Trojan attacks on deep neural networks (DNNs) exploit a backdoor embedded in a DNN model that can hijack any input with an attacker’s chosen signature trigger. Emerging defence mechanisms are mainly designed and validated on vision domain tasks (e.g., image classification) on 2D Convolutional Neural Network (CNN) model architectures a defence mechanism that is general across vision, text, and audio domain tasks is demanded. This work designs and evaluates a run-time Trojan detection method exploiting STRong Intentional Perturbation of inputs that is a multi-domain input-agnostic Trojan detection defence across Vision, Text and Audio domains—thus termed as STRIP-ViTA. Specifically, STRIP-ViTA is demonstratively independent of not only task domain but also model architectures. Most importantly, unlike other detection mechanisms, it requires neither machine learning expertise nor expensive computational resource, which are the reason behind DNN model outsourcing scenario—one main attack surface of Trojan attack. We have extensively evaluated the performance of STRIP-ViTA over: i) CIFAR10 and GTSRB datasets using 2D CNNs for vision tasks ii) IMDB and consumer complaint datasets using both LSTM and 1D CNNs for text tasks and iii) speech command dataset using both 1D CNNs and 2D CNNs for audio tasks. Experimental results based on more than 30 tested Trojaned models (including publicly Trojaned model) corroborate that STRIP-ViTA performs well across all nine architectures and five datasets. Overall, STRIP-ViTA can effectively detect trigger inputs with small false acceptance rate (FAR) with an acceptable preset false rejection rate (FRR). In particular, for vision tasks, we can always achieve a 0 percent FRR and FAR given strong attack success rate always preferred by the attacker. By setting FRR to be 3 percent, average FAR of 1.1 and 3.55 percent are achieved for text and audio tasks, respectively. Moreover, we have evaluated STRIP-ViTA against a number of advanced backdoor attacks and compare its effectiveness with other recent state-of-the-arts.",,IEEE
Analysis of Cross-Domain Security and Privacy Aspects of Cyber-Physical Systems,"Kamal Prasat, S. Sanjay, ... Ramani Selvanambi",International Journal of Wireless Information Networks,2022-07-04,"<a href=""Springer (2022-07-04) : Analysis of Cross-Domain Security and Privacy Aspects of Cyber-Physical Systems"" target=""_blank"">[https://link.springer.com/article/10.1007/s10776-022-00559-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10776-022-00559-6]</a>","Cyber-physical systems (CPSs) are the recent surfacing of engineered systems in which the integration of computing, communication, and control...",,Springer
A method for protecting neural networks from computer backdoor attacks based on the trigger identification,Menisov A.B.,"Scientific and Technical Journal of Information Technologies, Mechanics and Optics",2022-07-01,"<a href=""ScienceDirect (2022-07-01) : A method for protecting neural networks from computer backdoor attacks based on the trigger identification"" target=""_blank"">[https://doi.org/10.17586/2226-1494-2022-22-4-742-750]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.17586/2226-1494-2022-22-4-742-750]</a>",,,ScienceDirect
Backdoor Sets on Nowhere Dense SAT,Lokshtanov D.,"Leibniz International Proceedings in Informatics, LIPIcs",2022-07-01,"<a href=""ScienceDirect (2022-07-01) : Backdoor Sets on Nowhere Dense SAT"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.ICALP.2022.91]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.ICALP.2022.91]</a>",,,ScienceDirect
PTB: Robust physical backdoor attacks against deep neural networks in real world,Xue M.,Computers and Security,2022-07-01,"<a href=""ScienceDirect (2022-07-01) : PTB: Robust physical backdoor attacks against deep neural networks in real world"" target=""_blank"">[https://doi.org/10.1016/j.cose.2022.102726]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.cose.2022.102726]</a>",,,ScienceDirect
The triggers that open the NLP model backdoors are hidden in the adversarial samples,Shao K.,Computers and Security,2022-07-01,"<a href=""ScienceDirect (2022-07-01) : The triggers that open the NLP model backdoors are hidden in the adversarial samples"" target=""_blank"">[https://doi.org/10.1016/j.cose.2022.102730]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.cose.2022.102730]</a>",,,ScienceDirect
VulnerGAN: a backdoor attack through vulnerability amplification against machine learning-based network intrusion detection systems,Liu G.,Science China Information Sciences,2022-07-01,"<a href=""ScienceDirect (2022-07-01) : VulnerGAN: a backdoor attack through vulnerability amplification against machine learning-based network intrusion detection systems"" target=""_blank"">[https://doi.org/10.1007/s11432-021-3455-1]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/s11432-021-3455-1]</a>",,,ScienceDirect
Backdoor Attack is A Devil in Federated GAN-based Medical Image Synthesis,"Ruinan Jin, Xiaoxiao Li",arXiv,2022-07,"<a href=""DBLP (2022-07) : Backdoor Attack is A Devil in Federated GAN-based Medical Image Synthesis"" target=""_blank"">[https://doi.org/10.48550/arXiv.2207.00762]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2207.00762]</a>",,,DBLP
Finding Backdoors to Integer Programs: A Monte Carlo Tree Search Framework,Khalil E.B.,"Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",2022-06-30,"<a href=""ScienceDirect (2022-06-30) : Finding Backdoors to Integer Programs: A Monte Carlo Tree Search Framework"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
On Probabilistic Generalization of Backdoors in Boolean Satisfiability,Semenov A.,"Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",2022-06-30,"<a href=""ScienceDirect (2022-06-30) : On Probabilistic Generalization of Backdoors in Boolean Satisfiability"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Remotizing and Virtualizing Chips and Circuits for Hardware-based Capture-the-Flag Challenges,G. Roascio S. Y. Cerini P. Prinetto,2022 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW),2022-06-27,"<a href=""IEEE (2022-06-27) : Remotizing and Virtualizing Chips and Circuits for Hardware-based Capture-the-Flag Challenges"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9799296]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/EuroSPW55150.2022.00057]</a>","In the very rapid digital revolution we are experiencing, the availability of cybersecurity experts becomes critical in every organization and at multiple levels. However, classical and theory-oriented training seems to lack effectiveness and power of attraction, while professional selection and training processes based on cybersecurity gamification are being successfully experimented, among which Capture-the-Flag (CTF) competitions certainly stand out. Nevertheless, careful analysis reveals that such initiatives have a major shortcoming in addressing security issues when training people to tackle hardware-related security issues. Several motivations can be identified, including the inadequate technical knowledge of the White Teams charged of the challenges preparations, and the evident logistic problems posed by the availability of real hardware devices when the numbers of trainees significantly scales up. This paper presents a platform able to provide as a service hardware-based CTF challenges and exercises, involving circuits and chips that can be physically connected to a server or simulated, to deal with topics such as hardware bugs, flaws and backdoors, vulnerabilities in test infrastructures, and side-channel attacks. The platform is presented from a technical perspective, and data for deducting related efficiency, stability and scalability are offered.",,IEEE
"SP 800–22 and GM/T 0005–2012 Tests: Clearly Obsolete, Possibly Harmful",M. -J. O. Saarinen,2022 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW),2022-06-27,"<a href=""IEEE (2022-06-27) : SP 800–22 and GM/T 0005–2012 Tests: Clearly Obsolete, Possibly Harmful"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9799325]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/EuroSPW55150.2022.00011]</a>","When it comes to cryptographic random number generation, poor understanding of the security requirements and “mythical aura” of black-box statistical testing frequently leads it to be used as a substitute for cryptanalysis. To make things worse, a seemingly standard document, NIST SP 800–22, describes 15 statistical tests and suggests that they can be used to evaluate random and pseudorandom number generators in cryptographic applications. The Chi-nese standard GM/T 0005–2012 describes similar tests. These documents have not aged well. The weakest pseudorandom number generators will easily pass these tests, promoting false confidence in insecure systems. We strongly suggest that SP 800–22 be withdrawn by NIST we consider it to be not just irrelevant but actively harmful. We illustrate this by discussing the “reference generators” contained in the SP 800–22 document itself. None of these generators are suitable for modern cryptography, yet they pass the tests. For future development, we suggest focusing on stochastic modeling of entropy sources instead of model-free statistical tests. Random bit generators should also be reviewed for potential asymmetric backdoors via trapdoor one-way functions, and for security against quantum computing attacks.",,IEEE
ROOTECTOR: Robust Android Rooting Detection Framework Using Machine Learning Algorithms,"Wael F. Elsersy, Nor Badrul Anuar, Mohd Faizal Ab Razak",Arabian Journal for Science and Engineering,2022-06-26,"<a href=""Springer (2022-06-26) : ROOTECTOR: Robust Android Rooting Detection Framework Using Machine Learning Algorithms"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-022-06949-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-022-06949-5]</a>","Recently, the newly launched Google protect service alerts Android users from installing rooting tools. However, Android users lean toward rooting...",,Springer
"Federated recommenders: methods, challenges and future","Zareen Alamgir, Farwa K. Khan, Saira Karim",Cluster Computing,2022-06-25,"<a href=""Springer (2022-06-25) : Federated recommenders: methods, challenges and future"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-022-03644-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-022-03644-w]</a>","Abstract Web users are flooded with information on the internet, and they feel overwhelmed by the different choices they have to make online daily....",,Springer
A federated learning model based on filtering strategy,"Jingchen Yan, Mingyu Sun, ... Jianbin Li",World Wide Web,2022-06-24,"<a href=""Springer (2022-06-24) : A federated learning model based on filtering strategy"" target=""_blank"">[https://link.springer.com/article/10.1007/s11280-022-01074-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11280-022-01074-7]</a>","With the increase of IoT terminals, data has also shown explosive growth in recent years. Due to the dispersion, low replication cost, and value...",,Springer
Grey wolf based feature reduction for intrusion detection in WSN using LSTM,"S. Karthic, S. Manoj Kumar, P. N. Senthil Prakash",International Journal of Information Technology,2022-06-24,"<a href=""Springer (2022-06-24) : Grey wolf based feature reduction for intrusion detection in WSN using LSTM"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-022-01015-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-022-01015-7]</a>","In the recent days all the digital devices including devices used in Healthcare, Personal Digital Assistance etc. are connected to the network,...",,Springer
Natural Backdoor Datasets,"Emily Wenger, Roma Bhattacharjee, Arjun Nitin Bhagoji, Josephine Passananti, Emilio Andere, Haitao Zheng, Ben Y. Zhao","arXiv
arXiv","2022-06-21
2022-06","<a href=""arXiv (2022-06-21) : Natural Backdoor Datasets"" target=""_blank"">[http://arxiv.org/abs/2206.10673v1]</a>
<a href=""DBLP (2022-06) : Natural Backdoor Datasets"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.10673]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.10673]</a>","Extensive literature on backdoor poison attacks has studied attacks and defenses for backdoors using ""digital trigger patterns."" In contrast, ""physical backdoors"" use physical objects as triggers, have only recently been identified, and are qualitatively different enough to resist all defenses targeting digital trigger backdoors. Research on physical backdoors is limited by access to large datasets containing real images of physical objects co-located with targets of classification. Building these datasets is time- and labor-intensive. This works seeks to address the challenge of accessibility for research on physical backdoor attacks. We hypothesize that there may be naturally occurring physically co-located objects already present in popular datasets such as ImageNet. Once identified, a careful relabeling of these data can transform them into training samples for physical backdoor attacks. We propose a method to scalably identify these subsets of potential triggers in existing datasets, along with the specific classes they can poison. We call these naturally occurring trigger-class subsets natural backdoor datasets. Our techniques successfully identify natural backdoors in widely-available datasets, and produce models behaviorally equivalent to those trained on manually curated datasets. We release our code to allow the research community to create their own datasets for research on physical backdoor attacks.
","
","arXiv
DBLP"
Algorithm substitution attacks against receivers,"Marcel Armour, Bertram Poettering",International Journal of Information Security,2022-06-21,"<a href=""Springer (2022-06-21) : Algorithm substitution attacks against receivers"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00596-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-022-00596-5]</a>",This work describes a class of Algorithm Substitution Attack (ASA) generically targeting the receiver of a communication between two parties. Our...,,Springer
Intelligent networking in adversarial environment: challenges and opportunities,"Yi Zhao, Ke Xu, ... Min Zhu",Science China Information Sciences,2022-06-21,"<a href=""Springer (2022-06-21) : Intelligent networking in adversarial environment: challenges and opportunities"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-021-3463-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-021-3463-9]</a>","Although deep learning technologies have been widely exploited in many fields, they are vulnerable to adversarial attacks by adding small...",,Springer
Causality-Based Neural Network Repair,B. Sun J. Sun L. H. Pham T. Shi,2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE),2022-06-20,"<a href=""IEEE (2022-06-20) : Causality-Based Neural Network Repair"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9793926]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1145/3510003.3510080]</a>","Neural networks have had discernible achievements in a wide range of applications. The wide-spread adoption also raises the concern of their dependability and reliability. Similar to traditional decision-making programs, neural networks can have defects that need to be repaired. The defects may cause unsafe behaviors, raise security concerns or unjust societal impacts. In this work, we address the problem of repairing a neural network for desirable properties such as fairness and the absence of backdoor. The goal is to construct a neural network that satisfies the property by (minimally) adjusting the given neural network's parameters (i.e., weights). Specifically, we propose CARE (CAusality-based REpair), a causality-based neural network repair technique that 1) performs causality-based fault localization to identify the ‘guilty’ neurons and 2) optimizes the parameters of the identified neurons to reduce the misbehavior. We have empirically evaluated CARE on various tasks such as backdoor removal, neural network repair for fairness and safety properties. Our experiment results show that CARE is able to repair all neural networks efficiently and effectively. For fairness repair tasks, CARE successfully improves fairness by 61.91 % on average. For backdoor removal tasks, CARE reduces the attack success rate from over 98% to less than 1 %. For safety property repair tasks, CARE reduces the property violation rate to less than 1 %. Results also show that thanks to the causality-based fault localization, CARE's repair focuses on the misbehavior and preserves the accuracy of the neural networks.",,IEEE
ReMoS: Reducing Defect Inheritance in Transfer Learning via Relevant Model Slicing,Z. Zhang Y. Li J. Wang B. Liu D. Li Y. Guo X. Chen Y. Liu,2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE),2022-06-20,"<a href=""IEEE (2022-06-20) : ReMoS: Reducing Defect Inheritance in Transfer Learning via Relevant Model Slicing"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9793881]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1145/3510003.3510191]</a>","Transfer learning is a popular software reuse technique in the deep learning community that enables developers to build custom mod-els (students) based on sophisticated pretrained models (teachers). However, like vulnerability inheritance in traditional software reuse, some defects in the teacher model may also be inherited by students, such as well-known adversarial vulnerabilities and backdoors. Re-ducing such defects is challenging since the student is unaware of how the teacher is trained and/or attacked. In this paper, we propose ReMoS, a relevant model slicing technique to reduce defect inheri-tance during transfer learning while retaining useful knowledge from the teacher model. Specifically, ReMoS computes a model slice (a subset of model weights) that is relevant to the student task based on the neuron coverage information obtained by profiling the teacher model on the student task. Only the relevant slice is used to fine-tune the student model, while the irrelevant weights are retrained from scratch to minimize the risk of inheriting defects. Our experi-ments on seven DNN defects, four DNN models, and eight datasets demonstrate that ReMoS can reduce inherited defects effectively (by 63% to 86% for CV tasks and by 40% to 61 % for NLP tasks) and efficiently with minimal sacrifice of accuracy (3% on average).",,IEEE
DECK: Model Hardening for Defending Pervasive Backdoors,"Guanhong Tao, Yingqi Liu, Siyuan Cheng, Shengwei An, Zhuo Zhang, Qiuling Xu, Guangyu Shen, Xiangyu Zhang","arXiv
arXiv","2022-06-18
2022-06","<a href=""arXiv (2022-06-18) : DECK: Model Hardening for Defending Pervasive Backdoors"" target=""_blank"">[http://arxiv.org/abs/2206.09272v1]</a>
<a href=""DBLP (2022-06) : DECK: Model Hardening for Defending Pervasive Backdoors"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.09272]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.09272]</a>","Pervasive backdoors are triggered by dynamic and pervasive input perturbations. They can be intentionally injected by attackers or naturally exist in normally trained models. They have a different nature from the traditional static and localized backdoors that can be triggered by perturbing a small input area with some fixed pattern, e.g., a patch with solid color. Existing defense techniques are highly effective for traditional backdoors. However, they may not work well for pervasive backdoors, especially regarding backdoor removal and model hardening. In this paper, we propose a novel model hardening technique against pervasive backdoors, including both natural and injected backdoors. We develop a general pervasive attack based on an encoder-decoder architecture enhanced with a special transformation layer. The attack can model a wide range of existing pervasive backdoor attacks and quantify them by class distances. As such, using the samples derived from our attack in adversarial training can harden a model against these backdoor vulnerabilities. Our evaluation on 9 datasets with 15 model structures shows that our technique can enlarge class distances by 59.65% on average with less than 1% accuracy degradation and no robustness loss, outperforming five hardening techniques such as adversarial training, universal adversarial training, MOTH, etc. It can reduce the attack success rate of six pervasive backdoor attacks from 99.06% to 1.94%, surpassing seven state-of-the-art backdoor removal techniques.
","
","arXiv
DBLP"
Backdoor Attacks on Vision Transformers,"Akshayvarun Subramanya, Aniruddha Saha, Soroush Abbasi Koohpayegani, Ajinkya Tejankar, Hamed Pirsiavash","arXiv
arXiv","2022-06-16
2022-06","<a href=""arXiv (2022-06-16) : Backdoor Attacks on Vision Transformers"" target=""_blank"">[http://arxiv.org/abs/2206.08477v1]</a>
<a href=""DBLP (2022-06) : Backdoor Attacks on Vision Transformers"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.08477]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.08477]</a>","Vision Transformers (ViT) have recently demonstrated exemplary performance on a variety of vision tasks and are being used as an alternative to CNNs. Their design is based on a self-attention mechanism that processes images as a sequence of patches, which is quite different compared to CNNs. Hence it is interesting to study if ViTs are vulnerable to backdoor attacks. Backdoor attacks happen when an attacker poisons a small part of the training data for malicious purposes. The model performance is good on clean test images, but the attacker can manipulate the decision of the model by showing the trigger at test time. To the best of our knowledge, we are the first to show that ViTs are vulnerable to backdoor attacks. We also find an intriguing difference between ViTs and CNNs - interpretation algorithms effectively highlight the trigger on test images for ViTs but not for CNNs. Based on this observation, we propose a test-time image blocking defense for ViTs which reduces the attack success rate by a large margin. Code is available here: https://github.com/UCDvision/backdoor_transformer.git
","<a href=""arXiv"" target=""_blank"">[https://github.com/UCDvision/backdoor_transformer.git]</a>
","arXiv
DBLP"
RobustFL: Robust Federated Learning Against Poisoning Attacks in Industrial IoT Systems,J. Zhang C. Ge F. Hu B. Chen,IEEE Transactions on Industrial Informatics,2022-06-14,"<a href=""IEEE (2022-06-14) : RobustFL: Robust Federated Learning Against Poisoning Attacks in Industrial IoT Systems"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645297]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TII.2021.3132954]</a>","Industrial Internet of Things (IIoT) systems are key enabling infrastructures that sustain the functioning of production and manufacturing. To satisfy the intelligence demands, federated learning has been envisioned as a promising technique for IIoT applications with privacy training requirements. However, research works have shown that, by training the local model on crafted poisoning samples malicious participants can jeopardize the functionalities of the global model. In this article, we propose a robust federated learning method, named RobustFL, in IIoT systems to defend against poisoning attacks. The main idea is that we conduct an adversarial training framework, in which an extra logits-based predictive model is built at the server-side to predict which participant a given logit belongs to. Meanwhile, the federated model is adversarially trained to prevent this predictive behavior, thus mitigating the poisoning attack influences. We evaluate the poisoning attack and our defense method on three benchmark datasets. Experimental results demonstrate the superiority of our proposed method in terms of high accuracy and efficiency in defending against poisoning attacks.",,IEEE
Secure Partial Aggregation: Making Federated Learning More Robust for Industry 4.0 Applications,J. Gao B. Zhang X. Guo T. Baker M. Li Z. Liu,IEEE Transactions on Industrial Informatics,2022-06-14,"<a href=""IEEE (2022-06-14) : Secure Partial Aggregation: Making Federated Learning More Robust for Industry 4.0 Applications"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9692911]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TII.2022.3145837]</a>","Big data, due to its promotion for industrial intelligence, has become the cornerstone of the Industry 4.0 era. Federated learning, proposed by Google, can effectively integrate data from different devices and different domains to train models under the premise of privacy preservation. Unfortunately, this new training paradigm faces security risks both on the client side and server side. This article proposes a new federated learning scheme to defend from client-side malicious uploads (e.g., backdoor attacks). In addition, we use cryptography techniques to prevent server-side privacy attacks (e.g., membership inference). The secure partial aggregation protocol we designed improves the privacy and robustness of federated learning. The experiments show that models can achieve high accuracy of over 90% with a proper upload proportion, while the accuracy of the backdoor attack decreased from 99.5% to 0% with the best result. Meanwhile, we prove that our protocol can disable privacy attacks.",,IEEE
Enhancing Clean Label Backdoor Attack with Two-phase Specific Triggers,"Nan Luo, Yuanzhang Li, Yajie Wang, Shangbo Wu, Yu-an Tan, Quanxin Zhang","arXiv
arXiv","2022-06-10
2022-06","<a href=""arXiv (2022-06-10) : Enhancing Clean Label Backdoor Attack with Two-phase Specific Triggers"" target=""_blank"">[http://arxiv.org/abs/2206.04881v1]</a>
<a href=""DBLP (2022-06) : Enhancing Clean Label Backdoor Attack with Two-phase Specific Triggers"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.04881]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.04881]</a>","Backdoor attacks threaten Deep Neural Networks (DNNs). Towards stealthiness, researchers propose clean-label backdoor attacks, which require the adversaries not to alter the labels of the poisoned training datasets. Clean-label settings make the attack more stealthy due to the correct image-label pairs, but some problems still exist: first, traditional methods for poisoning training data are ineffective, second, traditional triggers are not stealthy which are still perceptible. To solve these problems, we propose a two-phase and image-specific triggers generation method to enhance clean-label backdoor attacks. Our methods are (1) powerful: our triggers can both promote the two phases (i.e., the backdoor implantation and activation phase) in backdoor attacks simultaneously, (2) stealthy: our triggers are generated from each image. They are image-specific instead of fixed triggers. Extensive experiments demonstrate that our approach can achieve a fantastic attack success rate~(98.98%) with low poisoning rate~(5%), high stealthiness under many evaluation metrics and is resistant to backdoor defense methods.
","
","arXiv
DBLP"
Membership Inference via Backdooring,"Hongsheng Hu, Zoran Salcic, Gillian Dobbie, Jinjun Chen, Lichao Sun, Xuyun Zhang","arXiv
IJCAI
arXiv","2022-06-10
2022
2022-06","<a href=""arXiv (2022-06-10) : Membership Inference via Backdooring"" target=""_blank"">[http://arxiv.org/abs/2206.04823v1]</a>
<a href=""DBLP (2022) : Membership Inference via Backdooring"" target=""_blank"">[https://doi.org/10.24963/ijcai.2022/532]</a>
<a href=""DBLP (2022-06) : Membership Inference via Backdooring"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.04823]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.24963/ijcai.2022/532]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.04823]</a>","Recently issued data privacy regulations like GDPR (General Data Protection Regulation) grant individuals the right to be forgotten. In the context of machine learning, this requires a model to forget about a training data sample if requested by the data owner (i.e., machine unlearning). As an essential step prior to machine unlearning, it is still a challenge for a data owner to tell whether or not her data have been used by an unauthorized party to train a machine learning model. Membership inference is a recently emerging technique to identify whether a data sample was used to train a target model, and seems to be a promising solution to this challenge. However, straightforward adoption of existing membership inference approaches fails to address the challenge effectively due to being originally designed for attacking membership privacy and suffering from several severe limitations such as low inference accuracy on well-generalized models. In this paper, we propose a novel membership inference approach inspired by the backdoor technology to address the said challenge. Specifically, our approach of Membership Inference via Backdooring (MIB) leverages the key observation that a backdoored model behaves very differently from a clean model when predicting on deliberately marked samples created by a data owner. Appealingly, MIB requires data owners' marking a small number of samples for membership inference and only black-box access to the target model, with theoretical guarantees for inference results. We perform extensive experiments on various datasets and deep neural network architectures, and the results validate the efficacy of our approach, e.g., marking only 0.1% of the training dataset is practically sufficient for effective membership inference.

","

","arXiv
DBLP
DBLP"
Backdoor attacks-resilient aggregation based on Robust Filtering of Outliers in federated learning for image classification,Rodríguez-Barroso N.,Knowledge-Based Systems,2022-06-07,"<a href=""ScienceDirect (2022-06-07) : Backdoor attacks-resilient aggregation based on Robust Filtering of Outliers in federated learning for image classification"" target=""_blank"">[https://doi.org/10.1016/j.knosys.2022.108588]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.knosys.2022.108588]</a>",,,ScienceDirect
A qualitative study of developers’ discussions of their problems and joys during the early COVID-19 months,"Gias Uddin, Omar Alam, Alexander Serebrenik",Empirical Software Engineering,2022-06-04,"<a href=""Springer (2022-06-04) : A qualitative study of developers’ discussions of their problems and joys during the early COVID-19 months"" target=""_blank"">[https://link.springer.com/article/10.1007/s10664-022-10156-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10664-022-10156-z]</a>",Many software developers started to work from home on a short notice during the early periods of COVID-19. A number of previous papers have studied...,,Springer
Kallima: A Clean-label Framework for Textual Backdoor Attacks,"Xiaoyi Chen, Yinpeng Dong, Zeyu Sun, Shengfang Zhai, Qingni Shen, Zhonghai Wu","arXiv
ESORICS
arXiv","2022-06-03
2022
2022-06","<a href=""arXiv (2022-06-03) : Kallima: A Clean-label Framework for Textual Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2206.01832v1]</a>
<a href=""DBLP (2022) : Kallima: A Clean-Label Framework for Textual Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1007/978-3-031-17140-6_22]</a>
<a href=""DBLP (2022-06) : Kallima: A Clean-label Framework for Textual Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.01832]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-17140-6_22]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.01832]</a>","Although Deep Neural Network (DNN) has led to unprecedented progress in various natural language processing (NLP) tasks, research shows that deep models are extremely vulnerable to backdoor attacks. The existing backdoor attacks mainly inject a small number of poisoned samples into the training dataset with the labels changed to the target one. Such mislabeled samples would raise suspicion upon human inspection, potentially revealing the attack. To improve the stealthiness of textual backdoor attacks, we propose the first clean-label framework Kallima for synthesizing mimesis-style backdoor samples to develop insidious textual backdoor attacks. We modify inputs belonging to the target class with adversarial perturbations, making the model rely more on the backdoor trigger. Our framework is compatible with most existing backdoor triggers. The experimental results on three benchmark datasets demonstrate the effectiveness of the proposed method.

","

","arXiv
DBLP
DBLP"
A temporal chrominance trigger for clean-label backdoor attack against anti-spoof rebroadcast detection,"Wei Guo, Benedetta Tondi, Mauro Barni","arXiv
arXiv","2022-06-02
2022-06","<a href=""arXiv (2022-06-02) : A temporal chrominance trigger for clean-label backdoor attack against anti-spoof rebroadcast detection"" target=""_blank"">[http://arxiv.org/abs/2206.01102v1]</a>
<a href=""DBLP (2022-06) : A temporal chrominance trigger for clean-label backdoor attack against anti-spoof rebroadcast detection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.01102]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.01102]</a>","We propose a stealthy clean-label video backdoor attack against Deep Learning (DL)-based models aiming at detecting a particular class of spoofing attacks, namely video rebroadcast attacks. The injected backdoor does not affect spoofing detection in normal conditions, but induces a misclassification in the presence of a specific triggering signal. The proposed backdoor relies on a temporal trigger altering the average chrominance of the video sequence. The backdoor signal is designed by taking into account the peculiarities of the Human Visual System (HVS) to reduce the visibility of the trigger, thus increasing the stealthiness of the backdoor. To force the network to look at the presence of the trigger in the challenging clean-label scenario, we choose the poisoned samples used for the injection of the backdoor following a so-called Outlier Poisoning Strategy (OPS). According to OPS, the triggering signal is inserted in the training samples that the network finds more difficult to classify. The effectiveness of the proposed backdoor attack and its generality are validated experimentally on different datasets and anti-spoofing rebroadcast detection architectures.
","
","arXiv
DBLP"
PerDoor: Persistent Non-Uniform Backdoors in Federated Learning using Adversarial Perturbations,"Manaar Alam, Esha Sarkar, Michail Maniatakos","arXiv
COINS
arXiv","2022-06-01
2023
2022-05","<a href=""arXiv (2022-06-01) : PerDoor: Persistent Non-Uniform Backdoors in Federated Learning using Adversarial Perturbations"" target=""_blank"">[http://arxiv.org/abs/2205.13523v2]</a>
<a href=""DBLP (2023) : PerDoor: Persistent Backdoors in Federated Learning using Adversarial Perturbations"" target=""_blank"">[https://doi.org/10.1109/COINS57856.2023.10189281]</a>
<a href=""DBLP (2022-05) : PerDoor: Persistent Non-Uniform Backdoors in Federated Learning using Adversarial Perturbations"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.13523]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/COINS57856.2023.10189281]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.13523]</a>","Federated Learning (FL) enables numerous participants to train deep learning models collaboratively without exposing their personal, potentially sensitive data, making it a promising solution for data privacy in collaborative training. The distributed nature of FL and unvetted data, however, makes it inherently vulnerable to backdoor attacks: In this scenario, an adversary injects backdoor functionality into the centralized model during training, which can be triggered to cause the desired misclassification for a specific adversary-chosen input. A range of prior work establishes successful backdoor injection in an FL system, however, these backdoors are not demonstrated to be long-lasting. The backdoor functionality does not remain in the system if the adversary is removed from the training process since the centralized model parameters continuously mutate during successive FL training rounds. Therefore, in this work, we propose PerDoor, a persistent-by-construction backdoor injection technique for FL, driven by adversarial perturbation and targeting parameters of the centralized model that deviate less in successive FL rounds and contribute the least to the main task accuracy. An exhaustive evaluation considering an image classification scenario portrays on average $10.5\times$ persistence over multiple FL rounds compared to traditional backdoor attacks. Through experiments, we further exhibit the potency of PerDoor in the presence of state-of-the-art backdoor prevention techniques in an FL system. Additionally, the operation of adversarial perturbation also assists PerDoor in developing non-uniform trigger patterns for backdoor inputs compared to uniform triggers (with fixed patterns and locations) of existing backdoor techniques, which are prone to be easily mitigated.

","

","arXiv
DBLP
DBLP"
A flexible approach for cyber threat hunting based on kernel audit records,"Fengyu Yang, Yanni Han, ... Zhen Xu",Cybersecurity,2022-06-01,"<a href=""Springer (2022-06-01) : A flexible approach for cyber threat hunting based on kernel audit records"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-022-00111-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-022-00111-2]</a>","Hunting the advanced threats hidden in the enterprise networks has always been a complex and difficult task. Due to the variety of attacking means,...",,Springer
Classic and backdoor pathways of androgen biosynthesis in human sexual development,Lee H.G.,Annals of Pediatric Endocrinology and Metabolism,2022-06-01,"<a href=""ScienceDirect (2022-06-01) : Classic and backdoor pathways of androgen biosynthesis in human sexual development"" target=""_blank"">[https://doi.org/10.6065/apem.2244124.062]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.6065/apem.2244124.062]</a>",,,ScienceDirect
DriNet: Dynamic Backdoor Attack against Automatic Speech Recognization Models,Ye J.,Applied Sciences (Switzerland),2022-06-01,"<a href=""ScienceDirect (2022-06-01) : DriNet: Dynamic Backdoor Attack against Automatic Speech Recognization Models"" target=""_blank"">[https://doi.org/10.3390/app12125786]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/app12125786]</a>",,,ScienceDirect
Model Agnostic Defence Against Backdoor Attacks in Machine Learning,Udeshi S.,IEEE Transactions on Reliability,2022-06-01,"<a href=""ScienceDirect (2022-06-01) : Model Agnostic Defence Against Backdoor Attacks in Machine Learning"" target=""_blank"">[https://doi.org/10.1109/TR.2022.3159784]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TR.2022.3159784]</a>",,,ScienceDirect
"A highly efficient, confidential, and continuous federated learning backdoor attack strategy","Jiarui Cao, liehuang Zhu","ICMLC '22: Proceedings of the 2022 14th International Conference on Machine Learning and Computing
ICMLC","2022-06
2022","<a href=""ACM (2022-06) : A highly efficient, confidential, and continuous federated learning backdoor attack strategy"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3529836.3529845]</a>
<a href=""DBLP (2022) : A highly efficient, confidential, and continuous federated learning backdoor attack strategy"" target=""_blank"">[https://doi.org/10.1145/3529836.3529845]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3529836.3529845]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3529836.3529845]</a>","Federated learning is a kind of distributed machine learning. Researchers have conducted extensive research on federated learning's security defences and backdoor attacks. However, most studies are based on the assumption federated learning participant's ...
","
","ACM
DBLP"
Can Backdoor Attacks Survive Time-Varying Models?,"Huiying Li, Arjun Nitin Bhagoji, Ben Y. Zhao, Haitao Zheng",arXiv,2022-06,"<a href=""DBLP (2022-06) : Can Backdoor Attacks Survive Time-Varying Models?"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.04677]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.04677]</a>",,,DBLP
Extracting a Minimal Trigger for an Efficient Backdoor Poisoning Attack Using the Activation Values of a Deep Neural Network,Na H.,WDC 2022 - Proceedings of the 1st Workshop on Security Implications of Deepfakes and Cheapfakes,2022-05-30,"<a href=""ScienceDirect (2022-05-30) : Extracting a Minimal Trigger for an Efficient Backdoor Poisoning Attack Using the Activation Values of a Deep Neural Network"" target=""_blank"">[https://doi.org/10.1145/3494109.3527192]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3494109.3527192]</a>",,,ScienceDirect
FLARE: Defending Federated Learning against Model Poisoning Attacks via Latent Space Representations,Wang N.,ASIA CCS 2022 - Proceedings of the 2022 ACM Asia Conference on Computer and Communications Security,2022-05-30,"<a href=""ScienceDirect (2022-05-30) : FLARE: Defending Federated Learning against Model Poisoning Attacks via Latent Space Representations"" target=""_blank"">[https://doi.org/10.1145/3488932.3517395]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3488932.3517395]</a>",,,ScienceDirect
The Detection of Malicious Modifications in the FPGA,Kamran Zahid,Journal of Electronic Testing,2022-05-30,"<a href=""Springer (2022-05-30) : The Detection of Malicious Modifications in the FPGA"" target=""_blank"">[https://link.springer.com/article/10.1007/s10836-022-06004-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10836-022-06004-z]</a>","Field Programmable Gate Arrays (FPGAs) are being widely used in a variety of embedded applications. Due to their programmable feature, FPGAs are the...",,Springer
Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World,"Hua Ma, Yinshan Li, Yansong Gao, Alsharif Abuadbba, Zhi Zhang, Anmin Fu, Hyoungshick Kim, Said F. Al-Sarawi, Nepal Surya, Derek Abbott","arXiv
arXiv","2022-05-29
2022-01","<a href=""arXiv (2022-05-29) : Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World"" target=""_blank"">[http://arxiv.org/abs/2201.08619v2]</a>
<a href=""DBLP (2022-01) : Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World"" target=""_blank"">[https://arxiv.org/abs/2201.08619]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2201.08619]</a>","Deep learning models have been shown to be vulnerable to recent backdoor attacks. A backdoored model behaves normally for inputs containing no attacker-secretly-chosen trigger and maliciously for inputs with the trigger. To date, backdoor attacks and countermeasures mainly focus on image classification tasks. And most of them are implemented in the digital world with digital triggers. Besides the classification tasks, object detection systems are also considered as one of the basic foundations of computer vision tasks. However, there is no investigation and understanding of the backdoor vulnerability of the object detector, even in the digital world with digital triggers. For the first time, this work demonstrates that existing object detectors are inherently susceptible to physical backdoor attacks. We use a natural T-shirt bought from a market as a trigger to enable the cloaking effect--the person bounding-box disappears in front of the object detector. We show that such a backdoor can be implanted from two exploitable attack scenarios into the object detector, which is outsourced or fine-tuned through a pretrained model. We have extensively evaluated three popular object detection algorithms: anchor-based Yolo-V3, Yolo-V4, and anchor-free CenterNet. Building upon 19 videos shot in real-world scenes, we confirm that the backdoor attack is robust against various factors: movement, distance, angle, non-rigid deformation, and lighting. Specifically, the attack success rate (ASR) in most videos is 100% or close to it, while the clean data accuracy of the backdoored model is the same as its clean counterpart. The latter implies that it is infeasible to detect the backdoor behavior merely through a validation set. The averaged ASR still remains sufficiently high to be 78% in the transfer learning attack scenarios evaluated on CenterNet. See the demo video on https://youtu.be/Q3HOF4OobbY.
","
","arXiv
DBLP"
Contributor-Aware Defenses Against Adversarial Backdoor Attacks,"Glenn Dawson, Muhammad Umer, Robi Polikar","arXiv
arXiv","2022-05-28
2022-06","<a href=""arXiv (2022-05-28) : Contributor-Aware Defenses Against Adversarial Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2206.03583v1]</a>
<a href=""DBLP (2022-06) : Contributor-Aware Defenses Against Adversarial Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.03583]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.03583]</a>","Deep neural networks for image classification are well-known to be vulnerable to adversarial attacks. One such attack that has garnered recent attention is the adversarial backdoor attack, which has demonstrated the capability to perform targeted misclassification of specific examples. In particular, backdoor attacks attempt to force a model to learn spurious relations between backdoor trigger patterns and false labels. In response to this threat, numerous defensive measures have been proposed, however, defenses against backdoor attacks focus on backdoor pattern detection, which may be unreliable against novel or unexpected types of backdoor pattern designs. We introduce a novel re-contextualization of the adversarial setting, where the presence of an adversary implicitly admits the existence of multiple database contributors. Then, under the mild assumption of contributor awareness, it becomes possible to exploit this knowledge to defend against backdoor attacks by destroying the false label associations. We propose a contributor-aware universal defensive framework for learning in the presence of multiple, potentially adversarial data sources that utilizes semi-supervised ensembles and learning from crowds to filter the false labels produced by adversarial triggers. Importantly, this defensive strategy is agnostic to backdoor pattern design, as it functions without needing -- or even attempting -- to perform either adversary identification or backdoor pattern detection during either training or inference. Our empirical studies demonstrate the robustness of the proposed framework against adversarial backdoor attacks from multiple simultaneous adversaries.
","
","arXiv
DBLP"
Defending Against Stealthy Backdoor Attacks,"Sangeet Sagar, Abhinav Bhatt, Abhijith Srinivas Bidaralli","arXiv
arXiv","2022-05-27
2022-05","<a href=""arXiv (2022-05-27) : Defending Against Stealthy Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2205.14246v1]</a>
<a href=""DBLP (2022-05) : Defending Against Stealthy Backdoor Attacks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.14246]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.14246]</a>","Defenses against security threats have been an interest of recent studies. Recent works have shown that it is not difficult to attack a natural language processing (NLP) model while defending against them is still a cat-mouse game. Backdoor attacks are one such attack where a neural network is made to perform in a certain way on specific samples containing some triggers while achieving normal results on other samples. In this work, we present a few defense strategies that can be useful to counter against such an attack. We show that our defense methodologies significantly decrease the performance on the attacked inputs while maintaining similar performance on benign inputs. We also show that some of our defenses have very less runtime and also maintain similarity with the original inputs.
","
","arXiv
DBLP"
Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks,"Xiangyu Qi, Tinghao Xie, Ruizhe Pan, Jifeng Zhu, Yong Yang, Kai Bu","arXiv
CVPR
arXiv","2022-05-26
2022
2021-11","<a href=""arXiv (2022-05-26) : Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2111.12965v2]</a>
<a href=""DBLP (2022) : Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.01299]</a>
<a href=""DBLP (2021-11) : Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2111.12965]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.01299]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2111.12965]</a>","One major goal of the AI security community is to securely and reliably produce and deploy deep learning models for real-world applications. To this end, data poisoning based backdoor attacks on deep neural networks (DNNs) in the production stage (or training stage) and corresponding defenses are extensively explored in recent years. Ironically, backdoor attacks in the deployment stage, which can often happen in unprofessional users' devices and are thus arguably far more threatening in real-world scenarios, draw much less attention of the community. We attribute this imbalance of vigilance to the weak practicality of existing deployment-stage backdoor attack algorithms and the insufficiency of real-world attack demonstrations. To fill the blank, in this work, we study the realistic threat of deployment-stage backdoor attacks on DNNs. We base our study on a commonly used deployment-stage attack paradigm -- adversarial weight attack, where adversaries selectively modify model weights to embed backdoor into deployed DNNs. To approach realistic practicality, we propose the first gray-box and physically realizable weights attack algorithm for backdoor injection, namely subnet replacement attack (SRA), which only requires architecture information of the victim model and can support physical triggers in the real world. Extensive experimental simulations and system-level real-world attack demonstrations are conducted. Our results not only suggest the effectiveness and practicality of the proposed attack algorithm, but also reveal the practical risk of a novel type of computer virus that may widely spread and stealthily inject backdoor into DNN models in user devices. By our study, we call for more attention to the vulnerability of DNNs in the deployment stage.

","

","arXiv
DBLP
DBLP"
A Survey on Long-Tailed Visual Recognition,"Lu Yang, He Jiang, ... Jun Guo",International Journal of Computer Vision,2022-05-25,"<a href=""Springer (2022-05-25) : A Survey on Long-Tailed Visual Recognition"" target=""_blank"">[https://link.springer.com/article/10.1007/s11263-022-01622-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11263-022-01622-8]</a>",The heavy reliance on data is one of the major reasons that currently limit the development of deep learning. Data quality directly dominates the...,,Springer
Anomaly-based intrusion detection system in IoT using kernel extreme learning machine,"Sawssen Bacha, Ahamed Aljuhani, ... Mamoun Alazab",Journal of Ambient Intelligence and Humanized Computing,2022-05-25,"<a href=""Springer (2022-05-25) : Anomaly-based intrusion detection system in IoT using kernel extreme learning machine"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-022-03887-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-022-03887-w]</a>","The Internet of Things (IoT) has developed rapidly and been integrated with a variety of domains. Such a technology allows devices to send, receive,...",,Springer
Reversing Kia Motors Head Unit to discover and exploit software vulnerabilities,"Gianpiero Costantino, Ilaria Matteucci",Journal of Computer Virology and Hacking Techniques,2022-05-25,"<a href=""Springer (2022-05-25) : Reversing Kia Motors Head Unit to discover and exploit software vulnerabilities"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-022-00430-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-022-00430-5]</a>","Modern vehicles resemble four-wheels computers connected to the Internet via their In-Vehicle Infotainment (IVI) systems. As with PCs in the past,...",,Springer
ORaBaN: an optimized radial basis neuro framework for anomaly detection in large networks,"N. G. Bhuvaneswari Amma, P. Valarmathi",International Journal of Information Technology,2022-05-22,"<a href=""Springer (2022-05-22) : ORaBaN: an optimized radial basis neuro framework for anomaly detection in large networks"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-022-00991-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-022-00991-0]</a>",The ever-expanding traffic over the Internet has resulted in a multitude of security threats as a result of the growth of traffic over large-scale...,,Springer
Design of AI Trojans for Evading Machine Learning-based Detection of Hardware Trojans,Z. Pan P. Mishra,"2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)",2022-05-19,"<a href=""IEEE (2022-05-19) : Design of AI Trojans for Evading Machine Learning-based Detection of Hardware Trojans"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774654]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.23919/DATE54114.2022.9774654]</a>","The globalized semiconductor supply chain significantly increases the risk of exposing System-on-Chip (SoC) designs to malicious implants, popularly known as hardware Trojans. Traditional simulation-based validation is unsuitable for detection of carefully-crafted hardware Trojans with extremely rare trigger conditions. While machine learning (ML) based Trojan detection approaches are promising due to their scalability as well as detection accuracy, ML-based methods themselves are vulnerable from Trojan attacks. In this paper, we propose a robust backdoor attack on ML-based Trojan detection algorithms to demonstrate this serious vulnerability. The proposed framework is able to design an AI Trojan and implant it inside the ML model that can be triggered by specific inputs. Experimental results demonstrate that the proposed AI Trojans can bypass state-of-the-art defense algorithms. Moreover, our approach provides a fast and cost-effective solution in achieving 100% attack success rate that significantly outperforms state-of-the art approaches based on adversarial attacks.",,IEEE
Mind the Scaling Factors: Resilience Analysis of Quantized Adversarially Robust CNNs,N. Fasfous L. Frickenstein M. Neumeier M. R. Vemparala A. Frickenstein E. Valpreda M. Martina W. Stechele,"2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)",2022-05-19,"<a href=""IEEE (2022-05-19) : Mind the Scaling Factors: Resilience Analysis of Quantized Adversarially Robust CNNs"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774686]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.23919/DATE54114.2022.9774686]</a>","As more deep learning algorithms enter safety-critical application domains, the importance of analyzing their resilience against hardware faults cannot be overstated. Most existing works focus on bit-flips in memory, fewer focus on compute errors, and almost none study the effect of hardware faults on adversarially trained convolutional neural networks (CNNs). In this work, we show that adversarially trained CNNs are more susceptible to failure due to hardware errors when compared to vanilla-trained models. We identify large differences in the quantization scaling factors of the CNNs which are resilient to hardware faults and those which are not. As adversarially trained CNNs learn robustness against input attack perturbations, their internal weight and activation distributions open a backdoor for injecting large magnitude hardware faults. We propose a simple weight decay remedy for adversarially trained models to maintain adversarial robustness and hardware resilience in the same CNN. We improve the fault resilience of an adversarially trained ResNet56 by 25% for large-scale bit-flip benchmarks on activation data while gaining slightly improved accuracy and adversarial robustness.",,IEEE
Backdoor Attacks on Bayesian Neural Networks using Reverse Distribution,"Zhixin Pan, Prabhat Mishra","arXiv
arXiv","2022-05-18
2022-05","<a href=""arXiv (2022-05-18) : Backdoor Attacks on Bayesian Neural Networks using Reverse Distribution"" target=""_blank"">[http://arxiv.org/abs/2205.09167v1]</a>
<a href=""DBLP (2022-05) : Backdoor Attacks on Bayesian Neural Networks using Reverse Distribution"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.09167]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.09167]</a>","Due to cost and time-to-market constraints, many industries outsource the training process of machine learning models (ML) to third-party cloud service providers, popularly known as ML-asa-Service (MLaaS). MLaaS creates opportunity for an adversary to provide users with backdoored ML models to produce incorrect predictions only in extremely rare (attacker-chosen) scenarios. Bayesian neural networks (BNN) are inherently immune against backdoor attacks since the weights are designed to be marginal distributions to quantify the uncertainty. In this paper, we propose a novel backdoor attack based on effective learning and targeted utilization of reverse distribution. This paper makes three important contributions. (1) To the best of our knowledge, this is the first backdoor attack that can effectively break the robustness of BNNs. (2) We produce reverse distributions to cancel the original distributions when the trigger is activated. (3) We propose an efficient solution for merging probability distributions in BNNs. Experimental results on diverse benchmark datasets demonstrate that our proposed attack can achieve the attack success rate (ASR) of 100%, while the ASR of the state-of-the-art attacks is lower than 60%.
","
","arXiv
DBLP"
Model-Contrastive Learning for Backdoor Defense,"Zhihao Yue, Jun Xia, Zhiwei Ling, Ming Hu, Ting Wang, Xian Wei, Mingsong Chen","arXiv
arXiv","2022-05-17
2022-05","<a href=""arXiv (2022-05-17) : Model-Contrastive Learning for Backdoor Defense"" target=""_blank"">[http://arxiv.org/abs/2205.04411v2]</a>
<a href=""DBLP (2022-05) : Model-Contrastive Learning for Backdoor Defense"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.04411]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.04411]</a>","Due to the popularity of Artificial Intelligence (AI) techniques, we are witnessing an increasing number of backdoor injection attacks that are designed to maliciously threaten Deep Neural Networks (DNNs) causing misclassification. Although there exist various defense methods that can effectively erase backdoors from DNNs, they greatly suffer from both high Attack Success Rate (ASR) and a non-negligible loss in Benign Accuracy (BA). Inspired by the observation that a backdoored DNN tends to form a new cluster in its feature spaces for poisoned data, in this paper we propose a novel two-stage backdoor defense method, named MCLDef, based on Model-Contrastive Learning (MCL). In the first stage, our approach performs trigger inversion based on trigger synthesis, where the resultant trigger can be used to generate poisoned data. In the second stage, under the guidance of MCL and our defined positive and negative pairs, MCLDef can purify the backdoored model by pulling the feature representations of poisoned data towards those of their clean data counterparts. Due to the shrunken cluster of poisoned data, the backdoor formed by end-to-end supervised learning is eliminated. Comprehensive experimental results show that, with only 5% of clean data, MCLDef significantly outperforms state-of-the-art defense methods by up to 95.79% reduction in ASR, while in most cases the BA degradation can be controlled within less than 2%. Our code is available at https://github.com/WeCanShow/MCL.
","<a href=""arXiv"" target=""_blank"">[https://github.com/WeCanShow/MCL]</a>
","arXiv
DBLP"
Towards enhanced PDF maldocs detection with feature engineering: design challenges,"Ahmed Falah, Shiva Raj Pokhrel, ... Anthony de Souza-Daw",Multimedia Tools and Applications,2022-05-17,"<a href=""Springer (2022-05-17) : Towards enhanced PDF maldocs detection with feature engineering: design challenges"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-022-11960-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-022-11960-x]</a>","In this paper, we perform an in-depth analysis of a large corpus of PDF maldocs to identify the key set of significantly important features and help...",,Springer
SGBA: A Stealthy Scapegoat Backdoor Attack against Deep Neural Networks,"Ying He, Zhili Shen, Chang Xia, Jingyu Hua, Wei Tong, Sheng Zhong",arXiv,2022-05-16,"<a href=""arXiv (2022-05-16) : SGBA: A Stealthy Scapegoat Backdoor Attack against Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2104.01026v3]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Outsourced deep neural networks have been demonstrated to suffer from patch-based trojan attacks, in which an adversary poisons the training sets to inject a backdoor in the obtained model so that regular inputs can be still labeled correctly while those carrying a specific trigger are falsely given a target label. Due to the severity of such attacks, many backdoor detection and containment systems have recently, been proposed for deep neural networks. One major category among them are various model inspection schemes, which hope to detect backdoors before deploying models from non-trusted third-parties. In this paper, we show that such state-of-the-art schemes can be defeated by a so-called Scapegoat Backdoor Attack, which introduces a benign scapegoat trigger in data poisoning to prevent the defender from reversing the real abnormal trigger. In addition, it confines the values of network parameters within the same variances of those from clean model during training, which further significantly enhances the difficulty of the defender to learn the differences between legal and illegal models through machine-learning approaches. Our experiments on 3 popular datasets show that it can escape detection by all five state-of-the-art model inspection schemes. Moreover, this attack brings almost no side-effects on the attack effectiveness and guarantees the universal feature of the trigger compared with original patch-based trojan attacks.",,arXiv
One-to-N & N-to-One: Two Advanced Backdoor Attacks Against Deep Learning Models,M. Xue C. He J. Wang W. Liu,IEEE Transactions on Dependable and Secure Computing,2022-05-12,"<a href=""IEEE (2022-05-12) : One-to-N & N-to-One: Two Advanced Backdoor Attacks Against Deep Learning Models"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9211729]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TDSC.2020.3028448]</a>","In recent years, deep learning models have been widely deployed in various application scenarios. The training processes of deep neural network (DNN) models are time-consuming, and require massive training data and large hardware overhead. These issues have led to the outsourced training procedure, pre-trained models supplied from third parties, or massive training data from untrusted users. However, a few recent researches indicate that, by injecting some well-designed backdoor instances into the training set, the attackers can create a concealed backdoor in the DNN model. In this way, the attacked model still works normally on the benign inputs, but when a backdoor instance is submitted, some specific abnormal behaviors will be triggered. Existing studies all focus on attacking a single target that triggered by a single backdoor (referred to as One-to-One attack), while the backdoor attacks against multiple target classes, and backdoor attacks triggered by multiple backdoors have not been studied yet. In this article, for the first time, we propose two advanced backdoor attacks, the multi-target backdoor attacks and multi-trigger backdoor attacks: 1) One-to-N attack, where the attacker can trigger multiple backdoor targets by controlling the different intensities of the same backdoor 2) N-to-One attack, where such attack is triggered only when all the $N$N backdoors are satisfied. Compared with existing One-to-One attacks, the proposed two backdoor attacks are more flexible, more powerful and more difficult to be detected. Besides, the proposed backdoor attacks can be applied under the weak attack model, where the attacker has no knowledge about the parameters and architectures of the DNN models. Experimental results show that these two attacks can achieve better or similar performances when injecting a much smaller proportion or same proportion of backdoor instances than those existing One-to-One backdoor attacks. The two attack methods can achieve high attack success rates (up to 100 percent in MNIST dataset and 92.22 percent in CIFAR-10 dataset), while the test accuracy of the DNN model has hardly dropped (as low as 0 percent in LeNet-5 model and 0.76 percent in VGG-16 model), thus will not raise administrator’s suspicions. Further, the two attacks are also evaluated on a large and realistic dataset (Youtube Aligned Face dataset), where the maximum attack success rate reaches 90 percent (One-to-N) and 94 percent (N-to-One), and the accuracy degradation of target face recognition model (VGGFace model) is only 0.05 percent. The proposed One-to-N and N-to-One attacks are demonstrated to be effective and stealthy against two state-of-the-art defense methods.",,IEEE
MESSB–LWE: multi-extractable somewhere statistically binding and learning with error-based integrity and authentication for cloud storage,"Dilli Babu Salvakkam, Rajendra Pamula",The Journal of Supercomputing,2022-05-05,"<a href=""Springer (2022-05-05) : MESSB–LWE: multi-extractable somewhere statistically binding and learning with error-based integrity and authentication for cloud storage"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-022-04497-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-022-04497-1]</a>",The concept of cloud envisioned cyber-physical systems is a practical technology that allows users to interact with each other while transferring...,,Springer
Few-Shot Backdoor Attacks on Visual Object Tracking,"Yiming Li, Haoxiang Zhong, Xingjun Ma, Yong Jiang, Shu-Tao Xia","arXiv
ICLR
arXiv","2022-05-04
2022
2022-01","<a href=""arXiv (2022-05-04) : Few-Shot Backdoor Attacks on Visual Object Tracking"" target=""_blank"">[http://arxiv.org/abs/2201.13178v2]</a>
<a href=""DBLP (2022) : Few-Shot Backdoor Attacks on Visual Object Tracking"" target=""_blank"">[https://openreview.net/forum?id=qSV5CuSaK_a]</a>
<a href=""DBLP (2022-01) : Few-Shot Backdoor Attacks on Visual Object Tracking"" target=""_blank"">[https://arxiv.org/abs/2201.13178]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/forum?id=qSV5CuSaK_a]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2201.13178]</a>","Visual object tracking (VOT) has been widely adopted in mission-critical applications, such as autonomous driving and intelligent surveillance systems. In current practice, third-party resources such as datasets, backbone networks, and training platforms are frequently used to train high-performance VOT models. Whilst these resources bring certain convenience, they also introduce new security threats into VOT models. In this paper, we reveal such a threat where an adversary can easily implant hidden backdoors into VOT models by tempering with the training process. Specifically, we propose a simple yet effective few-shot backdoor attack (FSBA) that optimizes two losses alternately: 1) a \emph{feature loss} defined in the hidden feature space, and 2) the standard \emph{tracking loss}. We show that, once the backdoor is embedded into the target model by our FSBA, it can trick the model to lose track of specific objects even when the \emph{trigger} only appears in one or a few frames. We examine our attack in both digital and physical-world settings and show that it can significantly degrade the performance of state-of-the-art VOT trackers. We also show that our attack is resistant to potential defenses, highlighting the vulnerability of VOT models to potential backdoor attacks.

","

","arXiv
DBLP
DBLP"
A Survey of Backdoor Attack in Deep Learning,Du W.,Journal of Cyber Security,2022-05-01,"<a href=""ScienceDirect (2022-05-01) : A Survey of Backdoor Attack in Deep Learning"" target=""_blank"">[https://doi.org/10.19363/J.cnki.cn10-1380/tn.2022.05.01]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.19363/J.cnki.cn10-1380/tn.2022.05.01]</a>",,,ScienceDirect
Enabling data-driven anomaly detection by design in cyber-physical production systems,"Rui Pinto, Gil Gonçalves, ... Eduardo Tovar",Cybersecurity,2022-05-01,"<a href=""Springer (2022-05-01) : Enabling data-driven anomaly detection by design in cyber-physical production systems"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-022-00114-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-022-00114-z]</a>","Designing and developing distributed cyber-physical production systems (CPPS) is a time-consuming, complex, and error-prone process. These systems...",,Springer
Mitigating the Backdoor Attack by Federated Filters for Industrial IoT Applications,Hou B.,IEEE Transactions on Industrial Informatics,2022-05-01,"<a href=""ScienceDirect (2022-05-01) : Mitigating the Backdoor Attack by Federated Filters for Industrial IoT Applications"" target=""_blank"">[https://doi.org/10.1109/TII.2021.3112100]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TII.2021.3112100]</a>",,,ScienceDirect
On the Neural Backdoor of Federated Generative Models in Edge Computing,Wang D.,ACM Transactions on Internet Technology,2022-05-01,"<a href=""ScienceDirect (2022-05-01) : On the Neural Backdoor of Federated Generative Models in Edge Computing"" target=""_blank"">[https://doi.org/10.1145/3425662]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3425662]</a>",,,ScienceDirect
Can You Hear It?: Backdoor Attacks via Ultrasonic Triggers,"Stefanos Koffas, Jing Xu, Mauro Conti, Stjepan Picek","WiseML '22: Proceedings of the 2022 ACM Workshop on Wireless Security and Machine Learning
arXiv
arXiv
WiseML@WiSec","2022-05
2022-03-06
2021-07
2022","<a href=""ACM (2022-05) : Can You Hear It?: Backdoor Attacks via Ultrasonic Triggers"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3522783.3529523]</a>
<a href=""arXiv (2022-03-06) : Can You Hear It? Backdoor Attacks via Ultrasonic Triggers"" target=""_blank"">[http://arxiv.org/abs/2107.14569v3]</a>
<a href=""DBLP (2021-07) : Can You Hear It? Backdoor Attacks via Ultrasonic Triggers"" target=""_blank"">[https://arxiv.org/abs/2107.14569]</a>
<a href=""DBLP (2022) : Can You Hear It?: Backdoor Attacks via Ultrasonic Triggers"" target=""_blank"">[https://doi.org/10.1145/3522783.3529523]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3522783.3529523]</a>
<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1145/3522783.3529523]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2107.14569]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3522783.3529523]</a>","This work explores backdoor attacks for automatic speech recognition systems where we inject inaudible triggers. By doing so, we make the backdoor attack challenging to detect for legitimate users and, consequently, potentially more dangerous. We ...
This work explores backdoor attacks for automatic speech recognition systems where we inject inaudible triggers. By doing so, we make the backdoor attack challenging to detect for legitimate users, and thus, potentially more dangerous. We conduct experiments on two versions of a speech dataset and three neural networks and explore the performance of our attack concerning the duration, position, and type of the trigger. Our results indicate that less than 1% of poisoned data is sufficient to deploy a backdoor attack and reach a 100% attack success rate. We observed that short, non-continuous triggers result in highly successful attacks. However, since our trigger is inaudible, it can be as long as possible without raising any suspicions making the attack more effective. Finally, we conducted our attack in actual hardware and saw that an adversary could manipulate inference in an Android application by playing the inaudible trigger over the air.

","


","ACM
arXiv
DBLP
DBLP"
ASAP: Algorithm Substitution Attacks on Cryptographic Protocols,"Sebastian Berndt, Jan Wichelmann, Claudius Pott, Tim-Henrik Traving, Thomas Eisenbarth",ASIA CCS '22: Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security,2022-05,"<a href=""ACM (2022-05) : ASAP: Algorithm Substitution Attacks on Cryptographic Protocols"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3488932.3517387]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3488932.3517387]</a>","The security of digital communication relies on few cryptographic protocols that are used to protect internet traffic, from web sessions to instant messaging. These protocols and the cryptographic primitives they rely on have been extensively studied ...",,ACM
Fight Poison with Poison: Detecting Backdoor Poison Samples via Decoupling Benign Correlations,"Xiangyu Qi, Tinghao Xie, Saeed Mahloujifar, Prateek Mittal",arXiv,2022-05,"<a href=""DBLP (2022-05) : Fight Poison with Poison: Detecting Backdoor Poison Samples via Decoupling Benign Correlations"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.13616]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.13616]</a>",,,DBLP
Textual Backdoor Attacks with Iterative Trigger Injection,"Jun Yan, Vansh Gupta, Xiang Ren",arXiv,2022-05,"<a href=""DBLP (2022-05) : Textual Backdoor Attacks with Iterative Trigger Injection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.12700]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.12700]</a>",,,DBLP
Towards a Defense against Backdoor Attacks in Continual Federated Learning,"Shuaiqi Wang, Jonathan Hayase, Giulia Fanti, Sewoong Oh",arXiv,2022-05,"<a href=""DBLP (2022-05) : Towards a Defense against Backdoor Attacks in Continual Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.11736]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.11736]</a>",,,DBLP
Universal Post-Training Backdoor Detection,"Hang Wang, Zhen Xiang, David J. Miller, George Kesidis",arXiv,2022-05,"<a href=""DBLP (2022-05) : Universal Post-Training Backdoor Detection"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.06900]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2205.06900]</a>",,,DBLP
Discovery of AI/ML Supply Chain Vulnerabilities within Automotive Cyber-Physical Systems,D. Williams C. Clark R. McGahan B. Potteiger D. Cohen P. Musau,2022 IEEE International Conference on Assured Autonomy (ICAA),2022-04-29,"<a href=""IEEE (2022-04-29) : Discovery of AI/ML Supply Chain Vulnerabilities within Automotive Cyber-Physical Systems"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9763651]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICAA52185.2022.00020]</a>","Steady advancement in Artificial Intelligence (AI) development over recent years has caused AI systems to become more readily adopted across industry and military use-cases globally. As powerful as these algorithms are, there are still gaping questions regarding their security and reliability. Beyond adversarial machine learning, software supply chain vulnerabilities and model backdoor injection exploits are emerging as potential threats to the physical safety of AI reliant CPS such as autonomous vehicles. In this work in progress paper, we introduce the concept of AI supply chain vulnerabilities with a provided proof of concept autonomous exploitation framework. We investigate the viability of algorithm backdoors and software third party library dependencies for applicability into modern AI attack kill chains. We leverage an autonomous vehicle case study for demonstrating the applicability of our offensive methodologies within a realistic AI CPS operating environment.",,IEEE
Detecting Backdoor Poisoning Attacks on Deep Neural Networks by Heatmap Clustering,"Lukas Schulth, Christian Berghoff, Matthias Neu","arXiv
arXiv","2022-04-27
2022-04","<a href=""arXiv (2022-04-27) : Detecting Backdoor Poisoning Attacks on Deep Neural Networks by Heatmap Clustering"" target=""_blank"">[http://arxiv.org/abs/2204.12848v1]</a>
<a href=""DBLP (2022-04) : Detecting Backdoor Poisoning Attacks on Deep Neural Networks by Heatmap Clustering"" target=""_blank"">[https://doi.org/10.48550/arXiv.2204.12848]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2204.12848]</a>","Predicitions made by neural networks can be fraudulently altered by so-called poisoning attacks. A special case are backdoor poisoning attacks. We study suitable detection methods and introduce a new method called Heatmap Clustering. There, we apply a $k$-means clustering algorithm on heatmaps produced by the state-of-the-art explainable AI method Layer-wise relevance propagation. The goal is to separate poisoned from un-poisoned data in the dataset. We compare this method with a similar method, called Activation Clustering, which also uses $k$-means clustering but applies it on the activation of certain hidden layers of the neural network as input. We test the performance of both approaches for standard backdoor poisoning attacks, label-consistent poisoning attacks and label-consistent poisoning attacks with reduced amplitude stickers. We show that Heatmap Clustering consistently performs better than Activation Clustering. However, when considering label-consistent poisoning attacks, the latter method also yields good detection performance.
","
","arXiv
DBLP"
Triggerless Backdoor Attack for NLP Tasks with Clean Labels,"Leilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Yi Yang, Shangwei Guo, Chun Fan","arXiv
arXiv
NAACL-HLT","2022-04-27
2021-11
2022","<a href=""arXiv (2022-04-27) : Triggerless Backdoor Attack for NLP Tasks with Clean Labels"" target=""_blank"">[http://arxiv.org/abs/2111.07970v2]</a>
<a href=""DBLP (2021-11) : Triggerless Backdoor Attack for NLP Tasks with Clean Labels"" target=""_blank"">[https://arxiv.org/abs/2111.07970]</a>
<a href=""DBLP (2022) : Triggerless Backdoor Attack for NLP Tasks with Clean Labels"" target=""_blank"">[https://doi.org/10.18653/v1/2022.naacl-main.214]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2111.07970]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2022.naacl-main.214]</a>","Backdoor attacks pose a new threat to NLP models. A standard strategy to construct poisoned data in backdoor attacks is to insert triggers (e.g., rare words) into selected sentences and alter the original label to a target label. This strategy comes with a severe flaw of being easily detected from both the trigger and the label perspectives: the trigger injected, which is usually a rare word, leads to an abnormal natural language expression, and thus can be easily detected by a defense model, the changed target label leads the example to be mistakenly labeled and thus can be easily detected by manual inspections. To deal with this issue, in this paper, we propose a new strategy to perform textual backdoor attacks which do not require an external trigger, and the poisoned samples are correctly labeled. The core idea of the proposed strategy is to construct clean-labeled examples, whose labels are correct but can lead to test label changes when fused with the training set. To generate poisoned clean-labeled examples, we propose a sentence generation model based on the genetic algorithm to cater to the non-differentiable characteristic of text data. Extensive experiments demonstrate that the proposed attacking strategy is not only effective, but more importantly, hard to defend due to its triggerless and clean-labeled nature. Our work marks the first step towards developing triggerless attacking strategies in NLP.

","

","arXiv
DBLP
DBLP"
Backdooring Explainable Machine Learning,"Maximilian Noppel, Lukas Peter, Christian Wressnegger","arXiv
arXiv","2022-04-20
2022-04","<a href=""arXiv (2022-04-20) : Backdooring Explainable Machine Learning"" target=""_blank"">[http://arxiv.org/abs/2204.09498v1]</a>
<a href=""DBLP (2022-04) : Backdooring Explainable Machine Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2204.09498]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2204.09498]</a>","Explainable machine learning holds great potential for analyzing and understanding learning-based systems. These methods can, however, be manipulated to present unfaithful explanations, giving rise to powerful and stealthy adversaries. In this paper, we demonstrate blinding attacks that can fully disguise an ongoing attack against the machine learning model. Similar to neural backdoors, we modify the model's prediction upon trigger presence but simultaneously also fool the provided explanation. This enables an adversary to hide the presence of the trigger or point the explanation to entirely different portions of the input, throwing a red herring. We analyze different manifestations of such attacks for different explanation types in the image domain, before we resume to conduct a red-herring attack against malware classification.
","
","arXiv
DBLP"
Planting Undetectable Backdoors in Machine Learning Models,"Shafi Goldwasser, Michael P. Kim, Vinod Vaikuntanathan, Or Zamir","arXiv
arXiv
FOCS","2022-04-14
2022-04
2022","<a href=""arXiv (2022-04-14) : Planting Undetectable Backdoors in Machine Learning Models"" target=""_blank"">[http://arxiv.org/abs/2204.06974v1]</a>
<a href=""DBLP (2022-04) : Planting Undetectable Backdoors in Machine Learning Models"" target=""_blank"">[https://doi.org/10.48550/arXiv.2204.06974]</a>
<a href=""DBLP (2022) : Planting Undetectable Backdoors in Machine Learning Models : [Extended Abstract]"" target=""_blank"">[https://doi.org/10.1109/FOCS54457.2022.00092]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2204.06974]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/FOCS54457.2022.00092]</a>","Given the computational cost and technical expertise required to train machine learning models, users may delegate the task of learning to a service provider. We show how a malicious learner can plant an undetectable backdoor into a classifier. On the surface, such a backdoored classifier behaves normally, but in reality, the learner maintains a mechanism for changing the classification of any input, with only a slight perturbation. Importantly, without the appropriate ""backdoor key"", the mechanism is hidden and cannot be detected by any computationally-bounded observer. We demonstrate two frameworks for planting undetectable backdoors, with incomparable guarantees. First, we show how to plant a backdoor in any model, using digital signature schemes. The construction guarantees that given black-box access to the original model and the backdoored version, it is computationally infeasible to find even a single input where they differ. This property implies that the backdoored model has generalization error comparable with the original model. Second, we demonstrate how to insert undetectable backdoors in models trained using the Random Fourier Features (RFF) learning paradigm or in Random ReLU networks. In this construction, undetectability holds against powerful white-box distinguishers: given a complete description of the network and the training data, no efficient distinguisher can guess whether the model is ""clean"" or contains a backdoor. Our construction of undetectable backdoors also sheds light on the related issue of robustness to adversarial examples. In particular, our construction can produce a classifier that is indistinguishable from an ""adversarially robust"" classifier, but where every input has an adversarial example! In summary, the existence of undetectable backdoors represent a significant theoretical roadblock to certifying adversarial robustness.

","

","arXiv
DBLP
DBLP"
FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis,"Yu Feng, Benteng Ma, Jing Zhang, Shanshan Zhao, Yong Xia, Dacheng Tao","arXiv
CVPR
arXiv","2022-04-14
2022
2021-12","<a href=""arXiv (2022-04-14) : FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis"" target=""_blank"">[http://arxiv.org/abs/2112.01148v2]</a>
<a href=""DBLP (2022) : FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.02021]</a>
<a href=""DBLP (2021-12) : FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis"" target=""_blank"">[https://arxiv.org/abs/2112.01148]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.02021]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2112.01148]</a>","In recent years, the security of AI systems has drawn increasing research attention, especially in the medical imaging realm. To develop a secure medical image analysis (MIA) system, it is a must to study possible backdoor attacks (BAs), which can embed hidden malicious behaviors into the system. However, designing a unified BA method that can be applied to various MIA systems is challenging due to the diversity of imaging modalities (e.g., X-Ray, CT, and MRI) and analysis tasks (e.g., classification, detection, and segmentation). Most existing BA methods are designed to attack natural image classification models, which apply spatial triggers to training images and inevitably corrupt the semantics of poisoned pixels, leading to the failures of attacking dense prediction models. To address this issue, we propose a novel Frequency-Injection based Backdoor Attack method (FIBA) that is capable of delivering attacks in various MIA tasks. Specifically, FIBA leverages a trigger function in the frequency domain that can inject the low-frequency information of a trigger image into the poisoned image by linearly combining the spectral amplitude of both images. Since it preserves the semantics of the poisoned image pixels, FIBA can perform attacks on both classification and dense prediction models. Experiments on three benchmarks in MIA (i.e., ISIC-2019 for skin lesion classification, KiTS-19 for kidney tumor segmentation, and EAD-2019 for endoscopic artifact detection), validate the effectiveness of FIBA and its superiority over state-of-the-art methods in attacking MIA models as well as bypassing backdoor defense. Source code will be available at https://github.com/HazardFY/FIBA.

","<a href=""arXiv"" target=""_blank"">[https://github.com/HazardFY/FIBA]</a>

","arXiv
DBLP
DBLP"
A novel multi-module integrated intrusion detection system for high-dimensional imbalanced data,"Jiyuan Cui, Liansong Zong, ... Mingwei Tang",Applied Intelligence,2022-04-14,"<a href=""Springer (2022-04-14) : A novel multi-module integrated intrusion detection system for high-dimensional imbalanced data"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-022-03361-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-022-03361-2]</a>","The high dimension, complexity, and imbalance of network data are hot issues in the field of intrusion detection. Nowadays, intrusion detection...",,Springer
Towards A Critical Evaluation of Robustness for Deep Learning Backdoor Countermeasures,"Huming Qiu, Hua Ma, Zhi Zhang, Alsharif Abuadbba, Wei Kang, Anmin Fu, Yansong Gao","arXiv
arXiv","2022-04-13
2022-04","<a href=""arXiv (2022-04-13) : Towards A Critical Evaluation of Robustness for Deep Learning Backdoor Countermeasures"" target=""_blank"">[http://arxiv.org/abs/2204.06273v1]</a>
<a href=""DBLP (2022-04) : Towards A Critical Evaluation of Robustness for Deep Learning Backdoor Countermeasures"" target=""_blank"">[https://doi.org/10.48550/arXiv.2204.06273]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2204.06273]</a>","Since Deep Learning (DL) backdoor attacks have been revealed as one of the most insidious adversarial attacks, a number of countermeasures have been developed with certain assumptions defined in their respective threat models. However, the robustness of these countermeasures is inadvertently ignored, which can introduce severe consequences, e.g., a countermeasure can be misused and result in a false implication of backdoor detection. For the first time, we critically examine the robustness of existing backdoor countermeasures with an initial focus on three influential model-inspection ones that are Neural Cleanse (S&P'19), ABS (CCS'19), and MNTD (S&P'21). Although the three countermeasures claim that they work well under their respective threat models, they have inherent unexplored non-robust cases depending on factors such as given tasks, model architectures, datasets, and defense hyper-parameter, which are \textit{not even rooted from delicate adaptive attacks}. We demonstrate how to trivially bypass them aligned with their respective threat models by simply varying aforementioned factors. Particularly, for each defense, formal proofs or empirical studies are used to reveal its two non-robust cases where it is not as robust as it claims or expects, especially the recent MNTD. This work highlights the necessity of thoroughly evaluating the robustness of backdoor countermeasures to avoid their misleading security implications in unknown non-robust cases.
","
","arXiv
DBLP"
A Secure JTAG Wrapper for SoC Testing and Debugging,K. -J. Lee Z. -Y. Lu S. -C. Yeh,IEEE Access,2022-04-13,"<a href=""IEEE (2022-04-13) : A Secure JTAG Wrapper for SoC Testing and Debugging"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9749066]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2022.3164712]</a>","IEEE Std. 1149.1, also known as the Joint Test Access Group (JTAG) standard, provides excellent controllability and observability for ICs and hence is widely used in IC testing, debugging, failure analysis, or even online chip control/monitoring. Unfortunately, it has also become a backdoor for attackers to manipulate the ICs or grab confidential information from the ICs. One way to address this problem is to disable JTAG pins after manufacturing testing. However this countermeasure prohibits the in-filed testing and debugging capability. Other countermeasures such as authentication and encryption/decryption methods based on specific static keys have also been proposed. However, these approaches may suffer from side-channel or memory attacks that may figure out the specific keys. This paper presents an authentication-based secure JTAG wrapper with a dynamic feature to defend against the attacks mentioned above. We generate different keys for different test data dynamically. Therefore, only legal test data can be updated to the test data registers (TDRs) through JTAG. Furthermore, the attackers will get fake responses if they shift in illegal test data, which makes it extremely difficult to break our proposed method. We can also employ the physical unclonable function (PUF) to distinguish the legal test data for different chips. Experiments on a RISC-V CPU processor called SCR1 show that our proposed method can have an area overhead of only 0.49%.",,IEEE
Defending against Membership Inference Attacks in Federated learning via Adversarial Example,Y. Xie B. Chen J. Zhang D. Wu,"2021 17th International Conference on Mobility, Sensing and Networking (MSN)",2022-04-13,"<a href=""IEEE (2022-04-13) : Defending against Membership Inference Attacks in Federated learning via Adversarial Example"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751527]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/MSN53354.2021.00036]</a>","Federated learning has attracted attention in recent years due to its native privacy-preserving features. However, it is still vulnerable to various membership inference attacks, such as backdoor, poisoning, and adversarial attacks. Membership Inference attack aims to discover the data used to train the model, which leads to privacy leaking ramifications on participants who use their local data to train the shared model. Recent research on countermeasure methods mainly focuses on protecting the parameters and has limitations in guaranteeing privacy while restraining the loss of the model. This paper proposes Fedefend, which applies adversarial examples to defend against membership inference attacks in federated learning. The proposed approach adds well-designed noise to the attack features of the target model of each iteration becomes an adversarial example. In addition, we also consider the utility loss of the model and use an adversarial method to generate noise to constrain the loss to a certain extent, which efficiently achieves a trade-off between privacy security and loss of the federated learning model. We evaluate the proposed Fedefend on two benchmark datasets, and the experimental results demonstrate that Fedefend has a good performance.",,IEEE
Backdoor Attack against NLP models with Robustness-Aware Perturbation defense,"Shaik Mohammed Maqsood, Viveros Manuela Ceron, Addluri GowthamKrishna","arXiv
arXiv","2022-04-08
2022-04","<a href=""arXiv (2022-04-08) : Backdoor Attack against NLP models with Robustness-Aware Perturbation defense"" target=""_blank"">[http://arxiv.org/abs/2204.05758v1]</a>
<a href=""DBLP (2022-04) : Backdoor Attack against NLP models with Robustness-Aware Perturbation defense"" target=""_blank"">[https://doi.org/10.48550/arXiv.2204.05758]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2204.05758]</a>","Backdoor attack intends to embed hidden backdoor into deep neural networks (DNNs), such that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed if the hidden backdoor is activated by the attacker defined trigger. This threat could happen when the training process is not fully controlled, such as training on third-party data-sets or adopting third-party models. There has been a lot of research and different methods to defend such type of backdoor attacks, one being robustness-aware perturbation-based defense method. This method mainly exploits big gap of robustness between poisoned and clean samples. In our work, we break this defense by controlling the robustness gap between poisoned and clean samples using adversarial training step.
","
","arXiv
DBLP"
An Adaptive Black-box Backdoor Detection Method for Deep Neural Networks,"Xinqiao Zhang, Huili Chen, Ke Huang, Farinaz Koushanfar",arXiv,2022-04,"<a href=""DBLP (2022-04) : An Adaptive Black-box Backdoor Detection Method for Deep Neural Networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2204.04329]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2204.04329]</a>",,,DBLP
Poisoning Attacks against Feature-Based Image Classification,"Robin Mayerhofer, Rudolf Mayer",CODASPY '22: Proceedings of the Twelfth ACM Conference on Data and Application Security and Privacy,2022-04,"<a href=""ACM (2022-04) : Poisoning Attacks against Feature-Based Image Classification"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3508398.3519363]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3508398.3519363]</a>","Adversarial machine learning and the robustness of machine learning is gaining attention, especially in image classification. Attacks based on data poisoning, with the aim to lower the integrity or availability of a model, showed high success rates, ...",,ACM
Model Agnostic Defence against Backdoor Attacks in Machine Learning,"Sakshi Udeshi, Shanshan Peng, Gerald Woo, Lionell Loh, Louth Rawshan, Sudipta Chattopadhyay","arXiv
arXiv","2022-03-31
2019-08","<a href=""arXiv (2022-03-31) : Model Agnostic Defence against Backdoor Attacks in Machine Learning"" target=""_blank"">[http://arxiv.org/abs/1908.02203v3]</a>
<a href=""DBLP (2019-08) : Model Agnostic Defence against Backdoor Attacks in Machine Learning"" target=""_blank"">[http://arxiv.org/abs/1908.02203]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1908.02203]</a>","Machine Learning (ML) has automated a multitude of our day-to-day decision making domains such as education, employment and driving automation. The continued success of ML largely depends on our ability to trust the model we are using. Recently, a new class of attacks called Backdoor Attacks have been developed. These attacks undermine the user's trust in ML models. In this work, we present NEO, a model agnostic framework to detect and mitigate such backdoor attacks in image classification ML models. For a given image classification model, our approach analyses the inputs it receives and determines if the model is backdoored. In addition to this feature, we also mitigate these attacks by determining the correct predictions of the poisoned images. An appealing feature of NEO is that it can, for the first time, isolate and reconstruct the backdoor trigger. NEO is also the first defence methodology, to the best of our knowledge that is completely blackbox. We have implemented NEO and evaluated it against three state of the art poisoned models. These models include highly critical applications such as traffic sign detection (USTS) and facial detection. In our evaluation, we show that NEO can detect $\approx$88% of the poisoned inputs on average and it is as fast as 4.4 ms per input image. We also reconstruct the poisoned input for the user to effectively test their systems.
","
","arXiv
DBLP"
Data-aware process discovery for malware detection: an empirical study,"Mario Luca Bernardi, Marta Cimitile, Fabrizio Maria Maggi",Machine Learning,2022-03-31,"<a href=""Springer (2022-03-31) : Data-aware process discovery for malware detection: an empirical study"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-022-06154-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-022-06154-3]</a>",Mobile devices are undeniably becoming essential in our lives and our daily activities. The adoption of mobile applications increases the human...,,Springer
Poisoning and Backdooring Contrastive Learning,"Nicholas Carlini, Andreas Terzis","arXiv
ICLR
arXiv","2022-03-28
2022
2021-06","<a href=""arXiv (2022-03-28) : Poisoning and Backdooring Contrastive Learning"" target=""_blank"">[http://arxiv.org/abs/2106.09667v2]</a>
<a href=""DBLP (2022) : Poisoning and Backdooring Contrastive Learning"" target=""_blank"">[https://openreview.net/forum?id=iC4UHbQ01Mp]</a>
<a href=""DBLP (2021-06) : Poisoning and Backdooring Contrastive Learning"" target=""_blank"">[https://arxiv.org/abs/2106.09667]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/forum?id=iC4UHbQ01Mp]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2106.09667]</a>","Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.

","

","arXiv
DBLP
DBLP"
PiDAn: A Coherence Optimization Approach for Backdoor Attack Detection and Mitigation in Deep Neural Networks,"Yue Wang, Wenqing Li, Esha Sarkar, Muhammad Shafique, Michail Maniatakos, Saif Eddin Jabari","arXiv
arXiv","2022-03-26
2022-03","<a href=""arXiv (2022-03-26) : PiDAn: A Coherence Optimization Approach for Backdoor Attack Detection and Mitigation in Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2203.09289v2]</a>
<a href=""DBLP (2022-03) : PiDAn: A Coherence Optimization Approach for Backdoor Attack Detection and Mitigation in Deep Neural Networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2203.09289]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2203.09289]</a>","Backdoor attacks impose a new threat in Deep Neural Networks (DNNs), where a backdoor is inserted into the neural network by poisoning the training dataset, misclassifying inputs that contain the adversary trigger. The major challenge for defending against these attacks is that only the attacker knows the secret trigger and the target class. The problem is further exacerbated by the recent introduction of ""Hidden Triggers"", where the triggers are carefully fused into the input, bypassing detection by human inspection and causing backdoor identification through anomaly detection to fail. To defend against such imperceptible attacks, in this work we systematically analyze how representations, i.e., the set of neuron activations for a given DNN when using the training data as inputs, are affected by backdoor attacks. We propose PiDAn, an algorithm based on coherence optimization purifying the poisoned data. Our analysis shows that representations of poisoned data and authentic data in the target class are still embedded in different linear subspaces, which implies that they show different coherence with some latent spaces. Based on this observation, the proposed PiDAn algorithm learns a sample-wise weight vector to maximize the projected coherence of weighted samples, where we demonstrate that the learned weight vector has a natural ""grouping effect"" and is distinguishable between authentic data and poisoned data. This enables the systematic detection and mitigation of backdoor attacks. Based on our theoretical analysis and experimental results, we demonstrate the effectiveness of PiDAn in defending against backdoor attacks that use different settings of poisoned samples on GTSRB and ILSVRC2012 datasets. Our PiDAn algorithm can detect more than 90% infected classes and identify 95% poisoned samples.
","
","arXiv
DBLP"
Trojan Horse Training for Breaking Defenses against Backdoor Attacks in Deep Learning,"Arezoo Rajabi, Bhaskar Ramasubramanian, Radha Poovendran","arXiv
arXiv","2022-03-25
2022-03","<a href=""arXiv (2022-03-25) : Trojan Horse Training for Breaking Defenses against Backdoor Attacks in Deep Learning"" target=""_blank"">[http://arxiv.org/abs/2203.15506v1]</a>
<a href=""DBLP (2022-03) : Trojan Horse Training for Breaking Defenses against Backdoor Attacks in Deep Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2203.15506]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2203.15506]</a>","Machine learning (ML) models that use deep neural networks are vulnerable to backdoor attacks. Such attacks involve the insertion of a (hidden) trigger by an adversary. As a consequence, any input that contains the trigger will cause the neural network to misclassify the input to a (single) target class, while classifying other inputs without a trigger correctly. ML models that contain a backdoor are called Trojan models. Backdoors can have severe consequences in safety-critical cyber and cyber physical systems when only the outputs of the model are available. Defense mechanisms have been developed and illustrated to be able to distinguish between outputs from a Trojan model and a non-Trojan model in the case of a single-target backdoor attack with accuracy > 96 percent. Understanding the limitations of a defense mechanism requires the construction of examples where the mechanism fails. Current single-target backdoor attacks require one trigger per target class. We introduce a new, more general attack that will enable a single trigger to result in misclassification to more than one target class. Such a misclassification will depend on the true (actual) class that the input belongs to. We term this category of attacks multi-target backdoor attacks. We demonstrate that a Trojan model with either a single-target or multi-target trigger can be trained so that the accuracy of a defense mechanism that seeks to distinguish between outputs coming from a Trojan and a non-Trojan model will be reduced. Our approach uses the non-Trojan model as a teacher for the Trojan model and solves a min-max optimization problem between the Trojan model and defense mechanism. Empirical evaluations demonstrate that our training procedure reduces the accuracy of a state-of-the-art defense mechanism from >96 to 0 percent.
","
","arXiv
DBLP"
Can We Trust Machine Learning for Electronic Design Automation?,K. Liu J. J. Zhang B. Tan D. Feng,2021 IEEE 34th International System-on-Chip Conference (SOCC),2022-03-24,"<a href=""IEEE (2022-03-24) : Can We Trust Machine Learning for Electronic Design Automation?"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739485]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SOCC52499.2021.9739485]</a>","Machine learning (ML) techniques, especially deep learning (DL), have been playing a vital role in developing electronic design automation (EDA) tools across the chip design flow. DL for EDA demonstrates comparable quality, faster turnaround time, and better generalization than traditional methods. However, DL techniques expose intrinsic vulnerabilities to adversarial perturbation attacks and backdooring attacks. Despite the expansive application and great promise of DL use in the computer-aided design (CAD) flow, compromised DL-based EDA tools can propagate undetected design errors, result in manufacturability issues, and eventually reduce downstream integrated circuit (IC) yields. With lithographic hotspot detection as a case study, this paper highlights current challenges and recent progress for securing and robustifying lithographic hotspot detectors and sheds insights into the future directions in enhancing the security and robustness of DL for EDA.",,IEEE
From distributed machine learning to federated learning: a survey,"Ji Liu, Jizhou Huang, ... Dejing Dou",Knowledge and Information Systems,2022-03-22,"<a href=""Springer (2022-03-22) : From distributed machine learning to federated learning: a survey"" target=""_blank"">[https://link.springer.com/article/10.1007/s10115-022-01664-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10115-022-01664-x]</a>","In recent years, data and computing resources are typically distributed in the devices of end users, various regions or organizations. Because of...",,Springer
Backdoor Learning Curves: Explaining Backdoor Poisoning Beyond Influence Functions,"Antonio Emanuele Cinà, Kathrin Grosse, Sebastiano Vascon, Ambra Demontis, Battista Biggio, Fabio Roli, Marcello Pelillo","arXiv
arXiv","2022-03-16
2021-06","<a href=""arXiv (2022-03-16) : Backdoor Learning Curves: Explaining Backdoor Poisoning Beyond Influence Functions"" target=""_blank"">[http://arxiv.org/abs/2106.07214v3]</a>
<a href=""DBLP (2021-06) : Backdoor Learning Curves: Explaining Backdoor Poisoning Beyond Influence Functions"" target=""_blank"">[https://arxiv.org/abs/2106.07214]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2106.07214]</a>","Backdoor attacks inject poisoning samples during training, with the goal of forcing a machine learning model to output an attacker-chosen class when presented a specific trigger at test time. Although backdoor attacks have been demonstrated in a variety of settings and against different models, the factors affecting their effectiveness are still not well understood. In this work, we provide a unifying framework to study the process of backdoor learning under the lens of incremental learning and influence functions. We show that the effectiveness of backdoor attacks depends on: (i) the complexity of the learning algorithm, controlled by its hyperparameters, (ii) the fraction of backdoor samples injected into the training set, and (iii) the size and visibility of the backdoor trigger. These factors affect how fast a model learns to correlate the presence of the backdoor trigger with the target class. Our analysis unveils the intriguing existence of a region in the hyperparameter space in which the accuracy on clean test samples is still high while backdoor attacks are ineffective, thereby suggesting novel criteria to improve existing defenses.
","
","arXiv
DBLP"
An Adaptive Black-box Defense against Trojan Attacks on Text Data,F. Alsharadgah A. Khreishah M. Al-Ayyoub Y. Jararweh G. Liu I. Khalil M. Almutiry N. Saeed,"2021 Eighth International Conference on Social Network Analysis, Management and Security (SNAMS)",2022-03-16,"<a href=""IEEE (2022-03-16) : An Adaptive Black-box Defense against Trojan Attacks on Text Data"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9732112]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SNAMS53716.2021.9732112]</a>","Trojan backdoor is a poisoning attack against Neural Network (NN) classifiers in which adversaries try to exploit the (highly desirable) model reuse property to implant Trojans into model parameters for backdoor breaches through a poisoned training process. Most of the proposed defenses against Trojan attacks assume a white-box setup, in which the defender either has access to the inner state of NN or can run back-propagation through it. Moreover, most of exiting works that propose white-box and black-box methods to defend Trojan backdoor focus on image data. Due to the the difference in the data structure, these defenses cannot be directly applied for textual data. We propose T-TROJDEF which is a more practical but challenging black-box defense method for text data that only needs to run forward-pass of the NN model. T-TROJDEF tries to identify and filter out Trojan inputs (i.e., inputs augmented with the Trojan trigger) by monitoring the changes in the prediction confidence when the input is repeatedly perturbed. The intuition is that Trojan inputs are more stable as the misclassification only depends on the trigger, while benign inputs will suffer when perturbed due to the perturbation of the classification features.",,IEEE
Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios,"Zhen Xiang, David J. Miller, George Kesidis","arXiv
ICLR
arXiv","2022-03-14
2022
2022-01","<a href=""arXiv (2022-03-14) : Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios"" target=""_blank"">[http://arxiv.org/abs/2201.08474v2]</a>
<a href=""DBLP (2022) : Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios"" target=""_blank"">[https://openreview.net/forum?id=MSgB8D4Hy51]</a>
<a href=""DBLP (2022-01) : Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios"" target=""_blank"">[https://arxiv.org/abs/2201.08474]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/forum?id=MSgB8D4Hy51]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2201.08474]</a>","Backdoor attacks (BAs) are an emerging threat to deep neural network classifiers. A victim classifier will predict to an attacker-desired target class whenever a test sample is embedded with the same backdoor pattern (BP) that was used to poison the classifier's training set. Detecting whether a classifier is backdoor attacked is not easy in practice, especially when the defender is, e.g., a downstream user without access to the classifier's training set. This challenge is addressed here by a reverse-engineering defense (RED), which has been shown to yield state-of-the-art performance in several domains. However, existing REDs are not applicable when there are only {\it two classes} or when {\it multiple attacks} are present. These scenarios are first studied in the current paper, under the practical constraints that the defender neither has access to the classifier's training set nor to supervision from clean reference classifiers trained for the same domain. We propose a detection framework based on BP reverse-engineering and a novel {\it expected transferability} (ET) statistic. We show that our ET statistic is effective {\it using the same detection threshold}, irrespective of the classification domain, the attack configuration, and the BP reverse-engineering algorithm that is used. The excellent performance of our method is demonstrated on six benchmark datasets. Notably, our detection framework is also applicable to multi-class scenarios with multiple attacks. Code is available at https://github.com/zhenxianglance/2ClassBADetection.

","<a href=""arXiv"" target=""_blank"">[https://github.com/zhenxianglance/2ClassBADetection]</a>

","arXiv
DBLP
DBLP"
SSVEP-based brain-computer interfaces are vulnerable to square wave attacks,"Rui Bian, Lubin Meng, Dongrui Wu",Science China Information Sciences,2022-03-14,"<a href=""Springer (2022-03-14) : SSVEP-based brain-computer interfaces are vulnerable to square wave attacks"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-022-3440-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-022-3440-5]</a>",Electroencephalogram (EEG) based brain-computer interfaces (BCIs) have attracted wide attention in recent years. Steady-state visual evoked potential...,,Springer
Semi-supervised machine learning framework for network intrusion detection,"Jieling Li, Hao Zhang, ... Zhihuang Liu",The Journal of Supercomputing,2022-03-14,"<a href=""Springer (2022-03-14) : Semi-supervised machine learning framework for network intrusion detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-022-04390-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-022-04390-x]</a>","Network intrusion detection plays an important role as tools for managing and identifying potential threats, which presents various challenges....",,Springer
Enhancing Backdoor Attacks with Multi-Level MMD Regularization,"Pengfei Xia, Hongjing Niu, Ziqiang Li, Bin Li",arXiv,2022-03-13,"<a href=""arXiv (2022-03-13) : Enhancing Backdoor Attacks with Multi-Level MMD Regularization"" target=""_blank"">[http://arxiv.org/abs/2111.05077v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","While Deep Neural Networks (DNNs) excel in many tasks, the huge training resources they require become an obstacle for practitioners to develop their own models. It has become common to collect data from the Internet or hire a third party to train models. Unfortunately, recent studies have shown that these operations provide a viable pathway for maliciously injecting hidden backdoors into DNNs. Several defense methods have been developed to detect malicious samples, with the common assumption that the latent representations of benign and malicious samples extracted by the infected model exhibit different distributions. However, a comprehensive study on the distributional differences is missing. In this paper, we investigate such differences thoroughly via answering three questions: 1) What are the characteristics of the distributional differences? 2) How can they be effectively reduced? 3) What impact does this reduction have on difference-based defense methods? First, the distributional differences of multi-level representations on the regularly trained backdoored models are verified to be significant by introducing Maximum Mean Discrepancy (MMD), Energy Distance (ED), and Sliced Wasserstein Distance (SWD) as the metrics. Then, ML-MMDR, a difference reduction method that adds multi-level MMD regularization into the loss, is proposed, and its effectiveness is testified on three typical difference-based defense methods. Across all the experimental settings, the F1 scores of these methods drop from 90%-100% on the regularly trained backdoored models to 60%-70% on the models trained with ML-MMDR. These results indicate that the proposed MMD regularization can enhance the stealthiness of existing backdoor attack methods. The prototype code of our method is now available at https://github.com/xpf/Multi-Level-MMD-Regularization.","<a href=""arXiv"" target=""_blank"">[https://github.com/xpf/Multi-Level-MMD-Regularization]</a>",arXiv
Practical algorithm substitution attack on extractable signatures,"Yi Zhao, Kaitai Liang, ... Emmanouil Panaousis","Designs, Codes and Cryptography",2022-03-05,"<a href=""Springer (2022-03-05) : Practical algorithm substitution attack on extractable signatures"" target=""_blank"">[https://link.springer.com/article/10.1007/s10623-022-01019-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10623-022-01019-1]</a>",An algorithm substitution attack (ASA) can undermine the security of cryptographic primitives by subverting the original implementation. An ASA...,,Springer
Dynamic Backdoors with Global Average Pooling,"Stefanos Koffas, Stjepan Picek, Mauro Conti","arXiv
AICAS
arXiv","2022-03-04
2022
2022-03","<a href=""arXiv (2022-03-04) : Dynamic Backdoors with Global Average Pooling"" target=""_blank"">[http://arxiv.org/abs/2203.02079v1]</a>
<a href=""DBLP (2022) : Dynamic Backdoors with Global Average Pooling"" target=""_blank"">[https://doi.org/10.1109/AICAS54282.2022.9869920]</a>
<a href=""DBLP (2022-03) : Dynamic Backdoors with Global Average Pooling"" target=""_blank"">[https://doi.org/10.48550/arXiv.2203.02079]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1109/AICAS54282.2022.9869920]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/AICAS54282.2022.9869920]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2203.02079]</a>","Outsourced training and machine learning as a service have resulted in novel attack vectors like backdoor attacks. Such attacks embed a secret functionality in a neural network activated when the trigger is added to its input. In most works in the literature, the trigger is static, both in terms of location and pattern. The effectiveness of various detection mechanisms depends on this property. It was recently shown that countermeasures in image classification, like Neural Cleanse and ABS, could be bypassed with dynamic triggers that are effective regardless of their pattern and location. Still, such backdoors are demanding as they require a large percentage of poisoned training data. In this work, we are the first to show that dynamic backdoor attacks could happen due to a global average pooling layer without increasing the percentage of the poisoned training data. Nevertheless, our experiments in sound classification, text sentiment analysis, and image classification show this to be very difficult in practice.

","

","arXiv
DBLP
DBLP"
A Dynamic-Key Based Secure Scan Architecture for Manufacturing and In-Field IC Testing,K. -J. Lee C. -A. Liu C. -C. Wu,IEEE Transactions on Emerging Topics in Computing,2022-03-03,"<a href=""IEEE (2022-03-03) : A Dynamic-Key Based Secure Scan Architecture for Manufacturing and In-Field IC Testing"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9194339]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TETC.2020.3021820]</a>","The design for testability (DFT) technology based on scan chains is widely used in industry to increase the testability of circuits. However, it also leads to a potential security problem that attackers can use scan chains as a backdoor to attack a system. Common methods to defend such attacks include disabling the scan chain after manufacturing test or employing some secret keys to encrypt/decrypt scan data or to verify the identities of users. The former would make in-field testing impossible and the latter would require storing keys in memory which might also undergo high risk of memory attacks. In this paper we propose a dynamic-key based secure scan architecture that works together with an intrinsic Physical Unclonable Function (PUF) of chips to defend both scan-based and memory attacks while facilitating both manufacturing and in-field testing. A system equipped with this secure architecture will shift out true circuit responses only when legal test patterns are shifted into the scan chains. Moreover, no test key will be stored in memory, hence no memory attacks are possible. We also leverage the PUF to distinct the legal test patterns for different manufactured chips so as to further protect chips. Analysis results show that our protection scheme can achieve a very high security level without sacrificing system performance, testability and diagnosability.",,IEEE
Development paradigms of cyberspace endogenous safety and security,Jiangxing Wu,Science China Information Sciences,2022-03-03,"<a href=""Springer (2022-03-03) : Development paradigms of cyberspace endogenous safety and security"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-021-3379-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-021-3379-2]</a>",,,Springer
How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data,"Zhiyuan Zhang, Lingjuan Lyu, Weiqiang Wang, Lichao Sun, Xu Sun","arXiv
ICLR
arXiv","2022-03-02
2022
2021-09","<a href=""arXiv (2022-03-02) : How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data"" target=""_blank"">[http://arxiv.org/abs/2109.01300v2]</a>
<a href=""DBLP (2022) : How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data"" target=""_blank"">[https://openreview.net/forum?id=Bn09TnDngN]</a>
<a href=""DBLP (2021-09) : How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data"" target=""_blank"">[https://arxiv.org/abs/2109.01300]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/forum?id=Bn09TnDngN]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2109.01300]</a>","Since training a large-scale backdoored model from scratch requires a large training dataset, several recent attacks have considered to inject backdoors into a trained clean model without altering model behaviors on the clean data. Previous work finds that backdoors can be injected into a trained clean model with Adversarial Weight Perturbation (AWP). Here AWPs refers to the variations of parameters that are small in backdoor learning. In this work, we observe an interesting phenomenon that the variations of parameters are always AWPs when tuning the trained clean model to inject backdoors. We further provide theoretical analysis to explain this phenomenon. We formulate the behavior of maintaining accuracy on clean data as the consistency of backdoored models, which includes both global consistency and instance-wise consistency. We extensively analyze the effects of AWPs on the consistency of backdoored models. In order to achieve better consistency, we propose a novel anchoring loss to anchor or freeze the model behaviors on the clean data, with a theoretical guarantee. Both the analytical and the empirical results validate the effectiveness of the anchoring loss in improving the consistency, especially the instance-wise consistency.

","

","arXiv
DBLP
DBLP"
A new approach for APT malware detection based on deep graph network for endpoint systems,"Cho Do Xuan, DT Huong",Applied Intelligence,2022-03-01,"<a href=""Springer (2022-03-01) : A new approach for APT malware detection based on deep graph network for endpoint systems"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-021-03138-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-021-03138-z]</a>",The form of spreading malware through end-users and thereby escalating and stealing data in organizations is one of the attack techniques widely used...,,Springer
Backdoor Attacks on Image Classification Models in Deep Neural Networks,Zhang Q.,Chinese Journal of Electronics,2022-03-01,"<a href=""ScienceDirect (2022-03-01) : Backdoor Attacks on Image Classification Models in Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1049/cje.2021.00.126]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1049/cje.2021.00.126]</a>",,,ScienceDirect
KLEPTOGRAPHIC (ALGORITHMIC) BACKDOORS IN THE RSA KEY GENERATOR,Markelova A.V.,Prikladnaya Diskretnaya Matematika,2022-03-01,"<a href=""ScienceDirect (2022-03-01) : KLEPTOGRAPHIC (ALGORITHMIC) BACKDOORS IN THE RSA KEY GENERATOR"" target=""_blank"">[https://doi.org/10.17223/20710410/55/2]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.17223/20710410/55/2]</a>",,,ScienceDirect
Clean-Annotation Backdoor Attack against Lane Detection Systems in the Wild,"Xingshuo Han, Guowen Xu, Yuan Zhou, Xuehuan Yang, Jiwei Li, Tianwei Zhang",arXiv,2022-03,"<a href=""DBLP (2022-03) : Clean-Annotation Backdoor Attack against Lane Detection Systems in the Wild"" target=""_blank"">[https://doi.org/10.48550/arXiv.2203.00858]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2203.00858]</a>",,,DBLP
Client-Wise Targeted Backdoor in Federated Learning,"Gorka Abad, Servio Paguada, Stjepan Picek, Víctor Julio Ramírez-Durán, Aitor Urbieta",arXiv,2022-03,"<a href=""DBLP (2022-03) : Client-Wise Targeted Backdoor in Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2203.08689]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2203.08689]</a>",,,DBLP
Invisible Backdoor Attack with Sample-Specific Triggers,Y. Li Y. Li B. Wu L. Li R. He S. Lyu,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)
Proceedings of the IEEE International Conference on Computer Vision
arXiv
ICCV","2022-02-28
2021-01-01
2021-08-13
2021","<a href=""IEEE (2022-02-28) : Invisible Backdoor Attack with Sample-Specific Triggers"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711191]</a>
<a href=""ScienceDirect (2021-01-01) : Invisible Backdoor Attack with Sample-Specific Triggers"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01615]</a>
<a href=""arXiv (2021-08-13) : Invisible Backdoor Attack with Sample-Specific Triggers"" target=""_blank"">[http://arxiv.org/abs/2012.03816v3]</a>
<a href=""DBLP (2021) : Invisible Backdoor Attack with Sample-Specific Triggers"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01615]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01615]</a>
<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01615]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01615]</a>","Recently, backdoor attacks pose a new security threat to the training process of deep neural networks (DNNs). Attackers intend to inject hidden backdoors into DNNs, such that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed if hidden backdoors are activated by the attacker-defined trigger. Existing backdoor attacks usually adopt the setting that triggers are sample-agnostic, i.e., different poisoned samples contain the same trigger, resulting in that the attacks could be easily mitigated by current backdoor defenses. In this work, we explore a novel attack paradigm, where backdoor triggers are sample-specific. In our attack, we only need to modify certain training samples with invisible perturbation, while not need to manipulate other training components (e.g., training loss, and model structure) as required in many existing attacks. Specifically, inspired by the recent advance in DNN-based image steganography, we generate sample-specific invisible additive noises as backdoor triggers by encoding an attacker-specified string into benign images through an encoder-decoder network. The mapping from the string to the target label will be generated when DNNs are trained on the poisoned dataset. Extensive experiments on benchmark datasets verify the effectiveness of our method in attacking models with or without defenses. The code will be available at https://github.com/yuezunli/ISSBA.

Recently, backdoor attacks pose a new security threat to the training process of deep neural networks (DNNs). Attackers intend to inject hidden backdoors into DNNs, such that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed if hidden backdoors are activated by the attacker-defined trigger. Existing backdoor attacks usually adopt the setting that triggers are sample-agnostic, $i.e.,$ different poisoned samples contain the same trigger, resulting in that the attacks could be easily mitigated by current backdoor defenses. In this work, we explore a novel attack paradigm, where backdoor triggers are sample-specific. In our attack, we only need to modify certain training samples with invisible perturbation, while not need to manipulate other training components ($e.g.$, training loss, and model structure) as required in many existing attacks. Specifically, inspired by the recent advance in DNN-based image steganography, we generate sample-specific invisible additive noises as backdoor triggers by encoding an attacker-specified string into benign images through an encoder-decoder network. The mapping from the string to the target label will be generated when DNNs are trained on the poisoned dataset. Extensive experiments on benchmark datasets verify the effectiveness of our method in attacking models with or without defenses.
","<a href=""IEEE"" target=""_blank"">[https://github.com/yuezunli/ISSBA]</a>


","IEEE
ScienceDirect
arXiv
DBLP"
Can Shape Structure Features Improve Model Robustness under Diverse Adversarial Settings?,M. Sun Z. Li C. Xiao H. Qiu B. Kailkhura M. Liu B. Li,2021 IEEE/CVF International Conference on Computer Vision (ICCV),2022-02-28,"<a href=""IEEE (2022-02-28) : Can Shape Structure Features Improve Model Robustness under Diverse Adversarial Settings?"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710511]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.00743]</a>","Recent studies show that convolutional neural networks (CNNs) are vulnerable under various settings, including adversarial attacks, common corruptions, and backdoor attacks. Motivated by the findings that human visual sys-tem pays more attention to global structure (e.g., shapes) for recognition while CNNs are biased towards local texture features in images, in this work we aim to analyze whether ""edge features"" could improve the recognition robustness in these scenarios, and if so, to what extent? To answer these questions and systematically evaluate the global structure features, we focus on shape features and pro-pose two edge-enabled pipelines EdgeNetRob and Edge-GANRob, forcing the CNNs to rely more on edge features. Specifically, EdgeNetRob and EdgeGANRob first explicitly extract shape structure features from a given image via an edge detection algorithm. Then EdgeNetRob trains down-stream learning tasks directly on the extracted edge features, while EdgeGANRob reconstructs a new image by refilling the texture information with a trained generative adversarial network (GANs). To reduce the sensitivity of edge detection algorithms to perturbations, we additionally propose a robust edge detection approach Robust Canny based on vanilla Canny. Based on our evaluation, we find that EdgeNetRob can help boost model robustness under different attack scenarios at the cost of the clean model accuracy. EdgeGANRob, on the other hand, is able to improve the clean model accuracy compared to EdgeNetRob while preserving the robustness. This shows that given such edge features, how to leverage them matters for robustness, and it also depends on data properties. Our systematic studies on edge structure features under different settings will shed light on future robust feature exploration and optimization.",,IEEE
Fooling LiDAR Perception via Adversarial Trajectory Perturbation,Y. Li C. Wen F. Juefei-Xu C. Feng,2021 IEEE/CVF International Conference on Computer Vision (ICCV),2022-02-28,"<a href=""IEEE (2022-02-28) : Fooling LiDAR Perception via Adversarial Trajectory Perturbation"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710897]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.00780]</a>","LiDAR point clouds collected from a moving vehicle are functions of its trajectories, because the sensor motion needs to be compensated to avoid distortions. When autonomous vehicles are sending LiDAR point clouds to deep networks for perception and planning, could the motion compensation consequently become a wide-open backdoor in those networks, due to both the adversarial vulnerability of deep learning and GPS-based vehicle trajectory estimation that is susceptible to wireless spoofing? We demonstrate such possibilities for the first time: instead of directly attacking point cloud coordinates which requires tampering with the raw LiDAR readings, only adversarial spoofing of a self-driving car’s trajectory with small perturbations is enough to make safety-critical objects undetectable or detected with incorrect positions. Moreover, polynomial trajectory perturbation is developed to achieve a temporally-smooth and highly-imperceptible attack. Extensive experiments on 3D object detection have shown that such attacks not only lower the performance of the state-of-the-art detectors effectively, but also transfer to other detectors, raising a red flag for the community. The code is available on https://ai4ce.github.io/FLAT/.","<a href=""IEEE"" target=""_blank"">[https://ai4ce.github.io/FLAT/]</a>",IEEE
Rethinking the Backdoor Attacks’ Triggers: A Frequency Perspective,Y. Zeng W. Park Z. M. Mao R. Jia,2021 IEEE/CVF International Conference on Computer Vision (ICCV),2022-02-28,"<a href=""IEEE (2022-02-28) : Rethinking the Backdoor Attacks’ Triggers: A Frequency Perspective"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710118]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01616]</a>","Backdoor attacks have been considered a severe security threat to deep learning. Such attacks can make models perform abnormally on inputs with predefined triggers and still retain state-of-the-art performance on clean data. While backdoor attacks have been thoroughly investigated in the image domain from both attackers’ and defenders’ sides, an analysis in the frequency domain has been missing thus far.This paper first revisits existing backdoor triggers from a frequency perspective and performs a comprehensive analysis. Our results show that many current backdoor attacks exhibit severe high-frequency artifacts, which persist across different datasets and resolutions. We further demonstrate these high-frequency artifacts enable a simple way to detect existing backdoor triggers at a detection rate of 98.50% without prior knowledge of the attack details and the target model. Acknowledging previous attacks’ weaknesses, we propose a practical way to create smooth backdoor triggers without high-frequency artifacts and study their detectability. We show that existing defense works can benefit by incorporating these smooth triggers into their design consideration. Moreover, we show that the detector tuned over stronger smooth triggers can generalize well to unseen weak smooth triggers. In short, our work emphasizes the importance of considering frequency analysis when designing both backdoor attacks and defenses in deep learning.",,IEEE
AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis,"Junfeng Guo, Ang Li, Cong Liu","arXiv
ICLR
arXiv","2022-02-24
2022
2021-10","<a href=""arXiv (2022-02-24) : AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis"" target=""_blank"">[http://arxiv.org/abs/2110.14880v4]</a>
<a href=""DBLP (2022) : AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis"" target=""_blank"">[https://openreview.net/forum?id=OM_lYiHXiCL]</a>
<a href=""DBLP (2021-10) : AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis"" target=""_blank"">[https://arxiv.org/abs/2110.14880]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/forum?id=OM_lYiHXiCL]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2110.14880]</a>","Deep neural networks (DNNs) are proved to be vulnerable against backdoor attacks. A backdoor is often embedded in the target DNNs through injecting a backdoor trigger into training examples, which can cause the target DNNs misclassify an input attached with the backdoor trigger. Existing backdoor detection methods often require the access to the original poisoned training data, the parameters of the target DNNs, or the predictive confidence for each given input, which are impractical in many real-world applications, e.g., on-device deployed DNNs. We address the black-box hard-label backdoor detection problem where the DNN is fully black-box and only its final output label is accessible. We approach this problem from the optimization perspective and show that the objective of backdoor detection is bounded by an adversarial objective. Further theoretical and empirical studies reveal that this adversarial objective leads to a solution with highly skewed distribution, a singularity is often observed in the adversarial map of a backdoor-infected example, which we call the adversarial singularity phenomenon. Based on this observation, we propose the adversarial extreme value analysis(AEVA) to detect backdoors in black-box neural networks. AEVA is based on an extreme value analysis of the adversarial map, computed from the monte-carlo gradient estimation. Evidenced by extensive experiments across multiple popular tasks and backdoor attacks, our approach is shown effective in detecting backdoor attacks under the black-box hard-label scenarios.

","

","arXiv
DBLP
DBLP"
"Cyber-physical security for IoT networks: a comprehensive review on traditional, blockchain and artificial intelligence based key-security","Ankit Attkan, Virender Ranga",Complex & Intelligent Systems,2022-02-24,"<a href=""Springer (2022-02-24) : Cyber-physical security for IoT networks: a comprehensive review on traditional, blockchain and artificial intelligence based key-security"" target=""_blank"">[https://link.springer.com/article/10.1007/s40747-022-00667-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s40747-022-00667-z]</a>",The recent years have garnered huge attention towards the Internet of Things (IoT) because it enables its consumers to improve their lifestyles and...,,Springer
On the Effectiveness of Adversarial Training against Backdoor Attacks,"Yinghua Gao, Dongxian Wu, Jingfeng Zhang, Guanhao Gan, Shu-Tao Xia, Gang Niu, Masashi Sugiyama","arXiv
arXiv","2022-02-22
2022-02","<a href=""arXiv (2022-02-22) : On the Effectiveness of Adversarial Training against Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2202.10627v1]</a>
<a href=""DBLP (2022-02) : On the Effectiveness of Adversarial Training against Backdoor Attacks"" target=""_blank"">[https://arxiv.org/abs/2202.10627]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2202.10627]</a>","DNNs' demand for massive data forces practitioners to collect data from the Internet without careful check due to the unacceptable cost, which brings potential risks of backdoor attacks. A backdoored model always predicts a target class in the presence of a predefined trigger pattern, which can be easily realized via poisoning a small amount of data. In general, adversarial training is believed to defend against backdoor attacks since it helps models to keep their prediction unchanged even if we perturb the input image (as long as within a feasible range). Unfortunately, few previous studies succeed in doing so. To explore whether adversarial training could defend against backdoor attacks or not, we conduct extensive experiments across different threat models and perturbation budgets, and find the threat model in adversarial training matters. For instance, adversarial training with spatial adversarial examples provides notable robustness against commonly-used patch-based backdoor attacks. We further propose a hybrid strategy which provides satisfactory robustness across different backdoor attacks.
","
","arXiv
DBLP"
Backdoor Defense in Federated Learning Using Differential Testing and Outlier Detection,"Yein Kim, Huili Chen, Farinaz Koushanfar","arXiv
arXiv","2022-02-21
2022-02","<a href=""arXiv (2022-02-21) : Backdoor Defense in Federated Learning Using Differential Testing and Outlier Detection"" target=""_blank"">[http://arxiv.org/abs/2202.11196v1]</a>
<a href=""DBLP (2022-02) : Backdoor Defense in Federated Learning Using Differential Testing and Outlier Detection"" target=""_blank"">[https://arxiv.org/abs/2202.11196]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2202.11196]</a>","The goal of federated learning (FL) is to train one global model by aggregating model parameters updated independently on edge devices without accessing users' private data. However, FL is susceptible to backdoor attacks where a small fraction of malicious agents inject a targeted misclassification behavior in the global model by uploading polluted model updates to the server. In this work, we propose DifFense, an automated defense framework to protect an FL system from backdoor attacks by leveraging differential testing and two-step MAD outlier detection, without requiring any previous knowledge of attack scenarios or direct access to local model parameters. We empirically show that our detection method prevents a various number of potential attackers while consistently achieving the convergence of the global model comparable to that trained under federated averaging (FedAvg). We further corroborate the effectiveness and generalizability of our method against prior defense techniques, such as Multi-Krum and coordinate-wise median aggregation. Our detection method reduces the average backdoor accuracy of the global model to below 4% and achieves a false negative rate of zero.
","
","arXiv
DBLP"
Label-Smoothed Backdoor Attack,"Minlong Peng, Zidi Xiong, Mingming Sun, Ping Li","arXiv
arXiv","2022-02-19
2022-02","<a href=""arXiv (2022-02-19) : Label-Smoothed Backdoor Attack"" target=""_blank"">[http://arxiv.org/abs/2202.11203v1]</a>
<a href=""DBLP (2022-02) : Label-Smoothed Backdoor Attack"" target=""_blank"">[https://arxiv.org/abs/2202.11203]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2202.11203]</a>","By injecting a small number of poisoned samples into the training set, backdoor attacks aim to make the victim model produce designed outputs on any input injected with pre-designed backdoors. In order to achieve a high attack success rate using as few poisoned training samples as possible, most existing attack methods change the labels of the poisoned samples to the target class. This practice often results in severe over-fitting of the victim model over the backdoors, making the attack quite effective in output control but easier to be identified by human inspection or automatic defense algorithms. In this work, we proposed a label-smoothing strategy to overcome the over-fitting problem of these attack methods, obtaining a \textit{Label-Smoothed Backdoor Attack} (LSBA). In the LSBA, the label of the poisoned sample $\bm{x}$ will be changed to the target class with a probability of $p_n(\bm{x})$ instead of 100\%, and the value of $p_n(\bm{x})$ is specifically designed to make the prediction probability the target class be only slightly greater than those of the other classes. Empirical studies on several existing backdoor attacks show that our strategy can considerably improve the stealthiness of these attacks and, at the same time, achieve a high attack success rate. In addition, our strategy makes it able to manually control the prediction probability of the design output through manipulating the applied and activated number of LSBAs\footnote{Source code will be published at \url{https://github.com/v-mipeng/LabelSmoothedAttack.git}}.
","<a href=""arXiv"" target=""_blank"">[https://github.com/v-mipeng/LabelSmoothedAttack.git}}]</a>
","arXiv
DBLP"
Debiasing Backdoor Attack: A Benign Application of Backdoor Attack in Eliminating Data Bias,"Shangxi Wu, Qiuyang He, Yi Zhang, Jitao Sang","arXiv
arXiv
Inf. Sci.","2022-02-18
2022-02
2023","<a href=""arXiv (2022-02-18) : Debiasing Backdoor Attack: A Benign Application of Backdoor Attack in Eliminating Data Bias"" target=""_blank"">[http://arxiv.org/abs/2202.10582v1]</a>
<a href=""DBLP (2022-02) : Debiasing Backdoor Attack: A Benign Application of Backdoor Attack in Eliminating Data Bias"" target=""_blank"">[https://arxiv.org/abs/2202.10582]</a>
<a href=""DBLP (2023) : Debiasing backdoor attack: A benign application of backdoor attack in eliminating data bias"" target=""_blank"">[https://doi.org/10.1016/j.ins.2023.119171]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2202.10582]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.ins.2023.119171]</a>","Backdoor attack is a new AI security risk that has emerged in recent years. Drawing on the previous research of adversarial attack, we argue that the backdoor attack has the potential to tap into the model learning process and improve model performance. Based on Clean Accuracy Drop (CAD) in backdoor attack, we found that CAD came out of the effect of pseudo-deletion of data. We provided a preliminary explanation of this phenomenon from the perspective of model classification boundaries and observed that this pseudo-deletion had advantages over direct deletion in the data debiasing problem. Based on the above findings, we proposed Debiasing Backdoor Attack (DBA). It achieves SOTA in the debiasing task and has a broader application scenario than undersampling.

","

","arXiv
DBLP
DBLP"
Resurrecting Trust in Facial Recognition: Mitigating Backdoor Attacks in Face Recognition to Prevent Potential Privacy Breaches,"Reena Zelenkova, Jack Swallow, M. A. P. Chamikara, Dongxi Liu, Mohan Baruwal Chhetri, Seyit Camtepe, Marthie Grobler, Mahathir Almashor","arXiv
arXiv","2022-02-18
2022-02","<a href=""arXiv (2022-02-18) : Resurrecting Trust in Facial Recognition: Mitigating Backdoor Attacks in Face Recognition to Prevent Potential Privacy Breaches"" target=""_blank"">[http://arxiv.org/abs/2202.10320v1]</a>
<a href=""DBLP (2022-02) : Resurrecting Trust in Facial Recognition: Mitigating Backdoor Attacks in Face Recognition to Prevent Potential Privacy Breaches"" target=""_blank"">[https://arxiv.org/abs/2202.10320]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2202.10320]</a>","Biometric data, such as face images, are often associated with sensitive information (e.g medical, financial, personal government records). Hence, a data breach in a system storing such information can have devastating consequences. Deep learning is widely utilized for face recognition (FR), however, such models are vulnerable to backdoor attacks executed by malicious parties. Backdoor attacks cause a model to misclassify a particular class as a target class during recognition. This vulnerability can allow adversaries to gain access to highly sensitive data protected by biometric authentication measures or allow the malicious party to masquerade as an individual with higher system permissions. Such breaches pose a serious privacy threat. Previous methods integrate noise addition mechanisms into face recognition models to mitigate this issue and improve the robustness of classification against backdoor attacks. However, this can drastically affect model accuracy. We propose a novel and generalizable approach (named BA-BAM: Biometric Authentication - Backdoor Attack Mitigation), that aims to prevent backdoor attacks on face authentication deep learning models through transfer learning and selective image perturbation. The empirical evidence shows that BA-BAM is highly robust and incurs a maximal accuracy drop of 2.4%, while reducing the attack success rate to a maximum of 20%. Comparisons with existing approaches show that BA-BAM provides a more practical backdoor mitigation approach for face recognition.
","
","arXiv
DBLP"
SAT Backdoors: Depth Beats Size,"Jan Dreier, Sebastian Ordyniak, Stefan Szeider","arXiv
ESA
arXiv","2022-02-16
2022
2022-02","<a href=""arXiv (2022-02-16) : SAT Backdoors: Depth Beats Size"" target=""_blank"">[http://arxiv.org/abs/2202.08326v1]</a>
<a href=""DBLP (2022) : SAT Backdoors: Depth Beats Size"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.ESA.2022.46]</a>
<a href=""DBLP (2022-02) : SAT Backdoors: Depth Beats Size"" target=""_blank"">[https://arxiv.org/abs/2202.08326]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.ESA.2022.46]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2202.08326]</a>","For several decades, much effort has been put into identifying classes of CNF formulas whose satisfiability can be decided in polynomial time. Classic results are the linear-time tractability of Horn formulas (Aspvall, Plass, and Tarjan, 1979) and Krom (i.e., 2CNF) formulas (Dowling and Gallier, 1984). Backdoors, introduced by Williams Gomes and Selman (2003), gradually extend such a tractable class to all formulas of bounded distance to the class. Backdoor size provides a natural but rather crude distance measure between a formula and a tractable class. Backdoor depth, introduced by M\""{a}hlmann, Siebertz, and Vigny (2021), is a more refined distance measure, which admits the utilization of different backdoor variables in parallel. Bounded backdoor size implies bounded backdoor depth, but there are formulas of constant backdoor depth and arbitrarily large backdoor size. We propose FPT approximation algorithms to compute backdoor depth into the classes Horn and Krom. This leads to a linear-time algorithm for deciding the satisfiability of formulas of bounded backdoor depth into these classes. We base our FPT approximation algorithm on a sophisticated notion of obstructions, extending M\""{a}hlmann et al.'s obstruction trees in various ways, including the addition of separator obstructions. We develop the algorithm through a new game-theoretic framework that simplifies the reasoning about backdoors. Finally, we show that bounded backdoor depth captures tractable classes of CNF formulas not captured by any known method.

","

","arXiv
DBLP
DBLP"
Backdoor Learning: A Survey,"Yiming Li, Yong Jiang, Zhifeng Li, Shu-Tao Xia","arXiv
arXiv
IEEE Trans. Neural Networks Learn. Syst.","2022-02-16
2020-07
2024","<a href=""arXiv (2022-02-16) : Backdoor Learning: A Survey"" target=""_blank"">[http://arxiv.org/abs/2007.08745v5]</a>
<a href=""DBLP (2020-07) : Backdoor Learning: A Survey"" target=""_blank"">[https://arxiv.org/abs/2007.08745]</a>
<a href=""DBLP (2024) : Backdoor Learning: A Survey"" target=""_blank"">[https://doi.org/10.1109/TNNLS.2022.3182979]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2007.08745]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TNNLS.2022.3182979]</a>","Backdoor attack intends to embed hidden backdoor into deep neural networks (DNNs), so that the attacked models perform well on benign samples, whereas their predictions will be maliciously changed if the hidden backdoor is activated by attacker-specified triggers. This threat could happen when the training process is not fully controlled, such as training on third-party datasets or adopting third-party models, which poses a new and realistic threat. Although backdoor learning is an emerging and rapidly growing research area, its systematic review, however, remains blank. In this paper, we present the first comprehensive survey of this realm. We summarize and categorize existing backdoor attacks and defenses based on their characteristics, and provide a unified framework for analyzing poisoning-based backdoor attacks. Besides, we also analyze the relation between backdoor attacks and relevant fields ($i.e.,$ adversarial attacks and data poisoning), and summarize widely adopted benchmark datasets. Finally, we briefly outline certain future research directions relying upon reviewed works. A curated list of backdoor-related resources is also available at \url{https://github.com/THUYimingLi/backdoor-learning-resources}.

","<a href=""arXiv"" target=""_blank"">[https://github.com/THUYimingLi/backdoor-learning-resources}]</a>

","arXiv
DBLP
DBLP"
Rethink the Evaluation for Attack Strength of Backdoor Attacks in Natural Language Processing,"Lingfeng Shen, Haiyun Jiang, Lemao Liu, Shuming Shi",arXiv,2022-02-16,"<a href=""arXiv (2022-02-16) : Rethink the Evaluation for Attack Strength of Backdoor Attacks in Natural Language Processing"" target=""_blank"">[http://arxiv.org/abs/2201.02993v2]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","It has been shown that natural language processing (NLP) models are vulnerable to a kind of security threat called the Backdoor Attack, which utilizes a `backdoor trigger' paradigm to mislead the models. The most threatening backdoor attack is the stealthy backdoor, which defines the triggers as text style or syntactic. Although they have achieved an incredible high attack success rate (ASR), we find that the principal factor contributing to their ASR is not the `backdoor trigger' paradigm. Thus the capacity of these stealthy backdoor attacks is overestimated when categorized as backdoor attacks. Therefore, to evaluate the real attack power of backdoor attacks, we propose a new metric called attack successful rate difference (ASRD), which measures the ASR difference between clean state and poison state models. Besides, since the defenses against stealthy backdoor attacks are absent, we propose Trigger Breaker, consisting of two too simple tricks that can defend against stealthy backdoor attacks effectively. Experiments show that our method achieves significantly better performance than state-of-the-art defense methods against stealthy backdoor attacks.",,arXiv
Multiphysics Simulation of EM Side-Channels from Silicon Backside with ML-based Auto-POI Identification,L. Lin D. Zhu J. Wen H. Chen Y. Lu N. Chang C. Chow H. Shrivastav C. -W. Chen K. Monta M. Nagata,2021 IEEE International Symposium on Hardware Oriented Security and Trust (HOST),2022-02-14,"<a href=""IEEE (2022-02-14) : Multiphysics Simulation of EM Side-Channels from Silicon Backside with ML-based Auto-POI Identification"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9702270]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/HOST49136.2021.9702270]</a>","The silicon substrate backside of modern ICs is increasingly recognized as a critical hardware vulnerability, which opens a backdoor for laser/optical probing, fault injection and side-channel attacks. In this work, a novel multiphysics simulation framework is proposed to assess near-field electromagnetic (EM) side-channel leakage. By modeling cell-level power, chip logic functionality and layout geometry, this framework efficiently generates time-domain EM traces at any virtual probe above the surface of silicon substrate. Moreover, an ML-based automatic POI (point-of-interest) identification algorithm is proposed to predict the most vulnerable leakage location, which can be 10-100x faster than a conventional correlation-based side-channel simulation approach. The simulation accuracy is further validated by silicon measurements of an AES crypto testchip in 130nm technology, with a matching leakage location pattern quantified by the required number of EM side-channel traces to disclose the secret keys. Our simulation result uncovers several unexpected data leakage issues from the silicon substrate, which is confirmed by measurements, thus demonstrating an approach that can effectively help prioritize pre-silicon design fixes or security ECOs (Engineering Change Orders).",,IEEE
Research on Webshell Detection Based on Semantic Analysis and Text-CNN,K. Cheng H. Wang G. Hu L. Zhang J. Chen W. Luo,2021 17th International Conference on Computational Intelligence and Security (CIS),2022-02-11,"<a href=""IEEE (2022-02-11) : Research on Webshell Detection Based on Semantic Analysis and Text-CNN"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9701708]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/CIS54983.2021.00115]</a>","Webshell, a kind of web page-based backdoor, is widely used in network attacks. There are many methods used for Webshell detection. A method based on bytecode of JSP scripts and Abstract Syntax Tree of PHP fails to see the similarities between different script languages. What's more, an approach based on raw script's content and CNN model only focuses on PHP Webshell and does poorly in the extraction of Webshell's features. Therefore, this paper proposes a generic Webshell detection model based on Text-CNN by extracting the node sequence of the Abstract Syntax Tree of PHP and JSP Webshell. The experimental results show that, compared to the traditional static detection model, the proposed detection model significantly improves detection rate, where the detection accuracy achieves as high as 99.5%.",,IEEE
False Memory Formation in Continual Learners Through Imperceptible Backdoor Trigger,"Muhammad Umer, Robi Polikar","arXiv
arXiv","2022-02-09
2022-02","<a href=""arXiv (2022-02-09) : False Memory Formation in Continual Learners Through Imperceptible Backdoor Trigger"" target=""_blank"">[http://arxiv.org/abs/2202.04479v1]</a>
<a href=""DBLP (2022-02) : False Memory Formation in Continual Learners Through Imperceptible Backdoor Trigger"" target=""_blank"">[https://arxiv.org/abs/2202.04479]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2202.04479]</a>","In this brief, we show that sequentially learning new information presented to a continual (incremental) learning model introduces new security risks: an intelligent adversary can introduce small amount of misinformation to the model during training to cause deliberate forgetting of a specific task or class at test time, thus creating ""false memory"" about that task. We demonstrate such an adversary's ability to assume control of the model by injecting ""backdoor"" attack samples to commonly used generative replay and regularization based continual learning approaches using continual learning benchmark variants of MNIST, as well as the more challenging SVHN and CIFAR 10 datasets. Perhaps most damaging, we show this vulnerability to be very acute and exceptionally effective: the backdoor pattern in our attack model can be imperceptible to human eye, can be provided at any point in time, can be added into the training data of even a single possibly unrelated task and can be achieved with as few as just 1\% of total training dataset of a single task.
","
","arXiv
DBLP"
Experimental Assessment of Wireless LANs against Rogue Access Points,N. Komanduri S. Sankaran,2021 IEEE International Symposium on Smart Electronic Systems (iSES),2022-02-08,"<a href=""IEEE (2022-02-08) : Experimental Assessment of Wireless LANs against Rogue Access Points"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9701078]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/iSES52644.2021.00098]</a>","Access Points (AP) are traditionally used to provide cost-effective, high speed Wi-Fi connectivity to homes, organizations and communities. Despite Wi-Fi providing numerous benefits such as flexibility, scalability and ease of deployment, it is susceptible to numerous vulnerabilities due to the presence of rogue access points (Rogue AP). In particular, intruders can eavesdrop, exploit, launch remote backdoors and manipulate legitimate clients and APs through Rogue APs thus leading to data breaches or possible network compromise. In this work, we build a real-time Wireless LAN testbed using commodity Wi-Fi devices such as Wi-Fi Pineapple Nano that acts as a rogue AP. Further, we perform different attacks on 802.11 Association process between clients and access points through the rogue AP and analyze their impact on the overall performance. Finally, we leverage a sniffer to capture genuine and malicious traffic and develop a mechanism for signature-based detection for mitigating the attacks caused by rogue APs. Evaluation shows that the proposed signature-based approach effectively detects the attacks caused by rogue APs with a detection rate of 91%.",,IEEE
Modeling for Endogenous Secure Domain Name System Based on Software Defined Networks,Q. Ren J. Wu Z. Li Z. Zhang,2021 3rd International Academic Exchange Conference on Science and Technology Innovation (IAECST),2022-02-07,"<a href=""IEEE (2022-02-07) : Modeling for Endogenous Secure Domain Name System Based on Software Defined Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9695571]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IAECST54258.2021.9695571]</a>","Cyberspace endogenous security (CES) aims to construct a new controllable and trusted system. It integrates the characteristics of dynamic heterogeneous redundant system, achieving the security defense and threat perception. This paper proposes an SDN-based Endogenous Secure Domain Name System (ESDNS) framework, and we adopt generalized stochastic Petri nets (GSPN) to describe system’s architectures and analyze the availability and awareness security of the ESDNS, and we analyze the influence of different attacking strength and recovering ability. Besides, we establish the prototype of ESDNS, the results of simulation show that the proposed method can effectively block the persistent attack of vulnerability backdoor, the cost of network communication delay and throughput performance is less than 10%, and the analysis of parameters gives the situation of degradation performance, ability of recovering and coordinated attack which has useful guidance to the engineering practice of endogenous secure systems.",,IEEE
Adversarial Unlearning of Backdoors via Implicit Hypergradient,"Yi Zeng, Si Chen, Won Park, Z. Morley Mao, Ming Jin, Ruoxi Jia","arXiv
arXiv
ICLR","2022-02-06
2021-10
2022","<a href=""arXiv (2022-02-06) : Adversarial Unlearning of Backdoors via Implicit Hypergradient"" target=""_blank"">[http://arxiv.org/abs/2110.03735v4]</a>
<a href=""DBLP (2021-10) : Adversarial Unlearning of Backdoors via Implicit Hypergradient"" target=""_blank"">[https://arxiv.org/abs/2110.03735]</a>
<a href=""DBLP (2022) : Adversarial Unlearning of Backdoors via Implicit Hypergradient"" target=""_blank"">[https://openreview.net/forum?id=MeeQkFYVbzW]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2110.03735]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/forum?id=MeeQkFYVbzW]</a>","We propose a minimax formulation for removing backdoors from a given poisoned model based on a small set of clean data. This formulation encompasses much of prior work on backdoor removal. We propose the Implicit Bacdoor Adversarial Unlearning (I-BAU) algorithm to solve the minimax. Unlike previous work, which breaks down the minimax into separate inner and outer problems, our algorithm utilizes the implicit hypergradient to account for the interdependence between inner and outer optimization. We theoretically analyze its convergence and the generalizability of the robustness gained by solving minimax on clean data to unseen test data. In our evaluation, we compare I-BAU with six state-of-art backdoor defenses on seven backdoor attacks over two datasets and various attack settings, including the common setting where the attacker targets one class as well as important but underexplored settings where multiple classes are targeted. I-BAU's performance is comparable to and most often significantly better than the best baseline. Particularly, its performance is more robust to the variation on triggers, attack settings, poison ratio, and clean data size. Moreover, I-BAU requires less computation to take effect, particularly, it is more than $13\times$ faster than the most efficient baseline in the single-target attack setting. Furthermore, it can remain effective in the extreme case where the defender can only access 100 clean samples -- a setting where all the baselines fail to produce acceptable results.

","

","arXiv
DBLP
DBLP"
Backdoor Defense via Decoupling the Training Process,"Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, Kui Ren","arXiv
ICLR
arXiv","2022-02-05
2022
2022-02","<a href=""arXiv (2022-02-05) : Backdoor Defense via Decoupling the Training Process"" target=""_blank"">[http://arxiv.org/abs/2202.03423v1]</a>
<a href=""DBLP (2022) : Backdoor Defense via Decoupling the Training Process"" target=""_blank"">[https://openreview.net/forum?id=TySnJ-0RdKI]</a>
<a href=""DBLP (2022-02) : Backdoor Defense via Decoupling the Training Process"" target=""_blank"">[https://arxiv.org/abs/2202.03423]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/forum?id=TySnJ-0RdKI]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2202.03423]</a>","Recent studies have revealed that deep neural networks (DNNs) are vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by poisoning a few training samples. The attacked model behaves normally on benign samples, whereas its prediction will be maliciously changed when the backdoor is activated. We reveal that poisoned samples tend to cluster together in the feature space of the attacked DNN model, which is mostly due to the end-to-end supervised training paradigm. Inspired by this observation, we propose a novel backdoor defense via decoupling the original end-to-end training process into three stages. Specifically, we first learn the backbone of a DNN model via \emph{self-supervised learning} based on training samples without their labels. The learned backbone will map samples with the same ground-truth label to similar locations in the feature space. Then, we freeze the parameters of the learned backbone and train the remaining fully connected layers via standard training with all (labeled) training samples. Lastly, to further alleviate side-effects of poisoned samples in the second stage, we remove labels of some `low-credible' samples determined based on the learned model and conduct a \emph{semi-supervised fine-tuning} of the whole model. Extensive experiments on multiple benchmark datasets and DNN models verify that the proposed defense is effective in reducing backdoor threats while preserving high accuracy in predicting benign samples. Our code is available at \url{https://github.com/SCLBD/DBD}.

","<a href=""arXiv"" target=""_blank"">[https://github.com/SCLBD/DBD}]</a>

","arXiv
DBLP
DBLP"
Research on Application of Mimic Defense in Industrial Control System Security,W. Dai S. Li L. Lu Y. Ye F. Meng D. Zhang,"2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)",2022-02-03,"<a href=""IEEE (2022-02-03) : Research on Application of Mimic Defense in Industrial Control System Security"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9688212]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICIBA52610.2021.9688212]</a>","The attacks and defenses in industrial control system(ICS) security has been in an asymmetrical situation. Due to the closeness, specificity and complexity of ICS, it has more vulnerabilities and deeper hidden backdoors. The traditional “remedy” type of defense is difficult to prevent increasingly complex attack methods. Starting from the attack chain model, this paper analyzes shortcomings of traditional ICS defense technology, and proposes an ICS security system structure based on mimic defense. Digital twin technology is used to build executive entities of mimic defense model, and increase the dynamic and heterogeneity of ICS. The system structure based on mimic defense has better protection against unknown vulnerabilities, and reduces the probability of being attacked successfully, improve the ICS's ability to withstand attacks through endogenous security at the system level, and alleviate the passive and lagged situation of ICS defense.",,IEEE
An integrated cyber security risk management framework and risk predication for the critical infrastructure protection,"Halima Ibrahim Kure, Shareeful Islam, Haralambos Mouratidis",Neural Computing and Applications,2022-02-02,"<a href=""Springer (2022-02-02) : An integrated cyber security risk management framework and risk predication for the critical infrastructure protection"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-022-06959-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-022-06959-2]</a>",Cyber security risk management plays an important role for today’s businesses due to the rapidly changing threat landscape and the existence of...,,Springer
Augmented Dual-Shuffle-based Moving Target Defense to Ensure CIA-triad in Federated Learning,Z. Zhou C. Xu M. Wang T. Ma S. Yu,2021 IEEE Global Communications Conference (GLOBECOM),2022-02-02,"<a href=""IEEE (2022-02-02) : Augmented Dual-Shuffle-based Moving Target Defense to Ensure CIA-triad in Federated Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9685154]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM46510.2021.9685154]</a>","In today's “Internet of Everything (IoE)” era, the collaboration from massive participants significantly boosts the performance and efficiency of model training. This trend also un-avoidably stirs up considerable concerns about multi-dimensional security problems. Under the circumstances, federated learning (FL) is enthusiastically adopted, as it protects privacy to a certain extent by only processing personal data locally. Nevertheless, FL's characteristics of concealment also pave the way for sev-eral emerging attacks during the training process, i.e., model inversion, poisoning, and backdoor. Currently, although partially mitigating attack effects, existing countermeasures against those threats are studied separately and orthogonal. This separation makes those defense methods mutually exclusive and restrictive in real-world application scenarios, far from satisfying. In this paper, we extensively model different attack paradigms into three types based on CIA-triad, the well-known information security primitive, and propose a novel dual-shuffle method to thwart aforementioned threats jointly. Concretely speaking, our primary model shuffling mechanism provides the confidentiality guarantee based on the information-theoretic notion of identifiability then, an augmented client shuffling mechanism purges the user group of adversaries proactively without any compromise of anonymous constraints. By conducting a series of experiments on bench-mark datasets, we demonstrate that our method could achieve significant security and convergence performance against three state-of-the-art attacks.",,IEEE
FedEqual: Defending Model Poisoning Attacks in Heterogeneous Federated Learning,L. -Y. Chen T. -C. Chiu A. -C. Pang L. -C. Cheng,2021 IEEE Global Communications Conference (GLOBECOM),2022-02-02,"<a href=""IEEE (2022-02-02) : FedEqual: Defending Model Poisoning Attacks in Heterogeneous Federated Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9685082]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM46510.2021.9685082]</a>","With the upcoming edge AI, federated learning (FL) is a privacy-preserving framework to meet the General Data Protection Regulation (GDPR). Unfortunately, FL is vulnerable to an up-to-date security threat, model poisoning attacks. By successfully replacing the global model with the targeted poisoned model, malicious end devices can trigger backdoor attacks and manipulate the whole learning process. The traditional researches under a homogeneous environment can ideally exclude the outliers with scarce side-effects on model performance. However, in privacy-preserving FL, each end device possibly owns a few data classes and different amounts of data, forming into a substantial heterogeneous environment where outliers could be malicious or benign. To achieve the system performance and robustness of FL's framework, we should not assertively remove any local model from the global model updating procedure. Therefore, in this paper, we propose a defending strategy called FedEqual to mitigate model poisoning attacks while preserving the learning task's performance without excluding any benign models. The results show that FedEqual outperforms other state-of-the-art baselines under different heterogeneous environments based on reproduced up-to-date model poisoning attacks.",,IEEE
"Threats, attacks and defenses to federated learning: issues, taxonomy and perspectives","Pengrui Liu, Xiangrui Xu, Wei Wang",Cybersecurity,2022-02-02,"<a href=""Springer (2022-02-02) : Threats, attacks and defenses to federated learning: issues, taxonomy and perspectives"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-021-00105-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-021-00105-6]</a>",Empirical attacks on Federated Learning (FL) systems indicate that FL is fraught with numerous attack surfaces throughout the FL execution. These...,,Springer
BlindNet backdoor: Attack on deep neural network using blind watermark,Kwon H.,Multimedia Tools and Applications,2022-02-01,"<a href=""ScienceDirect (2022-02-01) : BlindNet backdoor: Attack on deep neural network using blind watermark"" target=""_blank"">[https://doi.org/10.1007/s11042-021-11135-0]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/s11042-021-11135-0]</a>",,,ScienceDirect
ARIBA: Towards Accurate and Robust Identification of Backdoor Attacks in Federated Learning,"Yuxi Mi, Jihong Guan, Shuigeng Zhou",arXiv,2022-02,"<a href=""DBLP (2022-02) : ARIBA: Towards Accurate and Robust Identification of Backdoor Attacks in Federated Learning"" target=""_blank"">[https://arxiv.org/abs/2202.04311]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2202.04311]</a>",,,DBLP
Adversarial Fine-tuning for Backdoor Defense: Connect Adversarial Examples to Triggered Samples,"Bingxu Mu, Le Wang, Zhenxing Niu",arXiv,2022-02,"<a href=""DBLP (2022-02) : Adversarial Fine-tuning for Backdoor Defense: Connect Adversarial Examples to Triggered Samples"" target=""_blank"">[https://arxiv.org/abs/2202.06312]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2202.06312]</a>",,,DBLP
Backdoor Detection in Reinforcement Learning,"Junfeng Guo, Ang Li, Cong Liu",arXiv,2022-02,"<a href=""DBLP (2022-02) : Backdoor Detection in Reinforcement Learning"" target=""_blank"">[https://arxiv.org/abs/2202.03609]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2202.03609]</a>",,,DBLP
A new idea for RSA backdoors,Marco Cesati,"arXiv
arXiv","2022-01-31
2022-01","<a href=""arXiv (2022-01-31) : A new idea for RSA backdoors"" target=""_blank"">[http://arxiv.org/abs/2201.13153v1]</a>
<a href=""DBLP (2022-01) : A new idea for RSA backdoors"" target=""_blank"">[https://arxiv.org/abs/2201.13153]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2201.13153]</a>","This article proposes a new method to inject backdoors in RSA and other cryptographic primitives based on the Integer Factorization problem for balanced semi-primes. The method relies on mathematical congruences among the factors of the semi-primes modulo a large prime number, which acts as a ""designer key"" or ""escrow key"". In particular, two different backdoors are proposed, one targeting a single semi-prime and the other one a pair of semi-primes. The article also describes the results of tests performed on a SageMath implementation of the backdoors.
","
","arXiv
DBLP"
Imperceptible and Multi-channel Backdoor Attack against Deep Neural Networks,"Mingfu Xue, Shifeng Ni, Yinghao Wu, Yushu Zhang, Jian Wang, Weiqiang Liu","arXiv
arXiv","2022-01-31
2022-01","<a href=""arXiv (2022-01-31) : Imperceptible and Multi-channel Backdoor Attack against Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2201.13164v1]</a>
<a href=""DBLP (2022-01) : Imperceptible and Multi-channel Backdoor Attack against Deep Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2201.13164]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2201.13164]</a>","Recent researches demonstrate that Deep Neural Networks (DNN) models are vulnerable to backdoor attacks. The backdoored DNN model will behave maliciously when images containing backdoor triggers arrive. To date, existing backdoor attacks are single-trigger and single-target attacks, and the triggers of most existing backdoor attacks are obvious thus are easy to be detected or noticed. In this paper, we propose a novel imperceptible and multi-channel backdoor attack against Deep Neural Networks by exploiting Discrete Cosine Transform (DCT) steganography. Based on the proposed backdoor attack method, we implement two variants of backdoor attacks, i.e., N-to-N backdoor attack and N-to-One backdoor attack. Specifically, for a colored image, we utilize DCT steganography to construct the trigger on different channels of the image. As a result, the trigger is stealthy and natural. Based on the proposed method, we implement multi-target and multi-trigger backdoor attacks. Experimental results demonstrate that the average attack success rate of the N-to-N backdoor attack is 93.95% on CIFAR-10 dataset and 91.55% on TinyImageNet dataset, respectively. The average attack success rate of N-to-One attack is 90.22% and 89.53% on CIFAR-10 and TinyImageNet datasets, respectively. Meanwhile, the proposed backdoor attack does not affect the classification accuracy of the DNN model. Moreover, the proposed attack is demonstrated to be robust to the state-of-the-art backdoor defense (Neural Cleanse).
","
","arXiv
DBLP"
DFE: efficient IoT network intrusion detection using deep feature extraction,"Amir Basati, Mohammad Mehdi Faghih",Neural Computing and Applications,2022-01-29,"<a href=""Springer (2022-01-29) : DFE: efficient IoT network intrusion detection using deep feature extraction"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-021-06826-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-021-06826-6]</a>","In recent years, the Internet of Things (IoT) has received a lot of attention. It has been used in many applications such as the control industry,...",,Springer
Backdoors Stuck At The Frontdoor: Multi-Agent Backdoor Attacks That Backfire,"Siddhartha Datta, Nigel Shadbolt","arXiv
arXiv","2022-01-28
2022-01","<a href=""arXiv (2022-01-28) : Backdoors Stuck At The Frontdoor: Multi-Agent Backdoor Attacks That Backfire"" target=""_blank"">[http://arxiv.org/abs/2201.12211v1]</a>
<a href=""DBLP (2022-01) : Backdoors Stuck At The Frontdoor: Multi-Agent Backdoor Attacks That Backfire"" target=""_blank"">[https://arxiv.org/abs/2201.12211]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2201.12211]</a>","Malicious agents in collaborative learning and outsourced data collection threaten the training of clean models. Backdoor attacks, where an attacker poisons a model during training to successfully achieve targeted misclassification, are a major concern to train-time robustness. In this paper, we investigate a multi-agent backdoor attack scenario, where multiple attackers attempt to backdoor a victim model simultaneously. A consistent backfiring phenomenon is observed across a wide range of games, where agents suffer from a low collective attack success rate. We examine different modes of backdoor attack configurations, non-cooperation / cooperation, joint distribution shifts, and game setups to return an equilibrium attack success rate at the lower bound. The results motivate the re-evaluation of backdoor defense research for practical environments.
","
","arXiv
DBLP"
Auditable attribute-based data access control using blockchain in cloud storage,"V. Ezhil Arasi, K. Indra Gandhi, K. Kulothungan",The Journal of Supercomputing,2022-01-26,"<a href=""Springer (2022-01-26) : Auditable attribute-based data access control using blockchain in cloud storage"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-021-04293-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-021-04293-3]</a>",Data security in cloud data sharing system is effectively ensured by data access control mechanism. Data access control becomes more challenging...,,Springer
ROWBACK: RObust Watermarking for neural networks using BACKdoors,N. Chattopadhyay A. Chattopadhyay,"2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)
Proceedings - 20th IEEE International Conference on Machine Learning and Applications, ICMLA 2021
ICMLA","2022-01-25
2021-01-01
2021","<a href=""IEEE (2022-01-25) : ROWBACK: RObust Watermarking for neural networks using BACKdoors"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9680232]</a>
<a href=""ScienceDirect (2021-01-01) : ROWBACK: RObust Watermarking for neural networks using BACKdoors"" target=""_blank"">[https://doi.org/10.1109/ICMLA52953.2021.00274]</a>
<a href=""DBLP (2021) : ROWBACK: RObust Watermarking for neural networks using BACKdoors"" target=""_blank"">[https://doi.org/10.1109/ICMLA52953.2021.00274]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICMLA52953.2021.00274]</a>
<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICMLA52953.2021.00274]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICMLA52953.2021.00274]</a>","Claiming ownership of trained neural networks is critical towards stakeholders investing heavily in high performance neural networks. There is an associated cost for the entire pipeline starting from data curation to high performance computing infrastructure for neural architecture search and training the model. Watermarking neural networks is a potential solution to the problem, but standard techniques suffer from vulnerabilities demonstrated by attackers. In this paper, we propose a robust watermarking mechanism for neural architectures. Our proposed method ROWBACK turns two properties of neural networks, the presence of adversarial examples and the ability to trap backdoors in the network while training, into a scheme that guarantees strong proofs of ownership. We redesign the Trigger Set for watermarking using adversarial examples of the model which needs to be watermarked, and assign specific labels based on adversarial behaviour. We also mark every layer separately, during training, in order to ensure that removing watermarks requires complete retraining. We have tested ROWBACK for satisfying key indicative properties expected of a reliable watermarking scheme (generates accuracies within 1 - 2% of actual model, and a complete 100% match on the Trigger Set for verification), whilst being robust against state-of-the-art watermark removal attacks [1] (requires re-training of all layers with at least 60% samples and for at least more than 45% of epochs of actual training).

","

","IEEE
ScienceDirect
DBLP"
Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective,"Yi Zeng, Won Park, Z. Morley Mao, Ruoxi Jia","arXiv
ICCV
arXiv","2022-01-25
2021
2021-04","<a href=""arXiv (2022-01-25) : Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective"" target=""_blank"">[http://arxiv.org/abs/2104.03413v4]</a>
<a href=""DBLP (2021) : Rethinking the Backdoor Attacks&apos, Triggers: A Frequency Perspective"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01616]</a>
<a href=""DBLP (2021-04) : Rethinking the Backdoor Attacks&apos, Triggers: A Frequency Perspective"" target=""_blank"">[https://arxiv.org/abs/2104.03413]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01616]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2104.03413]</a>","Backdoor attacks have been considered a severe security threat to deep learning. Such attacks can make models perform abnormally on inputs with predefined triggers and still retain state-of-the-art performance on clean data. While backdoor attacks have been thoroughly investigated in the image domain from both attackers' and defenders' sides, an analysis in the frequency domain has been missing thus far. This paper first revisits existing backdoor triggers from a frequency perspective and performs a comprehensive analysis. Our results show that many current backdoor attacks exhibit severe high-frequency artifacts, which persist across different datasets and resolutions. We further demonstrate these high-frequency artifacts enable a simple way to detect existing backdoor triggers at a detection rate of 98.50% without prior knowledge of the attack details and the target model. Acknowledging previous attacks' weaknesses, we propose a practical way to create smooth backdoor triggers without high-frequency artifacts and study their detectability. We show that existing defense works can benefit by incorporating these smooth triggers into their design consideration. Moreover, we show that the detector tuned over stronger smooth triggers can generalize well to unseen weak smooth triggers. In short, our work emphasizes the importance of considering frequency analysis when designing both backdoor attacks and defenses in deep learning.

","

","arXiv
DBLP
DBLP"
Backdoor Defense with Machine Unlearning,"Yang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li Wang, Jianfeng Ma","arXiv
INFOCOM
arXiv","2022-01-24
2022
2022-01","<a href=""arXiv (2022-01-24) : Backdoor Defense with Machine Unlearning"" target=""_blank"">[http://arxiv.org/abs/2201.09538v1]</a>
<a href=""DBLP (2022) : Backdoor Defense with Machine Unlearning"" target=""_blank"">[https://doi.org/10.1109/INFOCOM48880.2022.9796974]</a>
<a href=""DBLP (2022-01) : Backdoor Defense with Machine Unlearning"" target=""_blank"">[https://arxiv.org/abs/2201.09538]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/INFOCOM48880.2022.9796974]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2201.09538]</a>","Backdoor injection attack is an emerging threat to the security of neural networks, however, there still exist limited effective defense methods against the attack. In this paper, we propose BAERASE, a novel method that can erase the backdoor injected into the victim model through machine unlearning. Specifically, BAERASE mainly implements backdoor defense in two key steps. First, trigger pattern recovery is conducted to extract the trigger patterns infected by the victim model. Here, the trigger pattern recovery problem is equivalent to the one of extracting an unknown noise distribution from the victim model, which can be easily resolved by the entropy maximization based generative model. Subsequently, BAERASE leverages these recovered trigger patterns to reverse the backdoor injection procedure and induce the victim model to erase the polluted memories through a newly designed gradient ascent based machine unlearning method. Compared with the previous machine unlearning solutions, the proposed approach gets rid of the reliance on the full access to training data for retraining and shows higher effectiveness on backdoor erasing than existing fine-tuning or pruning methods. Moreover, experiments show that BAERASE can averagely lower the attack success rates of three kinds of state-of-the-art backdoor attacks by 99\% on four benchmark datasets.

","

","arXiv
DBLP
DBLP"
Neighboring Backdoor Attacks on Graph Convolutional Network,"Liang Chen, Qibiao Peng, Jintang Li, Yang Liu, Jiawei Chen, Yong Li, Zibin Zheng","arXiv
arXiv","2022-01-17
2022-01","<a href=""arXiv (2022-01-17) : Neighboring Backdoor Attacks on Graph Convolutional Network"" target=""_blank"">[http://arxiv.org/abs/2201.06202v1]</a>
<a href=""DBLP (2022-01) : Neighboring Backdoor Attacks on Graph Convolutional Network"" target=""_blank"">[https://arxiv.org/abs/2201.06202]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2201.06202]</a>","Backdoor attacks have been widely studied to hide the misclassification rules in the normal models, which are only activated when the model is aware of the specific inputs (i.e., the trigger). However, despite their success in the conventional Euclidean space, there are few studies of backdoor attacks on graph structured data. In this paper, we propose a new type of backdoor which is specific to graph data, called neighboring backdoor. Considering the discreteness of graph data, how to effectively design the triggers while retaining the model accuracy on the original task is the major challenge. To address such a challenge, we set the trigger as a single node, and the backdoor is activated when the trigger node is connected to the target node. To preserve the model accuracy, the model parameters are not allowed to be modified. Thus, when the trigger node is not connected, the model performs normally. Under these settings, in this work, we focus on generating the features of the trigger node. Two types of backdoors are proposed: (1) Linear Graph Convolution Backdoor which finds an approximation solution for the feature generation (can be viewed as an integer programming problem) by looking at the linear part of GCNs. (2) Variants of existing graph attacks. We extend current gradient-based attack methods to our backdoor attack scenario. Extensive experiments on two social networks and two citation networks datasets demonstrate that all proposed backdoors can achieve an almost 100\% attack success rate while having no impact on predictive accuracy.
","
","arXiv
DBLP"
Design possibilities and challenges of DNN models: a review on the perspective of end devices,"Hanan Hussain, P. S. Tamizharasan, C. S. Rahul",Artificial Intelligence Review,2022-01-16,"<a href=""Springer (2022-01-16) : Design possibilities and challenges of DNN models: a review on the perspective of end devices"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-022-10138-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-022-10138-z]</a>",Deep Neural Network (DNN) models for both resource-rich environments and resource-constrained devices have become abundant in recent years. As of...,,Springer
Classification of IOT-Malware using Machine Learning,S. Madan M. Singh,2021 International Conference on Technological Advancements and Innovations (ICTAI),2022-01-14,"<a href=""IEEE (2022-01-14) : Classification of IOT-Malware using Machine Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9673185]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICTAI53825.2021.9673185]</a>","Every day, attackers target embedded IoT devices, causing damage to key cyber-infrastructure, obtaining users’ personal information, and misusing it to a greater extent. Data confidentiality, authentication, and privacy, denial of service, nonrepudiation, and digital content protection are only a few of the difficult security challenges that must be handled. To exploit these resource-constrained Cyber-physical systems, attackers use brute-force assaults, man-in-the-middle attacks, injecting malicious code, eavesdropping, and backdoors, among other methods. In this research, we present a hybrid analysis method for analyzing Linux-based IoT malware and event correlation for incident management using anomaly detection. For malware classification, the machine learning model is built using information from both static and dynamic analysis of harmful programs. For anomaly identification and event correlation, the anomalous DDoS traffic detection approach is also proposed. The F1-score is maximized for various DDoS attacks using the threshold selection approach, and the results are compared to the state-of-the-art literature.",,IEEE
APT-Dt-KC: advanced persistent threat detection based on kill-chain model,"Maryam Panahnejad, Meghdad Mirabi",The Journal of Supercomputing,2022-01-12,"<a href=""Springer (2022-01-12) : APT-Dt-KC: advanced persistent threat detection based on kill-chain model"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-021-04201-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-021-04201-9]</a>",Advanced persistent threat attacks are considered as a serious risk to almost any infrastructure since attackers are constantly changing and evolving...,,Springer
RFLBAT: A Robust Federated Learning Algorithm against Backdoor Attack,"Yongkang Wang, Dihua Zhai, Yufeng Zhan, Yuanqing Xia","arXiv
arXiv","2022-01-11
2022-01","<a href=""arXiv (2022-01-11) : RFLBAT: A Robust Federated Learning Algorithm against Backdoor Attack"" target=""_blank"">[http://arxiv.org/abs/2201.03772v1]</a>
<a href=""DBLP (2022-01) : RFLBAT: A Robust Federated Learning Algorithm against Backdoor Attack"" target=""_blank"">[https://arxiv.org/abs/2201.03772]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2201.03772]</a>","Federated learning (FL) is a distributed machine learning paradigm where enormous scattered clients (e.g. mobile devices or IoT devices) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. Unfortunately, FL is susceptible to a variety of attacks, including backdoor attack, which is made substantially worse in the presence of malicious attackers. Most of algorithms usually assume that the malicious at tackers no more than benign clients or the data distribution is independent identically distribution (IID). However, no one knows the number of malicious attackers and the data distribution is usually non identically distribution (Non-IID). In this paper, we propose RFLBAT which utilizes principal component analysis (PCA) technique and Kmeans clustering algorithm to defend against backdoor attack. Our algorithm RFLBAT does not bound the number of backdoored attackers and the data distribution, and requires no auxiliary information outside of the learning process. We conduct extensive experiments including a variety of backdoor attack types. Experimental results demonstrate that RFLBAT outperforms the existing state-of-the-art algorithms and is able to resist various backdoor attack scenarios including different number of attackers (DNA), different Non-IID scenarios (DNS), different number of clients (DNC) and distributed backdoor attack (DBA).
","
","arXiv
DBLP"
RF-DNN2: An ensemble learner for effective detection of PHP Webshells,A. Hannousse S. Yahiouche,2021 International Conference on Artificial Intelligence for Cyber Security Systems and Privacy (AI-CSP),2022-01-11,"<a href=""IEEE (2022-01-11) : RF-DNN2: An ensemble learner for effective detection of PHP Webshells"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671226]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/AI-CSP52968.2021.9671226]</a>","The explosion of web services has been accompanied by a rapid development of dangerous cyberattack methods. Webshells are considered among the easiest and most persistent cyberattack methods targeting web servers in the last few years. Webshells are malicious scripts injected into web servers to gain illegal persistent and remote access through simple and benign HTTP requests. Through webshells, hackers can access confidential data, execute system commands and compromise more machines connected to the target server. All those dangerous actions can be performed without being noticed by administrators and malware detectors. In this paper, we propose an ensemble learner model named RF-DNN2 for the detection of webshells written in PHP scripting language. The proposed model combines the predictions of two deep neural network models and uses Random Forest as a stacking meta classifier. The first deep neural network model is trained on vectorized source codes of webshells and the second deep neural network model is trained on vectorized opcode sequences generated from webshell sources. Individual deep neural network models and RF-DNN2 are compared with a set of traditional classifiers and other ensemble learners. The experiments show that the RF-DNN2 model has the best accuracy 98% and macro F1-score 97.45%.",,IEEE
Optimal hybrid heat transfer search and grey wolf optimization-based homomorphic encryption model to assure security in cloud-based IoT environment,"J. Thresa Jeniffer, A. Chandrasekar",Peer-to-Peer Networking and Applications,2022-01-09,"<a href=""Springer (2022-01-09) : Optimal hybrid heat transfer search and grey wolf optimization-based homomorphic encryption model to assure security in cloud-based IoT environment"" target=""_blank"">[https://link.springer.com/article/10.1007/s12083-021-01263-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12083-021-01263-7]</a>",Organizations often store the massive data generated by the IoT devices in the cloud for decision-making and enhancing the customer experience....,,Springer
Adversarial example detection for DNN models: a review and experimental comparison,"Ahmed Aldahdooh, Wassim Hamidouche, ... Olivier Déforges",Artificial Intelligence Review,2022-01-06,"<a href=""Springer (2022-01-06) : Adversarial example detection for DNN models: a review and experimental comparison"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-021-10125-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-021-10125-w]</a>","Deep learning (DL) has shown great success in many human-related tasks, which has led to its adoption in many computer vision based applications,...",,Springer
Development of the RISC-V entropy source interface,"Markku-Juhani O. Saarinen, G. Richard Newell, Ben Marshall",Journal of Cryptographic Engineering,2022-01-06,"<a href=""Springer (2022-01-06) : Development of the RISC-V entropy source interface"" target=""_blank"">[https://link.springer.com/article/10.1007/s13389-021-00275-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13389-021-00275-6]</a>",The RISC-V true random number generator (TRNG) architecture breaks with previous ISA TRNG practice by splitting the entropy source (ES) component...,,Springer
A Survey on Internet-of-Things Security: Threats and Emerging Countermeasures,"Dorsaf Swessi, Hanen Idoudi",Wireless Personal Communications,2022-01-05,"<a href=""Springer (2022-01-05) : A Survey on Internet-of-Things Security: Threats and Emerging Countermeasures"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-021-09420-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-021-09420-0]</a>","Internet of things (IoT) is a world wide network and set of paradigms that are intended to allow communications between anything, anytime and...",,Springer
Performance analysis of machine learning models for intrusion detection system using Gini Impurity-based Weighted Random Forest (GIWRF) feature selection technique,"Raisa Abedin Disha, Sajjad Waheed",Cybersecurity,2022-01-04,"<a href=""Springer (2022-01-04) : Performance analysis of machine learning models for intrusion detection system using Gini Impurity-based Weighted Random Forest (GIWRF) feature selection technique"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-021-00103-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-021-00103-8]</a>","To protect the network, resources, and sensitive data, the intrusion detection system (IDS) has become a fundamental component of organizations that...",,Springer
Real-time intrusion detection based on residual learning through ResNet algorithm,"Asma Shaikh, Preeti Gupta",International Journal of System Assurance Engineering and Management,2022-01-04,"<a href=""Springer (2022-01-04) : Real-time intrusion detection based on residual learning through ResNet algorithm"" target=""_blank"">[https://link.springer.com/article/10.1007/s13198-021-01558-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13198-021-01558-1]</a>","Over the years, attacks from the Internet have grown more advanced and can bypass simple security measures like antivirus scanners and firewalls....",,Springer
Compression-Resistant Backdoor Attack against Deep Neural Networks,"Mingfu Xue, Xin Wang, Shichang Sun, Yushu Zhang, Jian Wang, Weiqiang Liu","arXiv
arXiv","2022-01-03
2022-01","<a href=""arXiv (2022-01-03) : Compression-Resistant Backdoor Attack against Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2201.00672v1]</a>
<a href=""DBLP (2022-01) : Compression-Resistant Backdoor Attack against Deep Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2201.00672]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1007/s10489-023-04575-8]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2201.00672]</a>","In recent years, many backdoor attacks based on training data poisoning have been proposed. However, in practice, those backdoor attacks are vulnerable to image compressions. When backdoor instances are compressed, the feature of specific backdoor trigger will be destroyed, which could result in the backdoor attack performance deteriorating. In this paper, we propose a compression-resistant backdoor attack based on feature consistency training. To the best of our knowledge, this is the first backdoor attack that is robust to image compressions. First, both backdoor images and their compressed versions are input into the deep neural network (DNN) for training. Then, the feature of each image is extracted by internal layers of the DNN. Next, the feature difference between backdoor images and their compressed versions are minimized. As a result, the DNN treats the feature of compressed images as the feature of backdoor images in feature space. After training, the backdoor attack against DNN is robust to image compression. Furthermore, we consider three different image compressions (i.e., JPEG, JPEG2000, WEBP) in feature consistency training, so that the backdoor attack is robust to multiple image compression algorithms. Experimental results demonstrate the effectiveness and robustness of the proposed backdoor attack. When the backdoor instances are compressed, the attack success rate of common backdoor attack is lower than 10%, while the attack success rate of our compression-resistant backdoor is greater than 97%. The compression-resistant attack is still robust even when the backdoor images are compressed with low compression quality. In addition, extensive experiments have demonstrated that, our compression-resistant backdoor attack has the generalization ability to resist image compression which is not used in the training process.
","
","arXiv
DBLP"
"AI-Based Mobile Edge Computing for IoT: Applications, Challenges, and Future Scope","Ashish Singh, Suresh Chandra Satapathy, ... Adnan Gutub",Arabian Journal for Science and Engineering,2022-01-03,"<a href=""Springer (2022-01-03) : AI-Based Mobile Edge Computing for IoT: Applications, Challenges, and Future Scope"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-021-06348-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-021-06348-2]</a>",New technology is needed to meet the latency and bandwidth issues present in cloud computing architecture specially to support the currency of 5G...,,Springer
'It's Backdoor Accessibility': Disabled Students' Navigation of University Campus,Wertans E.,Journal of Disability Studies in Education,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : 'It's Backdoor Accessibility': Disabled Students' Navigation of University Campus"" target=""_blank"">[https://doi.org/10.1163/25888803-bja10013]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1163/25888803-bja10013]</a>",,,ScienceDirect
A Federated Learning Backdoor Attack Defense,Yan J.,"Proceedings - IEEE 8th International Conference on Big Data Computing Service and Applications, BigDataService 2022",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : A Federated Learning Backdoor Attack Defense"" target=""_blank"">[https://doi.org/10.1109/BigDataService55688.2022.00030]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/BigDataService55688.2022.00030]</a>",,,ScienceDirect
A Novel Method of Deep Neural Network Model Protection with Key and Backdoor Watermark,Cen D.,"Proceedings - 2022 4th International Symposium on Smart and Healthy Cities, ISHC 2022",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : A Novel Method of Deep Neural Network Model Protection with Key and Backdoor Watermark"" target=""_blank"">[https://doi.org/10.1109/ISHC56805.2022.00054]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ISHC56805.2022.00054]</a>",,,ScienceDirect
A Temporal-Pattern Backdoor Attack to Deep Reinforcement Learning,Yu Y.,"Proceedings - IEEE Global Communications Conference, GLOBECOM",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : A Temporal-Pattern Backdoor Attack to Deep Reinforcement Learning"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM48099.2022.10000751]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM48099.2022.10000751]</a>",,,ScienceDirect
A Trigger Sample Detection Scheme Based on Custom Backdoor Behaviors,Wang S.,Journal of Cyber Security,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : A Trigger Sample Detection Scheme Based on Custom Backdoor Behaviors"" target=""_blank"">[https://doi.org/10.19363/J.cnki.cn10-1380/tn.2022.11.03]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.19363/J.cnki.cn10-1380/tn.2022.11.03]</a>",,,ScienceDirect
A Universal Identity Backdoor Attack against Speaker Verification based on Siamese Network,Zhao H.,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : A Universal Identity Backdoor Attack against Speaker Verification based on Siamese Network"" target=""_blank"">[https://doi.org/10.21437/Interspeech.2022-446]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.21437/Interspeech.2022-446]</a>",,,ScienceDirect
ADVERSARIAL UNLEARNING OF BACKDOORS VIA IMPLICIT HYPERGRADIENT,Zeng Y.,ICLR 2022 - 10th International Conference on Learning Representations,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : ADVERSARIAL UNLEARNING OF BACKDOORS VIA IMPLICIT HYPERGRADIENT"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
AEVA: BLACK-BOX BACKDOOR DETECTION USING ADVERSARIAL EXTREME VALUE ANALYSIS,Guo J.,ICLR 2022 - 10th International Conference on Learning Representations,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : AEVA: BLACK-BOX BACKDOOR DETECTION USING ADVERSARIAL EXTREME VALUE ANALYSIS"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
An Improved Method for Making CNN Immune to Backdoor Attack by Activating Clustering,Zhou Y.,"Proceedings - 2022 6th International Symposium on Computer Science and Intelligent Control, ISCSIC 2022",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : An Improved Method for Making CNN Immune to Backdoor Attack by Activating Clustering"" target=""_blank"">[https://doi.org/10.1109/ISCSIC57216.2022.00012]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ISCSIC57216.2022.00012]</a>",,,ScienceDirect
An Improved Optimized Model for Invisible Backdoor Attack Creation Using Steganography,Alghazzawi D.M.,"Computers, Materials and Continua",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : An Improved Optimized Model for Invisible Backdoor Attack Creation Using Steganography"" target=""_blank"">[https://doi.org/10.32604/cmc.2022.022748]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.32604/cmc.2022.022748]</a>",,,ScienceDirect
An Invisible Backdoor Attack based on DCT-Injection,Xiao T.,"Proceedings of 2022 IEEE International Conference on Unmanned Systems, ICUS 2022",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : An Invisible Backdoor Attack based on DCT-Injection"" target=""_blank"">[https://doi.org/10.1109/ICUS55513.2022.9987040]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICUS55513.2022.9987040]</a>",,,ScienceDirect
An Invisible Black-Box Backdoor Attack Through Frequency Domain,Wang T.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2022-01-01,"<a href=""ScienceDirect (2022-01-01) : An Invisible Black-Box Backdoor Attack Through Frequency Domain"" target=""_blank"">[https://doi.org/10.1007/978-3-031-19778-9_23]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-19778-9_23]</a>",,,ScienceDirect
Asynchronous Evolutionary Algorithm for Finding Backdoors in Boolean Satisfiability,Pavlenko A.,"2022 IEEE Congress on Evolutionary Computation, CEC 2022 - Conference Proceedings",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Asynchronous Evolutionary Algorithm for Finding Backdoors in Boolean Satisfiability"" target=""_blank"">[https://doi.org/10.1109/CEC55065.2022.9870262]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CEC55065.2022.9870262]</a>",,,ScienceDirect
BACKDOOR DEFENSE VIA DECOUPLING THE TRAINING PROCESS,Huang K.,ICLR 2022 - 10th International Conference on Learning Representations,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : BACKDOOR DEFENSE VIA DECOUPLING THE TRAINING PROCESS"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
BADPRE: TASK-AGNOSTIC BACKDOOR ATTACKS TO PRE-TRAINED NLP FOUNDATION MODELS,Chen K.,ICLR 2022 - 10th International Conference on Learning Representations,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : BADPRE: TASK-AGNOSTIC BACKDOOR ATTACKS TO PRE-TRAINED NLP FOUNDATION MODELS"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Backdoor Attack Against Deep Learning-Based Autonomous Driving with Fogging,Liu J.,Communications in Computer and Information Science,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Backdoor Attack Against Deep Learning-Based Autonomous Driving with Fogging"" target=""_blank"">[https://doi.org/10.1007/978-981-19-7943-9_21]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-981-19-7943-9_21]</a>",,,ScienceDirect
Backdoor Attack Based On Feature In Federated Learning,Cao L.,Proceedings of SPIE - The International Society for Optical Engineering,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Backdoor Attack Based On Feature In Federated Learning"" target=""_blank"">[https://doi.org/10.1117/12.2653697]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1117/12.2653697]</a>",,,ScienceDirect
Backdoor Attack on Machine Learning Based Android Malware Detectors,Li C.,IEEE Transactions on Dependable and Secure Computing,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Backdoor Attack on Machine Learning Based Android Malware Detectors"" target=""_blank"">[https://doi.org/10.1109/TDSC.2021.3094824]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TDSC.2021.3094824]</a>",,,ScienceDirect
Backdoor Attacks Against Transfer Learning With Pre-Trained Deep Learning Models,Wang S.,IEEE Transactions on Services Computing,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Backdoor Attacks Against Transfer Learning With Pre-Trained Deep Learning Models"" target=""_blank"">[https://doi.org/10.1109/TSC.2020.3000900]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TSC.2020.3000900]</a>",,,ScienceDirect
Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient Ensembling,Yoo K.Y.,"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient Ensembling"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Backdoor Attacks on Self-Supervised Learning,Saha A.,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Backdoor Attacks on Self-Supervised Learning"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.01298]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.01298]</a>",,,ScienceDirect
Backdoor Defence for Voice Print Recognition Model Based on Speech Enhancement and Weight Pruning,Zhu J.,IEEE Access,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Backdoor Defence for Voice Print Recognition Model Based on Speech Enhancement and Weight Pruning"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2022.3217322]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2022.3217322]</a>",,,ScienceDirect
Backdoor Investigation and Incident Response: From Zero to Profit,Lai A.C.T.,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Backdoor Investigation and Incident Response: From Zero to Profit"" target=""_blank"">[https://doi.org/10.1007/978-3-031-06365-7_14]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-06365-7_14]</a>",,,ScienceDirect
BackdoorBench: A Comprehensive Benchmark of Backdoor Learning,Wu B.,Advances in Neural Information Processing Systems,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : BackdoorBench: A Comprehensive Benchmark of Backdoor Learning"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Backdoors in Neural Models of Source Code,Ramakrishnan G.,Proceedings - International Conference on Pattern Recognition,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Backdoors in Neural Models of Source Code"" target=""_blank"">[https://doi.org/10.1109/ICPR56361.2022.9956690]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICPR56361.2022.9956690]</a>",,,ScienceDirect
Better Trigger Inversion Optimization in Backdoor Scanning,Tao G.,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Better Trigger Inversion Optimization in Backdoor Scanning"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.01301]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.01301]</a>",,,ScienceDirect
Big Brother Is Watching You: A Closer Look at Backdoor Construction,Baksi A.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Big Brother Is Watching You: A Closer Look at Backdoor Construction"" target=""_blank"">[https://doi.org/10.1007/978-3-031-22829-2_5]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-22829-2_5]</a>",,,ScienceDirect
Breaking Distributed Backdoor Defenses for Federated Learning in Non-IID Settings,Yang J.,"Proceedings - 2022 18th International Conference on Mobility, Sensing and Networking, MSN 2022",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Breaking Distributed Backdoor Defenses for Federated Learning in Non-IID Settings"" target=""_blank"">[https://doi.org/10.1109/MSN57253.2022.00064]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/MSN57253.2022.00064]</a>",,,ScienceDirect
CRAB: CERTIFIED PATCH ROBUSTNESS AGAINST POISONING-BASED BACKDOOR ATTACKS,Ji H.,"Proceedings - International Conference on Image Processing, ICIP",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : CRAB: CERTIFIED PATCH ROBUSTNESS AGAINST POISONING-BASED BACKDOOR ATTACKS"" target=""_blank"">[https://doi.org/10.1109/ICIP46576.2022.9897387]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICIP46576.2022.9897387]</a>",,,ScienceDirect
Clean-label Backdoor Attack on Machine Learning-based Malware Detection Models and Countermeasures,Zheng W.,"Proceedings - 2022 IEEE 21st International Conference on Trust, Security and Privacy in Computing and Communications, TrustCom 2022",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Clean-label Backdoor Attack on Machine Learning-based Malware Detection Models and Countermeasures"" target=""_blank"">[https://doi.org/10.1109/TrustCom56396.2022.00171]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TrustCom56396.2022.00171]</a>",,,ScienceDirect
Clean-label poisoning attacks on federated learning for IoT,Yang J.,Expert Systems,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Clean-label poisoning attacks on federated learning for IoT"" target=""_blank"">[https://doi.org/10.1111/exsy.13161]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1111/exsy.13161]</a>",,,ScienceDirect
Combining Defences Against Data-Poisoning Based Backdoor Attacks on Neural Networks,Milakovic A.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Combining Defences Against Data-Poisoning Based Backdoor Attacks on Neural Networks"" target=""_blank"">[https://doi.org/10.1007/978-3-031-10684-2_3]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-10684-2_3]</a>",,,ScienceDirect
Constrained Optimization with Dynamic Bound-scaling for Effective NLP Backdoor Defense,Shen G.,Proceedings of Machine Learning Research,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Constrained Optimization with Dynamic Bound-scaling for Effective NLP Backdoor Defense"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
DEFENDING AGAINST BACKDOOR ATTACKS IN FEDERATED LEARNING WITH DIFFERENTIAL PRIVACY,Miao L.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : DEFENDING AGAINST BACKDOOR ATTACKS IN FEDERATED LEARNING WITH DIFFERENTIAL PRIVACY"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9747653]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9747653]</a>",,,ScienceDirect
DETECTING BACKDOOR ATTACKS AGAINST POINT CLOUD CLASSIFIERS,Xiang Z.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : DETECTING BACKDOOR ATTACKS AGAINST POINT CLOUD CLASSIFIERS"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9747194]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9747194]</a>",,,ScienceDirect
Data Leakage Attack via Backdoor Misclassification Triggers of Deep Learning Models,Yang X.,"Proceedings - 2022 4th International Conference on Data Intelligence and Security, ICDIS 2022",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Data Leakage Attack via Backdoor Misclassification Triggers of Deep Learning Models"" target=""_blank"">[https://doi.org/10.1109/ICDIS55630.2022.00017]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICDIS55630.2022.00017]</a>",,,ScienceDirect
Data Poisoning Attack and Defenses in Connectome-Based Predictive Models,Rosenblatt M.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Data Poisoning Attack and Defenses in Connectome-Based Predictive Models"" target=""_blank"">[https://doi.org/10.1007/978-3-031-23223-7_1]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-23223-7_1]</a>",,,ScienceDirect
Data-Efficient Backdoor Attacks,Xia P.,IJCAI International Joint Conference on Artificial Intelligence,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Data-Efficient Backdoor Attacks"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Data-Free Backdoor Removal Based on Channel Lipschitzness,Zheng R.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Data-Free Backdoor Removal Based on Channel Lipschitzness"" target=""_blank"">[https://doi.org/10.1007/978-3-031-20065-6_11]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-20065-6_11]</a>",,,ScienceDirect
DeepGuard: Backdoor Attack Detection and Identification Schemes in Privacy-Preserving Deep Neural Networks,Chen C.,Security and Communication Networks,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : DeepGuard: Backdoor Attack Detection and Identification Schemes in Privacy-Preserving Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1155/2022/2985308]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1155/2022/2985308]</a>",,,ScienceDirect
DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection,Rieger P.,"29th Annual Network and Distributed System Security Symposium, NDSS 2022",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection"" target=""_blank"">[https://doi.org/10.14722/ndss.2022.23156]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.14722/ndss.2022.23156]</a>",,,ScienceDirect
Defending Batch-Level Label Inference and Replacement Attacks in Vertical Federated Learning,Zou T.,IEEE Transactions on Big Data,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Defending Batch-Level Label Inference and Replacement Attacks in Vertical Federated Learning"" target=""_blank"">[https://doi.org/10.1109/TBDATA.2022.3192121]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TBDATA.2022.3192121]</a>",,,ScienceDirect
Distributed Swift and Stealthy Backdoor Attack on Federated Learning,Sundar A.P.,"2022 IEEE International Conference on Networking, Architecture and Storage, NAS 2022 - Proceedings",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Distributed Swift and Stealthy Backdoor Attack on Federated Learning"" target=""_blank"">[https://doi.org/10.1109/NAS55553.2022.9925353]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/NAS55553.2022.9925353]</a>",,,ScienceDirect
Dual-Key Multimodal Backdoors for Visual Question Answering,Walmer M.,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Dual-Key Multimodal Backdoors for Visual Question Answering"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.01494]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.01494]</a>",,,ScienceDirect
Dynamic Backdoor Attacks Against Machine Learning Models,Salem A.,"Proceedings - 7th IEEE European Symposium on Security and Privacy, Euro S and P 2022",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Dynamic Backdoor Attacks Against Machine Learning Models"" target=""_blank"">[https://doi.org/10.1109/EuroSP53844.2022.00049]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/EuroSP53844.2022.00049]</a>",,,ScienceDirect
Eliminating Backdoor Triggers for Deep Neural Networks Using Attention Relation Graph Distillation,Xia J.,IJCAI International Joint Conference on Artificial Intelligence,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Eliminating Backdoor Triggers for Deep Neural Networks Using Attention Relation Graph Distillation"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Evaluation of Various Defense Techniques Against Targeted Poisoning Attacks in Federated Learning∗,Richards C.,"Proceedings - 2022 IEEE 19th International Conference on Mobile Ad Hoc and Smart Systems, MASS 2022",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Evaluation of Various Defense Techniques Against Targeted Poisoning Attacks in Federated Learning∗"" target=""_blank"">[https://doi.org/10.1109/MASS56207.2022.00102]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/MASS56207.2022.00102]</a>",,,ScienceDirect
FEW-SHOT BACKDOOR ATTACKS ON VISUAL OBJECT TRACKING,Li Y.,ICLR 2022 - 10th International Conference on Learning Representations,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : FEW-SHOT BACKDOOR ATTACKS ON VISUAL OBJECT TRACKING"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Finding Naturally Occurring Physical Backdoors in Image Datasets,Wenger E.,Advances in Neural Information Processing Systems,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Finding Naturally Occurring Physical Backdoors in Image Datasets"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Foiling Training-Time Attacks on Neural Machine Translation Systems,Wang J.,Findings of the Association for Computational Linguistics: EMNLP 2022,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Foiling Training-Time Attacks on Neural Machine Translation Systems"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attacks,Liu T.Y.,Advances in Neural Information Processing Systems,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attacks"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
GAN-Enabled Robust Backdoor Attack for UAV Recognition,Xu M.,"Proceedings - 2022 7th International Conference on Communication, Image and Signal Processing, CCISP 2022",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : GAN-Enabled Robust Backdoor Attack for UAV Recognition"" target=""_blank"">[https://doi.org/10.1109/CCISP55629.2022.9974216]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CCISP55629.2022.9974216]</a>",,,ScienceDirect
HOW TO INJECT BACKDOORS WITH BETTER CONSISTENCY: LOGIT ANCHORING ON CLEAN DATA,Zhang Z.,ICLR 2022 - 10th International Conference on Learning Representations,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : HOW TO INJECT BACKDOORS WITH BETTER CONSISTENCY: LOGIT ANCHORING ON CLEAN DATA"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
INVISIBLE AND EFFICIENT BACKDOOR ATTACKS FOR COMPRESSED DEEP NEURAL NETWORKS,Phan H.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : INVISIBLE AND EFFICIENT BACKDOOR ATTACKS FOR COMPRESSED DEEP NEURAL NETWORKS"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9747582]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9747582]</a>",,,ScienceDirect
Imperceptible Backdoor Attack: From Input Space to Feature Representation,Zhong N.,IJCAI International Joint Conference on Artificial Intelligence,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Imperceptible Backdoor Attack: From Input Space to Feature Representation"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Inconspicuous Data Augmentation Based Backdoor Attack on Deep Neural Networks,Xu C.,International System on Chip Conference,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Inconspicuous Data Augmentation Based Backdoor Attack on Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1109/SOCC56010.2022.9908113]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SOCC56010.2022.9908113]</a>",,,ScienceDirect
Intelligent Fingerprinting to Detect Data Leakage Attacks on Spectrum Sensors,Celdrán A.H.,IEEE International Conference on Communications,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Intelligent Fingerprinting to Detect Data Leakage Attacks on Spectrum Sensors"" target=""_blank"">[https://doi.org/10.1109/ICC45855.2022.9839001]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICC45855.2022.9839001]</a>",,,ScienceDirect
Latent Space-Based Backdoor Attacks Against Deep Neural Networks,Kristanto A.,Proceedings of the International Joint Conference on Neural Networks,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Latent Space-Based Backdoor Attacks Against Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1109/IJCNN55064.2022.9892842]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/IJCNN55064.2022.9892842]</a>",,,ScienceDirect
Linux backdoor detection based on ensemble learning,Sun Y.,Proceedings of SPIE - The International Society for Optical Engineering,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Linux backdoor detection based on ensemble learning"" target=""_blank"">[https://doi.org/10.1117/12.2646330]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1117/12.2646330]</a>",,,ScienceDirect
Low-Poisoning Rate Invisible Backdoor Attack Based on Important Neurons,Yang X.g.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Low-Poisoning Rate Invisible Backdoor Attack Based on Important Neurons"" target=""_blank"">[https://doi.org/10.1007/978-3-031-19214-2_31]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-19214-2_31]</a>",,,ScienceDirect
Mitigating the Backdoor Attack by a Weight-Based Federated Aggregation,Hu F.,Lecture Notes in Electrical Engineering,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Mitigating the Backdoor Attack by a Weight-Based Federated Aggregation"" target=""_blank"">[https://doi.org/10.1007/978-981-19-6203-5_20]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-981-19-6203-5_20]</a>",,,ScienceDirect
Moderate-fitting as a Natural Backdoor Defender for Pre-trained Language Models,Zhu B.,Advances in Neural Information Processing Systems,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Moderate-fitting as a Natural Backdoor Defender for Pre-trained Language Models"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Multi-Model Selective Backdoor Attack with Different Trigger Positions,Kwon H.,IEICE Transactions on Information and Systems,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Multi-Model Selective Backdoor Attack with Different Trigger Positions"" target=""_blank"">[https://doi.org/10.1587/transinf.2021EDL8054]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1587/transinf.2021EDL8054]</a>",,,ScienceDirect
Natural Backdoor Attacks on Deep Neural Networks via Raindrops,Zhao F.,Security and Communication Networks,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Natural Backdoor Attacks on Deep Neural Networks via Raindrops"" target=""_blank"">[https://doi.org/10.1155/2022/4593002]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1155/2022/4593002]</a>",,,ScienceDirect
Never Too Late: Tracing and Mitigating Backdoor Attacks in Federated Learning,Zeng H.,Proceedings of the IEEE Symposium on Reliable Distributed Systems,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Never Too Late: Tracing and Mitigating Backdoor Attacks in Federated Learning"" target=""_blank"">[https://doi.org/10.1109/SRDS55811.2022.00017]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SRDS55811.2022.00017]</a>",,,ScienceDirect
OBJECT-ORIENTED BACKDOOR ATTACK AGAINST IMAGE CAPTIONING,Li M.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : OBJECT-ORIENTED BACKDOOR ATTACK AGAINST IMAGE CAPTIONING"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9746440]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9746440]</a>",,,ScienceDirect
"One-to-N &amp, N-to-One: Two Advanced Backdoor Attacks Against Deep Learning Models",Xue M.,IEEE Transactions on Dependable and Secure Computing,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : One-to-N &amp, N-to-One: Two Advanced Backdoor Attacks Against Deep Learning Models"" target=""_blank"">[https://doi.org/10.1109/TDSC.2020.3028448]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TDSC.2020.3028448]</a>",,,ScienceDirect
Optimized Client-side Detection of Model Poisoning Attacks in Federated learning,Zhang G.,"Proceedings - 24th IEEE International Conference on High Performance Computing and Communications, 8th IEEE International Conference on Data Science and Systems, 20th IEEE International Conference on Smart City and 8th IEEE International Conference on Dependability in Sensor, Cloud and Big Data Systems and Application, HPCC/DSS/SmartCity/DependSys 2022",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Optimized Client-side Detection of Model Poisoning Attacks in Federated learning"" target=""_blank"">[https://doi.org/10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00188]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00188]</a>",,,ScienceDirect
POST-TRAINING DETECTION OF BACKDOOR ATTACKS FOR TWO-CLASS AND MULTI-ATTACK SCENARIOS,Xiang Z.,ICLR 2022 - 10th International Conference on Learning Representations,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : POST-TRAINING DETECTION OF BACKDOOR ATTACKS FOR TWO-CLASS AND MULTI-ATTACK SCENARIOS"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Patch-Based Backdoors Detection and Mitigation with Feature Masking,Wang T.,Communications in Computer and Information Science,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Patch-Based Backdoors Detection and Mitigation with Feature Masking"" target=""_blank"">[https://doi.org/10.1007/978-981-19-7242-3_15]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-981-19-7242-3_15]</a>",,,ScienceDirect
Planting Undetectable Backdoors in Machine Learning Models: [Extended Abstract],Goldwasser S.,"Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Planting Undetectable Backdoors in Machine Learning Models: [Extended Abstract]"" target=""_blank"">[https://doi.org/10.1109/FOCS54457.2022.00092]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/FOCS54457.2022.00092]</a>",,,ScienceDirect
Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks,Shan S.,"Proceedings of the 31st USENIX Security Symposium, Security 2022",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Poison Ink: Robust and Invisible Backdoor Attack,Zhang J.,IEEE Transactions on Image Processing,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Poison Ink: Robust and Invisible Backdoor Attack"" target=""_blank"">[https://doi.org/10.1109/TIP.2022.3201472]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TIP.2022.3201472]</a>",,,ScienceDirect
Power Analysis Attack on Locking SIB based IJTAG Achitecture,Kumar G.,"IEEE/IFIP International Conference on VLSI and System-on-Chip, VLSI-SoC",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Power Analysis Attack on Locking SIB based IJTAG Achitecture"" target=""_blank"">[https://doi.org/10.1109/VLSI-SoC54400.2022.9939634]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/VLSI-SoC54400.2022.9939634]</a>",,,ScienceDirect
Pre-activation Distributions Expose Backdoor Neurons,Zheng R.,Advances in Neural Information Processing Systems,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Pre-activation Distributions Expose Backdoor Neurons"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
RIBAC: Towards Robust and Imperceptible Backdoor Attack against Compact DNN,Phan H.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2022-01-01,"<a href=""ScienceDirect (2022-01-01) : RIBAC: Towards Robust and Imperceptible Backdoor Attack against Compact DNN"" target=""_blank"">[https://doi.org/10.1007/978-3-031-19772-7_41]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-19772-7_41]</a>",,,ScienceDirect
Reputation-Based Defense Scheme Against Backdoor Attacks on Federated Learning,Su L.,Lecture Notes on Data Engineering and Communications Technologies,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Reputation-Based Defense Scheme Against Backdoor Attacks on Federated Learning"" target=""_blank"">[https://doi.org/10.1007/978-981-16-7469-3_107]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-981-16-7469-3_107]</a>",,,ScienceDirect
Rule-Based Runtime Mitigation Against Poison Attacks on Neural Networks,Usman M.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Rule-Based Runtime Mitigation Against Poison Attacks on Neural Networks"" target=""_blank"">[https://doi.org/10.1007/978-3-031-17196-3_4]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-17196-3_4]</a>",,,ScienceDirect
SBPA: Sybil-Based Backdoor Poisoning Attacks for Distributed Big Data in AIoT-Based Federated Learning System,Xiao X.,IEEE Transactions on Big Data,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : SBPA: Sybil-Based Backdoor Poisoning Attacks for Distributed Big Data in AIoT-Based Federated Learning System"" target=""_blank"">[https://doi.org/10.1109/TBDATA.2022.3224392]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TBDATA.2022.3224392]</a>",,,ScienceDirect
STEALTHY BACKDOOR ATTACK WITH ADVERSARIAL TRAINING,Feng L.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : STEALTHY BACKDOOR ATTACK WITH ADVERSARIAL TRAINING"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9746008]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9746008]</a>",,,ScienceDirect
Security of operating system using the Metasploit framework by creating a backdoor from remote setup,Thapa R.,"2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering, ICACITE 2022",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Security of operating system using the Metasploit framework by creating a backdoor from remote setup"" target=""_blank"">[https://doi.org/10.1109/ICACITE53722.2022.9823460]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICACITE53722.2022.9823460]</a>",,,ScienceDirect
Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch,Souri H.,Advances in Neural Information Processing Systems,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
TEST-TIME DETECTION OF BACKDOOR TRIGGERS FOR POISONED DEEP NEURAL NETWORKS,Li X.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : TEST-TIME DETECTION OF BACKDOOR TRIGGERS FOR POISONED DEEP NEURAL NETWORKS"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9746573]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9746573]</a>",,,ScienceDirect
Targeted Data Poisoning Attacks Against Continual Learning Neural Networks,Li H.,Proceedings of the International Joint Conference on Neural Networks,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Targeted Data Poisoning Attacks Against Continual Learning Neural Networks"" target=""_blank"">[https://doi.org/10.1109/IJCNN55064.2022.9892774]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/IJCNN55064.2022.9892774]</a>",,,ScienceDirect
The Devil Is in the GAN: Backdoor Attacks and Defenses in Deep Generative Models,Rawat A.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2022-01-01,"<a href=""ScienceDirect (2022-01-01) : The Devil Is in the GAN: Backdoor Attacks and Defenses in Deep Generative Models"" target=""_blank"">[https://doi.org/10.1007/978-3-031-17143-7_41]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-17143-7_41]</a>",,,ScienceDirect
TnT Attacks! Universal Naturalistic Adversarial Patches Against Deep Neural Network Systems,Doan B.G.,IEEE Transactions on Information Forensics and Security,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : TnT Attacks! Universal Naturalistic Adversarial Patches Against Deep Neural Network Systems"" target=""_blank"">[https://doi.org/10.1109/TIFS.2022.3198857]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TIFS.2022.3198857]</a>",,,ScienceDirect
Toward Backdoor Attacks for Image Captioning Model in Deep Neural Networks,Kwon H.,Security and Communication Networks,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Toward Backdoor Attacks for Image Captioning Model in Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1155/2022/1525052]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1155/2022/1525052]</a>",,,ScienceDirect
Towards Backdoor Attack on Deep Learning based Time Series Classification,Ding D.,Proceedings - International Conference on Data Engineering,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Towards Backdoor Attack on Deep Learning based Time Series Classification"" target=""_blank"">[https://doi.org/10.1109/ICDE53745.2022.00100]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICDE53745.2022.00100]</a>",,,ScienceDirect
Towards Class-Oriented Poisoning Attacks Against Neural Networks,Zhao B.,"Proceedings - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Towards Class-Oriented Poisoning Attacks Against Neural Networks"" target=""_blank"">[https://doi.org/10.1109/WACV51458.2022.00230]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/WACV51458.2022.00230]</a>",,,ScienceDirect
Trap and Replace: Defending Backdoor Attacks by Trapping Them into an Easy-to-Replace Subnetwork,Wang H.,Advances in Neural Information Processing Systems,2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Trap and Replace: Defending Backdoor Attacks by Trapping Them into an Easy-to-Replace Subnetwork"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Universal Evasion Attacks on Summarization Scoring,Mu W.,"BlackboxNLP 2022 - BlackboxNLP Analyzing and Interpreting Neural Networks for NLP, Proceedings of the Workshop",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Universal Evasion Attacks on Summarization Scoring"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Verifying Neural Networks Against Backdoor Attacks,Pham L.H.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Verifying Neural Networks Against Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1007/978-3-031-13185-1_9]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-13185-1_9]</a>",,,ScienceDirect
WHEN DOES BACKDOOR ATTACK SUCCEED IN IMAGE RECONSTRUCTION? A STUDY OF HEURISTICS VS. BI-LEVEL SOLUTION,Taneja V.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : WHEN DOES BACKDOOR ATTACK SUCCEED IN IMAGE RECONSTRUCTION? A STUDY OF HEURISTICS VS. BI-LEVEL SOLUTION"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9746433]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9746433]</a>",,,ScienceDirect
Where to Attack: A Dynamic Locator Model for Backdoor Attack in Text Classifications,Lu H.Y.,"Proceedings - International Conference on Computational Linguistics, COLING",2022-01-01,"<a href=""ScienceDirect (2022-01-01) : Where to Attack: A Dynamic Locator Model for Backdoor Attack in Text Classifications"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Hiding Behind Backdoors: Self-Obfuscation Against Generative Models,"Siddhartha Datta, Nigel Shadbolt",arXiv,2022-01,"<a href=""DBLP (2022-01) : Hiding Behind Backdoors: Self-Obfuscation Against Generative Models"" target=""_blank"">[https://arxiv.org/abs/2201.09774]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2201.09774]</a>",,,DBLP
Model Transferring Attacks to Backdoor HyperNetwork in Personalized Federated Learning,"Phung Lai, NhatHai Phan, Abdallah Khreishah, Issa Khalil, Xintao Wu",arXiv,2022-01,"<a href=""DBLP (2022-01) : Model Transferring Attacks to Backdoor HyperNetwork in Personalized Federated Learning"" target=""_blank"">[https://arxiv.org/abs/2201.07063]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2201.07063]</a>",,,DBLP
Rethink Stealthy Backdoor Attacks in Natural Language Processing,"Lingfeng Shen, Haiyun Jiang, Lemao Liu, Shuming Shi",arXiv,2022-01,"<a href=""DBLP (2022-01) : Rethink Stealthy Backdoor Attacks in Natural Language Processing"" target=""_blank"">[https://arxiv.org/abs/2201.02993]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2201.02993]</a>",,,DBLP
Backdoor Attack is a Devil in Federated GAN-Based Medical Image Synthesis,"Ruinan Jin, Xiaoxiao Li","Simulation and Synthesis in Medical Imaging
arXiv
SASHIMI@MICCAI","2022
2022-07-30
2022","<a href=""Springer (2022) : Backdoor Attack is a Devil in Federated GAN-Based Medical Image Synthesis"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-16980-9_15]</a>
<a href=""arXiv (2022-07-30) : Backdoor Attack is a Devil in Federated GAN-based Medical Image Synthesis"" target=""_blank"">[http://arxiv.org/abs/2207.00762v2]</a>
<a href=""DBLP (2022) : Backdoor Attack is a Devil in Federated GAN-Based Medical Image Synthesis"" target=""_blank"">[https://doi.org/10.1007/978-3-031-16980-9_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-16980-9_15]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-16980-9_15]</a>","Deep Learning-based image synthesis techniques have been applied in healthcare research for generating medical images to support open research....
Deep Learning-based image synthesis techniques have been applied in healthcare research for generating medical images to support open research. Training generative adversarial neural networks (GAN) usually requires large amounts of training data. Federated learning (FL) provides a way of training a central model using distributed data from different medical institutions while keeping raw data locally. However, FL is vulnerable to backdoor attack, an adversarial by poisoning training data, given the central server cannot access the original data directly. Most backdoor attack strategies focus on classification models and centralized domains. In this study, we propose a way of attacking federated GAN (FedGAN) by treating the discriminator with a commonly used data poisoning strategy in backdoor attack classification models. We demonstrate that adding a small trigger with size less than 0.5 percent of the original image size can corrupt the FL-GAN model. Based on the proposed attack, we provide two effective defense strategies: global malicious detection and local training regularization. We show that combining the two defense strategies yields a robust medical image generation.
","

","Springer
arXiv
DBLP"
A Pragmatic Label-Specific Backdoor Attack,"Yu Wang, Haomiao Yang, ... Mengyu Ge","Frontiers in Cyber Security
FCS","2022
2022","<a href=""Springer (2022) : A Pragmatic Label-Specific Backdoor Attack"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-8445-7_10]</a>
<a href=""DBLP (2022) : A Pragmatic Label-Specific Backdoor Attack"" target=""_blank"">[https://doi.org/10.1007/978-981-19-8445-7_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-8445-7_10]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-981-19-8445-7_10]</a>","Backdoor attacks, as an insidious security threat to deep neural networks (DNNs), are adept at injecting triggers into DNNs. A malicious attacker can...
","
","Springer
DBLP"
Energy-Based Learning for Preventing Backdoor Attack,"Xiangyu Gao, Meikang Qiu","Knowledge Science, Engineering and Management
KSEM","2022
2022","<a href=""Springer (2022) : Energy-Based Learning for Preventing Backdoor Attack"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-10989-8_56]</a>
<a href=""DBLP (2022) : Energy-Based Learning for Preventing Backdoor Attack"" target=""_blank"">[https://doi.org/10.1007/978-3-031-10989-8_56]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-10989-8_56]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-10989-8_56]</a>","The popularity of machine learning has motivated the idea of Energy-Based Learning (EBL), which used Energy-Based Models (EBMs) proposed by Prof....
","
","Springer
DBLP"
Fooling a Face Recognition System with a Marker-Free Label-Consistent Backdoor Attack,"Nino Cauli, Alessandro Ortis, Sebastiano Battiato","Image Analysis and Processing – ICIAP 2022
ICIAP","2022
2022","<a href=""Springer (2022) : Fooling a Face Recognition System with a Marker-Free Label-Consistent Backdoor Attack"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-06430-2_15]</a>
<a href=""DBLP (2022) : Fooling a Face Recognition System with a Marker-Free Label-Consistent Backdoor Attack"" target=""_blank"">[https://doi.org/10.1007/978-3-031-06430-2_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-06430-2_15]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-06430-2_15]</a>","Modern face recognition systems are mostly based on deep learning models. These models need a large amount of data and high computational power to be...
","
","Springer
DBLP"
How to Backdoor (Classic) McEliece and How to Guard Against Backdoors,"Tobias Hemmert, Alexander May, ... Carl Richard Theodor Schneider","Post-Quantum Cryptography
PQCrypto","2022
2022","<a href=""Springer (2022) : How to Backdoor (Classic) McEliece and How to Guard Against Backdoors"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17234-2_2]</a>
<a href=""DBLP (2022) : How to Backdoor (Classic) McEliece and How to Guard Against Backdoors"" target=""_blank"">[https://doi.org/10.1007/978-3-031-17234-2_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17234-2_2]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-17234-2_2]</a>","We show how to backdoor the McEliece cryptosystem such that a backdoored public key is indistinguishable from a usual public key, but allows to...
","
","Springer
DBLP"
Practical Backdoor Attack Against Speaker Recognition System,"Yuxiao Luo, Jianwei Tai, ... Shengzhi Zhang","Information Security Practice and Experience
ISPEC","2022
2022","<a href=""Springer (2022) : Practical Backdoor Attack Against Speaker Recognition System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-21280-2_26]</a>
<a href=""DBLP (2022) : Practical Backdoor Attack Against Speaker Recognition System"" target=""_blank"">[https://doi.org/10.1007/978-3-031-21280-2_26]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-21280-2_26]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-031-21280-2_26]</a>","Deep learning-based models have achieved state-of-the-art performance in a wide variety of classification and recognition tasks. Although such models...
","
","Springer
DBLP"
A Backdoor Embedding Method for Backdoor Detection in Deep Neural Networks,"Meirong Liu, Hong Zheng, ... Yinglong Dai","Ubiquitous Security
UbiSec","2022
2021","<a href=""Springer (2022) : A Backdoor Embedding Method for Backdoor Detection in Deep Neural Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-0468-4_1]</a>
<a href=""DBLP (2021) : A Backdoor Embedding Method for Backdoor Detection in Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1007/978-981-19-0468-4_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-0468-4_1]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-981-19-0468-4_1]</a>","As the coming of artificial intelligence (AI) era, deep learning models are widely applied on many aspects of our daily lives, such as face...
","
","Springer
DBLP"
A Brief Survey of Cloud Data Auditing Mechanism,"Yash Anand, Bhargavi Sirmour, ... Soumyadev Maity",Innovative Data Communication Technologies and Application,2022,"<a href=""Springer (2022) : A Brief Survey of Cloud Data Auditing Mechanism"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-7167-8_42]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-7167-8_42]</a>","In today’s digital world, the demand for cloud storage is increasing. Cloud has attracted a vast number of users in today’s environment because it...",,Springer
A Combination Reduction Algorithm and Its Application,"Wei Yang, Shaojun Yang, ... Yong Zhao",Network and System Security,2022,"<a href=""Springer (2022) : A Combination Reduction Algorithm and Its Application"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-23020-2_38]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-23020-2_38]</a>","After the Snowden incident, cryptographic subversion attack has attracted widespread attentions. Subversion attack is an unconventional attack inside...",,Springer
A Comprehensive Study on Mobile Malwares: Mobile Covert Channels—Threats and Security,"Ketaki Pattani, Sunil Gautam",Soft Computing and Optimization,2022,"<a href=""Springer (2022) : A Comprehensive Study on Mobile Malwares: Mobile Covert Channels—Threats and Security"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-6406-0_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-6406-0_8]</a>","In current scenario, Android operating system powers billions of users, and everyday numerous Android devices are activated. Android OS bridging...",,Springer
A Design Scheme and Security Analysis of Unmanned Aerial Vehicle,"Dongyu Yang, Yue Zhao, ... Haiyang Peng",Wireless and Satellite Systems,2022,"<a href=""Springer (2022) : A Design Scheme and Security Analysis of Unmanned Aerial Vehicle"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-93398-2_50]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-93398-2_50]</a>","Since the 21st century, informatization, modernization and intellectualization have become an important direction of national science and technology...",,Springer
A Feature-Map-Based Large-Payload DNN Watermarking Algorithm,"Yue Li, Lydia Abady, ... Mauro Barni",Digital Forensics and Watermarking,2022,"<a href=""Springer (2022) : A Feature-Map-Based Large-Payload DNN Watermarking Algorithm"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-95398-0_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-95398-0_10]</a>",Watermarking has recently been proposed as a solution to protect the Intellectual Property Rights (IPR) of Deep Neural Networks (DNN). Dynamic DNN...,,Springer
A Framework for APT Detection Based on Host Destination and Packet—Analysis,"R. C. Veena, S. H. Brahmananda",Computer Networks and Inventive Communication Technologies,2022,"<a href=""Springer (2022) : A Framework for APT Detection Based on Host Destination and Packet—Analysis"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-3728-5_62]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-3728-5_62]</a>","In cybersecurity, advanced persistent threats have gained more attention. Even after that, a variety of techniques, like change control, sandboxing,...",,Springer
A General Backdoor Attack to Graph Neural Networks Based on Explanation Method,"Luyao Chen, Na Yan, Boyang Zhang, Zhaoyang Wang, Yu Wen, Yanfei Hu",TrustCom,2022,"<a href=""DBLP (2022) : A General Backdoor Attack to Graph Neural Networks Based on Explanation Method"" target=""_blank"">[https://doi.org/10.1109/TrustCom56396.2022.00107]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TrustCom56396.2022.00107]</a>",,,DBLP
A Generalized Comprehensive Security Architecture Framework for IoT Applications Against Cyber-Attacks,"M. Nakkeeran, Senthilkumar Mathi",Artificial Intelligence and Technologies,2022,"<a href=""Springer (2022) : A Generalized Comprehensive Security Architecture Framework for IoT Applications Against Cyber-Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-6448-9_46]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-6448-9_46]</a>",Integration of apps through emerging Internet of Things (IoT) technology with edge computing opens up increased possibilities for building a smart...,,Springer
A Generic Enhancer for Backdoor Attacks on Deep Neural Networks,"Bilal Hussain Abbasi, Qi Zhong, Leo Yu Zhang, Shang Gao, Antonio Robles-Kelly, Robin Doss",ICONIP,2022,"<a href=""DBLP (2022) : A Generic Enhancer for Backdoor Attacks on Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1007/978-981-99-1648-1_25]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-981-99-1648-1_25]</a>",,,DBLP
A High Performance Intrusion Detection System Using LightGBM Based on Oversampling and Undersampling,"Hao Zhang, Lina Ge, Zhe Wang",Intelligent Computing Theories and Application,2022,"<a href=""Springer (2022) : A High Performance Intrusion Detection System Using LightGBM Based on Oversampling and Undersampling"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-13870-6_53]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-13870-6_53]</a>","Intrusion detection system plays an important role in network security, however, the problem with data imbalance limits the detection ability of...",,Springer
A Hybrid Semi-supervised Learning with Nature-Inspired Optimization for Intrusion Detection System in IoT Environment,"Dukka Karun Kumar Reddy, Janmenjoy Nayak, H. S. Behera",Computational Intelligence in Pattern Recognition,2022,"<a href=""Springer (2022) : A Hybrid Semi-supervised Learning with Nature-Inspired Optimization for Intrusion Detection System in IoT Environment"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-3089-8_55]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-3089-8_55]</a>","The purpose of IDS is to protect the confidentiality, integrity, and availability of a system. As computer networks become more vulnerable to...",,Springer
A Novel Backdoor Attack Adapted to Transfer Learning,"Peihao Li, Jie Huang, Shuaishuai Zhang, Chunyang Qi, Chuang Liang, Yang Peng",SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta,2022,"<a href=""DBLP (2022) : A Novel Backdoor Attack Adapted to Transfer Learning"" target=""_blank"">[https://doi.org/10.1109/SmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00246]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00246]</a>",,,DBLP
A Novel Framework for NIDS Using Stacked Ensemble Learning,"H. Jagruthi, C. Kavitha",Soft Computing for Security Applications,2022,"<a href=""Springer (2022) : A Novel Framework for NIDS Using Stacked Ensemble Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5301-8_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5301-8_9]</a>","With growing information technology and increasing digitization, the data is increasing largely every year, data security becomes the largest concern...",,Springer
A Proposed Intrusion Detection Method Based on Machine Learning Used for Internet of Things Systems,"Neder Karmous, Mohamed Ould-Elhassen Aoueileyine, ... Neji Youssef",Advanced Information Networking and Applications,2022,"<a href=""Springer (2022) : A Proposed Intrusion Detection Method Based on Machine Learning Used for Internet of Things Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-99619-2_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-99619-2_4]</a>",This paper presents an improved method using supervised machine-learning techniques of the Internet of things (IoT) systems to ensure security in...,,Springer
A Review of Machine Learning-Based Intrusion Detection Systems on the Cloud,"Nishtha Srivastava, Ashish Chaudhari, ... Udai Pratap Rao","Security, Privacy and Data Analytics",2022,"<a href=""Springer (2022) : A Review of Machine Learning-Based Intrusion Detection Systems on the Cloud"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-9089-1_25]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-9089-1_25]</a>","Organizations and individuals adopted cloud computing due to its scalability and flexibility. At the same time, it has become more vulnerable due to...",,Springer
A Review on Security Issues in Healthcare Cyber-Physical Systems,"V. S. Abhijith, B. Sowmiya, ... P. Varalakshmi",Cyber Intelligence and Information Retrieval,2022,"<a href=""Springer (2022) : A Review on Security Issues in Healthcare Cyber-Physical Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-4284-5_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-4284-5_4]</a>",The rapid advancements in computation methodologies and cloud computing along with the rapid expansion of the Internet of things have resulted in...,,Springer
A Secured Network Layer and Information Security for Financial Institutions: A Case Study,"Md Rahat Ibne Sattar, Shrabonti Mitra, ... Mayeen Uddin Khandaker",Intelligent Computing & Optimization,2022,"<a href=""Springer (2022) : A Secured Network Layer and Information Security for Financial Institutions: A Case Study"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-93247-3_94]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-93247-3_94]</a>","Communication networks are used to transfer valuable and confidential information for a variety of purposes. As a result, they attract the attention...",,Springer
A Security Provocation in Cloud-Based Computing,"Aritra Dutta, Rajesh Bose, ... Sandip Roy",Pattern Recognition and Data Analysis with Applications,2022,"<a href=""Springer (2022) : A Security Provocation in Cloud-Based Computing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-1520-8_27]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-1520-8_27]</a>",Cloud computing makes a massive trend for research organizations because highly demand. There is different type of causes that many organizations...,,Springer
A Self-supervised Adversarial Learning Approach for Network Intrusion Detection System,"Lirui Deng, Youjian Zhao, Heng Bao",Cyber Security,2022,"<a href=""Springer (2022) : A Self-supervised Adversarial Learning Approach for Network Intrusion Detection System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-8285-9_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-8285-9_5]</a>",The network intrusion detection system (NIDS) plays an essential role in network security. Although many data-driven approaches from the field of...,,Springer
"A Study on Cloud Environment: Confidentiality Problems, Security Threats, and Challenges","Megha Gupta, Laxmi Ahuja, Ashish Seth",Soft Computing for Security Applications,2022,"<a href=""Springer (2022) : A Study on Cloud Environment: Confidentiality Problems, Security Threats, and Challenges"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5301-8_49]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5301-8_49]</a>","In terms of computing services, cloud computing has expanded significantly. It is a set of computing tools that may be accessed through the Internet....",,Springer
A Summary of 5G WiFi Security Issues,"Cui Zhuohan, Li Zheng, ... Ai Xiaoxi","Communications, Signal Processing, and Systems",2022,"<a href=""Springer (2022) : A Summary of 5G WiFi Security Issues"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-0390-8_117]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-0390-8_117]</a>","With the continuous innovation of wireless communication technology, 5G WiFi devices has been popularized in the average families, and its high-speed...",,Springer
A Supervised Rare Anomaly Detection Technique via Cooperative Co-evolution-Based Feature Selection Using Benchmark UNSW_NB15 Dataset,"A. N. M. Bazlur Rashid, Mohiuddin Ahmed, Sheikh Rabiul Islam",Ubiquitous Security,2022,"<a href=""Springer (2022) : A Supervised Rare Anomaly Detection Technique via Cooperative Co-evolution-Based Feature Selection Using Benchmark UNSW_NB15 Dataset"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-0468-4_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-0468-4_21]</a>","Anomaly detection is important in many domains, including cybersecurity. There are a number of rare anomalies in cybersecurity datasets, and...",,Springer
A Survey of Adversarial Attacks on Wireless Communications,"Xiangyu Luo, Quan Qin, ... Meng Xue","Edge Computing and IoT: Systems, Management and Security",2022,"<a href=""Springer (2022) : A Survey of Adversarial Attacks on Wireless Communications"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-04231-7_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-04231-7_7]</a>","As the deep neural network (DNN) has been applied in various fields in wireless communications, the potential security problems of DNNs in wireless...",,Springer
A Survey of Network Features for Machine Learning Algorithms to Detect Network Attacks,"Joveria Rubab, Hammad Afzal, Waleed Bin Shahid",Intelligent Information and Database Systems,2022,"<a href=""Springer (2022) : A Survey of Network Features for Machine Learning Algorithms to Detect Network Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-21967-2_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-21967-2_7]</a>","The immeasurable amount of data in network traffic has increased its vulnerability. Therefore, monitoring and analyzing traffic for threat hunting is...",,Springer
A collaborative deep learning microservice for backdoor defenses in Industrial IoT networks,"Qin Liu, Liqiong Chen, Hongbo Jiang, Jie Wu, Tian Wang, Tao Peng, Guojun Wang",Ad Hoc Networks,2022,"<a href=""DBLP (2022) : A collaborative deep learning microservice for backdoor defenses in Industrial IoT networks"" target=""_blank"">[https://doi.org/10.1016/j.adhoc.2021.102727]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.adhoc.2021.102727]</a>",,,DBLP
A multitarget backdooring attack on deep neural networks with random location trigger,"Xiao Yu, Liu Cong, Mingwen Zheng, Yajie Wang, Xinrui Liu, Song Shuxiao, Ma Yuexuan, Zheng Jun",Int. J. Intell. Syst.,2022,"<a href=""DBLP (2022) : A multitarget backdooring attack on deep neural networks with random location trigger"" target=""_blank"">[https://doi.org/10.1002/int.22785]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1002/int.22785]</a>",,,DBLP
ACTSS: Input Detection Defense against Backdoor Attacks via Activation Subset Scanning,"Yuexin Xuan, Xiaojun Chen, Zhendong Zhao, Yangyang Ding, Jianming Lv",IJCNN,2022,"<a href=""DBLP (2022) : ACTSS: Input Detection Defense against Backdoor Attacks via Activation Subset Scanning"" target=""_blank"">[https://doi.org/10.1109/IJCNN55064.2022.9891900]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/IJCNN55064.2022.9891900]</a>",,,DBLP
APT Attack Heuristic Induction Honeypot Platform Based on Snort and OpenFlow,"Bo Dai, Zhenhai Zhang, ... Yuan Liu",Smart Computing and Communication,2022,"<a href=""Springer (2022) : APT Attack Heuristic Induction Honeypot Platform Based on Snort and OpenFlow"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-97774-0_31]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-97774-0_31]</a>",The honeypot can record attacker’s aggressive behavior and analyze methods of attack in order to develop more intelligent protection policies in the...,,Springer
ATTACK-DEFENSE MODELING OF MATERIAL EXTRUSION ADDITIVE MANUFACTURING SYSTEMS,"Alyxandra Van Stockum, Elizabeth Kurkowski, ... Sujeet Shenoi",Critical Infrastructure Protection XVI,2022,"<a href=""Springer (2022) : ATTACK-DEFENSE MODELING OF MATERIAL EXTRUSION ADDITIVE MANUFACTURING SYSTEMS"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-20137-0_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-20137-0_5]</a>","The use of additive manufacturing in the critical infrastructure makes it an attractive target for cyber attacks. However, research on additive...",,Springer
ATTEQ-NN: Attention-based QoE-aware Evasive Backdoor Attacks,"Xueluan Gong, Yanjiao Chen, Jianshuo Dong, Qian Wang",NDSS,2022,"<a href=""DBLP (2022) : ATTEQ-NN: Attention-based QoE-aware Evasive Backdoor Attacks"" target=""_blank"">[https://www.ndss-symposium.org/ndss-paper/auto-draft-238/]</a>","<a href=""DBLP"" target=""_blank"">[https://www.ndss-symposium.org/ndss-paper/auto-draft-238/]</a>",,,DBLP
"Active intellectual property protection for deep neural networks through stealthy backdoor and users&apos, identities authentication","Mingfu Xue, Shichang Sun, Yushu Zhang, Jian Wang, Weiqiang Liu",Appl. Intell.,2022,"<a href=""DBLP (2022) : Active intellectual property protection for deep neural networks through stealthy backdoor and users&apos, identities authentication"" target=""_blank"">[https://doi.org/10.1007/s10489-022-03339-0]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/s10489-022-03339-0]</a>",,,DBLP
Adaptive Multiparty NIKE,"Venkata Koppula, Brent Waters, Mark Zhandry",Theory of Cryptography,2022,"<a href=""Springer (2022) : Adaptive Multiparty NIKE"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22365-5_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22365-5_9]</a>",We construct adaptively secure multiparty non-interactive key exchange (NIKE) from polynomially-hard indistinguishability obfuscation and other...,,Springer
Advances in Adversarial Attacks and Defenses in Intrusion Detection System: A Survey,"Mariama Mbow, Kouichi Sakurai, Hiroshi Koide",Science of Cyber Security - SciSec 2022 Workshops,2022,"<a href=""Springer (2022) : Advances in Adversarial Attacks and Defenses in Intrusion Detection System: A Survey"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-7769-5_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-7769-5_15]</a>",Machine learning is one of the predominant methods used in computer science and has been widely and successfully applied in many areas such as...,,Springer
Against Backdoor Attacks In Federated Learning With Differential Privacy,"Lu Miao, Wei Yang, Rong Hu, Lu Li, Liusheng Huang",ICASSP,2022,"<a href=""DBLP (2022) : Against Backdoor Attacks In Federated Learning With Differential Privacy"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9747653]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9747653]</a>",,,DBLP
Algorithm Substitution Attacks on Identity-Based Encryption,"Yifei Wang, Yuliang Lin, ... Lin Liu",Security and Privacy in Social Networks and Big Data,2022,"<a href=""Springer (2022) : Algorithm Substitution Attacks on Identity-Based Encryption"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-7242-3_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-7242-3_2]</a>",Algorithm substitution attack (ASA) was proposed by Bellare et al. at CRYPTO 2014 and has been studied in the context of various cryptographic...,,Springer
An Adaptive and Collaborative Method Based on GMRA for Intrusion Detection,"Shaohua Teng, Yongzhi Zhang, ... Lu Liang",Computer Supported Cooperative Work and Social Computing,2022,"<a href=""Springer (2022) : An Adaptive and Collaborative Method Based on GMRA for Intrusion Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-4546-5_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-4546-5_6]</a>","Generally, the detection configuration in the normal state cannot cope with the detection during sudden network peaks. Moreover, a kind of attack...",,Springer
An Analysis of Byzantine-Tolerant Aggregation Mechanisms on Model Poisoning in Federated Learning,"Mary Roszel, Robert Norvill, Radu State",Modeling Decisions for Artificial Intelligence,2022,"<a href=""Springer (2022) : An Analysis of Byzantine-Tolerant Aggregation Mechanisms on Model Poisoning in Federated Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-13448-7_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-13448-7_12]</a>",Federated learning is a distributed setting where multiple participants jointly train a machine learning model without exchanging data. Recent work...,,Springer
An Analysis of Cyber Espionage Process,"Richard Rivera, Leandro Pazmiño, ... Jhonattan Barriga",Developments and Advances in Defense and Security,2022,"<a href=""Springer (2022) : An Analysis of Cyber Espionage Process"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-4884-7_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-4884-7_1]</a>",The recent increasing cases released worldwide on espionage require a knowledge systematization study in this area. This paper presents a general...,,Springer
An Efficient Detection and Prevention Approach of Unknown Malicious Attack: A Novel Honeypot Approach,"Aatif Sarfaraz, Atul Jha, ... Radha Tamal Goswami",Cyber Security and Digital Forensics,2022,"<a href=""Springer (2022) : An Efficient Detection and Prevention Approach of Unknown Malicious Attack: A Novel Honeypot Approach"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-3961-6_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-3961-6_2]</a>","In this modern era, security has gotten to be the foremost broadly concerned in each domain as recently approaching malware postures a danger to the...",,Springer
An Improved Needham-Schroeder Session Key Distribution Protocol for In-Vehicle CAN Network,"Yin Long, Jian Xu, ... Zihao Wang",Security and Privacy in New Computing Environments,2022,"<a href=""Springer (2022) : An Improved Needham-Schroeder Session Key Distribution Protocol for In-Vehicle CAN Network"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-96791-8_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-96791-8_3]</a>","With the rapid development of automobile technology, the internal network of automobiles is facing more and more security problems. Many CAN-based...",,Springer
An Intrusion Detection Approach for Small-Sized Networks,"Phong Cao Nguyen, Van The Ho, ... Hai Thanh Nguyen",Inventive Computation and Information Technologies,2022,"<a href=""Springer (2022) : An Intrusion Detection Approach for Small-Sized Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-6723-7_67]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-6723-7_67]</a>","In any network system, the intrusion is undesirable, and organizations are constantly searching for solutions that could effectively detect intrusion...",,Springer
An Investigation of Vulnerabilities in Internet of Health Things,"Saifur Rahman, Tance Suleski, ... A. S. M. Kayes",Cognitive Radio Oriented Wireless Networks and Wireless Internet,2022,"<a href=""Springer (2022) : An Investigation of Vulnerabilities in Internet of Health Things"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-98002-3_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-98002-3_22]</a>",Medical devices are the machines or instruments that play a vital role in diagnosis or treatment for patients in a healthcare ecosystem. As...,,Springer
Analysis and Detection Against Overlapping Phenomenon of Behavioral Attribute in Network Attacks,"Jiang Xie, Shuhao Li, Peishuai Sun",Science of Cyber Security,2022,"<a href=""Springer (2022) : Analysis and Detection Against Overlapping Phenomenon of Behavioral Attribute in Network Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17551-0_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17551-0_14]</a>",Various network attacks have brought great threats to cyber security. It is beneficial to build various datasets for detecting these network attacks....,,Springer
Analysis and Evaluation of Machine Learning Classifiers for IoT Attack Dataset,"H. Jagruthi, C. Kavitha",Machine Learning and Autonomous Systems,2022,"<a href=""Springer (2022) : Analysis and Evaluation of Machine Learning Classifiers for IoT Attack Dataset"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-7996-4_34]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-7996-4_34]</a>",IoT connects millions of smart devices through internet. IoT is vulnerable to cyber-attacks because of its constrained/controlled environment. To...,,Springer
Analysis of Computer Network Security Problems and Countermeasures Based on Cognitive Big Data,"Jie Dong, Peng Wang, ... Yazhen Gu",International Conference on Cognitive based Information Processing and Applications (CIPA 2021),2022,"<a href=""Springer (2022) : Analysis of Computer Network Security Problems and Countermeasures Based on Cognitive Big Data"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5854-9_126]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5854-9_126]</a>","Big data provides convenience for social development and work, and computer network has been widely used for its convenience and efficiency. However,...",,Springer
Analysis of State-of-Art Attack Detection Methods Using Recurrent Neural Network,"Priyanka Dixit, Sanjay Silakari","Proceedings of the International Conference on Paradigms of Communication, Computing and Data Sciences",2022,"<a href=""Springer (2022) : Analysis of State-of-Art Attack Detection Methods Using Recurrent Neural Network"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5747-4_68]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5747-4_68]</a>",Recurrent Neural Networks (RNN’s) are deep neural (DL) networks that can be trained on large volume of databases and performed well on natural...,,Springer
Analysis of Various Supervised Machine Learning Algorithms for Intrusion Detection,"Kabir Nagpal, Niyati Jain, ... Sunita Singhal","Cyber Warfare, Security and Space Research",2022,"<a href=""Springer (2022) : Analysis of Various Supervised Machine Learning Algorithms for Intrusion Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-15784-4_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-15784-4_3]</a>",Computer network intrusion detection systems help recognize unauthorized access and abnormal attacks over secured networks. It is an important...,,Springer
Android Malware Detection with Classification Based on Hybrid Analysis and N-gram Feature Extraction,"Eslavath Ravi, Mummadi Upendra Kumar",Advancements in Smart Computing and Information Security,2022,"<a href=""Springer (2022) : Android Malware Detection with Classification Based on Hybrid Analysis and N-gram Feature Extraction"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-23095-0_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-23095-0_13]</a>",Mobile devices will have the potential to expose to various cyber-attacks with the explosive growth of mobile networks. Unknown malware may...,,Springer
Anthropomorphism in Computer Security Terminology Through the Prizm of Smart Cognitive Framing,"Ekaterina Isaeva, Olga Baiburova, Oksana Manzhula",Science and Global Challenges of the 21st Century - Science and Technology,2022,"<a href=""Springer (2022) : Anthropomorphism in Computer Security Terminology Through the Prizm of Smart Cognitive Framing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-89477-1_46]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-89477-1_46]</a>",Semantic framing is undergoing a revolution in terms of its application to information processing and cognitive manipulation. Deliberate usage of...,,Springer
Anti-Neuron Watermarking: Protecting Personal Data Against Unauthorized Neural Networks,"Zihang Zou, Boqing Gong, Liqiang Wang",Computer Vision – ECCV 2022,2022,"<a href=""Springer (2022) : Anti-Neuron Watermarking: Protecting Personal Data Against Unauthorized Neural Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-19778-9_26]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-19778-9_26]</a>",We study protecting a user’s data (images in this work) against a learner’s unauthorized use in training neural networks. It is especially...,,Springer
Approach to Machine Learning for Secured Cloud Computing,"Amarnath Jambhaiyanahatti Lalyanaik, Pritam G. Shah, ... Vinayak B. Joshi",Cognition and Recognition,2022,"<a href=""Springer (2022) : Approach to Machine Learning for Secured Cloud Computing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22405-8_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22405-8_6]</a>","Machine Learning (ML) is the bigger picture of this technology driven world we live in right now. With machine learning, there are clearly a lot of...",,Springer
Are Backdoor Mandates Ethical? - A Position Paper,"Raphaël Khoury, Sylvain Hallé",IEEE Technol. Soc. Mag.,2022,"<a href=""DBLP (2022) : Are Backdoor Mandates Ethical? - A Position Paper"" target=""_blank"">[https://doi.org/10.1109/MTS.2022.3217699]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/MTS.2022.3217699]</a>",,,DBLP
Are Graph Neural Network Explainers Robust to Graph Noises?,"Yiqiao Li, Sunny Verma, ... Fang Chen",AI 2022: Advances in Artificial Intelligence,2022,"<a href=""Springer (2022) : Are Graph Neural Network Explainers Robust to Graph Noises?"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22695-3_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22695-3_12]</a>","With the rapid deployment of graph neural networks (GNNs) based techniques in a wide range of applications such as link prediction, community...",,Springer
Assisting Backdoor Federated Learning with Whole Population Knowledge Alignment in Mobile Edge Computing,"Tian Liu, Xueyang Hu, Tao Shu",SECON,2022,"<a href=""DBLP (2022) : Assisting Backdoor Federated Learning with Whole Population Knowledge Alignment in Mobile Edge Computing"" target=""_blank"">[https://doi.org/10.1109/SECON55815.2022.9918550]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SECON55815.2022.9918550]</a>",,,DBLP
AttacKG: Constructing Technique Knowledge Graph from Cyber Threat Intelligence Reports,"Zhenyuan Li, Jun Zeng, ... Zhenkai Liang",Computer Security – ESORICS 2022,2022,"<a href=""Springer (2022) : AttacKG: Constructing Technique Knowledge Graph from Cyber Threat Intelligence Reports"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17140-6_29]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17140-6_29]</a>","Cyber attacks are becoming more sophisticated and diverse, making attack detection increasingly challenging. To combat these attacks, security...",,Springer
Attack Strategies and Countermeasures in Transport-Based Time Synchronization Solutions,"Diana Gratiela Berbecaru, Antonio Lioy",Intelligent Distributed Computing XIV,2022,"<a href=""Springer (2022) : Attack Strategies and Countermeasures in Transport-Based Time Synchronization Solutions"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-96627-0_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-96627-0_19]</a>","The security, availability, and accuracy of time information transmitted over transport networks are getting increased attention since different...",,Springer
Attacks on ML Systems: From Security Analysis to Attack Mitigation,"Qingtian Zou, Lan Zhang, ... Peng Liu",Information Systems Security,2022,"<a href=""Springer (2022) : Attacks on ML Systems: From Security Analysis to Attack Mitigation"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-23690-7_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-23690-7_7]</a>",The past several years have witnessed rapidly increasing use of machine learning (ML) systems in multiple industry sectors. Since security analysis...,,Springer
Backdoor Attacks against Deep Neural Networks by Personalized Audio Steganography,"Peng Liu, Shuyi Zhang, Chuanjian Yao, Wenzhe Ye, Xianxian Li",ICPR,2022,"<a href=""DBLP (2022) : Backdoor Attacks against Deep Neural Networks by Personalized Audio Steganography"" target=""_blank"">[https://doi.org/10.1109/ICPR56361.2022.9956521]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICPR56361.2022.9956521]</a>",,,DBLP
Backdoor Federated Learning-Based mmWave Beam Selection,"Zhengming Zhang, Ruming Yang, Xiangyu Zhang, Chunguo Li, Yongming Huang, Luxi Yang",IEEE Trans. Commun.,2022,"<a href=""DBLP (2022) : Backdoor Federated Learning-Based mmWave Beam Selection"" target=""_blank"">[https://doi.org/10.1109/TCOMM.2022.3200111]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TCOMM.2022.3200111]</a>",,,DBLP
Backdoor Poisoning of Encrypted Traffic Classifiers,"John T. Holodnak, Olivia M. Brown, Jason Matterer, Andrew Lemke",ICDM,2022,"<a href=""DBLP (2022) : Backdoor Poisoning of Encrypted Traffic Classifiers"" target=""_blank"">[https://doi.org/10.1109/ICDMW58026.2022.00080]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICDMW58026.2022.00080]</a>",,,DBLP
Backdoor smoothing: Demystifying backdoor attacks on deep neural networks,"Kathrin Grosse, Taesung Lee, Battista Biggio, Youngja Park, Michael Backes, Ian M. Molloy",Comput. Secur.,2022,"<a href=""DBLP (2022) : Backdoor smoothing: Demystifying backdoor attacks on deep neural networks"" target=""_blank"">[https://doi.org/10.1016/j.cose.2022.102814]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.cose.2022.102814]</a>",,,DBLP
Backdoors Against Natural Language Processing: A Review,"Shaofeng Li, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Suguo Du, Haojin Zhu",IEEE Secur. Priv.,2022,"<a href=""DBLP (2022) : Backdoors Against Natural Language Processing: A Review"" target=""_blank"">[https://doi.org/10.1109/MSEC.2022.3181001]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/MSEC.2022.3181001]</a>",,,DBLP
Big Brother Is Watching You: A Closer Look At Backdoor Construction,"Anubhab Baksi, Arghya Bhattacharjee, Jakub Breier, Takanori Isobe, Mridul Nandi",IACR Cryptol. ePrint Arch.,2022,"<a href=""DBLP (2022) : Big Brother Is Watching You: A Closer Look At Backdoor Construction"" target=""_blank"">[https://eprint.iacr.org/2022/953]</a>","<a href=""DBLP"" target=""_blank"">[https://eprint.iacr.org/2022/953]</a>",,,DBLP
BlockRAT: An Enhanced Remote Access Trojan Framework via Blockchain,"Yanze Kang, Xiaobo Yu, ... Yining Liu",Science of Cyber Security,2022,"<a href=""Springer (2022) : BlockRAT: An Enhanced Remote Access Trojan Framework via Blockchain"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17551-0_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17551-0_2]</a>","Remote Access Trojan (RAT) is a type of malicious software, aiming to infect victims’ computers through targeted attacks. Most existing RATs require...",,Springer
Blockchain-Based Authentication Scheme to Secure Supermarket Transactions—A Survey,"L. Bilvashree, H. A. Brinda Nadig, ... K. Harshitha",ICT Systems and Sustainability,2022,"<a href=""Springer (2022) : Blockchain-Based Authentication Scheme to Secure Supermarket Transactions—A Survey"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5987-4_58]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5987-4_58]</a>",Internet of things (IoT) takes an important part in authorizing in converting physical assets to smart establishments through the existing network...,,Springer
Boosting the Performance of CDCL-Based SAT Solvers by Exploiting Backbones and Backdoors,"Tasniem Nasser Al-Yahya, Mohamed El Bachir Menai, Hassan Mathkour",Algorithms,2022,"<a href=""DBLP (2022) : Boosting the Performance of CDCL-Based SAT Solvers by Exploiting Backbones and Backdoors"" target=""_blank"">[https://doi.org/10.3390/a15090302]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.3390/a15090302]</a>",,,DBLP
Bridging Formal Methods and Machine Learning with Global Optimisation,"Xiaowei Huang, Wenjie Ruan, ... Xingyu Zhao",Formal Methods and Software Engineering,2022,"<a href=""Springer (2022) : Bridging Formal Methods and Machine Learning with Global Optimisation"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17244-1_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17244-1_1]</a>",Formal methods and machine learning are two research fields with drastically different foundations and philosophies. Formal methods utilise...,,Springer
CRAB: Certified Patch Robustness Against Poisoning-Based Backdoor Attacks,"Huxiao Ji, Jie Li, Chentao Wu",ICIP,2022,"<a href=""DBLP (2022) : CRAB: Certified Patch Robustness Against Poisoning-Based Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1109/ICIP46576.2022.9897387]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICIP46576.2022.9897387]</a>",,,DBLP
Campus Network Intrusion Detection Method Based on Convolutional Neural Network in Big Data Environment,"Chao Yuan, Yubian Wang",Application of Intelligent Systems in Multi-modal Information Analytics,2022,"<a href=""Springer (2022) : Campus Network Intrusion Detection Method Based on Convolutional Neural Network in Big Data Environment"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-05237-8_117]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-05237-8_117]</a>","In the big data (BD) environment, network intrusion detection (NID) technology has become a research hotspot at home and abroad. As one of the...",,Springer
"Can Collaborative Learning Be Private, Robust and Scalable?","Dmitrii Usynin, Helena Klause, ... Georgios Kaissis","Distributed, Collaborative, and Federated Learning, and Affordable AI and Healthcare for Resource Diverse Global Health",2022,"<a href=""Springer (2022) : Can Collaborative Learning Be Private, Robust and Scalable?"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18523-6_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18523-6_4]</a>","In federated learning for medical image analysis, the safety of the learning protocol is paramount. Such settings can often be compromised by...",,Springer
Cloud Attacks and Defence Mechanism for SaaS: A Survey,"Akram Harun Shaikh, B. B. Meshram",Intelligent Computing and Networking,2022,"<a href=""Springer (2022) : Cloud Attacks and Defence Mechanism for SaaS: A Survey"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-4863-2_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-4863-2_4]</a>",Cloud computing systems are the de-facto deployments for any user data and processing requirements. Due to a wide variety of cloud systems available...,,Springer
Complex Backdoor Detection by Symmetric Feature Differencing,"Yingqi Liu, Guangyu Shen, Guanhong Tao, Zhenting Wang, Shiqing Ma, Xiangyu Zhang",CVPR,2022,"<a href=""DBLP (2022) : Complex Backdoor Detection by Symmetric Feature Differencing"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.01458]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.01458]</a>",,,DBLP
Constructing and Deconstructing Intentional Weaknesses in Symmetric Ciphers,"Christof Beierle, Tim Beyne, ... Gregor Leander",Advances in Cryptology – CRYPTO 2022,2022,"<a href=""Springer (2022) : Constructing and Deconstructing Intentional Weaknesses in Symmetric Ciphers"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-15982-4_25]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-15982-4_25]</a>","Deliberately weakened ciphers are of great interest in political discussion on law enforcement, as in the constantly recurring crypto wars, and have...",,Springer
Coordinated Backdoor Attacks against Federated Learning with Model-Dependent Triggers,"Xueluan Gong, Yanjiao Chen, Huayang Huang, Yuqing Liao, Shuai Wang, Qian Wang",IEEE Netw.,2022,"<a href=""DBLP (2022) : Coordinated Backdoor Attacks against Federated Learning with Model-Dependent Triggers"" target=""_blank"">[https://doi.org/10.1109/MNET.011.2000783]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/MNET.011.2000783]</a>",,,DBLP
Cyberpunk and Cypherpunk: A Philosophical Analysis Comparing Two Views of the Metaverse,Ma Hanlin,Metaverse – METAVERSE 2022,2022,"<a href=""Springer (2022) : Cyberpunk and Cypherpunk: A Philosophical Analysis Comparing Two Views of the Metaverse"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-23518-4_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-23518-4_7]</a>","As ancestors of the idea of the metaverse - Cyberpunks and Cypherpunks - propose two distinct attitudes towards Cyberspace, and this difference also...",,Springer
DEFEAT: Deep Hidden Feature Backdoor Attacks by Imperceptible Perturbation and Latent Representation Constraints,"Zhendong Zhao, Xiaojun Chen, Yuexin Xuan, Ye Dong, Dakui Wang, Kaitai Liang",CVPR,2022,"<a href=""DBLP (2022) : DEFEAT: Deep Hidden Feature Backdoor Attacks by Imperceptible Perturbation and Latent Representation Constraints"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.01478]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CVPR52688.2022.01478]</a>",,,DBLP
DNNdroid: Android Malware Detection Framework Based on Federated Learning and Edge Computing,"Arvind Mahindru, Himani Arora",Advancements in Smart Computing and Information Security,2022,"<a href=""Springer (2022) : DNNdroid: Android Malware Detection Framework Based on Federated Learning and Edge Computing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-23095-0_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-23095-0_7]</a>",The fact that apps are available for free via Android’s official store has helped the platform become more popular. The functionality of Android apps...,,Springer
DOMAIN-Based Intelligent Network Intrusion Detection System,"Nithil Jose, J. Govindarajan",Inventive Computation and Information Technologies,2022,"<a href=""Springer (2022) : DOMAIN-Based Intelligent Network Intrusion Detection System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-6723-7_34]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-6723-7_34]</a>","The state-of-the-art presently in the network intrusion detection, both in the network-level intrusion detection system and the host-level intrusion...",,Springer
Deep Reinforcement Learning for Cybersecurity Threat Detection and Protection: A Review,"Mohit Sewak, Sanjay K. Sahay, Hemant Rathore",Secure Knowledge Management In The Artificial Intelligence Era,2022,"<a href=""Springer (2022) : Deep Reinforcement Learning for Cybersecurity Threat Detection and Protection: A Review"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-97532-6_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-97532-6_4]</a>",The cybersecurity threat landscape has lately become overly complex. Threat actors leverage weaknesses in the network and endpoint security in a very...,,Springer
Defense against backdoor attack in federated learning,"Shiwei Lu, Ruihu Li, Wenbin Liu, Xuan Chen",Comput. Secur.,2022,"<a href=""DBLP (2022) : Defense against backdoor attack in federated learning"" target=""_blank"">[https://doi.org/10.1016/j.cose.2022.102819]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.cose.2022.102819]</a>",,,DBLP
Defense of Scapegoating Attack in Network Tomography,"Xiaojia Xu, Yongcai Wang, ... Deying Li",Algorithmic Aspects in Information and Management,2022,"<a href=""Springer (2022) : Defense of Scapegoating Attack in Network Tomography"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-16081-3_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-16081-3_15]</a>","Defending of scapegoating attack is a critical problem in network tomography. Theoretically, the ideal defending scheme is to add monitoring paths to...",,Springer
Deletion-Backdoors for Argumentation Frameworks with Collective Attacks,"Wolfgang Dvorák, Matthias König, Stefan Woltran",SAFA@COMMA,2022,"<a href=""DBLP (2022) : Deletion-Backdoors for Argumentation Frameworks with Collective Attacks"" target=""_blank"">[https://ceur-ws.org/Vol-3236/paper8.pdf]</a>","<a href=""DBLP"" target=""_blank"">[https://ceur-ws.org/Vol-3236/paper8.pdf]</a>",,,DBLP
Detecting Backdoor Attacks against Point Cloud Classifiers,"Zhen Xiang, David J. Miller, Siheng Chen, Xi Li, George Kesidis",ICASSP,2022,"<a href=""DBLP (2022) : Detecting Backdoor Attacks against Point Cloud Classifiers"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9747194]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9747194]</a>",,,DBLP
Detecting Backdoor Attacks on Deep Neural Networks Based on Model Parameters Analysis,"Mingyuan Ma, Hu Li, Xiaohui Kuang",ICTAI,2022,"<a href=""DBLP (2022) : Detecting Backdoor Attacks on Deep Neural Networks Based on Model Parameters Analysis"" target=""_blank"">[https://doi.org/10.1109/ICTAI56018.2022.00098]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICTAI56018.2022.00098]</a>",,,DBLP
Detecting Stegomalware: Malicious Image Steganography and Its Intrusion in Windows,"Vinita Verma, Sunil K. Muttoo, V. B. Singh","Security, Privacy and Data Analytics",2022,"<a href=""Springer (2022) : Detecting Stegomalware: Malicious Image Steganography and Its Intrusion in Windows"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-9089-1_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-9089-1_9]</a>","Steganography, a data hiding technique has trended into hiding the malware within digital media, giving rise to stegomalware. Specifically, digital...",,Springer
Detection Accuracy for Evaluating Compositional Explanations of Units,"Sayo M. Makinwa, Biagio La Rosa, Roberto Capobianco",AIxIA 2021 – Advances in Artificial Intelligence,2022,"<a href=""Springer (2022) : Detection Accuracy for Evaluating Compositional Explanations of Units"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-08421-8_38]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-08421-8_38]</a>",The recent success of deep learning models in solving complex problems and in different domains has increased interest in understanding what they...,,Springer
Detection of Anomalies in IoT Systems by Neuroevolution Algorithms,"Alexander Fatin, Evgeny Pavlenko, Peter Zegzhda",Algorithms and Solutions Based on Computer Technology,2022,"<a href=""Springer (2022) : Detection of Anomalies in IoT Systems by Neuroevolution Algorithms"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-93872-7_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-93872-7_5]</a>","The aim of the study is to detect anomalies in the operation of Internet of Things (IoT) systems, cyber-physical systems and Supervisory Control And...",,Springer
Detection of Signature-Based Attacks in Cloud Infrastructure Using Support Vector Machine,"B. Radha, D. Sakthivel",Mobile Radio Communications and 5G Networks,2022,"<a href=""Springer (2022) : Detection of Signature-Based Attacks in Cloud Infrastructure Using Support Vector Machine"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-7018-3_38]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-7018-3_38]</a>","In modern years, cloud computing has arisen as a widely utilized innovation in IT area. With increase in the use of cloud computing, it has become...",,Springer
DiSSECT: Distinguisher of Standard and Simulated Elliptic Curves via Traits,"Vladimir Sedlacek, Vojtech Suchanek, ... Vashek Matyas",Progress in Cryptology - AFRICACRYPT 2022,2022,"<a href=""Springer (2022) : DiSSECT: Distinguisher of Standard and Simulated Elliptic Curves via Traits"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17433-9_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17433-9_21]</a>","It can be tricky to trust elliptic curves standardized in a non-transparent way. To rectify this, we propose a systematic methodology for analyzing...",,Springer
Dictionary-Based DGAs Variants Detection,"Raja Azlina Raja Mahmood, Azizol Abdullah, ... Nur Izura Udzir",Advances on Intelligent Informatics and Computing,2022,"<a href=""Springer (2022) : Dictionary-Based DGAs Variants Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-98741-1_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-98741-1_22]</a>","Domain Generation Algorithm (DGA) has been used by botnets to obfuscate the connections between the bot master and its bots. The recent DGAs, namely...",,Springer
Distributed (Correlation) Samplers: How to Remove a Trusted Dealer in One Round,"Damiano Abram, Peter Scholl, Sophia Yakoubov",Advances in Cryptology – EUROCRYPT 2022,2022,"<a href=""Springer (2022) : Distributed (Correlation) Samplers: How to Remove a Trusted Dealer in One Round"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-06944-4_27]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-06944-4_27]</a>",Structured random strings (SRSs) and correlated randomness are important for many cryptographic protocols. In settings where interaction is...,,Springer
Do Dark Web and Cryptocurrencies Empower Cybercriminals?,"Milad Taleby Ahvanooey, Mark Xuefang Zhu, ... Kim-Kwang Raymond Choo",Digital Forensics and Cyber Crime,2022,"<a href=""Springer (2022) : Do Dark Web and Cryptocurrencies Empower Cybercriminals?"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-06365-7_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-06365-7_17]</a>","The dark web is often associated with criminal activities such as the sale of exploit kits using cryptocurrencies as payment. However, the difficulty...",,Springer
Effective Backdoor Defense by Exploiting Sensitivity of Poisoned Samples,"Weixin Chen, Baoyuan Wu, Haoqian Wang",NeurIPS,2022,"<a href=""DBLP (2022) : Effective Backdoor Defense by Exploiting Sensitivity of Poisoned Samples"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/3f9bbf77fbd858e5b6e39d39fe84ed2e-Abstract-Conference.html]</a>","<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/3f9bbf77fbd858e5b6e39d39fe84ed2e-Abstract-Conference.html]</a>",,,DBLP
Employing Feature Selection to Improve the Performance of Intrusion Detection Systems,"Ricardo Avila, Raphaël Khoury, ... Kobra Khanmohammadi",Foundations and Practice of Security,2022,"<a href=""Springer (2022) : Employing Feature Selection to Improve the Performance of Intrusion Detection Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-08147-7_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-08147-7_7]</a>",Intrusion detection systems use datasets with various features to detect attacks and protect computers and network systems from these attacks....,,Springer
Energy-Based Learning for Polluted Outlier Detection in Backdoor,"Xiangyu Gao, Meikang Qiu",SmartCloud,2022,"<a href=""DBLP (2022) : Energy-Based Learning for Polluted Outlier Detection in Backdoor"" target=""_blank"">[https://doi.org/10.1109/SmartCloud55982.2022.00014]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SmartCloud55982.2022.00014]</a>",,,DBLP
Enhanced Intrusion Detection System Based on AutoEncoder Network and Support Vector Machine,"Sihem Dadi, Mohamed Abid","Networking, Intelligent Systems and Security",2022,"<a href=""Springer (2022) : Enhanced Intrusion Detection System Based on AutoEncoder Network and Support Vector Machine"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-3637-0_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-3637-0_23]</a>","In the recent years, Internet of Vehicles (IoV) became the subject of many searches. IoV relies on Vehicular Ad-hoc NETwork (VANET) which is based on...",,Springer
Enhancing System Security by Intrusion Detection Using Deep Learning,"Lakshit Sama, Hua Wang, Paul Watters",Databases Theory and Applications,2022,"<a href=""Springer (2022) : Enhancing System Security by Intrusion Detection Using Deep Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-15512-3_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-15512-3_14]</a>",Network intrusion detection has become a hot topic in cyber security research due to better advancements in deep learning. The research is lacking an...,,Springer
Evaluation of Kernel-Level IoT Security and QoS Aware Models from an Empirical Perspective,"Bharat S. Dhak, Prabhakar L. Ramteke","Proceedings of the 3rd International Conference on Communication, Devices and Computing",2022,"<a href=""Springer (2022) : Evaluation of Kernel-Level IoT Security and QoS Aware Models from an Empirical Perspective"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-9154-6_68]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-9154-6_68]</a>","Internet of Things (IoT) based devices have proven to be highly useful during this pandemic. From home automation to industrial monitoring, these...",,Springer
Experimental Study of Fault Injection Attack on Image Sensor Interface for Triggering Backdoored DNN Models,"Tatsuya Oyama, Shunsuke Okura, Kota Yoshida, Takeshi Fujino",IEICE Trans. Fundam. Electron. Commun. Comput. Sci.,2022,"<a href=""DBLP (2022) : Experimental Study of Fault Injection Attack on Image Sensor Interface for Triggering Backdoored DNN Models"" target=""_blank"">[https://doi.org/10.1587/transfun.2021cip0019]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1587/transfun.2021cip0019]</a>",,,DBLP
Extending Machine Learning Techniques Using Multi-level Approach to Detect and Classify Anomalies in a Network on UNSW-NB15 dataset,"Utkarsh Rodge, Vinod Pathari","Computational Intelligence, Cyber Security and Computational Models. Recent Trends in Computational Models, Intelligent and Secure Systems",2022,"<a href=""Springer (2022) : Extending Machine Learning Techniques Using Multi-level Approach to Detect and Classify Anomalies in a Network on UNSW-NB15 dataset"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-15556-7_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-15556-7_4]</a>","With the increase in the number of internet-connected devices and widespread adoption of IoT devices, there is a massive growth in network activity...",,Springer
FLAS: A Platform for Studying Attacks on Federated Learning,"Yuanchao Loh, Zichen Chen, ... Han Yu","Social Computing and Social Media: Design, User Experience and Impact",2022,"<a href=""Springer (2022) : FLAS: A Platform for Studying Attacks on Federated Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-05061-9_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-05061-9_12]</a>","Smartphones have become a part of everyday life, and users are contributing to Machine Learning with a simple touch (ML). Federated Learning (FL) is...",,Springer
FSAFA-stacking2: An Effective Ensemble Learning Model for Intrusion Detection with Firefly Algorithm Based Feature Selection,"Guo Chen, Junyao Zheng, ... Weigang Wu",Algorithms and Architectures for Parallel Processing,2022,"<a href=""Springer (2022) : FSAFA-stacking2: An Effective Ensemble Learning Model for Intrusion Detection with Firefly Algorithm Based Feature Selection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-95388-1_37]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-95388-1_37]</a>",This paper presents a two-layer ensemble learning model stacking2 based on the Stacking framework to deal with the problems of lack of generalization...,,Springer
Faster Algorithms for Weak Backdoors,"Serge Gaspers, Andrew Kaploun",AAAI,2022,"<a href=""DBLP (2022) : Faster Algorithms for Weak Backdoors"" target=""_blank"">[https://doi.org/10.1609/aaai.v36i4.20288]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v36i4.20288]</a>",,,DBLP
FedBC: An Efficient and Privacy-Preserving Federated Consensus Scheme,"Mengfan Xu, Xinghua Li",Security and Privacy in Social Networks and Big Data,2022,"<a href=""Springer (2022) : FedBC: An Efficient and Privacy-Preserving Federated Consensus Scheme"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-7242-3_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-7242-3_10]</a>",The capacity of federated learning (FL) to tackle the issue of “Data Island” while maintaining data privacy has garnered significant attention....,,Springer
FedMCS: A Privacy-Preserving Mobile Crowdsensing Defense Scheme,"Mengfan Xu, Xinghua Li",Cyberspace Safety and Security,2022,"<a href=""Springer (2022) : FedMCS: A Privacy-Preserving Mobile Crowdsensing Defense Scheme"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18067-5_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18067-5_18]</a>","Mobile crowdsensing (MCS) provides a promising avenue for distributed data sharing. However, the increased privacy of workers and the existence of...",,Springer
Flow-Based Intrusion Detection Systems: A Survey,"Aliaa Al-Bakaa, Bahaa Al-Musawi",Applications and Techniques in Information Security,2022,"<a href=""Springer (2022) : Flow-Based Intrusion Detection Systems: A Survey"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-1166-8_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-1166-8_10]</a>","After developing IoT devices, information security takes a critical role than any other period. Most IoT devices use weak passwords, insecure...",,Springer
Fusion of Traffic Data and Alert Log Based on Sensitive Information,"Jie Cheng, Ru Zhang, ... Shulin Zhang","Proceeding of 2021 International Conference on Wireless Communications, Networking and Applications",2022,"<a href=""Springer (2022) : Fusion of Traffic Data and Alert Log Based on Sensitive Information"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-2456-9_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-2456-9_9]</a>","At present, the attack behavior that occurs in the network has gradually developed from a single-step, simple attack method to a complex multi-step...",,Springer
Harmonic Group Mix: A Framework for Anonymous and Authenticated Broadcast Messages in Vehicle-to-Vehicle Environments,"Mirja Nitschke, Christian Roth, ... Doğan Kesdoğan",Information Systems Security and Privacy,2022,"<a href=""Springer (2022) : Harmonic Group Mix: A Framework for Anonymous and Authenticated Broadcast Messages in Vehicle-to-Vehicle Environments"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-94900-6_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-94900-6_9]</a>","Nowadays Vehicle-to-Vehicle communication (V2V) plays an increasingly important role, not only in terms of safety, but also in other areas of...",,Springer
Hibernated Backdoor: A Mutual Information Empowered Backdoor Attack to Deep Neural Networks,"Rui Ning, Jiang Li, Chunsheng Xin, Hongyi Wu, Chonggang Wang",AAAI,2022,"<a href=""DBLP (2022) : Hibernated Backdoor: A Mutual Information Empowered Backdoor Attack to Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1609/aaai.v36i9.21272]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v36i9.21272]</a>",,,DBLP
Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation,"Xudong Pan, Mi Zhang, Beina Sheng, Jiaming Zhu, Min Yang",USENIX Security Symposium,2022,"<a href=""DBLP (2022) : Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity22/presentation/pan-hidden]</a>","<a href=""DBLP"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity22/presentation/pan-hidden]</a>",,,DBLP
Hide and Seek: On the Stealthiness of Attacks Against Deep Learning Systems,"Zeyan Liu, Fengjun Li, ... Bo Luo",Computer Security – ESORICS 2022,2022,"<a href=""Springer (2022) : Hide and Seek: On the Stealthiness of Attacks Against Deep Learning Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17143-7_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17143-7_17]</a>","With the growing popularity of artificial intelligence (AI) and machine learning (ML), a wide spectrum of attacks against deep learning (DL) models...",,Springer
How Can Organizations Prevent Cyber Attacks Using Proper Cloud Computing Security?,"Ismail Mohammed Bahkali, Sudhanshu Kumar Semwal","Proceedings of the Future Technologies Conference (FTC) 2021, Volume 3",2022,"<a href=""Springer (2022) : How Can Organizations Prevent Cyber Attacks Using Proper Cloud Computing Security?"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-89912-7_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-89912-7_14]</a>",Cloud Computing is a collection of services and products delivered over a network or the Internet. The intruder no longer has to attack the user’s...,,Springer
How to Backdoor (Classical) McEliece and How to Guard Against Backdoors,"Alexander May, Carl Richard Theodor Schneider",IACR Cryptol. ePrint Arch.,2022,"<a href=""DBLP (2022) : How to Backdoor (Classical) McEliece and How to Guard Against Backdoors"" target=""_blank"">[https://eprint.iacr.org/2022/362]</a>","<a href=""DBLP"" target=""_blank"">[https://eprint.iacr.org/2022/362]</a>",,,DBLP
How to backdoor LWE-like cryptosystems,Tobias Hemmert,IACR Cryptol. ePrint Arch.,2022,"<a href=""DBLP (2022) : How to backdoor LWE-like cryptosystems"" target=""_blank"">[https://eprint.iacr.org/2022/1381]</a>","<a href=""DBLP"" target=""_blank"">[https://eprint.iacr.org/2022/1381]</a>",,,DBLP
I Know Your Triggers: Defending Against Textual Backdoor Attacks with Benign Backdoor Augmentation,"Yue Gao, Jack W. Stokes, Manoj Ajith Prasad, Andrew T. Marshall, Kassem Fawaz, Emre Kiciman",MILCOM,2022,"<a href=""DBLP (2022) : I Know Your Triggers: Defending Against Textual Backdoor Attacks with Benign Backdoor Augmentation"" target=""_blank"">[https://doi.org/10.1109/MILCOM55135.2022.10017466]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/MILCOM55135.2022.10017466]</a>",,,DBLP
IBD: An Interpretable Backdoor-Detection Method via Multivariate Interactions,"Yixiao Xu, Xiaolei Liu, Kangyi Ding, Bangzhou Xin",Sensors,2022,"<a href=""DBLP (2022) : IBD: An Interpretable Backdoor-Detection Method via Multivariate Interactions"" target=""_blank"">[https://doi.org/10.3390/s22228697]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.3390/s22228697]</a>",,,DBLP
Information Security Incidents in the Last 5 Years and Vulnerabilities of Automated Information Systems in the Fleet,"German Danilin, Sergey Sokolov, ... Vijendra Singh",International Scientific Siberian Transport Forum TransSiberia - 2021,2022,"<a href=""Springer (2022) : Information Security Incidents in the Last 5 Years and Vulnerabilities of Automated Information Systems in the Fleet"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-96383-5_172]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-96383-5_172]</a>",The trend towards the introduction of unmanned ships has been outlined for a long time: the number of crews on river and sea ships has been...,,Springer
Interpretability-Guided Defense Against Backdoor Attacks to Deep Neural Networks,"Wei Jiang, Xiangyu Wen, Jinyu Zhan, Xupeng Wang, Ziwei Song",IEEE Trans. Comput. Aided Des. Integr. Circuits Syst.,2022,"<a href=""DBLP (2022) : Interpretability-Guided Defense Against Backdoor Attacks to Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1109/TCAD.2021.3111123]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TCAD.2021.3111123]</a>",,,DBLP
Invisible and Efficient Backdoor Attacks for Compressed Deep Neural Networks,"Huy Phan, Yi Xie, Jian Liu, Yingying Chen, Bo Yuan",ICASSP,2022,"<a href=""DBLP (2022) : Invisible and Efficient Backdoor Attacks for Compressed Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9747582]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9747582]</a>",,,DBLP
"KRAKEN: A Knowledge-Based Recommender System for Analysts, to Kick Exploration up a Notch","Romain Brisse, Simon Boche, ... Jean-Francois Lalande",Innovative Security Solutions for Information Technology and Communications,2022,"<a href=""Springer (2022) : KRAKEN: A Knowledge-Based Recommender System for Analysts, to Kick Exploration up a Notch"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17510-7_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17510-7_1]</a>","During a computer security investigation, a security analyst has to explore the logs available to understand what happened in the compromised system....",,Springer
LinkBreaker: Breaking the Backdoor-Trigger Link in DNNs via Neurons Consistency Check,"Zhenzhu Chen, Shang Wang, Anmin Fu, Yansong Gao, Shui Yu, Robert H. Deng",IEEE Trans. Inf. Forensics Secur.,2022,"<a href=""DBLP (2022) : LinkBreaker: Breaking the Backdoor-Trigger Link in DNNs via Neurons Consistency Check"" target=""_blank"">[https://doi.org/10.1109/TIFS.2022.3175616]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2022.3175616]</a>",,,DBLP
Local Intrinsic Dimensionality of IoT Networks for Unsupervised Intrusion Detection,"Matt Gorbett, Hossein Shirazi, Indrakshi Ray",Data and Applications Security and Privacy XXXVI,2022,"<a href=""Springer (2022) : Local Intrinsic Dimensionality of IoT Networks for Unsupervised Intrusion Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-10684-2_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-10684-2_9]</a>","The Internet of Things (IoT) is revolutionizing society by connecting people, devices, and environments seamlessly and providing enhanced user...",,Springer
Long-Short History of Gradients Is All You Need: Detecting Malicious and Unreliable Clients in Federated Learning,"Ashish Gupta, Tie Luo, ... Sajal K. Das",Computer Security – ESORICS 2022,2022,"<a href=""Springer (2022) : Long-Short History of Gradients Is All You Need: Detecting Malicious and Unreliable Clients in Federated Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17143-7_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17143-7_22]</a>",Federated learning offers a framework of training a machine learning model in a distributed fashion while preserving privacy of the participants. As...,,Springer
MANIPULATION OF G-CODE TOOLPATH FILES IN 3D PRINTERS: ATTACKS AND MITIGATIONS,"Elizabeth Kurkowski, Alyxandra Van Stockum, ... Sujeet Shenoi",Critical Infrastructure Protection XVI,2022,"<a href=""Springer (2022) : MANIPULATION OF G-CODE TOOLPATH FILES IN 3D PRINTERS: ATTACKS AND MITIGATIONS"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-20137-0_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-20137-0_6]</a>",Additive manufacturing or 3D printing is commonly used to create mission-critical parts in the critical infrastructure. This research focuses on...,,Springer
MP-BADNet+: Secure and effective backdoor attack detection and mitigation protocols among multi-participants in private DNNs,"Congcong Chen, Lifei Wei, Lei Zhang, Ya Peng, Jianting Ning",Peer-to-Peer Netw. Appl.,2022,"<a href=""DBLP (2022) : MP-BADNet+: Secure and effective backdoor attack detection and mitigation protocols among multi-participants in private DNNs"" target=""_blank"">[https://doi.org/10.1007/s12083-022-01377-6]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/s12083-022-01377-6]</a>",,,DBLP
Machine Learning Models for Malicious Traffic Detection in IoT Networks /IoT-23 Dataset/,"Chibueze Victor Oha, Fathima Shakoora Farouk, ... Sergey Butakov",Machine Learning for Networking,2022,"<a href=""Springer (2022) : Machine Learning Models for Malicious Traffic Detection in IoT Networks /IoT-23 Dataset/"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-98978-1_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-98978-1_5]</a>",Connected devices are penetrating markets with an unprecedented speed. Networks that carry Internet of Things (IoT) traffic need highly adaptable...,,Springer
MaleficNet: Hiding Malware into Deep Neural Networks Using Spread-Spectrum Channel Coding,"Dorjan Hitaj, Giulio Pagnotta, ... Fernando Perez-Cruz",Computer Security – ESORICS 2022,2022,"<a href=""Springer (2022) : MaleficNet: Hiding Malware into Deep Neural Networks Using Spread-Spectrum Channel Coding"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17143-7_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17143-7_21]</a>","The training and development of good deep learning models is often a challenging task, thus leading individuals (developers, researchers, and...",,Springer
Malware Prediction Using LSTM Networks,"Saba Iqbal, Abrar Ullah, ... Ahmad Ryad Soobhany",Proceedings of International Conference on Information Technology and Applications,2022,"<a href=""Springer (2022) : Malware Prediction Using LSTM Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-7618-5_51]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-7618-5_51]</a>","With a recent increase in the use of the Internet, there has been a rise in malware attacks. Malware attacks can lead to stealing confidential data...",,Springer
Malware Techniques and Its Effect: A Survey,"Deepali Yadav, Gautam Kumar, ... Sheo Kumar",ICCCE 2021,2022,"<a href=""Springer (2022) : Malware Techniques and Its Effect: A Survey"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-7985-8_127]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-7985-8_127]</a>",Malware is worldwide outbreak which tends to mitigate attacks and collects private data. Studies said that the impact were surpass. There are a...,,Springer
MasterFace Watermarking for IPR Protection of Siamese Network for Face Verification,"Wei Guo, Benedetta Tondi, Mauro Barni",Digital Forensics and Watermarking,2022,"<a href=""Springer (2022) : MasterFace Watermarking for IPR Protection of Siamese Network for Face Verification"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-95398-0_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-95398-0_13]</a>",Deep Neural Network (DNN) watermarking is receiving increasing attention as means to protect the Intellectual Property Rights (IPR) of DNN models....,,Springer
Measuring the Effectiveness of SAT-Based Guess-and-Determine Attacks in Algebraic Cryptanalysis,"Andrey Gladush, Irina Gribanova, ... Alexander Semenov",Parallel Computational Technologies,2022,"<a href=""Springer (2022) : Measuring the Effectiveness of SAT-Based Guess-and-Determine Attacks in Algebraic Cryptanalysis"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-11623-0_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-11623-0_11]</a>",This paper studies the problem of algebraic cryptanalysis where state-of-the-art SAT solvers are used to invert some cryptographic function. We...,,Springer
Methodology of ISMS Establishment Against Modern Cybersecurity Threats,"Vitalii Susukailo, Ivan Opirsky, Oleh Yaremko",Future Intent-Based Networking,2022,"<a href=""Springer (2022) : Methodology of ISMS Establishment Against Modern Cybersecurity Threats"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92435-5_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92435-5_15]</a>","The chapter addresses the Information Security Management System (ISMS) establishment approach, ensuring necessary controls to avoid widespread...",,Springer
Mitigating Targeted Bit-Flip Attacks via Data Augmentation: An Empirical Study,"Ziyuan Zhang, Meiqi Wang, ... Meikang Qiu","Knowledge Science, Engineering and Management",2022,"<a href=""Springer (2022) : Mitigating Targeted Bit-Flip Attacks via Data Augmentation: An Empirical Study"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-10989-8_48]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-10989-8_48]</a>","As deep neural networks (DNNs) become more widely used in various safety-critical applications, protecting their security has been an urgent and...",,Springer
Multicriteria Optimization Techniques in SVM Method for the Classification Problem,Anastasia A. Andrianova,Mesh Methods for Boundary-Value Problems and Applications,2022,"<a href=""Springer (2022) : Multicriteria Optimization Techniques in SVM Method for the Classification Problem"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-87809-2_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-87809-2_2]</a>",The paper considers the procedures for solving the multiclass classification problem using a series of support vector machine optimization problems...,,Springer
New Generation Power System Security Protection Technology Based on Dynamic Defense,"Xiaowei Chen, Hefang Jiang, ... Muhammad Shafiq",Artificial Intelligence and Security,2022,"<a href=""Springer (2022) : New Generation Power System Security Protection Technology Based on Dynamic Defense"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-06791-4_56]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-06791-4_56]</a>","With the development of power information systems and the evolution of attack means, the traditional security protection scheme is difficult to deal...",,Springer
ODDITY: An Ensemble Framework Leverages Contrastive Representation Learning for Superior Anomaly Detection,"Hongyi Peng, Vinay Sachidananda, ... Mohan Gurusamy",Information and Communications Security,2022,"<a href=""Springer (2022) : ODDITY: An Ensemble Framework Leverages Contrastive Representation Learning for Superior Anomaly Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-15777-6_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-15777-6_23]</a>","Ensemble approaches are promising for anomaly detection due to the heterogeneity of network traffic. However, existing ensemble approaches lack...",,Springer
Object-Oriented Backdoor Attack Against Image Captioning,"Meiling Li, Nan Zhong, Xinpeng Zhang, Zhenxing Qian, Sheng Li",ICASSP,2022,"<a href=""DBLP (2022) : Object-Oriented Backdoor Attack Against Image Captioning"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9746440]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9746440]</a>",,,DBLP
On the Susceptibility of Texas Instruments SimpleLink Platform Microcontrollers to Non-invasive Physical Attacks,"Lennert Wouters, Benedikt Gierlichs, Bart Preneel",Constructive Side-Channel Analysis and Secure Design,2022,"<a href=""Springer (2022) : On the Susceptibility of Texas Instruments SimpleLink Platform Microcontrollers to Non-invasive Physical Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-99766-3_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-99766-3_7]</a>",We investigate the susceptibility of the Texas Instruments SimpleLink platform microcontrollers to non-invasive physical attacks. We extracted the...,,Springer
Optimization of Support Vector Machine for Classification of Spyware Using Symbiotic Organism Search for Features Selection,"Noah Ndakotsu Gana, Shafi’i Muhammad Abdulhamid, ... Ambrose Azeta",Information Systems and Management Science,2022,"<a href=""Springer (2022) : Optimization of Support Vector Machine for Classification of Spyware Using Symbiotic Organism Search for Features Selection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-86223-7_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-86223-7_2]</a>","Malware’s key target is to compromise system security pillars, the confidentiality, integrity and availability. Spyware is a form of malware program...",,Springer
PPBR-FL: A Privacy-Preserving and Byzantine-Robust Federated Learning System,"Ying Lin, Shengfu Ning, ... Huan Pi","Knowledge Science, Engineering and Management",2022,"<a href=""Springer (2022) : PPBR-FL: A Privacy-Preserving and Byzantine-Robust Federated Learning System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-10989-8_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-10989-8_4]</a>","As a distributed machine learning framework, federated learning enables a multitude of participants to train a joint model privately by keeping...",,Springer
PPT: Backdoor Attacks on Pre-trained Models via Poisoned Prompt Tuning,"Wei Du, Yichun Zhao, Boqun Li, Gongshen Liu, Shilin Wang",IJCAI,2022,"<a href=""DBLP (2022) : PPT: Backdoor Attacks on Pre-trained Models via Poisoned Prompt Tuning"" target=""_blank"">[https://doi.org/10.24963/ijcai.2022/96]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.24963/ijcai.2022/96]</a>",,,DBLP
Performance of Secure Data Deduplication Framework in Cloud Services,"Rajesh Banala, Vicky Nair, P. Nagaraj",Artificial Intelligence and Data Science,2022,"<a href=""Springer (2022) : Performance of Secure Data Deduplication Framework in Cloud Services"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-21385-4_32]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-21385-4_32]</a>",Data Deduplication makes it convenient to utilize the available disk space efficiently and proves as a solution for preventing data redundancy....,,Springer
Phishing URL Detection and Vulnerability Assessment of Web Applications Using IVS Attributes with XAI,"Vivek John Sudhakar, Srimathi Mahalingam, ... V. Vetriselvi",ICT Analysis and Applications,2022,"<a href=""Springer (2022) : Phishing URL Detection and Vulnerability Assessment of Web Applications Using IVS Attributes with XAI"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5655-2_89]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5655-2_89]</a>","Sudhakar, Vivek John Mahalingam, Srimathi Venkatesh, Vaman Vetriselvi, V.The number of cyber attacks which are carried out are increasing day by day...",,Springer
Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches,"Zhiyuan Cheng, James Liang, ... Xiangyu Zhang",Computer Vision – ECCV 2022,2022,"<a href=""Springer (2022) : Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-19839-7_30]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-19839-7_30]</a>","Deep learning has substantially boosted the performance of Monocular Depth Estimation (MDE), a critical component in fully vision-based autonomous...",,Springer
Piccolo: Exposing Complex Backdoors in NLP Transformer Models,"Yingqi Liu, Guangyu Shen, Guanhong Tao, Shengwei An, Shiqing Ma, Xiangyu Zhang",SP,2022,"<a href=""DBLP (2022) : Piccolo: Exposing Complex Backdoors in NLP Transformer Models"" target=""_blank"">[https://doi.org/10.1109/SP46214.2022.9833579]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SP46214.2022.9833579]</a>",,,DBLP
Preliminary Investigation of Mobile Banking Attacks in West Africa: Feedback from Orange Money Customers in Burkina Faso,"Arthur D. Sawadogo, Zakaria Sawadogo, ... Issa Boussim",e-Infrastructure and e-Services for Developing Countries,2022,"<a href=""Springer (2022) : Preliminary Investigation of Mobile Banking Attacks in West Africa: Feedback from Orange Money Customers in Burkina Faso"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-06374-9_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-06374-9_2]</a>","Mobile banking is used to perform balance checks, account transactions, payments, credit applications, and other banking transactions via a mobile...",,Springer
"Preparation of a Social Engineering Attack, from Scratch to Compromise: A USB Dropper and Impersonation Approach","Jorge Sánchez Freire, Benjamín Garcés",Information and Communication Technologies,2022,"<a href=""Springer (2022) : Preparation of a Social Engineering Attack, from Scratch to Compromise: A USB Dropper and Impersonation Approach"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18272-3_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18272-3_19]</a>","Although there are different kind of tools to fight against cyber attackers, there is one vulnerability that is exploited daily due to the nature...",,Springer
Prevention of GAN-Based Privacy Inferring Attacks Towards Federated Learning,"Hongbo Cao, Yongsheng Zhu, ... Wei Wang","Collaborative Computing: Networking, Applications and Worksharing",2022,"<a href=""Springer (2022) : Prevention of GAN-Based Privacy Inferring Attacks Towards Federated Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-24386-8_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-24386-8_3]</a>","With the increasing amount of data, data privacy has drawn great concern in machine learning among the public. Federated Learning, which is a new...",,Springer
PromptAttack: Prompt-Based Attack for Language Models via Gradient Search,"Yundi Shi, Piji Li, ... Zhe Liu",Natural Language Processing and Chinese Computing,2022,"<a href=""Springer (2022) : PromptAttack: Prompt-Based Attack for Language Models via Gradient Search"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17120-8_53]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-17120-8_53]</a>","As the pre-trained language models (PLMs) continue to grow, so do the hardware and data requirements for fine-tuning PLMs. Therefore, the researchers...",,Springer
Randomized Channel Shuffling: Minimal-Overhead Backdoor Attack Detection without Clean Datasets,"Ruisi Cai, Zhenyu Zhang, Tianlong Chen, Xiaohan Chen, Zhangyang Wang",NeurIPS,2022,"<a href=""DBLP (2022) : Randomized Channel Shuffling: Minimal-Overhead Backdoor Attack Detection without Clean Datasets"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/db1d5c63576587fc1d40d33a75190c71-Abstract-Conference.html]</a>","<a href=""DBLP"" target=""_blank"">[http://papers.nips.cc/paper_files/paper/2022/hash/db1d5c63576587fc1d40d33a75190c71-Abstract-Conference.html]</a>",,,DBLP
Research on Application of Data Encryption in Computer Network Security,"Lanlan Yin, Feng Mo, ... Yin Long",Proceedings of the 11th International Conference on Computer Engineering and Networks,2022,"<a href=""Springer (2022) : Research on Application of Data Encryption in Computer Network Security"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-6554-7_75]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-6554-7_75]</a>","The rapid development of network information technology, bringing a massive impact on people’s lives, has completely broken the traditional way of...",,Springer
Robust backdoor injection with the capability of resisting network transfer,"Le Feng, Sheng Li, Zhenxing Qian, Xinpeng Zhang",Inf. Sci.,2022,"<a href=""DBLP (2022) : Robust backdoor injection with the capability of resisting network transfer"" target=""_blank"">[https://doi.org/10.1016/j.ins.2022.08.123]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.ins.2022.08.123]</a>",,,DBLP
SVD Mark: A Novel Black-Box Watermarking for Protecting Intellectual Property of Deep Neural Network Model,"Haojie Lv, Shuyuan Shen, ... Delin Duan",Advances in Artificial Intelligence and Security,2022,"<a href=""Springer (2022) : SVD Mark: A Novel Black-Box Watermarking for Protecting Intellectual Property of Deep Neural Network Model"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-06764-8_31]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-06764-8_31]</a>","With the rapid development of deep learning technology, more and more researchers have paid attention to protecting the intellectual property rights...",,Springer
Saisiyat Is Where It Is At! Insights Into Backdoors And Debiasing Of Cross Lingual Transformers For Named Entity Recognition,"Ricardo A. Calix, J. J. Ben-Joseph, Nina Lopatina, Ryan Ashley, Mona Gogia, George Sieniawski, Andrea Brennen",IEEE Big Data,2022,"<a href=""DBLP (2022) : Saisiyat Is Where It Is At! Insights Into Backdoors And Debiasing Of Cross Lingual Transformers For Named Entity Recognition"" target=""_blank"">[https://doi.org/10.1109/BigData55660.2022.10020403]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/BigData55660.2022.10020403]</a>",,,DBLP
Sample-Specific Backdoor based Active Intellectual Property Protection for Deep Neural Networks,"Yinghao Wu, Mingfu Xue, Dujuan Gu, Yushu Zhang, Weiqiang Liu",AICAS,2022,"<a href=""DBLP (2022) : Sample-Specific Backdoor based Active Intellectual Property Protection for Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1109/AICAS54282.2022.9869927]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/AICAS54282.2022.9869927]</a>",,,DBLP
Securing Data from Active Attacks in IoT: An Extensive Study,"C. Silpa, G. Niranjana, K. Ramani","Proceedings of International Conference on Deep Learning, Computing and Intelligence",2022,"<a href=""Springer (2022) : Securing Data from Active Attacks in IoT: An Extensive Study"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5652-1_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5652-1_5]</a>",The Internet of Things (IoT) is one of the most important technologies aimed at improving the quality of human lives. IoT plays a promising role in...,,Springer
Security Issues and Defenses in Virtualization,"Rouaa Al Zoubi, Bayan Mahfood, Sohail Abbas",Proceedings of International Conference on Information Technology and Applications,2022,"<a href=""Springer (2022) : Security Issues and Defenses in Virtualization"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-7618-5_52]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-7618-5_52]</a>","Virtualization, the process of allowing efficient utilization of physical computer hardware, is the core of many new technologies. With this comes...",,Springer
"Security Issues in Self-organized Ad-Hoc Networks (MANET, VANET, and FANET): A Survey","Sihem Goumiri, Mohamed Amine Riahla, M’hamed Hamadouche",Artificial Intelligence and Its Applications,2022,"<a href=""Springer (2022) : Security Issues in Self-organized Ad-Hoc Networks (MANET, VANET, and FANET): A Survey"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-96311-8_29]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-96311-8_29]</a>","Self-organized AdHoc networks have become one of the most interested and studied domains, especially with the rapid development of communication...",,Springer
Security and Privacy Concerns for Healthcare Wearable Devices and Emerging Alternative Approaches,"Eleni Boumpa, Vasileios Tsoukas, ... Athanasios Kakarountas",Wireless Mobile Communication and Healthcare,2022,"<a href=""Springer (2022) : Security and Privacy Concerns for Healthcare Wearable Devices and Emerging Alternative Approaches"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-06368-8_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-06368-8_2]</a>",The wide use of wearable devices rises a lot of concerns about the privacy and security of personal data that are collected and stored by such...,,Springer
Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare,"Hyejun Jeong, Tai-Myoung Chung","Future Data and Security Engineering. Big Data, Security and Privacy, Smart City and Industry 4.0 Applications",2022,"<a href=""Springer (2022) : Security and Privacy Issues and Solutions in Federated Learning for Digital Healthcare"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-8069-5_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-8069-5_21]</a>",The advent of Federated Learning has enabled the creation of a high-performing model as if it had been trained on a considerable amount of data. A...,,Springer
Security in Industrial Control Systems Using Machine Learning Algorithms: An Overview,"Pallavi Arora, Baljeet Kaur, Marcio Andrey Teixeira",ICT Analysis and Applications,2022,"<a href=""Springer (2022) : Security in Industrial Control Systems Using Machine Learning Algorithms: An Overview"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5655-2_34]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5655-2_34]</a>","Since tiny pocket-sized individual gadgets (e.g., smartphones) together with big computing appliances or solutions (e.g., cloud computing or net...",,Springer
SegPGD: An Effective and Efficient Adversarial Attack for Evaluating and Boosting Segmentation Robustness,"Jindong Gu, Hengshuang Zhao, ... Philip H. S. Torr",Computer Vision – ECCV 2022,2022,"<a href=""Springer (2022) : SegPGD: An Effective and Efficient Adversarial Attack for Evaluating and Boosting Segmentation Robustness"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-19818-2_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-19818-2_18]</a>",Deep neural network-based image classifications are vulnerable to adversarial perturbations. The image classifications can be easily fooled by adding...,,Springer
SentMod: Hidden Backdoor Attack on Unstructured Textual Data,"Saquib Irtiza, Latifur Khan, Kevin W. Hamlen",BigDataSecurity/HPSC/IDS,2022,"<a href=""DBLP (2022) : SentMod: Hidden Backdoor Attack on Unstructured Textual Data"" target=""_blank"">[https://doi.org/10.1109/BigDataSecurityHPSCIDS54978.2022.00050]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/BigDataSecurityHPSCIDS54978.2022.00050]</a>",,,DBLP
SmartIDS: A Comparative Study of Intelligent Intrusion Detection Systems for Internet of Things,"Ghada Abdelmoumin, Danda B. Rawat","Proceedings of the Future Technologies Conference (FTC) 2021, Volume 1",2022,"<a href=""Springer (2022) : SmartIDS: A Comparative Study of Intelligent Intrusion Detection Systems for Internet of Things"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-89906-6_28]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-89906-6_28]</a>","Traditional intrusion detection systems (IDSs) are not scalable and efficient in detecting intrusions in IoT systems, hence, protecting them against...",,Springer
SoK: Decentralized Randomness Beacon Protocols,"Mayank Raikwar, Danilo Gligoroski",Information Security and Privacy,2022,"<a href=""Springer (2022) : SoK: Decentralized Randomness Beacon Protocols"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22301-3_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22301-3_21]</a>",The scientific interest in the area of Decentralized Randomness Beacon (DRB) protocols has been thriving recently. Partially that interest is due to...,,Springer
Statistical Analysis-Based Intrusion Detection for Software Defined Network,"Talha Naqash, M. Hassan Tanveer, ... Muhammad Salman",Smart Trends in Computing and Communications,2022,"<a href=""Springer (2022) : Statistical Analysis-Based Intrusion Detection for Software Defined Network"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-4016-2_27]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-4016-2_27]</a>","Software-defined network (SDN) consists of two layers, control and data layer that makes SDN more flexible and scalable. Open flow protocol used for...",,Springer
Stealthy Backdoor Attack with Adversarial Training,"Le Feng, Sheng Li, Zhenxing Qian, Xinpeng Zhang",ICASSP,2022,"<a href=""DBLP (2022) : Stealthy Backdoor Attack with Adversarial Training"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9746008]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9746008]</a>",,,DBLP
Steganography-Free Zero-Knowledge,"Behzad Abdolmaleki, Nils Fleischhacker, ... Giulio Malavolta",Theory of Cryptography,2022,"<a href=""Springer (2022) : Steganography-Free Zero-Knowledge"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22318-1_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-22318-1_6]</a>",We revisit the well-studied problem of preventing steganographic communication in multi-party communications. While this is known to be a provably...,,Springer
Substitution Attacks Against Sigma Protocols,"Yuliang Lin, Rongmao Chen, ... Lin Liu",Cyberspace Safety and Security,2022,"<a href=""Springer (2022) : Substitution Attacks Against Sigma Protocols"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18067-5_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18067-5_14]</a>","Inspired by the Snowden revelations, Bellare, Paterson, and Rogaway proposed the notion of Algorithm Substitution Attack (ASA) where the attacker...",,Springer
Suppressing Poisoning Attacks on Federated Learning for Medical Imaging,"Naif Alkhunaizi, Dmitry Kamzolov, ... Karthik Nandakumar",Medical Image Computing and Computer Assisted Intervention – MICCAI 2022,2022,"<a href=""Springer (2022) : Suppressing Poisoning Attacks on Federated Learning for Medical Imaging"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-16452-1_64]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-16452-1_64]</a>","Collaboration among multiple data-owning entities (e.g., hospitals) can accelerate the training process and yield better machine learning models due...",,Springer
Survey on Recent Malware Detection Techniques for IoT,"Sangeeta Kakati, Debasish Chouhan, ... Subir Panja",Pattern Recognition and Data Analysis with Applications,2022,"<a href=""Springer (2022) : Survey on Recent Malware Detection Techniques for IoT"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-1520-8_53]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-1520-8_53]</a>","Malware has been growing at a rapid rate in recent times, and studying detection techniques has become a vital step. There are a varieties of malware...",,Springer
"Susceptibility &amp, defense of satellite image-trained convolutional networks to backdoor attacks","Ethan Brewer, Jason Lin, Daniel S. Miller Runfola",Inf. Sci.,2022,"<a href=""DBLP (2022) : Susceptibility &amp, defense of satellite image-trained convolutional networks to backdoor attacks"" target=""_blank"">[https://doi.org/10.1016/j.ins.2022.05.004]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.ins.2022.05.004]</a>",,,DBLP
TextBack: Watermarking Text Classifiers using Backdooring,"Nandish Chattopadhyay, Rajan Kataria, Anupam Chattopadhyay",DSD,2022,"<a href=""DBLP (2022) : TextBack: Watermarking Text Classifiers using Backdooring"" target=""_blank"">[https://doi.org/10.1109/DSD57027.2022.00053]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/DSD57027.2022.00053]</a>",,,DBLP
Toward Cleansing Backdoored Neural Networks in Federated Learning,"Chen Wu, Xian Yang, Sencun Zhu, Prasenjit Mitra",ICDCS,2022,"<a href=""DBLP (2022) : Toward Cleansing Backdoored Neural Networks in Federated Learning"" target=""_blank"">[https://doi.org/10.1109/ICDCS54860.2022.00084]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICDCS54860.2022.00084]</a>",,,DBLP
Towards Effective and Robust Neural Trojan Defenses via Input Filtering,"Kien Do, Haripriya Harikumar, ... Svetha Venkatesh",Computer Vision – ECCV 2022,2022,"<a href=""Springer (2022) : Towards Effective and Robust Neural Trojan Defenses via Input Filtering"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-20065-6_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-20065-6_17]</a>","Trojan attacks on deep neural networks are both dangerous and surreptitious. Over the past few years, Trojan attacks have advanced from using only a...",,Springer
Towards Optimizing Malware Detection: An Approach Based on Generative Adversarial Networks and Transformers,"Ayyub Alzahem, Wadii Boulila, ... Iman Almomani",Computational Collective Intelligence,2022,"<a href=""Springer (2022) : Towards Optimizing Malware Detection: An Approach Based on Generative Adversarial Networks and Transformers"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-16014-1_47]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-16014-1_47]</a>","Nowadays, cybercriminals are carrying out many forms of cyberattacks. Malware attacks, in particular, have emerged as one of the most challenging...",,Springer
Towards a Webshell Detection Approach Using Rule-Based and Deep HTTP Traffic Analysis,"Ha V. Le, Hoang V. Vo, ... Hung T. Du",Computational Collective Intelligence,2022,"<a href=""Springer (2022) : Towards a Webshell Detection Approach Using Rule-Based and Deep HTTP Traffic Analysis"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-16014-1_45]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-16014-1_45]</a>",Web applications are highly vulnerable to injecting malicious code (webshell) attacks. The static analysis is considered the best method to detect...,,Springer
Tractable Abstract Argumentation via Backdoor-Treewidth,"Wolfgang Dvorák, Markus Hecher, Matthias König, André Schidler, Stefan Szeider, Stefan Woltran",AAAI,2022,"<a href=""DBLP (2022) : Tractable Abstract Argumentation via Backdoor-Treewidth"" target=""_blank"">[https://doi.org/10.1609/aaai.v36i5.20501]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v36i5.20501]</a>",,,DBLP
Triggerability of Backdoor Attacks in Multi-Source Transfer Learning-based Intrusion Detection,"Nour Alhussien, Ahmed Aleroud, Reza Rahaeimehr, Alexander Schwarzmann",BDCAT,2022,"<a href=""DBLP (2022) : Triggerability of Backdoor Attacks in Multi-Source Transfer Learning-based Intrusion Detection"" target=""_blank"">[https://doi.org/10.1109/BDCAT56447.2022.00013]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/BDCAT56447.2022.00013]</a>",,,DBLP
TrojanFlow: A Neural Backdoor Attack to Deep Learning-based Network Traffic Classifiers,"Rui Ning, Chunsheng Xin, Hongyi Wu",INFOCOM,2022,"<a href=""DBLP (2022) : TrojanFlow: A Neural Backdoor Attack to Deep Learning-based Network Traffic Classifiers"" target=""_blank"">[https://doi.org/10.1109/INFOCOM48880.2022.9796878]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/INFOCOM48880.2022.9796878]</a>",,,DBLP
Un-Fair Trojan: Targeted Backdoor Attacks Against Model Fairness,"Nicholas Furth, Abdallah Khreishah, Guanxiong Liu, NhatHai Phan, Yaser Jararweh",SDS,2022,"<a href=""DBLP (2022) : Un-Fair Trojan: Targeted Backdoor Attacks Against Model Fairness"" target=""_blank"">[https://doi.org/10.1109/SDS57574.2022.10062890]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SDS57574.2022.10062890]</a>",,,DBLP
Understanding the Security of Deepfake Detection,"Xiaoyu Cao, Neil Zhenqiang Gong",Digital Forensics and Cyber Crime,2022,"<a href=""Springer (2022) : Understanding the Security of Deepfake Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-06365-7_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-06365-7_22]</a>","Deepfakes pose growing challenges to the trust of information on the Internet. Thus, detecting deepfakes has attracted increasing attentions from...",,Springer
Unlabeled Backdoor Poisoning in Semi-Supervised Learning,"Le Feng, Sheng Li, Zhenxing Qian, Xinpeng Zhang",ICME,2022,"<a href=""DBLP (2022) : Unlabeled Backdoor Poisoning in Semi-Supervised Learning"" target=""_blank"">[https://doi.org/10.1109/ICME52920.2022.9859941]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICME52920.2022.9859941]</a>",,,DBLP
VPN: Verification of Poisoning in Neural Networks,"Youcheng Sun, Muhammad Usman, ... Corina S. Păsăreanu",Software Verification and Formal Methods for ML-Enabled Autonomous Systems,2022,"<a href=""Springer (2022) : VPN: Verification of Poisoning in Neural Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-21222-2_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-21222-2_1]</a>","Neural networks are successfully used in a variety of applications, many of them having safety and security concerns. As a result researchers have...",,Springer
Verifiable Isogeny Walks: Towards an Isogeny-Based Postquantum VDF,"Jorge Chavez-Saab, Francisco Rodríguez-Henríquez, Mehdi Tibouchi",Selected Areas in Cryptography,2022,"<a href=""Springer (2022) : Verifiable Isogeny Walks: Towards an Isogeny-Based Postquantum VDF"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-99277-4_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-99277-4_21]</a>","In this paper, we investigate the problem of constructing postquantum-secure verifiable delay functions (VDFs), particularly based on supersingular...",,Springer
Voting Classifier-Based Intrusion Detection for IoT Networks,"Muhammad Almas Khan, Muazzam A. Khan Khattk, ... Jawad Ahmad",Advances on Smart and Soft Computing,2022,"<a href=""Springer (2022) : Voting Classifier-Based Intrusion Detection for IoT Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5559-3_26]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-5559-3_26]</a>",Internet of Things (IoT) is transforming human lives by paving the way for the management of physical devices on the edge. These interconnected IoT...,,Springer
Vulnerability Analysis and Robust Training with Additive Noise for FGSM Attack on Transfer Learning-Based Brain Tumor Detection from MRI,"Debashis Gupta, Biprodip Pal","Proceedings of the International Conference on Big Data, IoT, and Machine Learning",2022,"<a href=""Springer (2022) : Vulnerability Analysis and Robust Training with Additive Noise for FGSM Attack on Transfer Learning-Based Brain Tumor Detection from MRI"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-6636-0_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-6636-0_9]</a>",Deep learning-based high-precision computerized brain tumor diagnosis helps to obtain significant clinical features for proper treatment. Research...,,Springer
Web Attack Detection Using Machine Learning,"Ruturaj Malavade, Harshali Upadhye, ... Jagannath Aghav","Data, Engineering and Applications",2022,"<a href=""Springer (2022) : Web Attack Detection Using Machine Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-4687-5_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-19-4687-5_22]</a>","Network security is critical in the new age of the Internet. Web attacks tend to target sites such as e-commerce sites, social media, and email...",,Springer
When Does Backdoor Attack Succeed in Image Reconstruction? A Study of Heuristics vs. Bi-Level Solution,"Vardaan Taneja, Pin-Yu Chen, Yuguang Yao, Sijia Liu",ICASSP,2022,"<a href=""DBLP (2022) : When Does Backdoor Attack Succeed in Image Reconstruction? A Study of Heuristics vs. Bi-Level Solution"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9746433]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9746433]</a>",,,DBLP
Working Mechanism of Eternalblue and Its Application in Ransomworm,"Zian Liu, Chao Chen, ... Shang Gao",Cyberspace Safety and Security,2022,"<a href=""Springer (2022) : Working Mechanism of Eternalblue and Its Application in Ransomworm"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18067-5_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-031-18067-5_13]</a>","After the leaking of exploit Eternalblue, some ransomworms utilizing this exploit have been developed to sweep over the world in recent years....",,Springer
Structural Watermarking to Deep Neural Networks via Network Channel Pruning,X. Zhao Y. Yao H. Wu X. Zhang,2021 IEEE International Workshop on Information Forensics and Security (WIFS),2021-12-24,"<a href=""IEEE (2021-12-24) : Structural Watermarking to Deep Neural Networks via Network Channel Pruning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9648376]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/WIFS53200.2021.9648376]</a>","In order to protect the intellectual property (IP) of deep neural networks (DNNs), many existing DNN watermarking techniques either embed watermarks directly into the DNN parameters or insert backdoor watermarks by fine-tuning the DNN parameters, which, however, cannot resist against various attack methods that remove watermarks by altering DNN parameters. In this paper, we bypass such attacks by introducing a structural watermarking scheme that utilizes channel pruning to embed the watermark into the host DNN architecture instead of crafting the DNN parameters. To be specific, during watermark embedding, we prune the internal channels of the host DNN with the channel pruning rates controlled by the watermark. During watermark extraction, the watermark is retrieved by identifying the channel pruning rates from the architecture of the target DNN model. Due to the superiority of pruning mechanism, the performance of the DNN model on its original task is reserved during watermark embedding. Experimental results have shown that, the proposed work enables the embedded watermark to be reliably recovered and provides a sufficient payload, without sacrificing the usability of the DNN model. It is also demonstrated that the proposed work is robust against common transforms and attacks designed for conventional watermarking approaches.",,IEEE
The Dynamic Nature of Insider Threat Indicators,"Frank L. Greitzer, Justin Purl",SN Computer Science,2021-12-22,"<a href=""Springer (2021-12-22) : The Dynamic Nature of Insider Threat Indicators"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-021-00990-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-021-00990-1]</a>",Insider threat indicators are not equally indicative of potential insider threat activity. Indicator risk assessments depend not only on the number...,,Springer
Fruit-classification model resilience under adversarial attack,Raheel Siddiqi,SN Applied Sciences,2021-12-21,"<a href=""Springer (2021-12-21) : Fruit-classification model resilience under adversarial attack"" target=""_blank"">[https://link.springer.com/article/10.1007/s42452-021-04917-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42452-021-04917-6]</a>","An accurate and robust fruit image classifier can have a variety of real-life and industrial applications including automated pricing, intelligent...",,Springer
Challenges and future directions of secure federated learning: a survey,"Kaiyue Zhang, Xuan Song, ... Shui Yu",Frontiers of Computer Science,2021-12-10,"<a href=""Springer (2021-12-10) : Challenges and future directions of secure federated learning: a survey"" target=""_blank"">[https://link.springer.com/article/10.1007/s11704-021-0598-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11704-021-0598-z]</a>","Federated learning came into being with the increasing concern of privacy security, as people’s sensitive information is being exposed under the era...",,Springer
Are You a Good Client? Client Classification in Federated Learning,H. Jeong J. An J. Jeong,2021 International Conference on Information and Communication Technology Convergence (ICTC),2021-12-07,"<a href=""IEEE (2021-12-07) : Are You a Good Client? Client Classification in Federated Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9620836]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICTC52510.2021.9620836]</a>","Federated Learning (FL) is a distributed machine learning framework, where any raw data do not leave the participating clients' machines aiming for privacy preservation. Due to its distributed nature, federated learning is especially vulnerable to data poisoning attacks which degrade overall performance of the framework. Hence there is an arising need of early identification and removal of malicious clients. However, correctly identifying malicious clients is difficult. Clients with non-IID (Independently and Identically Distributed) data and those with malicious data, for example, are hardly distinguishable due to the dissimilar distribution of non-IID data and normal data. Prior works focus on improving the performance with either non-IID data or malicious data, but not both. On the other hand, this paper proposes a mechanism that identifies and classifies three types of clients: clients having IID, non-IID, and malicious data. Our findings can help future studies to remove malicious clients efficiently while training a model with diverse data.",,IEEE
Test-Time Detection of Backdoor Triggers for Poisoned Deep Neural Networks,"Xi Li, Zhen Xiang, David J. Miller, George Kesidis","arXiv
ICASSP
arXiv","2021-12-06
2022
2021-12","<a href=""arXiv (2021-12-06) : Test-Time Detection of Backdoor Triggers for Poisoned Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2112.03350v1]</a>
<a href=""DBLP (2022) : Test-Time Detection of Backdoor Triggers for Poisoned Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9746573]</a>
<a href=""DBLP (2021-12) : Test-Time Detection of Backdoor Triggers for Poisoned Deep Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2112.03350]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICASSP43922.2022.9746573]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2112.03350]</a>","Backdoor (Trojan) attacks are emerging threats against deep neural networks (DNN). A DNN being attacked will predict to an attacker-desired target class whenever a test sample from any source class is embedded with a backdoor pattern, while correctly classifying clean (attack-free) test samples. Existing backdoor defenses have shown success in detecting whether a DNN is attacked and in reverse-engineering the backdoor pattern in a ""post-training"" regime: the defender has access to the DNN to be inspected and a small, clean dataset collected independently, but has no access to the (possibly poisoned) training set of the DNN. However, these defenses neither catch culprits in the act of triggering the backdoor mapping, nor mitigate the backdoor attack at test-time. In this paper, we propose an ""in-flight"" defense against backdoor attacks on image classification that 1) detects use of a backdoor trigger at test-time, and 2) infers the class of origin (source class) for a detected trigger example. The effectiveness of our defense is demonstrated experimentally against different strong backdoor attacks.

","

","arXiv
DBLP
DBLP"
Robust Federated Learning with Adaptable Learning Rate,J. Zhou Z. Zhong J. Wang X. Zhang F. Chen C. Yan,2021 7th International Conference on Big Data and Information Analytics (BigDIA),2021-12-06,"<a href=""IEEE (2021-12-06) : Robust Federated Learning with Adaptable Learning Rate"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9619731]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/BigDIA53151.2021.9619731]</a>","Federated learning(FL), one of the most popular distributed machine learning algorithms, allows participants to collaborate on updating models without sharing local data. At the same time, FL is vulnerable to malicious attacks due to the secrecy of the data. Backdoor attacks, as one of many malicious attacks, attempt to add a backdoor to the global model, which will lead to misclassification after activation. Backdoor attacks are difficult to detect. To defend against backdoor attacks, we designed a defense method deployed on the central server side. The method autonomously adjusts the learning rate based on the received parameter signs form all participants to reduce the impact of malicious updates. It is shown experimentally that the method can significantly reduce the impact of backdoor attacks while maintaining high model accuracy on clean datasets when the correct threshold is set.",,IEEE
Efficient Key-Gate Placement and Dynamic Scan Obfuscation Towards Robust Logic Encryption,R. Karmakar H. Kumar S. Chattopadhyay,IEEE Transactions on Emerging Topics in Computing,2021-12-03,"<a href=""IEEE (2021-12-03) : Efficient Key-Gate Placement and Dynamic Scan Obfuscation Towards Robust Logic Encryption"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946748]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TETC.2019.2963094]</a>","Logic encryption has emerged to be a potential solution to the problem of Intellectual Property (IP)-Piracy and counterfeiting. However, in the recent past, several attacks have been mounted on existing logic encryption strategies to extract the secret key. SAT attack, the most predominant one among them, exploits the unprotected Design-for-Testability (DfT) infrastructure as a backdoor to launch attacks on sequential circuits. Protecting the DfT infrastructure is of paramount importance to ensure the security of an Integrated Chip (IC). In this paper, we propose a new logic encryption scheme which dynamically obfuscates the scan operation for an unauthorized attempt of scan access. A detailed security analysis on the proposed secure DfT infrastructure demonstrates its ability to thwart SAT attack without compromising the testability of the design. A methodical key-gate placement strategy enables the proposed scheme to eliminate the leakage of key information through weak key-gate locations, offering protection against path sensitization and logic cone based attacks. Unlike other state-of-the-art SAT preventive schemes, our proposed method does not suffer from poor output corruption, which is a fundamental requirement of a logic encryption scheme.",,IEEE
Certified Robustness of Nearest Neighbors against Data Poisoning and Backdoor Attacks,"Jinyuan Jia, Yupei Liu, Xiaoyu Cao, Neil Zhenqiang Gong","arXiv
AAAI","2021-12-02
2022","<a href=""arXiv (2021-12-02) : Certified Robustness of Nearest Neighbors against Data Poisoning and Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2012.03765v3]</a>
<a href=""DBLP (2022) : Certified Robustness of Nearest Neighbors against Data Poisoning and Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1609/aaai.v36i9.21191]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v36i9.21191]</a>","Data poisoning attacks and backdoor attacks aim to corrupt a machine learning classifier via modifying, adding, and/or removing some carefully selected training examples, such that the corrupted classifier makes incorrect predictions as the attacker desires. The key idea of state-of-the-art certified defenses against data poisoning attacks and backdoor attacks is to create a majority vote mechanism to predict the label of a testing example. Moreover, each voter is a base classifier trained on a subset of the training dataset. Classical simple learning algorithms such as k nearest neighbors (kNN) and radius nearest neighbors (rNN) have intrinsic majority vote mechanisms. In this work, we show that the intrinsic majority vote mechanisms in kNN and rNN already provide certified robustness guarantees against data poisoning attacks and backdoor attacks. Moreover, our evaluation results on MNIST and CIFAR10 show that the intrinsic certified robustness guarantees of kNN and rNN outperform those provided by state-of-the-art certified defenses. Our results serve as standard baselines for future certified defenses against data poisoning attacks and backdoor attacks.
","
","arXiv
DBLP"
A study on robustness of malware detection model,"Wanjia Zheng, Kazumasa Omote",Annals of Telecommunications,2021-12-02,"<a href=""Springer (2021-12-02) : A study on robustness of malware detection model"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-021-00899-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-021-00899-z]</a>","In recent years, machine learning–based techniques are used to prevent cyberattacks caused by malware, and special attention is paid to the risks...",,Springer
Anti-Backdoor Learning: Training Clean Models on Poisoned Data,"Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma","arXiv
NeurIPS
arXiv","2021-12-01
2021
2021-10","<a href=""arXiv (2021-12-01) : Anti-Backdoor Learning: Training Clean Models on Poisoned Data"" target=""_blank"">[http://arxiv.org/abs/2110.11571v3]</a>
<a href=""DBLP (2021) : Anti-Backdoor Learning: Training Clean Models on Poisoned Data"" target=""_blank"">[https://proceedings.neurips.cc/paper/2021/hash/7d38b1e9bd793d3f45e0e212a729a93c-Abstract.html]</a>
<a href=""DBLP (2021-10) : Anti-Backdoor Learning: Training Clean Models on Poisoned Data"" target=""_blank"">[https://arxiv.org/abs/2110.11571]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://proceedings.neurips.cc/paper/2021/hash/7d38b1e9bd793d3f45e0e212a729a93c-Abstract.html]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2110.11571]</a>","Backdoor attack has emerged as a major security threat to deep neural networks (DNNs). While existing defense methods have demonstrated promising results on detecting or erasing backdoors, it is still not clear whether robust training methods can be devised to prevent the backdoor triggers being injected into the trained model in the first place. In this paper, we introduce the concept of \emph{anti-backdoor learning}, aiming to train \emph{clean} models given backdoor-poisoned data. We frame the overall learning process as a dual-task of learning the \emph{clean} and the \emph{backdoor} portions of data. From this view, we identify two inherent characteristics of backdoor attacks as their weaknesses: 1) the models learn backdoored data much faster than learning with clean data, and the stronger the attack the faster the model converges on backdoored data, 2) the backdoor task is tied to a specific class (the backdoor target class). Based on these two weaknesses, we propose a general learning scheme, Anti-Backdoor Learning (ABL), to automatically prevent backdoor attacks during training. ABL introduces a two-stage \emph{gradient ascent} mechanism for standard training to 1) help isolate backdoor examples at an early training stage, and 2) break the correlation between backdoor examples and the target class at a later training stage. Through extensive experiments on multiple benchmark datasets against 10 state-of-the-art attacks, we empirically show that ABL-trained models on backdoor-poisoned data achieve the same performance as they were trained on purely clean data. Code is available at \url{https://github.com/bboylyg/ABL}.

","<a href=""arXiv"" target=""_blank"">[https://github.com/bboylyg/ABL}]</a>

","arXiv
DBLP
DBLP"
Adversarial Attacks on Intrusion Detection Systems Using the LSTM Classifier,"D. A. Kulikov, V. V. Platonov",Automatic Control and Computer Sciences,2021-12-01,"<a href=""Springer (2021-12-01) : Adversarial Attacks on Intrusion Detection Systems Using the LSTM Classifier"" target=""_blank"">[https://link.springer.com/article/10.3103/S0146411621080174]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.3103/S0146411621080174]</a>","Abstract In this paper, adversarial attacks on machine learning models and their classification are considered. Methods for assessing the resistance...",,Springer
Bringing disgust in through the backdoor in healthy food promotion: a phenomenological perspective,de Boer B.,"Medicine, Health Care and Philosophy",2021-12-01,"<a href=""ScienceDirect (2021-12-01) : Bringing disgust in through the backdoor in healthy food promotion: a phenomenological perspective"" target=""_blank"">[https://doi.org/10.1007/s11019-021-10037-0]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/s11019-021-10037-0]</a>",,,ScienceDirect
Detecting Anomalies in Cyber-Physical Systems Using Graph Neural Networks,"K. V. Vasil’eva, D. S. Lavrova",Automatic Control and Computer Sciences,2021-12-01,"<a href=""Springer (2021-12-01) : Detecting Anomalies in Cyber-Physical Systems Using Graph Neural Networks"" target=""_blank"">[https://link.springer.com/article/10.3103/S0146411621080320]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.3103/S0146411621080320]</a>",Abstract Application of convolutional graph neural networks for detecting anomalies in cyber-physical systems is proposed. The graph model reflecting...,,Springer
On the Detection of Exploitation of Vulnerabilities That Leads to the Execution of a Malicious Code,Y. V. Kosolapov,Automatic Control and Computer Sciences,2021-12-01,"<a href=""Springer (2021-12-01) : On the Detection of Exploitation of Vulnerabilities That Leads to the Execution of a Malicious Code"" target=""_blank"">[https://link.springer.com/article/10.3103/S0146411621070233]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.3103/S0146411621070233]</a>","Abstract Software protection from exploitation of possible unknown vulnerabilities can be ensured both by searching for (for example, using symbolic...",,Springer
Using the Neat-Hypercube Mechanism to Detect Cyber Attacks on IoT Systems,"A. D. Fatin, E. Yu. Pavlenko",Automatic Control and Computer Sciences,2021-12-01,"<a href=""Springer (2021-12-01) : Using the Neat-Hypercube Mechanism to Detect Cyber Attacks on IoT Systems"" target=""_blank"">[https://link.springer.com/article/10.3103/S0146411621080101]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.3103/S0146411621080101]</a>","Abstract — This paper considers a method for detecting abnormal behavior in cyber-physical systems, Internet of Things (IoT) systems, and distributed...",,Springer
BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements,"Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, Yang Zhang","ACSAC '21: Proceedings of the 37th Annual Computer Security Applications Conference
arXiv
ACSAC","2021-12
2021-10-04
2021","<a href=""ACM (2021-12) : BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3485832.3485837]</a>
<a href=""arXiv (2021-10-04) : BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements"" target=""_blank"">[http://arxiv.org/abs/2006.01043v2]</a>
<a href=""DBLP (2021) : BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements"" target=""_blank"">[https://doi.org/10.1145/3485832.3485837]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3485832.3485837]</a>
<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1145/3485832.3485837]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3485832.3485837]</a>","Deep neural networks (DNNs) have progressed rapidly during the past decade and have been deployed in various real-world applications. Meanwhile, DNN models have been shown to be vulnerable to security and privacy attacks. One such attack that has ...
Deep neural networks (DNNs) have progressed rapidly during the past decade and have been deployed in various real-world applications. Meanwhile, DNN models have been shown to be vulnerable to security and privacy attacks. One such attack that has attracted a great deal of attention recently is the backdoor attack. Specifically, the adversary poisons the target model's training set to mislead any input with an added secret trigger to a target class. Previous backdoor attacks predominantly focus on computer vision (CV) applications, such as image classification. In this paper, we perform a systematic investigation of backdoor attack on NLP models, and propose BadNL, a general NLP backdoor attack framework including novel attack methods. Specifically, we propose three methods to construct triggers, namely BadChar, BadWord, and BadSentence, including basic and semantic-preserving variants. Our attacks achieve an almost perfect attack success rate with a negligible effect on the original model's utility. For instance, using the BadChar, our backdoor attack achieves a 98.9% attack success rate with yielding a utility improvement of 1.5% on the SST-5 dataset when only poisoning 3% of the original set. Moreover, we conduct a user study to prove that our triggers can well preserve the semantics from humans perspective.
","

","ACM
arXiv
DBLP"
A Survey on Feature Extraction Methods of Heuristic Backdoor Detection,"ZhiMin Guo, WeiJian Zhang, Wen Yang, Xin Che, Zheng Zhang, MingYang Li","ICFEICT 2021: International Conference on Frontiers of Electronics, Information and Computation Technologies",2021-12,"<a href=""ACM (2021-12) : A Survey on Feature Extraction Methods of Heuristic Backdoor Detection"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3474198.3478137]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3474198.3478137]</a>","In the age of the Internet, while the network is sending a lot of information to people, hackers also transmit a lot of malicious code through the network. Hackers use these malicious codes to steal sensitive information from infected people and damage ...",,ACM
Defending Label Inference and Backdoor Attacks in Vertical Federated Learning,"Yang Liu, Zhihao Yi, Yan Kang, Yuanqin He, Wenhan Liu, Tianyuan Zou, Qiang Yang",arXiv,2021-12,"<a href=""DBLP (2021-12) : Defending Label Inference and Backdoor Attacks in Vertical Federated Learning"" target=""_blank"">[https://arxiv.org/abs/2112.05409]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2112.05409]</a>",,,DBLP
Backdoor Attack through Frequency Domain,"Tong Wang, Yuan Yao, Feng Xu, Shengwei An, Hanghang Tong, Ting Wang","arXiv
arXiv","2021-11-30
2021-11","<a href=""arXiv (2021-11-30) : Backdoor Attack through Frequency Domain"" target=""_blank"">[http://arxiv.org/abs/2111.10991v2]</a>
<a href=""DBLP (2021-11) : Backdoor Attack through Frequency Domain"" target=""_blank"">[https://arxiv.org/abs/2111.10991]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2111.10991]</a>","Backdoor attacks have been shown to be a serious threat against deep learning systems such as biometric authentication and autonomous driving. An effective backdoor attack could enforce the model misbehave under certain predefined conditions, i.e., triggers, but behave normally otherwise. However, the triggers of existing attacks are directly injected in the pixel space, which tend to be detectable by existing defenses and visually identifiable at both training and inference stages. In this paper, we propose a new backdoor attack FTROJAN through trojaning the frequency domain. The key intuition is that triggering perturbations in the frequency domain correspond to small pixel-wise perturbations dispersed across the entire image, breaking the underlying assumptions of existing defenses and making the poisoning images visually indistinguishable from clean ones. We evaluate FTROJAN in several datasets and tasks showing that it achieves a high attack success rate without significantly degrading the prediction accuracy on benign inputs. Moreover, the poisoning images are nearly invisible and retain high perceptual quality. We also evaluate FTROJAN against state-of-the-art defenses as well as several adaptive defenses that are designed on the frequency domain. The results show that FTROJAN can robustly elude or significantly degenerate the performance of these defenses.
","
","arXiv
DBLP"
Anomaly Localization in Model Gradients Under Backdoor Attacks Against Federated Learning,Zeki Bilgin,"arXiv
arXiv","2021-11-29
2021-11","<a href=""arXiv (2021-11-29) : Anomaly Localization in Model Gradients Under Backdoor Attacks Against Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2111.14683v1]</a>
<a href=""DBLP (2021-11) : Anomaly Localization in Model Gradients Under Backdoor Attacks Against Federated Learning"" target=""_blank"">[https://arxiv.org/abs/2111.14683]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2111.14683]</a>","Inserting a backdoor into the joint model in federated learning (FL) is a recent threat raising concerns. Existing studies mostly focus on developing effective countermeasures against this threat, assuming that backdoored local models, if any, somehow reveal themselves by anomalies in their gradients. However, this assumption needs to be elaborated by identifying specifically which gradients are more likely to indicate an anomaly to what extent under which conditions. This is an important issue given that neural network models usually have huge parametric space and consist of a large number of weights. In this study, we make a deep gradient-level analysis on the expected variations in model gradients under several backdoor attack scenarios against FL. Our main novel finding is that backdoor-induced anomalies in local model updates (weights or gradients) appear in the final layer bias weights of the malicious local models. We support and validate our findings by both theoretical and experimental analysis in various FL settings. We also investigate the impact of the number of malicious clients, learning rate, and malicious data rate on the observed anomaly. Our implementation is publicly available\footnote{\url{ https://github.com/ArcelikAcikKaynak/Federated_Learning.git}}.
","<a href=""arXiv"" target=""_blank"">[https://github.com/ArcelikAcikKaynak/Federated_Learning.git}}]</a>
","arXiv
DBLP"
Big knowledge-based semantic correlation for detecting slow and low-level advanced persistent threats,"Amir Mohammadzade Lajevardi, Morteza Amini",Journal of Big Data,2021-11-27,"<a href=""Springer (2021-11-27) : Big knowledge-based semantic correlation for detecting slow and low-level advanced persistent threats"" target=""_blank"">[https://link.springer.com/article/10.1186/s40537-021-00532-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s40537-021-00532-9]</a>","Targeted cyber attacks, which today are known as Advanced Persistent Threats (APTs), use low and slow patterns to bypass intrusion detection and...",,Springer
Embedding and extraction of knowledge in tree ensemble classifiers,"Wei Huang, Xingyu Zhao, Xiaowei Huang",Machine Learning,2021-11-24,"<a href=""Springer (2021-11-24) : Embedding and extraction of knowledge in tree ensemble classifiers"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-021-06068-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-021-06068-6]</a>","The embedding and extraction of knowledge is a recent trend in machine learning applications, e.g., to supplement training datasets that are small....",,Springer
Stronger data poisoning attacks break data sanitization defenses,"Pang Wei Koh, Jacob Steinhardt, Percy Liang",Machine Learning,2021-11-24,"<a href=""Springer (2021-11-24) : Stronger data poisoning attacks break data sanitization defenses"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-021-06119-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10994-021-06119-y]</a>",Machine learning models trained on data from the outside world can be corrupted by data poisoning attacks that inject malicious points into the...,,Springer
Trojan Signatures in DNN Weights,G. Fields M. Samragh M. Javaheripi F. Koushanfar T. Javidi,2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW),2021-11-24,"<a href=""IEEE (2021-11-24) : Trojan Signatures in DNN Weights"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607407]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICCVW54120.2021.00008]</a>","Deep neural networks have been shown to be vulnerable to backdoor, or trojan, attacks where an adversary has embedded a trigger in the network at training time such that the model correctly classifies all standard inputs, but generates a targeted, incorrect classification on any input which contains the trigger. In this paper, we present the first ultra light-weight and highly effective trojan detection method that does not require access to the training/test data, does not involve any expensive computations, and makes no assumptions on the nature of the trojan trigger. Our approach focuses on analysis of the weights of the final, linear layer of the network. We empirically demonstrate several characteristics of these weights that occur frequently in trojaned networks, but not in benign networks. In particular, we show that the distribution of the weights associated with the trojan target class is clearly distinguishable from the weights associated with other classes. Using this, we demonstrate the effectiveness of our proposed detection method against state-of-the-art attacks across a variety of architectures, datasets, and trigger types.",,IEEE
User Perception of Data Breaches,Z. Hassanzadeh R. Biddle S. Marsen,IEEE Transactions on Professional Communication,2021-11-24,"<a href=""IEEE (2021-11-24) : User Perception of Data Breaches"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9580451]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TPC.2021.3110545]</a>","Background: Data breaches happen when an unauthorized party gains access to personally identifiable information. They are becoming more common and impactful, raising serious concerns for individuals as well as companies. Literature review: Although there is considerable literature on users’ mental models in security and privacy, there has been limited study of mental models related to data breaches. Research questions: 1. How do users understand data breaches? 2. What are their perceptions of the causes, responsibilities, and consequences, as well as possible prevention and appropriate follow up? Methodology: We explored end-user understanding of internet data breaches by conducting a study with 35 participants. They were asked to draw their understanding of data breaches and answer some open-ended and closed-ended questions afterwards. Results/discussion: Although their drawings varied in detail and complexity, we identified four patterns in the participants’ drawings: they illustrated abstractions of attacks to gain administrator access, end-user access, backdoor access, or access using database server vulnerabilities. We found that participants had a basic model of how an internet data breach happens, but with significant uncertainties regarding system vulnerabilities, causes, consequences, prevention methods, and follow-up steps after a breach. Conclusions: In all, end-user mental models of internet data breaches are basic and show gaps that emphasize the need for improved communication to increase users’ awareness and help them hold companies accountable.",,IEEE
DBIA: Data-free Backdoor Injection Attack against Transformer Networks,"Peizhuo Lv, Hualong Ma, Jiachen Zhou, Ruigang Liang, Kai Chen, Shengzhi Zhang, Yunfei Yang","arXiv
ICME
arXiv","2021-11-22
2023
2021-11","<a href=""arXiv (2021-11-22) : DBIA: Data-free Backdoor Injection Attack against Transformer Networks"" target=""_blank"">[http://arxiv.org/abs/2111.11870v1]</a>
<a href=""DBLP (2023) : DBIA: Data-Free Backdoor Attack Against Transformer Networks"" target=""_blank"">[https://doi.org/10.1109/ICME55011.2023.00479]</a>
<a href=""DBLP (2021-11) : DBIA: Data-free Backdoor Injection Attack against Transformer Networks"" target=""_blank"">[https://arxiv.org/abs/2111.11870]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICME55011.2023.00479]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2111.11870]</a>","Recently, transformer architecture has demonstrated its significance in both Natural Language Processing (NLP) and Computer Vision (CV) tasks. Though other network models are known to be vulnerable to the backdoor attack, which embeds triggers in the model and controls the model behavior when the triggers are presented, little is known whether such an attack is still valid on the transformer models and if so, whether it can be done in a more cost-efficient manner. In this paper, we propose DBIA, a novel data-free backdoor attack against the CV-oriented transformer networks, leveraging the inherent attention mechanism of transformers to generate triggers and injecting the backdoor using the poisoned surrogate dataset. We conducted extensive experiments based on three benchmark transformers, i.e., ViT, DeiT and Swin Transformer, on two mainstream image classification tasks, i.e., CIFAR10 and ImageNet. The evaluation results demonstrate that, consuming fewer resources, our approach can embed backdoors with a high success rate and a low impact on the performance of the victim transformers. Our code is available at https://anonymous.4open.science/r/DBIA-825D.

","

","arXiv
DBLP
DBLP"
NTD: Non-Transferability Enabled Backdoor Detection,"Yinshan Li, Hua Ma, Zhi Zhang, Yansong Gao, Alsharif Abuadbba, Anmin Fu, Yifeng Zheng, Said F. Al-Sarawi, Derek Abbott","arXiv
arXiv","2021-11-22
2021-11","<a href=""arXiv (2021-11-22) : NTD: Non-Transferability Enabled Backdoor Detection"" target=""_blank"">[http://arxiv.org/abs/2111.11157v1]</a>
<a href=""DBLP (2021-11) : NTD: Non-Transferability Enabled Backdoor Detection"" target=""_blank"">[https://arxiv.org/abs/2111.11157]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2111.11157]</a>","A backdoor deep learning (DL) model behaves normally upon clean inputs but misbehaves upon trigger inputs as the backdoor attacker desires, posing severe consequences to DL model deployments. State-of-the-art defenses are either limited to specific backdoor attacks (source-agnostic attacks) or non-user-friendly in that machine learning (ML) expertise or expensive computing resources are required. This work observes that all existing backdoor attacks have an inevitable intrinsic weakness, non-transferability, that is, a trigger input hijacks a backdoored model but cannot be effective to another model that has not been implanted with the same backdoor. With this key observation, we propose non-transferability enabled backdoor detection (NTD) to identify trigger inputs for a model-under-test (MUT) during run-time.Specifically, NTD allows a potentially backdoored MUT to predict a class for an input. In the meantime, NTD leverages a feature extractor (FE) to extract feature vectors for the input and a group of samples randomly picked from its predicted class, and then compares similarity between the input and the samples in the FE's latent space. If the similarity is low, the input is an adversarial trigger input, otherwise, benign. The FE is a free pre-trained model privately reserved from open platforms. As the FE and MUT are from different sources, the attacker is very unlikely to insert the same backdoor into both of them. Because of non-transferability, a trigger effect that does work on the MUT cannot be transferred to the FE, making NTD effective against different types of backdoor attacks. We evaluate NTD on three popular customized tasks such as face recognition, traffic sign recognition and general animal classification, results of which affirm that NDT has high effectiveness (low false acceptance rate) and usability (low false rejection rate) with low detection latency.
","
","arXiv
DBLP"
Detecting Malicious Gradients from Asynchronous SGD on Variational Autoencoder,Z. Gu Y. Yang H. Shi,2021 40th International Symposium on Reliable Distributed Systems (SRDS),2021-11-22,"<a href=""IEEE (2021-11-22) : Detecting Malicious Gradients from Asynchronous SGD on Variational Autoencoder"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9603557]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SRDS53918.2021.00039]</a>","In asynchronous distributed learning, the parameter server updates the global model as soon as a new gradient is received from any device. The asynchronous systems are designed to address the existence of lagging devices which is inevitable due to device heterogeneity and network unreliability. However, the lack of synchrony incurs additional noise and makes detecting and defending against malicious model gradients a challenging task. Unlike existing works that struggle to design robust methods to tolerate untargeted model poisoning gradients, the paper considers detecting and removing targeted model poisoning gradients from the normal asynchronous training process. This paper proposes Asynvae, a robust distributed asynchronous learning framework where the parameter server uses variational autoencoder to detect and exclude malicious gradients. Since the reconstruction error of malicious updates is much larger than that of benign ones, it can be used as an anomaly score. We formulate a threshold of reconstruction error to differentiate malicious updates from normal ones based on this idea. Asynvae is tested with extensive experiments on distributed learning benchmarks, showing a competitive performance over existing distributed learning methods under untargeted model poisoning attack, targeted model poisoning attack and lagging attack.",,IEEE
WAFFLE: Watermarking in Federated Learning,B. G. A. Tekgul Y. Xia S. Marchal N. Asokan,2021 40th International Symposium on Reliable Distributed Systems (SRDS),2021-11-22,"<a href=""IEEE (2021-11-22) : WAFFLE: Watermarking in Federated Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9603498]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SRDS53918.2021.00038]</a>","Federated learning is a distributed learning technique where machine learning models are trained on client devices in which the local training data resides. The training is coordinated via a central server which is, typically, controlled by the intended owner of the resulting model. By avoiding the need to transport the training data to the central server, federated learning improves privacy and efficiency. But it raises the risk of model theft by clients because the resulting model is available on every client device. Even if the application software used for local training may attempt to prevent direct access to the model, a malicious client may bypass any such restrictions by reverse engineering the application software. Watermarking is a well-known deterrence method against model theft by providing the means for model owners to demonstrate ownership of their models. Several recent deep neural network (DNN) watermarking techniques use backdooring: training the models with additional mislabeled data. Backdooring requires full access to the training data and control of the training process. This is feasible when a single party trains the model in a centralized manner, but not in a federated learning setting where the training process and training data are distributed among several client devices. In this paper, we present WAFFLE, the first approach to watermark DNN models trained using federated learning. It introduces a retraining step at the server after each aggregation of local models into the global model. We show that WAFFLE efficiently embeds a resilient watermark into models incurring only negligible degradation in test accuracy (-0.17%), and does not require access to training data. We also introduce a novel technique to generate the backdoor used as a watermark. It outperforms prior techniques, imposing no communication, and low computational (+3.2%) overhead<sup>1</sup><sup>1</sup>The research report version of this paper is also available in https://arxiv.org/abs/2008.07298, and the code for reproducing our work can be found at https://github.com/ssg-research/WAFFLE.","<a href=""IEEE"" target=""_blank"">[https://github.com/ssg-research/WAFFLE]</a>",IEEE
Explainable artificial intelligence: a comprehensive review,"Dang Minh, H. Xiang Wang, ... Tan N. Nguyen",Artificial Intelligence Review,2021-11-18,"<a href=""Springer (2021-11-18) : Explainable artificial intelligence: a comprehensive review"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-021-10088-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-021-10088-y]</a>","Thanks to the exponential growth in computing power and vast amounts of data, artificial intelligence (AI) has witnessed remarkable developments in...",,Springer
Work-in-Progress: A Physically Realizable Backdoor Attack on 3D Point Cloud Deep Learning,C. Bian W. Jiang J. Zhan Z. Song X. Wen H. Lei,2021 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS),2021-11-18,"<a href=""IEEE (2021-11-18) : Work-in-Progress: A Physically Realizable Backdoor Attack on 3D Point Cloud Deep Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9603381]</a>","<a href=""IEEE"" target=""_blank"">[]</a>","Modern autonomous driving has widely used deep learning to process point cloud data. This application is widely deployed on embedded edge computing devices and has high security requirements. We found that backdoor attacks can pose an extremely serious threat to point cloud deep learning systems, but this attack method has not been explored in point cloud deep learning tasks. In this paper, we propose a physically implementable backdoor attack method for the point cloud deep learning model. This method can achieve good performance in the attack effect and physical realization, evaluating by preliminary experiments.",,IEEE
A descriptive study of assumptions in STRIDE security threat modeling,"Dimitri Van Landuyt, Wouter Joosen",Software and Systems Modeling,2021-11-17,"<a href=""Springer (2021-11-17) : A descriptive study of assumptions in STRIDE security threat modeling"" target=""_blank"">[https://link.springer.com/article/10.1007/s10270-021-00941-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10270-021-00941-7]</a>","Security threat modeling involves the systematic elicitation of plausible threat scenarios, and leads to the identification and articulation of the...",,Springer
An Overview of Backdoor Attacks Against Deep Neural Networks and Possible Defences,"Wei Guo, Benedetta Tondi, Mauro Barni","arXiv
arXiv","2021-11-16
2021-11","<a href=""arXiv (2021-11-16) : An Overview of Backdoor Attacks Against Deep Neural Networks and Possible Defences"" target=""_blank"">[http://arxiv.org/abs/2111.08429v1]</a>
<a href=""DBLP (2021-11) : An Overview of Backdoor Attacks Against Deep Neural Networks and Possible Defences"" target=""_blank"">[https://arxiv.org/abs/2111.08429]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2111.08429]</a>","Together with impressive advances touching every aspect of our society, AI technology based on Deep Neural Networks (DNN) is bringing increasing security concerns. While attacks operating at test time have monopolised the initial attention of researchers, backdoor attacks, exploiting the possibility of corrupting DNN models by interfering with the training process, represents a further serious threat undermining the dependability of AI techniques. In a backdoor attack, the attacker corrupts the training data so to induce an erroneous behaviour at test time. Test time errors, however, are activated only in the presence of a triggering event corresponding to a properly crafted input sample. In this way, the corrupted network continues to work as expected for regular inputs, and the malicious behaviour occurs only when the attacker decides to activate the backdoor hidden within the network. In the last few years, backdoor attacks have been the subject of an intense research activity focusing on both the development of new classes of attacks, and the proposal of possible countermeasures. The goal of this overview paper is to review the works published until now, classifying the different types of attacks and defences proposed so far. The classification guiding the analysis is based on the amount of control that the attacker has on the training process, and the capability of the defender to verify the integrity of the data used for training, and to monitor the operations of the DNN at training and test time. As such, the proposed analysis is particularly suited to highlight the strengths and weaknesses of both attacks and defences with reference to the application scenarios they are operating in.
","
","arXiv
DBLP"
"Federated learning attack surface: taxonomy, cyber defences, challenges, and future directions","Attia Qammar, Jianguo Ding, Huansheng Ning",Artificial Intelligence Review,2021-11-15,"<a href=""Springer (2021-11-15) : Federated learning attack surface: taxonomy, cyber defences, challenges, and future directions"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-021-10098-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-021-10098-w]</a>",Federated learning (FL) has received a great deal of research attention in the context of privacy protection restrictions. By jointly training deep...,,Springer
Backdoor Pre-trained Models Can Transfer to All,Shen L.,Proceedings of the ACM Conference on Computer and Communications Security,2021-11-12,"<a href=""ScienceDirect (2021-11-12) : Backdoor Pre-trained Models Can Transfer to All"" target=""_blank"">[https://doi.org/10.1145/3460120.3485370]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3460120.3485370]</a>",,,ScienceDirect
Towards a Standard Feature Set for Network Intrusion Detection System Datasets,"Mohanad Sarhan, Siamak Layeghy, Marius Portmann",Mobile Networks and Applications,2021-11-10,"<a href=""Springer (2021-11-10) : Towards a Standard Feature Set for Network Intrusion Detection System Datasets"" target=""_blank"">[https://link.springer.com/article/10.1007/s11036-021-01843-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11036-021-01843-0]</a>",Network Intrusion Detection Systems (NIDSs) are important tools for the protection of computer networks against increasingly frequent and...,,Springer
Influencing factors identification in smart society for insider threat in law enforcement agency using a mixed method approach,"Karthiggaibalan Kisenasamy, Sundresan Perumal, ... Balveer Singh Mahindar Singh",International Journal of System Assurance Engineering and Management,2021-11-09,"<a href=""Springer (2021-11-09) : Influencing factors identification in smart society for insider threat in law enforcement agency using a mixed method approach"" target=""_blank"">[https://link.springer.com/article/10.1007/s13198-021-01378-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13198-021-01378-3]</a>",One of the main principle goals for threat protection is to understand the behavior of the employee. An employee who is trusted will have the...,,Springer
AID: Attesting the Integrity of Deep Neural Networks,O. Aramoon P. -Y. Chen G. Qu,2021 58th ACM/IEEE Design Automation Conference (DAC),2021-11-08,"<a href=""IEEE (2021-11-08) : AID: Attesting the Integrity of Deep Neural Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586290]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/DAC18074.2021.9586290]</a>","Due to their crucial role in many decision-making tasks, Deep Neural Networks (DNNs) are common targets for a large array of integrity breaches. In this paper, we propose AID, a novel methodology to Attest the Integrity of DNNs. AID generates a set of test cases called edge-points that can reveal whether a model has been compromised. AID does not require access to parameters of the DNN and can work with a restricted black-box access to the model, which makes it applicable to most real life scenarios. Experimental results show that AID is highly effective and reliable. With at most four edge-points, AID is able to detect eight representative integrity breaches including backdoor, poisoning, and compression attacks, with zero false-positive.",,IEEE
Excess Capacity and Backdoor Poisoning,"Naren Sarayu Manoj, Avrim Blum","arXiv
NeurIPS
arXiv","2021-11-03
2021
2021-09","<a href=""arXiv (2021-11-03) : Excess Capacity and Backdoor Poisoning"" target=""_blank"">[http://arxiv.org/abs/2109.00685v3]</a>
<a href=""DBLP (2021) : Excess Capacity and Backdoor Poisoning"" target=""_blank"">[https://proceedings.neurips.cc/paper/2021/hash/aaebdb8bb6b0e73f6c3c54a0ab0c6415-Abstract.html]</a>
<a href=""DBLP (2021-09) : Excess Capacity and Backdoor Poisoning"" target=""_blank"">[https://arxiv.org/abs/2109.00685]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://proceedings.neurips.cc/paper/2021/hash/aaebdb8bb6b0e73f6c3c54a0ab0c6415-Abstract.html]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2109.00685]</a>","A backdoor data poisoning attack is an adversarial attack wherein the attacker injects several watermarked, mislabeled training examples into a training set. The watermark does not impact the test-time performance of the model on typical data, however, the model reliably errs on watermarked examples. To gain a better foundational understanding of backdoor data poisoning attacks, we present a formal theoretical framework within which one can discuss backdoor data poisoning attacks for classification problems. We then use this to analyze important statistical and computational issues surrounding these attacks. On the statistical front, we identify a parameter we call the memorization capacity that captures the intrinsic vulnerability of a learning problem to a backdoor attack. This allows us to argue about the robustness of several natural learning problems to backdoor attacks. Our results favoring the attacker involve presenting explicit constructions of backdoor attacks, and our robustness results show that some natural problem settings cannot yield successful backdoor attacks. From a computational standpoint, we show that under certain assumptions, adversarial training can detect the presence of backdoors in a training set. We then show that under similar assumptions, two closely related problems we call backdoor filtering and robust generalization are nearly equivalent. This implies that it is both asymptotically necessary and sufficient to design algorithms that can identify watermarked examples in the training set in order to obtain a learning algorithm that both generalizes well to unseen data and is robust to backdoors.

","

","arXiv
DBLP
DBLP"
"A survey on internet of energy security: related fields, challenges, threats and emerging technologies","Mazin Mohammed Mogadem, Yan Li, Daniel Limenew Meheretie",Cluster Computing,2021-11-02,"<a href=""Springer (2021-11-02) : A survey on internet of energy security: related fields, challenges, threats and emerging technologies"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-021-03423-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-021-03423-z]</a>",The need for security has never been more than this for the structure of today’s world. Emerging industrial revolutions started to shapeshift many...,,Springer
Backdoor Smoothing: Demystifying Backdoor Attacks on Deep Neural Networks,"Kathrin Grosse, Taesung Lee, Battista Biggio, Youngja Park, Michael Backes, Ian Molloy",arXiv,2021-11-02,"<a href=""arXiv (2021-11-02) : Backdoor Smoothing: Demystifying Backdoor Attacks on Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2006.06721v4]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","Backdoor attacks mislead machine-learning models to output an attacker-specified class when presented a specific trigger at test time. These attacks require poisoning the training data to compromise the learning algorithm, e.g., by injecting poisoning samples containing the trigger into the training set, along with the desired class label. Despite the increasing number of studies on backdoor attacks and defenses, the underlying factors affecting the success of backdoor attacks, along with their impact on the learning algorithm, are not yet well understood. In this work, we aim to shed light on this issue by unveiling that backdoor attacks induce a smoother decision function around the triggered samples -- a phenomenon which we refer to as \textit{backdoor smoothing}. To quantify backdoor smoothing, we define a measure that evaluates the uncertainty associated to the predictions of a classifier around the input samples. Our experiments show that smoothness increases when the trigger is added to the input samples, and that this phenomenon is more pronounced for more successful attacks. We also provide preliminary evidence that backdoor triggers are not the only smoothing-inducing patterns, but that also other artificial patterns can be detected by our approach, paving the way towards understanding the limitations of current defenses and designing novel ones.",,arXiv
BDDR: An Effective Defense Against Textual Backdoor Attacks,Shao K.,Computers and Security,2021-11-01,"<a href=""ScienceDirect (2021-11-01) : BDDR: An Effective Defense Against Textual Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1016/j.cose.2021.102433]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.cose.2021.102433]</a>",,,ScienceDirect
Boundary augment: A data augment method to defend poison attack,Chen X.,IET Image Processing,2021-11-01,"<a href=""ScienceDirect (2021-11-01) : Boundary augment: A data augment method to defend poison attack"" target=""_blank"">[https://doi.org/10.1049/ipr2.12325]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1049/ipr2.12325]</a>",,,ScienceDirect
Federated Learning Backdoor Attack Scheme Based on Generative Adversarial Network,Chen D.,Jisuanji Yanjiu yu Fazhan/Computer Research and Development,2021-11-01,"<a href=""ScienceDirect (2021-11-01) : Federated Learning Backdoor Attack Scheme Based on Generative Adversarial Network"" target=""_blank"">[https://doi.org/10.7544/issn1000-1239.2021.20210659]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.7544/issn1000-1239.2021.20210659]</a>",,,ScienceDirect
Textual backdoor defense via poisoned sample recognition,Shao K.,Applied Sciences (Switzerland),2021-11-01,"<a href=""ScienceDirect (2021-11-01) : Textual backdoor defense via poisoned sample recognition"" target=""_blank"">[https://doi.org/10.3390/app11219938]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/app11219938]</a>",,,ScienceDirect
Work-in-Progress: Generative Strategy based Backdoor Attacks to 3D Point Clouds,X. Wen W. Jiang J. Zhan C. Bian Z. Song,2021 International Conference on Embedded Software (EMSOFT),2021-11-01,"<a href=""IEEE (2021-11-01) : Work-in-Progress: Generative Strategy based Backdoor Attacks to 3D Point Clouds"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9589486]</a>","<a href=""IEEE"" target=""_blank"">[]</a>","3D deep learning has been applied in safety-critical scenarios, e.g., autonomous driving. Several works have raised the security problems of 3D deep learnings mainly from the perspective of adversarial attacks. In this paper, we propose a novel backdoor attack method to threaten 3D deep learning without the original training data. Several neurons are selected and made sensitive to backdoor triggers. The backdoor triggers are generated by reversing neural network, and the shape of which is constrained to map the objects in the physical world. Sufficient training data can be also generated by reverse engineering. Finally, retraining with the generated 3D trigger and training data is applied to inject backdoors, which is in no need of accessing the original training process and data.",,IEEE
Hidden Backdoors in Human-Centric Language Models,"Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, Jialiang Lu","CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security
arXiv
CCS
arXiv","2021-11
2021-09-28
2021
2021-05","<a href=""ACM (2021-11) : Hidden Backdoors in Human-Centric Language Models"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3460120.3484576]</a>
<a href=""arXiv (2021-09-28) : Hidden Backdoors in Human-Centric Language Models"" target=""_blank"">[http://arxiv.org/abs/2105.00164v3]</a>
<a href=""DBLP (2021) : Hidden Backdoors in Human-Centric Language Models"" target=""_blank"">[https://doi.org/10.1145/3460120.3484576]</a>
<a href=""DBLP (2021-05) : Hidden Backdoors in Human-Centric Language Models"" target=""_blank"">[https://arxiv.org/abs/2105.00164]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3460120.3484576]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3460120.3484576]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2105.00164]</a>","Natural language processing (NLP) systems have been proven to be vulnerable to backdoor attacks, whereby hidden features (backdoors) are trained into a language model and may only be activated by specific inputs (called triggers), to trick the model ...
Natural language processing (NLP) systems have been proven to be vulnerable to backdoor attacks, whereby hidden features (backdoors) are trained into a language model and may only be activated by specific inputs (called triggers), to trick the model into producing unexpected behaviors. In this paper, we create covert and natural triggers for textual backdoor attacks, \textit{hidden backdoors}, where triggers can fool both modern language models and human inspection. We deploy our hidden backdoors through two state-of-the-art trigger embedding methods. The first approach via homograph replacement, embeds the trigger into deep neural networks through the visual spoofing of lookalike character replacement. The second approach uses subtle differences between text generated by language models and real natural text to produce trigger sentences with correct grammar and high fluency. We demonstrate that the proposed hidden backdoors can be effective across three downstream security-critical NLP tasks, representative of modern human-centric NLP systems, including toxic comment detection, neural machine translation (NMT), and question answering (QA). Our two hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at least $97\%$ with an injection rate of only $3\%$ in toxic comment detection, $95.1\%$ ASR in NMT with less than $0.5\%$ injected data, and finally $91.12\%$ ASR against QA updated with only 27 poisoning data samples on a model previously trained with 92,024 samples (0.029\%). We are able to demonstrate the adversary's high success rate of attacks, while maintaining functionality for regular users, with triggers inconspicuous by the human administrators.

","


","ACM
arXiv
DBLP
DBLP"
DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection,"Yuanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, Yunxin Liu","ICSE '21: Proceedings of the 43rd International Conference on Software Engineering
arXiv
ICSE
arXiv","2021-11
2021-01-18
2021
2021-01","<a href=""ACM (2021-11) : DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection"" target=""_blank"">[https://dl.acm.org/doi/10.1109/ICSE43902.2021.00035]</a>
<a href=""arXiv (2021-01-18) : DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection"" target=""_blank"">[http://arxiv.org/abs/2101.06896v1]</a>
<a href=""DBLP (2021) : DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection"" target=""_blank"">[https://doi.org/10.1109/ICSE43902.2021.00035]</a>
<a href=""DBLP (2021-01) : DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection"" target=""_blank"">[https://arxiv.org/abs/2101.06896]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1109/ICSE43902.2021.00035]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICSE43902.2021.00035]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2101.06896]</a>","Deep learning models are increasingly used in mobile applications as critical components. Unlike the program bytecode whose vulnerabilities and threats have been widely-discussed, whether and how the deep learning models deployed in the applications can ...
Deep learning models are increasingly used in mobile applications as critical components. Unlike the program bytecode whose vulnerabilities and threats have been widely-discussed, whether and how the deep learning models deployed in the applications can be compromised are not well-understood since neural networks are usually viewed as a black box. In this paper, we introduce a highly practical backdoor attack achieved with a set of reverse-engineering techniques over compiled deep learning models. The core of the attack is a neural conditional branch constructed with a trigger detector and several operators and injected into the victim model as a malicious payload. The attack is effective as the conditional logic can be flexibly customized by the attacker, and scalable as it does not require any prior knowledge from the original model. We evaluated the attack effectiveness using 5 state-of-the-art deep learning models and real-world samples collected from 30 users. The results demonstrated that the injected backdoor can be triggered with a success rate of 93.5%, while only brought less than 2ms latency overhead and no more than 1.4% accuracy decrease. We further conducted an empirical study on real-world mobile deep learning apps collected from Google Play. We found 54 apps that were vulnerable to our attack, including popular and security-critical ones. The results call for the awareness of deep learning application developers and auditors to enhance the protection of deployed models.

","


","ACM
arXiv
DBLP
DBLP"
Backdoor Attack on Deep Neural Networks Triggered by Fault Injection Attack on Image Sensor Interface,"Tatsuya Oyama, Shunsuke Okura, Kota Yoshida, Takeshi Fujino","ASHES '21: Proceedings of the 5th Workshop on Attacks and Solutions in Hardware Security
ASHES@CCS
Sensors","2021-11
2021
2023","<a href=""ACM (2021-11) : Backdoor Attack on Deep Neural Networks Triggered by Fault Injection Attack on Image Sensor Interface"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3474376.3487287]</a>
<a href=""DBLP (2021) : Backdoor Attack on Deep Neural Networks Triggered by Fault Injection Attack on Image Sensor Interface"" target=""_blank"">[https://doi.org/10.1145/3474376.3487287]</a>
<a href=""DBLP (2023) : Backdoor Attack on Deep Neural Networks Triggered by Fault Injection Attack on Image Sensor Interface"" target=""_blank"">[https://doi.org/10.3390/s23104742]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3474376.3487287]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3474376.3487287]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.3390/s23104742]</a>","Recent automobiles use image sensors to take in the physical world information, and deep neural networks (DNNs) are used to recognize the surroundings to control the vehicles. Adversarial examples and backdoor attacks that induce misclassification by ...

","

","ACM
DBLP
DBLP"
A Statistical Difference Reduction Method for Escaping Backdoor Detection,"Pengfei Xia, Hongjing Niu, Ziqiang Li, Bin Li",arXiv,2021-11,"<a href=""DBLP (2021-11) : A Statistical Difference Reduction Method for Escaping Backdoor Detection"" target=""_blank"">[https://arxiv.org/abs/2111.05077]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2111.05077]</a>",,,DBLP
Subpopulation Data Poisoning Attacks,"Matthew Jagielski, Giorgio Severi, Niklas Pousette Harger, Alina Oprea",CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security,2021-11,"<a href=""ACM (2021-11) : Subpopulation Data Poisoning Attacks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3460120.3485368]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3460120.3485368]</a>","Machine learning systems are deployed in critical settings, but they might fail in unexpected ways, impacting the accuracy of their predictions. Poisoning attacks against machine learning induce adversarial modification of data used by a machine ...",,ACM
Adversarial Neuron Pruning Purifies Backdoored Deep Models,"Dongxian Wu, Yisen Wang","arXiv
NeurIPS
arXiv","2021-10-27
2021
2021-10","<a href=""arXiv (2021-10-27) : Adversarial Neuron Pruning Purifies Backdoored Deep Models"" target=""_blank"">[http://arxiv.org/abs/2110.14430v1]</a>
<a href=""DBLP (2021) : Adversarial Neuron Pruning Purifies Backdoored Deep Models"" target=""_blank"">[https://proceedings.neurips.cc/paper/2021/hash/8cbe9ce23f42628c98f80fa0fac8b19a-Abstract.html]</a>
<a href=""DBLP (2021-10) : Adversarial Neuron Pruning Purifies Backdoored Deep Models"" target=""_blank"">[https://arxiv.org/abs/2110.14430]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://proceedings.neurips.cc/paper/2021/hash/8cbe9ce23f42628c98f80fa0fac8b19a-Abstract.html]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2110.14430]</a>","As deep neural networks (DNNs) are growing larger, their requirements for computational resources become huge, which makes outsourcing training more popular. Training in a third-party platform, however, may introduce potential risks that a malicious trainer will return backdoored DNNs, which behave normally on clean samples but output targeted misclassifications whenever a trigger appears at the test time. Without any knowledge of the trigger, it is difficult to distinguish or recover benign DNNs from backdoored ones. In this paper, we first identify an unexpected sensitivity of backdoored DNNs, that is, they are much easier to collapse and tend to predict the target label on clean samples when their neurons are adversarially perturbed. Based on these observations, we propose a novel model repairing method, termed Adversarial Neuron Pruning (ANP), which prunes some sensitive neurons to purify the injected backdoor. Experiments show, even with only an extremely small amount of clean data (e.g., 1%), ANP effectively removes the injected backdoor without causing obvious performance degradation.

","

","arXiv
DBLP
DBLP"
Evaluation of Machine Learning Algorithms for Detection of Malicious Traffic in SCADA Network,"L. Rajesh, Penke Satyanarayana",Journal of Electrical Engineering & Technology,2021-10-25,"<a href=""Springer (2021-10-25) : Evaluation of Machine Learning Algorithms for Detection of Malicious Traffic in SCADA Network"" target=""_blank"">[https://link.springer.com/article/10.1007/s42835-021-00931-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42835-021-00931-1]</a>",Industrial Process Control Systems (IPCS) like Supervisory Control and Data Acquisition (SCADA) systems are more vulnerable to cyber-attacks....,,Springer
Detecting Backdoor Attacks Against Point Cloud Classifiers,"Zhen Xiang, David J. Miller, Siheng Chen, Xi Li, George Kesidis","arXiv
arXiv","2021-10-20
2021-10","<a href=""arXiv (2021-10-20) : Detecting Backdoor Attacks Against Point Cloud Classifiers"" target=""_blank"">[http://arxiv.org/abs/2110.10354v1]</a>
<a href=""DBLP (2021-10) : Detecting Backdoor Attacks Against Point Cloud Classifiers"" target=""_blank"">[https://arxiv.org/abs/2110.10354]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2110.10354]</a>","Backdoor attacks (BA) are an emerging threat to deep neural network classifiers. A classifier being attacked will predict to the attacker's target class when a test sample from a source class is embedded with the backdoor pattern (BP). Recently, the first BA against point cloud (PC) classifiers was proposed, creating new threats to many important applications including autonomous driving. Such PC BAs are not detectable by existing BA defenses due to their special BP embedding mechanism. In this paper, we propose a reverse-engineering defense that infers whether a PC classifier is backdoor attacked, without access to its training set or to any clean classifiers for reference. The effectiveness of our defense is demonstrated on the benchmark ModeNet40 dataset for PCs.
","
","arXiv
DBLP"
TRAPDOOR: Repurposing backdoors to detect dataset bias in machine learning-based genomic analysis,"Esha Sarkar, Michail Maniatakos","arXiv
arXiv","2021-10-20
2021-08","<a href=""arXiv (2021-10-20) : TRAPDOOR: Repurposing backdoors to detect dataset bias in machine learning-based genomic analysis"" target=""_blank"">[http://arxiv.org/abs/2108.10132v2]</a>
<a href=""DBLP (2021-08) : TRAPDOOR: Repurposing backdoors to detect dataset bias in machine learning-based genomic analysis"" target=""_blank"">[https://arxiv.org/abs/2108.10132]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2108.10132]</a>","Machine Learning (ML) has achieved unprecedented performance in several applications including image, speech, text, and data analysis. Use of ML to understand underlying patterns in gene mutations (genomics) has far-reaching results, not only in overcoming diagnostic pitfalls, but also in designing treatments for life-threatening diseases like cancer. Success and sustainability of ML algorithms depends on the quality and diversity of data collected and used for training. Under-representation of groups (ethnic groups, gender groups, etc.) in such a dataset can lead to inaccurate predictions for certain groups, which can further exacerbate systemic discrimination issues. In this work, we propose TRAPDOOR, a methodology for identification of biased datasets by repurposing a technique that has been mostly proposed for nefarious purposes: Neural network backdoors. We consider a typical collaborative learning setting of the genomics supply chain, where data may come from hospitals, collaborative projects, or research institutes to a central cloud without awareness of bias against a sensitive group. In this context, we develop a methodology to leak potential bias information of the collective data without hampering the genuine performance using ML backdooring catered for genomic applications. Using a real-world cancer dataset, we analyze the dataset with the bias that already existed towards white individuals and also introduced biases in datasets artificially, and our experimental result show that TRAPDOOR can detect the presence of dataset bias with 100% accuracy, and furthermore can also extract the extent of bias by recovering the percentage with a small error.
","
","arXiv
DBLP"
PROUD-MAL: static analysis-based progressive framework for deep unsupervised malware classification of windows portable executable,"Syed Khurram Jah Rizvi, Warda Aslam, ... Muhammad Moazam Fraz",Complex & Intelligent Systems,2021-10-12,"<a href=""Springer (2021-10-12) : PROUD-MAL: static analysis-based progressive framework for deep unsupervised malware classification of windows portable executable"" target=""_blank"">[https://link.springer.com/article/10.1007/s40747-021-00560-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s40747-021-00560-1]</a>","Enterprises are striving to remain protected against malware-based cyber-attacks on their infrastructure, facilities, networks and systems. Static...",,Springer
Widen The Backdoor To Let More Attackers In,"Siddhartha Datta, Giulio Lovisotto, Ivan Martinovic, Nigel Shadbolt","arXiv
arXiv","2021-10-09
2021-10","<a href=""arXiv (2021-10-09) : Widen The Backdoor To Let More Attackers In"" target=""_blank"">[http://arxiv.org/abs/2110.04571v1]</a>
<a href=""DBLP (2021-10) : Widen The Backdoor To Let More Attackers In"" target=""_blank"">[https://arxiv.org/abs/2110.04571]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2110.04571]</a>","As collaborative learning and the outsourcing of data collection become more common, malicious actors (or agents) which attempt to manipulate the learning process face an additional obstacle as they compete with each other. In backdoor attacks, where an adversary attempts to poison a model by introducing malicious samples into the training data, adversaries have to consider that the presence of additional backdoor attackers may hamper the success of their own backdoor. In this paper, we investigate the scenario of a multi-agent backdoor attack, where multiple non-colluding attackers craft and insert triggered samples in a shared dataset which is used by a model (a defender) to learn a task. We discover a clear backfiring phenomenon: increasing the number of attackers shrinks each attacker's attack success rate (ASR). We then exploit this phenomenon to minimize the collective ASR of attackers and maximize defender's robustness accuracy by (i) artificially augmenting the number of attackers, and (ii) indexing to remove the attacker's sub-dataset from the model for inference, hence proposing 2 defenses.
","
","arXiv
DBLP"
Dyn-Backdoor: Backdoor Attack on Dynamic Link Prediction,"Jinyin Chen, Haiyang Xiong, Haibin Zheng, Jian Zhang, Guodong Jiang, Yi Liu","arXiv
arXiv
IEEE Trans. Netw. Sci. Eng.","2021-10-08
2021-10
2024","<a href=""arXiv (2021-10-08) : Dyn-Backdoor: Backdoor Attack on Dynamic Link Prediction"" target=""_blank"">[http://arxiv.org/abs/2110.03875v1]</a>
<a href=""DBLP (2021-10) : Dyn-Backdoor: Backdoor Attack on Dynamic Link Prediction"" target=""_blank"">[https://arxiv.org/abs/2110.03875]</a>
<a href=""DBLP (2024) : Dyn-Backdoor: Backdoor Attack on Dynamic Link Prediction"" target=""_blank"">[https://doi.org/10.1109/TNSE.2023.3301673]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2110.03875]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TNSE.2023.3301673]</a>","Dynamic link prediction (DLP) makes graph prediction based on historical information. Since most DLP methods are highly dependent on the training data to achieve satisfying prediction performance, the quality of the training data is crucial. Backdoor attacks induce the DLP methods to make wrong prediction by the malicious training data, i.e., generating a subgraph sequence as the trigger and embedding it to the training data. However, the vulnerability of DLP toward backdoor attacks has not been studied yet. To address the issue, we propose a novel backdoor attack framework on DLP, denoted as Dyn-Backdoor. Specifically, Dyn-Backdoor generates diverse initial-triggers by a generative adversarial network (GAN). Then partial links of the initial-triggers are selected to form a trigger set, according to the gradient information of the attack discriminator in the GAN, so as to reduce the size of triggers and improve the concealment of the attack. Experimental results show that Dyn-Backdoor launches successful backdoor attacks on the state-of-the-art DLP models with success rate more than 90%. Additionally, we conduct a possible defense against Dyn-Backdoor to testify its resistance in defensive settings, highlighting the needs of defenses for backdoor attacks on DLP.

","

","arXiv
DBLP
DBLP"
Cassandra: Detecting Trojaned Networks From Adversarial Perturbations,X. Zhang R. Gupta A. Mian N. Rahnavard M. Shah,IEEE Access,2021-10-08,"<a href=""IEEE (2021-10-08) : Cassandra: Detecting Trojaned Networks From Adversarial Perturbations"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9502110]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2021.3101289]</a>","Deep neural networks are being widely deployed for critical tasks. In many cases, pre-trained models are sourced from vendors who may have disrupted the training pipeline to insert Trojan behaviors. These malicious behaviors can be triggered at the adversary’s will, which is a serious security threat. To verify the integrity of a deep model, we propose a method that captures its fingerprint with adversarial perturbations. Inserting backdoors into a network alters its decision boundaries which are effectively encoded by adversarial perturbations. Our proposed Trojan detection network learns features from adversarial patterns and its properties to encode the unknown trigger shape and deviations in the decision boundaries caused by backdoors. Our method works completely without or with limited clean samples for improved performance. Our method also performs anomaly detection to identify the target class of a Trojaned network and is invariant to the trigger type, trigger size, network architecture and does not require any triggered samples. Experiments are performed on MNIST, NIST-TrojAI and Odysseus datasets, with 5000 pre-trained models in total, making this the largest study to date on Trojaned detection and the new state-of-the-art accuracy is achieved.",,IEEE
BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models,"Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei Li, Chun Fan","arXiv
ICLR
arXiv","2021-10-06
2022
2021-10","<a href=""arXiv (2021-10-06) : BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models"" target=""_blank"">[http://arxiv.org/abs/2110.02467v1]</a>
<a href=""DBLP (2022) : BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models"" target=""_blank"">[https://openreview.net/forum?id=Mng8CQ9eBW]</a>
<a href=""DBLP (2021-10) : BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models"" target=""_blank"">[https://arxiv.org/abs/2110.02467]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/forum?id=Mng8CQ9eBW]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2110.02467]</a>","Pre-trained Natural Language Processing (NLP) models can be easily adapted to a variety of downstream language tasks. This significantly accelerates the development of language models. However, NLP models have been shown to be vulnerable to backdoor attacks, where a pre-defined trigger word in the input text causes model misprediction. Previous NLP backdoor attacks mainly focus on some specific tasks. This makes those attacks less general and applicable to other kinds of NLP models and tasks. In this work, we propose \Name, the first task-agnostic backdoor attack against the pre-trained NLP models. The key feature of our attack is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pre-trained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor, even after the extensive transfer learning process. We further design a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that our approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way.

","

","arXiv
DBLP
DBLP"
Backdoor attacks to deep neural network-based system for covid-19 detection from chest x-ray images,Matsuo Y.,Applied Sciences (Switzerland),2021-10-01,"<a href=""ScienceDirect (2021-10-01) : Backdoor attacks to deep neural network-based system for covid-19 detection from chest x-ray images"" target=""_blank"">[https://doi.org/10.3390/app11209556]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/app11209556]</a>",,,ScienceDirect
Bias Busters: Robustifying DL-Based Lithographic Hotspot Detectors against Backdooring Attacks,Liu K.,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2021-10-01,"<a href=""ScienceDirect (2021-10-01) : Bias Busters: Robustifying DL-Based Lithographic Hotspot Detectors against Backdooring Attacks"" target=""_blank"">[https://doi.org/10.1109/TCAD.2020.3033749]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TCAD.2020.3033749]</a>",,,ScienceDirect
Reasoning short cuts in infinite domain constraint satisfaction: Algorithms and lower bounds for backdoors,Jonsson P.,"Leibniz International Proceedings in Informatics, LIPIcs",2021-10-01,"<a href=""ScienceDirect (2021-10-01) : Reasoning short cuts in infinite domain constraint satisfaction: Algorithms and lower bounds for backdoors"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.CP.2021.32]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.CP.2021.32]</a>",,,ScienceDirect
A physically realizable backdoor attack on 3D point cloud deep learning: work-in-progress,"Chen Bian, Wei Jiang, Jinyu Zhan, Ziwei Song, Xiangyu Wen, Hong Lei","CODES/ISSS '21: Proceedings of the 2021 International Conference on Hardware/Software Codesign and System Synthesis
CODES+ISSS","2021-10
2021","<a href=""ACM (2021-10) : A physically realizable backdoor attack on 3D point cloud deep learning: work-in-progress"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3478684.3479254]</a>
<a href=""DBLP (2021) : A physically realizable backdoor attack on 3D point cloud deep learning: work-in-progress"" target=""_blank"">[https://doi.org/10.1145/3478684.3479254]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3478684.3479254]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3478684.3479254]</a>","Modern autonomous driving has widely used deep learning to process point cloud data. This application is widely deployed on embedded edge computing devices and has high security requirements. We found that backdoor attacks can pose an extremely serious ...
","
","ACM
DBLP"
Anti-Distillation Backdoor Attacks: Backdoors Can Really Survive in Knowledge Distillation,"Yunjie Ge, Qian Wang, Baolin Zheng, Xinlu Zhuang, Qi Li, Chao Shen, Cong Wang","MM '21: Proceedings of the 29th ACM International Conference on Multimedia
ACM Multimedia","2021-10
2021","<a href=""ACM (2021-10) : Anti-Distillation Backdoor Attacks: Backdoors Can Really Survive in Knowledge Distillation"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3474085.3475254]</a>
<a href=""DBLP (2021) : Anti-Distillation Backdoor Attacks: Backdoors Can Really Survive in Knowledge Distillation"" target=""_blank"">[https://doi.org/10.1145/3474085.3475254]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3474085.3475254]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3474085.3475254]</a>","Motivated by resource-limited scenarios, knowledge distillation (KD) has received growing attention, effectively and quickly producing lightweight yet high-performance student models by transferring the dark knowledge from large teacher models. However, ...
","
","ACM
DBLP"
MP-BADNet: A Backdoor-Attack Detection and Identification Protocol among Multi-Participants in Private Deep Neural Networks,"Congcong Chen, Lifei Wei, Lei Zhang, Jianting Ning","ACM TURC '21: Proceedings of the ACM Turing Award Celebration Conference - China
ACM TUR-C","2021-10
2021","<a href=""ACM (2021-10) : MP-BADNet: A Backdoor-Attack Detection and Identification Protocol among Multi-Participants in Private Deep Neural Networks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3472634.3472660]</a>
<a href=""DBLP (2021) : MP-BADNet: A Backdoor-Attack Detection and Identification Protocol among Multi-Participants in Private Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1145/3472634.3472660]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3472634.3472660]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3472634.3472660]</a>","Deep Neural Networks (DNNs) are vulnerable to backdoor attacks where the adversary can inject malicious data during the DNN training. Such kind of attacks is always activated when the input is stamped with a pre-specified trigger which results in a pre-...
","
","ACM
DBLP"
An Optimization Perspective on Realizing Backdoor Injection Attacks on Deep Neural Networks in Hardware,"M. Caner Tol, Saad Islam, Berk Sunar, Ziming Zhang",arXiv,2021-10,"<a href=""DBLP (2021-10) : An Optimization Perspective on Realizing Backdoor Injection Attacks on Deep Neural Networks in Hardware"" target=""_blank"">[https://arxiv.org/abs/2110.07683]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2110.07683]</a>",,,DBLP
A physically realizable backdoor attack on 3D point cloud deep learning: Work-in-progress,Bian C.,"Proceedings - 2021 International Conference on Hardware/Software Codesign and System Synthesis, CODES+ISSS 2021",2021-09-30,"<a href=""ScienceDirect (2021-09-30) : A physically realizable backdoor attack on 3D point cloud deep learning: Work-in-progress"" target=""_blank"">[https://doi.org/10.1145/3478684.3479254]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3478684.3479254]</a>",,,ScienceDirect
Backdoor Attacks on Federated Learning with Lottery Ticket Hypothesis,"Zeyuan Yin, Ye Yuan, Panfeng Guo, Pan Zhou","arXiv
arXiv","2021-09-22
2021-09","<a href=""arXiv (2021-09-22) : Backdoor Attacks on Federated Learning with Lottery Ticket Hypothesis"" target=""_blank"">[http://arxiv.org/abs/2109.10512v1]</a>
<a href=""DBLP (2021-09) : Backdoor Attacks on Federated Learning with Lottery Ticket Hypothesis"" target=""_blank"">[https://arxiv.org/abs/2109.10512]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2109.10512]</a>","Edge devices in federated learning usually have much more limited computation and communication resources compared to servers in a data center. Recently, advanced model compression methods, like the Lottery Ticket Hypothesis, have already been implemented on federated learning to reduce the model size and communication cost. However, Backdoor Attack can compromise its implementation in the federated learning scenario. The malicious edge device trains the client model with poisoned private data and uploads parameters to the center, embedding a backdoor to the global shared model after unwitting aggregative optimization. During the inference phase, the model with backdoors classifies samples with a certain trigger as one target category, while shows a slight decrease in inference accuracy to clean samples. In this work, we empirically demonstrate that Lottery Ticket models are equally vulnerable to backdoor attacks as the original dense models, and backdoor attacks can influence the structure of extracted tickets. Based on tickets' similarities between each other, we provide a feasible defense for federated learning against backdoor attacks on various datasets.
","
","arXiv
DBLP"
BFClass: A Backdoor-free Text Classification Framework,"Zichao Li, Dheeraj Mekala, Chengyu Dong, Jingbo Shang","arXiv
EMNLP
arXiv","2021-09-22
2021
2021-09","<a href=""arXiv (2021-09-22) : BFClass: A Backdoor-free Text Classification Framework"" target=""_blank"">[http://arxiv.org/abs/2109.10855v1]</a>
<a href=""DBLP (2021) : BFClass: A Backdoor-free Text Classification Framework"" target=""_blank"">[https://doi.org/10.18653/v1/2021.findings-emnlp.40]</a>
<a href=""DBLP (2021-09) : BFClass: A Backdoor-free Text Classification Framework"" target=""_blank"">[https://arxiv.org/abs/2109.10855]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2021.findings-emnlp.40]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2109.10855]</a>","Backdoor attack introduces artificial vulnerabilities into the model by poisoning a subset of the training data via injecting triggers and modifying labels. Various trigger design strategies have been explored to attack text classifiers, however, defending such attacks remains an open problem. In this work, we propose BFClass, a novel efficient backdoor-free training framework for text classification. The backbone of BFClass is a pre-trained discriminator that predicts whether each token in the corrupted input was replaced by a masked language model. To identify triggers, we utilize this discriminator to locate the most suspicious token from each training sample and then distill a concise set by considering their association strengths with particular labels. To recognize the poisoned subset, we examine the training samples with these identified triggers as the most suspicious token, and check if removing the trigger will change the poisoned model's prediction. Extensive experiments demonstrate that BFClass can identify all the triggers, remove 95% poisoned training samples with very limited false alarms, and achieve almost the same performance as the models trained on the benign training data.

","

","arXiv
DBLP
DBLP"
Network security situation prediction based on feature separation and dual attention mechanism,"Zhijian Li, Dongmei Zhao, ... Hongbin Zhang",EURASIP Journal on Wireless Communications and Networking,2021-09-21,"<a href=""Springer (2021-09-21) : Network security situation prediction based on feature separation and dual attention mechanism"" target=""_blank"">[https://link.springer.com/article/10.1186/s13638-021-02050-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13638-021-02050-x]</a>","With the development of smart cities, network security has become more and more important. In order to improve the safety of smart cities, a...",,Springer
Adversarial Targeted Forgetting in Regularization and Generative Based Continual Learning Models,M. Umer R. Polikar,2021 International Joint Conference on Neural Networks (IJCNN),2021-09-20,"<a href=""IEEE (2021-09-20) : Adversarial Targeted Forgetting in Regularization and Generative Based Continual Learning Models"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533400]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IJCNN52387.2021.9533400]</a>","Continual (or “incremental”) learning approaches are employed when additional knowledge or tasks need to be learned from subsequent batches or from streaming data. However these approaches are typically adversary agnostic, i.e., they do not consider the possibility of a malicious attack. In our prior work, we explored the vulnerabilities of Elastic Weight Consolidation (EWC) to the perceptible misinformation. We now explore the vulnerabilities of other regularization-based as well as generative replay-based continual learning algorithms, and also extend the attack to imperceptible misinformation. We show that an intelligent adversary can take advantage of a continual learning algorithm's capabilities of retaining existing knowledge over time, and force it to learn and retain deliberately introduced misinformation. To demonstrate this vulnerability, we inject backdoor attack samples into the training data. These attack samples constitute the misinformation, allowing the attacker to capture control of the model at test time. We evaluate the extent of this vulnerability on both rotated and split benchmark variants of the MNIST dataset under two important domain and class incremental learning scenarios. We show that the adversary can create a “false memory” about any task by inserting carefully-designed backdoor samples to the test instances of that task thereby controlling the amount of forgetting of any task of its choosing. Perhaps most importantly, we show this vulnerability to be very acute and damaging: the model memory can be easily compromised with the addition of backdoor samples into as little as 1% of the training data, even when the misinformation is imperceptible to human eye.",,IEEE
Privacy-Enhanced Federated Learning Against Poisoning Adversaries,X. Liu H. Li G. Xu Z. Chen X. Huang R. Lu,IEEE Transactions on Information Forensics and Security,2021-09-15,"<a href=""IEEE (2021-09-15) : Privacy-Enhanced Federated Learning Against Poisoning Adversaries"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524709]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TIFS.2021.3108434]</a>","Federated learning (FL), as a distributed machine learning setting, has received considerable attention in recent years. To alleviate privacy concerns, FL essentially promises that multiple parties jointly train the model by exchanging gradients rather than raw data. However, intrinsic privacy issue still exists in FL, e.g., user’s training samples could be revealed by solely inferring gradients. Moreover, the emerging poisoning attack also poses a crucial security threat to FL. In particular, due to the distributed nature of FL, malicious users may submit crafted gradients during the training process to undermine the integrity and availability of the model. Furthermore, there exists a contradiction in simultaneously addressing two issues, that is, privacy-preserving FL solutions are dedicated to ensuring gradients indistinguishability, whereas the defenses against poisoning attacks tend to remove outliers based on their similarity. To solve such a dilemma, in this paper, we aim to build a bridge between the two issues. Specifically, we present a privacy-enhanced FL (PEFL) framework that adopts homomorphic encryption as the underlying technology and provides the server with a channel to punish poisoners via the effective gradient data extraction of the logarithmic function. To the best of our knowledge, the PEFL is the first effort to efficiently detect the poisoning behaviors in FL under ciphertext. Detailed theoretical analyses illustrate the security and convergence properties of the scheme. Moreover, the experiments conducted on real-world datasets show that the PEFL can effectively defend against label-flipping and backdoor attacks, two representative poisoning attacks in FL.",,IEEE
Intrusion Detection System in Smart Home Network Using Bidirectional LSTM and Convolutional Neural Networks Hybrid Model,N. Elsayed Z. S. Zaghloul S. W. Azumah C. Li,2021 IEEE International Midwest Symposium on Circuits and Systems (MWSCAS),2021-09-13,"<a href=""IEEE (2021-09-13) : Intrusion Detection System in Smart Home Network Using Bidirectional LSTM and Convolutional Neural Networks Hybrid Model"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9531683]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/MWSCAS47672.2021.9531683]</a>","Internet of Things (IoT) allowed smart homes to improve the quality and the comfort of our daily lives. However, these conveniences introduced several security concerns that increase rapidly. IoT devices, smart home hubs, and gateway raise various security risks. The smart home gateways act as a centralized point of communication between the IoT devices, which can create a backdoor into network data for hackers. One of the common and effective ways to detect such attacks is intrusion detection in the network traffic. In this paper, we proposed an intrusion detection system (IDS) to detect anomalies in a smart home network using a bidirectional long short-term memory (BiLSTM) and convolutional neural network (CNN) hybrid model. The BiLSTM recurrent behavior provides the intrusion detection model to preserve the learned information through time, and the CNN extracts perfectly the data features. The proposed model can be applied to any smart home network gateway.",,IEEE
Backdoor Attack and Defense for Deep Regression,"Xi Li, George Kesidis, David J. Miller, Vladimir Lucic","arXiv
arXiv","2021-09-06
2021-09","<a href=""arXiv (2021-09-06) : Backdoor Attack and Defense for Deep Regression"" target=""_blank"">[http://arxiv.org/abs/2109.02381v1]</a>
<a href=""DBLP (2021-09) : Backdoor Attack and Defense for Deep Regression"" target=""_blank"">[https://arxiv.org/abs/2109.02381]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2109.02381]</a>","We demonstrate a backdoor attack on a deep neural network used for regression. The backdoor attack is localized based on training-set data poisoning wherein the mislabeled samples are surrounded by correctly labeled ones. We demonstrate how such localization is necessary for attack success. We also study the performance of a backdoor defense using gradient-based discovery of local error maximizers. Local error maximizers which are associated with significant (interpolation) error, and are proximal to many training samples, are suspicious. This method is also used to accurately train for deep regression in the first place by active (deep) learning leveraging an ""oracle"" capable of providing real-valued supervision (a regression target) for samples. Such oracles, including traditional numerical solvers of PDEs or SDEs using finite difference or Monte Carlo approximations, are far more computationally costly compared to deep regression.
","
","arXiv
DBLP"
SAFELearning: Enable Backdoor Detectability In Federated Learning With Secure Aggregation,"Zhuosheng Zhang, Jiarui Li, Shucheng Yu, Christian Makaya","arXiv
arXiv","2021-09-03
2021-02","<a href=""arXiv (2021-09-03) : SAFELearning: Enable Backdoor Detectability In Federated Learning With Secure Aggregation"" target=""_blank"">[http://arxiv.org/abs/2102.02402v2]</a>
<a href=""DBLP (2021-02) : SAFELearning: Enable Backdoor Detectability In Federated Learning With Secure Aggregation"" target=""_blank"">[https://arxiv.org/abs/2102.02402]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1109/TIFS.2023.3280032]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2102.02402]</a>","For model privacy, local model parameters in federated learning shall be obfuscated before sent to the remote aggregator. This technique is referred to as \emph{secure aggregation}. However, secure aggregation makes model poisoning attacks such backdooring more convenient considering that existing anomaly detection methods mostly require access to plaintext local models. This paper proposes SAFELearning which supports backdoor detection for secure aggregation. We achieve this through two new primitives - \emph{oblivious random grouping (ORG)} and \emph{partial parameter disclosure (PPD)}. ORG partitions participants into one-time random subgroups with group configurations oblivious to participants, PPD allows secure partial disclosure of aggregated subgroup models for anomaly detection without leaking individual model privacy. SAFELearning can significantly reduce backdoor model accuracy without jeopardizing the main task accuracy under common backdoor strategies. Extensive experiments show SAFELearning is robust against malicious and faulty participants, whilst being more efficient than the state-of-art secure aggregation protocol in terms of both communication and computation costs.
","
","arXiv
DBLP"
A review study on blockchain-based IoT security and forensics,"Randa Kamal, Ezz El-Din Hemdan, Nawal El-Fishway",Multimedia Tools and Applications,2021-09-01,"<a href=""Springer (2021-09-01) : A review study on blockchain-based IoT security and forensics"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-021-11350-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-021-11350-9]</a>",The term Internet of Things (IoT) represents all communicating countless heterogeneous devices to share data and resources via the internet. The...,,Springer
Generative strategy based backdoor attacks to 3D point clouds: work-in-progress,"Xiangyu Wen, Wei Jiang, Jinyu Zhan, Chen Bian, Ziwei Song","EMSOFT '21: Proceedings of the 2021 International Conference on Embedded Software
EMSOFT","2021-09
2021","<a href=""ACM (2021-09) : Generative strategy based backdoor attacks to 3D point clouds: work-in-progress"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3477244.3477611]</a>
<a href=""DBLP (2021) : Generative strategy based backdoor attacks to 3D point clouds: work-in-progress"" target=""_blank"">[https://doi.org/10.1145/3477244.3477611]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3477244.3477611]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3477244.3477611]</a>","3D deep learning has been applied in safety-critical scenarios, e.g., autonomous driving. Several works have raised the security problems of 3D deep learnings mainly from the perspective of adversarial attacks. In this paper, we propose a novel backdoor ...
","
","ACM
DBLP"
Clean-label Backdoor Attack against Deep Hashing based Retrieval,"Kuofeng Gao, Jiawang Bai, Bin Chen, Dongxian Wu, Shu-Tao Xia",arXiv,2021-09,"<a href=""DBLP (2021-09) : Clean-label Backdoor Attack against Deep Hashing based Retrieval"" target=""_blank"">[https://arxiv.org/abs/2109.08868]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2109.08868]</a>",,,DBLP
Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning,"Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, Xipeng Qiu","arXiv
EMNLP
arXiv","2021-08-31
2021
2021-08","<a href=""arXiv (2021-08-31) : Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning"" target=""_blank"">[http://arxiv.org/abs/2108.13888v1]</a>
<a href=""DBLP (2021) : Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning"" target=""_blank"">[https://doi.org/10.18653/v1/2021.emnlp-main.241]</a>
<a href=""DBLP (2021-08) : Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning"" target=""_blank"">[https://arxiv.org/abs/2108.13888]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2021.emnlp-main.241]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2108.13888]</a>","\textbf{P}re-\textbf{T}rained \textbf{M}odel\textbf{s} have been widely applied and recently proved vulnerable under backdoor attacks: the released pre-trained weights can be maliciously poisoned with certain triggers. When the triggers are activated, even the fine-tuned model will predict pre-defined labels, causing a security threat. These backdoors generated by the poisoning methods can be erased by changing hyper-parameters during fine-tuning or detected by finding the triggers. In this paper, we propose a stronger weight-poisoning attack method that introduces a layerwise weight poisoning strategy to plant deeper backdoors, we also introduce a combinatorial trigger that cannot be easily detected. The experiments on text classification tasks show that previous defense methods cannot resist our weight-poisoning method, which indicates that our method can be widely applied and may provide hints for future model robustness studies.

","

","arXiv
DBLP
DBLP"
Hyperparameter search based convolution neural network with Bi-LSTM model for intrusion detection system in multimedia big data environment,"Irina V. Pustokhina, Denis A. Pustokhin, ... K. Shankar",Multimedia Tools and Applications,2021-08-31,"<a href=""Springer (2021-08-31) : Hyperparameter search based convolution neural network with Bi-LSTM model for intrusion detection system in multimedia big data environment"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-021-11271-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-021-11271-7]</a>","In recent years, there is an exponential increase in the growth of the multimedia data, which is being generated from zettabyte to petabyte scale. At...",,Springer
Poisonous Label Attack: Black-Box Data Poisoning Attack with Enhanced Conditional DCGAN,"Haiqing Liu, Daoxing Li, Yuancheng Li",Neural Processing Letters,2021-08-28,"<a href=""Springer (2021-08-28) : Poisonous Label Attack: Black-Box Data Poisoning Attack with Enhanced Conditional DCGAN"" target=""_blank"">[https://link.springer.com/article/10.1007/s11063-021-10584-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11063-021-10584-w]</a>",Data poisoning is identified as a security threat for machine learning models. This paper explores the poisoning attack against the convolutional...,,Springer
Shielding Collaborative Learning: Mitigating Poisoning Attacks Through Client-Side Detection,L. Zhao S. Hu Q. Wang J. Jiang C. Shen X. Luo P. Hu,IEEE Transactions on Dependable and Secure Computing,2021-08-27,"<a href=""IEEE (2021-08-27) : Shielding Collaborative Learning: Mitigating Poisoning Attacks Through Client-Side Detection"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066920]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TDSC.2020.2986205]</a>","Collaborative learning allows multiple clients to train a joint model without sharing their data with each other. Each client performs training locally and then submits the model updates to a central server for aggregation. Since the server has no visibility into the process of generating the updates, collaborative learning is vulnerable to poisoning attacks where a malicious client can generate a poisoned update to introduce backdoor functionality to the joint model. The existing solutions for detecting poisoned updates, however, fail to defend against the recently proposed attacks, especially in the non-IID (independent and identically distributed) setting. In this article, we present a novel defense scheme to detect anomalous updates in both IID and non-IID settings. Our key idea is to realize client-side cross-validation, where each update is evaluated over other clients' local data. The server will adjust the weights of the updates based on the evaluation results when performing aggregation. To adapt to the unbalanced distribution of data in the non-IID setting, a dynamic client allocation mechanism is designed to assign detection tasks to the most suitable clients. During the detection process, we also protect the client-level privacy to prevent malicious clients from knowing the participations of other clients, by integrating differential privacy with our design without degrading the detection performance. Our experimental evaluations on three real-world datasets show that our scheme is significantly robust to two representative poisoning attacks.",,IEEE
Stop-and-Go: Exploring Backdoor Attacks on Deep Reinforcement Learning-based Traffic Congestion Control Systems,"Yue Wang, Esha Sarkar, Wenqing Li, Michail Maniatakos, Saif Eddin Jabari","arXiv
IEEE Trans. Inf. Forensics Secur.","2021-08-26
2021","<a href=""arXiv (2021-08-26) : Stop-and-Go: Exploring Backdoor Attacks on Deep Reinforcement Learning-based Traffic Congestion Control Systems"" target=""_blank"">[http://arxiv.org/abs/2003.07859v4]</a>
<a href=""DBLP (2021) : Stop-and-Go: Exploring Backdoor Attacks on Deep Reinforcement Learning-Based Traffic Congestion Control Systems"" target=""_blank"">[https://doi.org/10.1109/TIFS.2021.3114024]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1109/TIFS.2021.3114024]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2021.3114024]</a>","Recent work has shown that the introduction of autonomous vehicles (AVs) in traffic could help reduce traffic jams. Deep reinforcement learning methods demonstrate good performance in complex control problems, including autonomous vehicle control, and have been used in state-of-the-art AV controllers. However, deep neural networks (DNNs) render automated driving vulnerable to machine learning-based attacks. In this work, we explore the backdooring/trojanning of DRL-based AV controllers. We develop a trigger design methodology that is based on well-established principles of traffic physics. The malicious actions include vehicle deceleration and acceleration to cause stop-and-go traffic waves to emerge (congestion attacks) or AV acceleration resulting in the AV crashing into the vehicle in front (insurance attack). We test our attack on single-lane and two-lane circuits. Our experimental results show that the backdoored model does not compromise normal operation performance, with the maximum decrease in cumulative rewards being 1%. Still, it can be maliciously activated to cause a crash or congestion when the corresponding triggers appear.
","
","arXiv
DBLP"
An edge based hybrid intrusion detection framework for mobile edge computing,"Ashish Singh, Kakali Chatterjee, Suresh Chandra Satapathy",Complex & Intelligent Systems,2021-08-25,"<a href=""Springer (2021-08-25) : An edge based hybrid intrusion detection framework for mobile edge computing"" target=""_blank"">[https://link.springer.com/article/10.1007/s40747-021-00498-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s40747-021-00498-4]</a>",The Mobile Edge Computing (MEC) model attracts more users to its services due to its characteristics and rapid delivery approach. This network...,,Springer
PointBA: Towards Backdoor Attacks in 3D Point Cloud,"Xinke Li, Zhirui Chen, Yue Zhao, Zekun Tong, Yabang Zhao, Andrew Lim, Joey Tianyi Zhou","arXiv
ICCV
arXiv","2021-08-23
2021
2021-03","<a href=""arXiv (2021-08-23) : PointBA: Towards Backdoor Attacks in 3D Point Cloud"" target=""_blank"">[http://arxiv.org/abs/2103.16074v3]</a>
<a href=""DBLP (2021) : PointBA: Towards Backdoor Attacks in 3D Point Cloud"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01618]</a>
<a href=""DBLP (2021-03) : PointBA: Towards Backdoor Attacks in 3D Point Cloud"" target=""_blank"">[https://arxiv.org/abs/2103.16074]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01618]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2103.16074]</a>","3D deep learning has been increasingly more popular for a variety of tasks including many safety-critical applications. However, recently several works raise the security issues of 3D deep models. Although most of them consider adversarial attacks, we identify that backdoor attack is indeed a more serious threat to 3D deep learning systems but remains unexplored. We present the backdoor attacks in 3D point cloud with a unified framework that exploits the unique properties of 3D data and networks. In particular, we design two attack approaches on point cloud: the poison-label backdoor attack (PointPBA) and the clean-label backdoor attack (PointCBA). The first one is straightforward and effective in practice, while the latter is more sophisticated assuming there are certain data inspections. The attack algorithms are mainly motivated and developed by 1) the recent discovery of 3D adversarial samples suggesting the vulnerability of deep models under spatial transformation, 2) the proposed feature disentanglement technique that manipulates the feature of the data through optimization methods and its potential to embed a new task. Extensive experiments show the efficacy of the PointPBA with over 95% success rate across various 3D datasets and models, and the more stealthy PointCBA with around 50% success rate. Our proposed backdoor attack in 3D point cloud is expected to perform as a baseline for improving the robustness of 3D deep models.

","

","arXiv
DBLP
DBLP"
On distributed object storage architecture based on mimic defense,H. Yu H. Li X. Yang H. Ma,China Communications,2021-08-23,"<a href=""IEEE (2021-08-23) : On distributed object storage architecture based on mimic defense"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521170]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.23919/JCC.2021.08.009]</a>","With the advent of the era of big data, cloud computing, Internet of things, and other information industries continue to develop. There is an increasing amount of unstructured data such as pictures, audio, and video on the Internet. And the distributed object storage system has become the mainstream cloud storage solution. With the increasing number of distributed applications, data security in the distributed object storage system has become the focus. For the distributed object storage system, traditional defenses are means that fix discovered system vulnerabilities and backdoors by patching, or means to modify the corresponding structure and upgrade. However, these two kinds of means are hysteretic and hardly deal with unknown security threats. Based on mimic defense theory, this paper constructs the principle framework of the distributed object storage system and introduces the dynamic redundancy and heterogeneous function in the distributed object storage system architecture, which increases the attack cost, and greatly improves the security and availability of data.",,IEEE
Byzantine Resistant Secure Blockchained Federated Learning at the Edge,Z. Li H. Yu T. Zhou L. Luo M. Fan Z. Xu G. Sun,IEEE Network,2021-08-20,"<a href=""IEEE (2021-08-20) : Byzantine Resistant Secure Blockchained Federated Learning at the Edge"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382023]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/MNET.011.2000604]</a>","The emerging blockchained federated learning, known for its security properties such as decentralization, immutability and traceability, is evolving into an important direction of next-generation AI. With the booming edge computing technologies, blockchained federated learning can take advantage of computing, communication and storage resources geo-distributed at the edge, so that blockchained federated learning can gather edge intelligence from more widely distributed devices more efficiently. However, untrustworthy devices at the edge also bring serious security threats, namely byzantine attacks. Existing solutions focus on selecting local models that are most likely to be honest, rather than detecting byzantine models and identifying attackers, because verifying each local model separately brings intolerable verification delay. In this paper, we propose a byzantine resistant secure blockchained federated learning framework named BytoChain. BytoChain improves the efficiency of model verification by introducing verifiers to execute heavy verification workflows in parallel, and detects byzantine attacks through a byzantine resistant consensus Proof-of-Accuracy (PoA). We analyze how BytoChain can mitigate five types of attacks, and demonstrate its effectiveness by simulations. Finally, we envision some open issues about security, including attacks on privacy, confidentiality, and backdoors.",,IEEE
AEGR: a simple approach to gradient reversal in autoencoders for network anomaly detection,"Kasra Babaei, Zhi Yuan Chen, Tomas Maul",Soft Computing,2021-08-17,"<a href=""Springer (2021-08-17) : AEGR: a simple approach to gradient reversal in autoencoders for network anomaly detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-021-06110-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-021-06110-8]</a>",Anomaly detection is referred to a process in which the aim is to detect data points that follow a different pattern from the majority of data...,,Springer
Towards building data analytics benchmarks for IoT intrusion detection,"Rasheed Ahmad, Izzat Alsmadi, ... Lo’ai Tawalbeh",Cluster Computing,2021-08-16,"<a href=""Springer (2021-08-16) : Towards building data analytics benchmarks for IoT intrusion detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-021-03388-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-021-03388-z]</a>",Data analytics projects span all types of domains and applications. Researchers publish results using certain datasets and classification models....,,Springer
Graph Backdoor,"Zhaohan Xi, Ren Pang, Shouling Ji, Ting Wang","arXiv
USENIX Security Symposium
arXiv","2021-08-10
2021
2020-06","<a href=""arXiv (2021-08-10) : Graph Backdoor"" target=""_blank"">[http://arxiv.org/abs/2006.11890v5]</a>
<a href=""DBLP (2021) : Graph Backdoor"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity21/presentation/xi]</a>
<a href=""DBLP (2020-06) : Graph Backdoor"" target=""_blank"">[https://arxiv.org/abs/2006.11890]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity21/presentation/xi]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2006.11890]</a>","One intriguing property of deep neural networks (DNNs) is their inherent vulnerability to backdoor attacks -- a trojan model responds to trigger-embedded inputs in a highly predictable manner while functioning normally otherwise. Despite the plethora of prior work on DNNs for continuous data (e.g., images), the vulnerability of graph neural networks (GNNs) for discrete-structured data (e.g., graphs) is largely unexplored, which is highly concerning given their increasing use in security-sensitive domains. To bridge this gap, we present GTA, the first backdoor attack on GNNs. Compared with prior work, GTA departs in significant ways: graph-oriented -- it defines triggers as specific subgraphs, including both topological structures and descriptive features, entailing a large design spectrum for the adversary, input-tailored -- it dynamically adapts triggers to individual graphs, thereby optimizing both attack effectiveness and evasiveness, downstream model-agnostic -- it can be readily launched without knowledge regarding downstream models or fine-tuning strategies, and attack-extensible -- it can be instantiated for both transductive (e.g., node classification) and inductive (e.g., graph classification) tasks, constituting severe threats for a range of security-critical applications. Through extensive evaluation using benchmark datasets and state-of-the-art models, we demonstrate the effectiveness of GTA. We further provide analytical justification for its effectiveness and discuss potential countermeasures, pointing to several promising research directions.

","

","arXiv
DBLP
DBLP"
Ensemble classification for intrusion detection via feature extraction based on deep Learning,"Maryam Yousefnezhad, Javad Hamidzadeh, Mohammad Aliannejadi",Soft Computing,2021-08-10,"<a href=""Springer (2021-08-10) : Ensemble classification for intrusion detection via feature extraction based on deep Learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-021-06067-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-021-06067-8]</a>",An intrusion detection system is a security system that aims to detect sabotage and intrusions on networks to inform experts of the attack and abuse...,,Springer
Statistical Analysis Based Intrusion Detection System for Ultra-High-Speed Software Defined Network,"Talha Naqash, Sajjad Hussain Shah, Muhammad Najam Ul Islam",International Journal of Parallel Programming,2021-08-09,"<a href=""Springer (2021-08-09) : Statistical Analysis Based Intrusion Detection System for Ultra-High-Speed Software Defined Network"" target=""_blank"">[https://link.springer.com/article/10.1007/s10766-021-00715-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10766-021-00715-0]</a>","Internet users and internet services are increasing day by day, which increases the internet traffic from zeta-bytes to petabytes with...",,Springer
Decamouflage: A Framework to Detect Image-Scaling Attacks on CNN,B. Kim A. Abuadbba Y. Gao Y. Zheng M. E. Ahmed S. Nepal H. Kim,2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN),2021-08-06,"<a href=""IEEE (2021-08-06) : Decamouflage: A Framework to Detect Image-Scaling Attacks on CNN"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9505142]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/DSN48987.2021.00023]</a>","Image-scaling is a typical operation that processes the input image before feeding it into convolutional neural network models. However, it is vulnerable to the newly revealed image-scaling attack. This work presents an image-scaling attack detection framework, Decamouflage, consisting of three independent detection methods: scaling, filtering, and steganalysis, to detect the attack through examining distinct image characteristics. Decamouflage has a pre-determined detection threshold that is generic. More precisely, as we have validated, the threshold determined from one dataset is also applicable to other different datasets. Extensive experiments show that Decamouflage achieves detection accuracy of 99.9% and 98.5% in the white-box and the black-box settings, respectively. We also measured its running time overhead on a PC with an Intel i5 CPU and 8GB RAM. The experimental results show that image-scaling attacks can be detected in milliseconds. Moreover, Decamouflage is highly robust against adaptive image-scaling attacks (e.g., attack image size variances).",,IEEE
MT-MTD: Muti-Training based Moving Target Defense Trojaning Attack in Edged-AI network,Y. Qiu J. Wu S. Mumtaz J. Li A. Al-Dulaimi J. J. P. C. Rodrigues,ICC 2021 - IEEE International Conference on Communications,2021-08-06,"<a href=""IEEE (2021-08-06) : MT-MTD: Muti-Training based Moving Target Defense Trojaning Attack in Edged-AI network"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9500545]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICC42927.2021.9500545]</a>","The evolution of deep learning has promoted the popularization of smart devices. However, due to the insufficient development of computing hardware, the ability to conduct local training on smart devices is greatly restricted, and it is usually necessary to deploy ready-made models. This opacity makes smart devices vulnerable to deep learning backdoor attacks. Some existing countermeasures against backdoor attacks are based on the attacker’s ignorance of defense. Once the attacker knows the defense mechanism, he can easily overturn it. In this paper, we propose a Trojaning attack defense framework based on moving target defense(MTD) strategy. According to the analysis of attack-defense game types and confrontation process, the moving target defense model based on signaling game was constructed. The simulation results show that in most cases, our technology can greatly increase the attack cost of the attacker, thereby ensuring the availability of Deep Neural Networks(DNN) and protecting it from Trojaning attacks.",,IEEE
Securing Neural Networks Using Homomorphic Encryption,A. Dalvi A. Jain S. Moradiya R. Nirmal J. Sanghavi I. Siddavatam,2021 International Conference on Intelligent Technologies (CONIT),2021-08-04,"<a href=""IEEE (2021-08-04) : Securing Neural Networks Using Homomorphic Encryption"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9498376]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/CONIT51480.2021.9498376]</a>","Neural networks are becoming increasingly popular within the modern world, and they are often implemented without much consideration of their potential flaws, which makes them vulnerable and are easily being hacked by hackers. One of such vulnerabilities, namely, a backdoor attack is studied in this paper. A backdoor attacked neural network involves inducing unique misclassification rules or patterns as triggers in the neural network such that, upon encountering the trigger, the neural network will only predict the output based upon the misclassification rules, giving the attacker control over the output of the neural network. To prevent such a vulnerability, we propose to employ homomorphic encryption as a solution. Homomorphic Encrypted Data has a special property where certain operations can be performed on encrypted data to in-turn directly perform the operations on the plain-text data itself, without the need of any special mechanism. This ability of homomorphic encryption can be used in conjunction with the vulnerable neural network, to revoke the control of the attacker from the neural network. Thereby, in this paper, we will be securing a vulnerable neural network from backdoor attack using homomorphic encryption.",,IEEE
"UAV Communications with Machine Learning: Challenges, Applications and Open Issues","Sana Ben Aissa, Asma Ben Letaifa",Arabian Journal for Science and Engineering,2021-08-03,"<a href=""Springer (2021-08-03) : UAV Communications with Machine Learning: Challenges, Applications and Open Issues"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-021-05932-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-021-05932-w]</a>",Unmanned aerial vehicles (UAV) have recently proved their ability to afford reliable and cost-effective solutions for many real-world scenarios. The...,,Springer
Defense-Resistant Backdoor Attacks against Deep Neural Networks in Outsourced Cloud Environment,Gong X.,IEEE Journal on Selected Areas in Communications,2021-08-01,"<a href=""ScienceDirect (2021-08-01) : Defense-Resistant Backdoor Attacks against Deep Neural Networks in Outsourced Cloud Environment"" target=""_blank"">[https://doi.org/10.1109/JSAC.2021.3087237]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/JSAC.2021.3087237]</a>",,,ScienceDirect
What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors,"Yi-Shan Lin, Wen-Chuan Lee, Z. Berkay Celik","KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining
KDD","2021-08
2021","<a href=""ACM (2021-08) : What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3447548.3467213]</a>
<a href=""DBLP (2021) : What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors"" target=""_blank"">[https://doi.org/10.1145/3447548.3467213]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3447548.3467213]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3447548.3467213]</a>","EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the input parts deemed important to arrive at a decision for a specific target. However, it remains ...
","
","ACM
DBLP"
Backdoor Attacks on Network Certification via Data Poisoning,"Tobias Lorenz, Marta Kwiatkowska, Mario Fritz",arXiv,2021-08,"<a href=""DBLP (2021-08) : Backdoor Attacks on Network Certification via Data Poisoning"" target=""_blank"">[https://arxiv.org/abs/2108.11299]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2108.11299]</a>",,,DBLP
Quantization Backdoors to Deep Learning Models,"Hua Ma, Huming Qiu, Yansong Gao, Zhi Zhang, Alsharif Abuadbba, Anmin Fu, Said F. Al-Sarawi, Derek Abbott",arXiv,2021-08,"<a href=""DBLP (2021-08) : Quantization Backdoors to Deep Learning Models"" target=""_blank"">[https://arxiv.org/abs/2108.09187]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2108.09187]</a>",,,DBLP
Jintide: Utilizing Low-Cost Reconfigurable External Monitors to Substantially Enhance Hardware Security of Large-Scale CPU Clusters,J. Zhu A. Luo G. Li B. Zhang Y. Wang G. Shan Y. Li J. Pan C. Deng S. Yin S. Wei L. Liu,IEEE Journal of Solid-State Circuits,2021-07-26,"<a href=""IEEE (2021-07-26) : Jintide: Utilizing Low-Cost Reconfigurable External Monitors to Substantially Enhance Hardware Security of Large-Scale CPU Clusters"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9366896]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/JSSC.2021.3058551]</a>","Nowadays, hardware security has become a serious concern for modern CPUs. State-of-the-art detection approaches rely heavily on trustworthy and intimate internal states, incurring significant design/operation overheads and additional risks to security and intellectual property. This article proposes an architecture called Jintide, which utilizes trusted external monitors to validate an untrusted CPU chip at runtime. This architecture records, replays, and analyzes the CPU’s IO and memory behavior with the architectural states. The Jintide simultaneously verifies whether the records are correctly replayed with the instruction set architecture and whether the records involve malicious behavior. Consequently, not only architectural but also micro-architectural threats can be detected. The Jintide adopts the states from the untrusted source because it has a built-in function to detect spurious states. The monitors comprise three types of chips (with 28-/40-nm TSMC technology): a tracer chip to record the behavior of IO ports, multiple tracer chips to record the behavior of DDR4 DIMMs, and a reconfigurable chip to verify these records with software states. As runtime external monitors, the Jintide would be especially suitable to constitute distributed large-scale clusters, which can amortize operation overheads. This scheme is effective in detecting pervasive hardware security issues, including vulnerabilities, backdoors, and hardware Trojans. The measured results show that a system composed of 300 000 Jintide CPUs containing Intel Xeon Skylake processors can detect over 99.8% of recognizable attacks at the cost of 0.98% performance loss. Hence, the Jintide is an extensible, low-cost, and effective solution to improve the hardware security of large-scale CPU clusters.",,IEEE
A model of digital identity for better information security in e-learning systems,"Dragan Korać, Boris Damjanović, Dejan Simić",The Journal of Supercomputing,2021-07-23,"<a href=""Springer (2021-07-23) : A model of digital identity for better information security in e-learning systems"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-021-03981-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-021-03981-4]</a>",The trend of rapid development of information technologies is creating new challenges in information security such as security management of...,,Springer
ESRFuzzer: an enhanced fuzzing framework for physical SOHO router devices to discover multi-Type vulnerabilities,"Yu Zhang, Wei Huo, ... Baoxu Liu",Cybersecurity,2021-07-19,"<a href=""Springer (2021-07-19) : ESRFuzzer: an enhanced fuzzing framework for physical SOHO router devices to discover multi-Type vulnerabilities"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-021-00091-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-021-00091-9]</a>","SOHO (small office/home office) routers provide services for end devices to connect to the Internet, playing an important role in cyberspace....",,Springer
Research on Key Technology of Industrial Network Boundary Protection based on Endogenous Security,F. Yu Q. Wei Y. Geng Y. Wang,"2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)",2021-07-19,"<a href=""IEEE (2021-07-19) : Research on Key Technology of Industrial Network Boundary Protection based on Endogenous Security"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9482240]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IMCEC51613.2021.9482240]</a>","Industrial network boundary protection equipment faces threats from attackers when protecting the industrial control system network. The similarity and static characteristics caused by large-scale and long-term deployment determine that it could only defend against known attacks but could not deal with unknown APT threats, which leads to the breakthrough of one defense line is equivalent to the breakthrough of all defense lines and may bring challenges to industrial production safety. This paper proposes a mimic defense model of industrial isolation gateway based on endogenous security. With the dynamic scheduling mechanism to transform the attack surface, the gateway selects multiple heterogeneous filter executors to process the same packet simultaneously. By comparing the processing results of each executor, anomaly detection is carried out to realize the dynamic defense of the industrial isolation gateway. The experimental results show that the industrial isolation gateway based on mimic architecture can significantly increase the difficulty of backdoor utilization, such as paralysis, rule tampering, and information theft, and effectively defend the industrial control system from the threats caused by the backdoors and vulnerabilities of the isolation gateway while exerting the normal boundary protection function.",,IEEE
Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting,"Xiangyu Qi, Jifeng Zhu, Chulin Xie, Yong Yang","arXiv
arXiv","2021-07-15
2021-07","<a href=""arXiv (2021-07-15) : Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting"" target=""_blank"">[http://arxiv.org/abs/2107.07240v1]</a>
<a href=""DBLP (2021-07) : Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting"" target=""_blank"">[https://arxiv.org/abs/2107.07240]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2107.07240]</a>","We study the realistic potential of conducting backdoor attack against deep neural networks (DNNs) during deployment stage. Specifically, our goal is to design a deployment-stage backdoor attack algorithm that is both threatening and realistically implementable. To this end, we propose Subnet Replacement Attack (SRA), which is capable of embedding backdoor into DNNs by directly modifying a limited number of model parameters. Considering the realistic practicability, we abandon the strong white-box assumption widely adopted in existing studies, instead, our algorithm works in a gray-box setting, where architecture information of the victim model is available but the adversaries do not have any knowledge of parameter values. The key philosophy underlying our approach is -- given any neural network instance (regardless of its specific parameter values) of a certain architecture, we can always embed a backdoor into that model instance, by replacing a very narrow subnet of a benign model (without backdoor) with a malicious backdoor subnet, which is designed to be sensitive (fire large activation value) to a particular backdoor trigger pattern.
","
","arXiv
DBLP"
Backdoor filter: Mitigating visible backdoor triggers in dataset,Wei Z.,"Proceedings 2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence, DTPI 2021",2021-07-15,"<a href=""ScienceDirect (2021-07-15) : Backdoor filter: Mitigating visible backdoor triggers in dataset"" target=""_blank"">[https://doi.org/10.1109/DTPI52967.2021.9540109]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/DTPI52967.2021.9540109]</a>",,,ScienceDirect
Implement of a secure selective ultrasonic microphone jammer,"Yike Chen, Ming Gao, ... Jinsong Han",CCF Transactions on Pervasive Computing and Interaction,2021-07-14,"<a href=""Springer (2021-07-14) : Implement of a secure selective ultrasonic microphone jammer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42486-021-00074-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42486-021-00074-2]</a>",Eavesdropping via microphones has been a serious threat to security and privacy. Recent advances in utilizing non-linearity property of microphone...,,Springer
AdvDoor: Adversarial backdoor attack of deep learning system,Zhang Q.,ISSTA 2021 - Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis,2021-07-11,"<a href=""ScienceDirect (2021-07-11) : AdvDoor: Adversarial backdoor attack of deep learning system"" target=""_blank"">[https://doi.org/10.1145/3460319.3464809]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3460319.3464809]</a>",,,ScienceDirect
"A survey on intrusion detection system: feature selection, model, performance measures, application perspective, challenges, and future research directions","Ankit Thakkar, Ritika Lohiya",Artificial Intelligence Review,2021-07-01,"<a href=""Springer (2021-07-01) : A survey on intrusion detection system: feature selection, model, performance measures, application perspective, challenges, and future research directions"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-021-10037-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-021-10037-9]</a>","With the increase in the usage of the Internet, a large amount of information is exchanged between different communicating devices. The data should...",,Springer
BaFFLe: Backdoor detection via feedback-based federated learning,Andreina S.,Proceedings - International Conference on Distributed Computing Systems,2021-07-01,"<a href=""ScienceDirect (2021-07-01) : BaFFLe: Backdoor detection via feedback-based federated learning"" target=""_blank"">[https://doi.org/10.1109/ICDCS51616.2021.00086]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICDCS51616.2021.00086]</a>",,,ScienceDirect
AdvDoor: adversarial backdoor attack of deep learning system,"Quan Zhang, Yifeng Ding, Yongqiang Tian, Jianmin Guo, Min Yuan, Yu Jiang","ISSTA 2021: Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis
ISSTA","2021-07
2021","<a href=""ACM (2021-07) : AdvDoor: adversarial backdoor attack of deep learning system"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3460319.3464809]</a>
<a href=""DBLP (2021) : AdvDoor: adversarial backdoor attack of deep learning system"" target=""_blank"">[https://doi.org/10.1145/3460319.3464809]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3460319.3464809]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3460319.3464809]</a>","Deep Learning (DL) system has been widely used in many critical applications, such as autonomous vehicles and unmanned aerial vehicles. However, their security is threatened by backdoor attack, which is achieved by adding artificial patterns on specific ...
","
","ACM
DBLP"
A N-binary Classification and Grouping-based Approach to Improve the Performance of Anomaly Detection,"Omkar Shende, R. K. Pateriya, Priyanka Verma",Arabian Journal for Science and Engineering,2021-06-30,"<a href=""Springer (2021-06-30) : A N-binary Classification and Grouping-based Approach to Improve the Performance of Anomaly Detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-021-05871-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-021-05871-6]</a>","In today’s world, the growth of computer networks is exponential as networking is an essential part of the latest technologies like Internet of...",,Springer
Explainability-based Backdoor Attacks against Graph Neural Networks,Xu J.,WiseML 2021 - Proceedings of the 3rd ACM Workshop on Wireless Security and Machine Learning,2021-06-28,"<a href=""ScienceDirect (2021-06-28) : Explainability-based Backdoor Attacks against Graph Neural Networks"" target=""_blank"">[https://doi.org/10.1145/3468218.3469046]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3468218.3469046]</a>",,,ScienceDirect
Inaudible Manipulation of Voice-Enabled Devices through BackDoor Using Robust Adversarial Audio Attacks: Invited Paper,Kasher M.,WiseML 2021 - Proceedings of the 3rd ACM Workshop on Wireless Security and Machine Learning,2021-06-28,"<a href=""ScienceDirect (2021-06-28) : Inaudible Manipulation of Voice-Enabled Devices through BackDoor Using Robust Adversarial Audio Attacks: Invited Paper"" target=""_blank"">[https://doi.org/10.1145/3468218.3469048]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3468218.3469048]</a>",,,ScienceDirect
A novel malicious remote administration tool using stealth and self-defense techniques,"Ioannis Kazoleas, Panagiotis Karampelas",International Journal of Information Security,2021-06-26,"<a href=""Springer (2021-06-26) : A novel malicious remote administration tool using stealth and self-defense techniques"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-021-00559-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-021-00559-2]</a>","As a result of technology advancement, the impact of threats against computer operating systems has increased significantly. The category of...",,Springer
Cyber-attack detection in healthcare using cyber-physical system and machine learning techniques,"Ahmad Ali AlZubi, Mohammed Al-Maitah, Abdulaziz Alarifi",Soft Computing,2021-06-26,"<a href=""Springer (2021-06-26) : Cyber-attack detection in healthcare using cyber-physical system and machine learning techniques"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-021-05926-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-021-05926-8]</a>",Cyber-physical systems have been extensively utilized in healthcare domains to deliver high-quality patient treatment in multifaceted clinical...,,Springer
BatFL: Backdoor Detection on Federated Learning in e-Health,Xi B.,"2021 IEEE/ACM 29th International Symposium on Quality of Service, IWQOS 2021",2021-06-25,"<a href=""ScienceDirect (2021-06-25) : BatFL: Backdoor Detection on Federated Learning in e-Health"" target=""_blank"">[https://doi.org/10.1109/IWQOS52092.2021.9521339]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/IWQOS52092.2021.9521339]</a>",,,ScienceDirect
Detecting Backdoor in Deep Neural Networks via Intentional Adversarial Perturbations,"Mingfu Xue, Yinghao Wu, Zhiyu Wu, Yushu Zhang, Jian Wang, Weiqiang Liu","arXiv
arXiv","2021-06-22
2021-05","<a href=""arXiv (2021-06-22) : Detecting Backdoor in Deep Neural Networks via Intentional Adversarial Perturbations"" target=""_blank"">[http://arxiv.org/abs/2105.14259v2]</a>
<a href=""DBLP (2021-05) : Detecting Backdoor in Deep Neural Networks via Intentional Adversarial Perturbations"" target=""_blank"">[https://arxiv.org/abs/2105.14259]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1016/j.ins.2023.03.112]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2105.14259]</a>","Recent researches show that deep learning model is susceptible to backdoor attacks. Many defenses against backdoor attacks have been proposed. However, existing defense works require high computational overhead or backdoor attack information such as the trigger size, which is difficult to satisfy in realistic scenarios. In this paper, a novel backdoor detection method based on adversarial examples is proposed. The proposed method leverages intentional adversarial perturbations to detect whether an image contains a trigger, which can be applied in both the training stage and the inference stage (sanitize the training set in training stage and detect the backdoor instances in inference stage). Specifically, given an untrusted image, the adversarial perturbation is added to the image intentionally. If the prediction of the model on the perturbed image is consistent with that on the unperturbed image, the input image will be considered as a backdoor instance. Compared with most existing defense works, the proposed adversarial perturbation based method requires low computational resources and maintains the visual quality of the images. Experimental results show that, the backdoor detection rate of the proposed defense method is 99.63%, 99.76% and 99.91% on Fashion-MNIST, CIFAR-10 and GTSRB datasets, respectively. Besides, the proposed method maintains the visual quality of the image as the l2 norm of the added perturbation are as low as 2.8715, 3.0513 and 2.4362 on Fashion-MNIST, CIFAR-10 and GTSRB datasets, respectively. In addition, it is also demonstrated that the proposed method can achieve high defense performance against backdoor attacks under different attack settings (trigger transparency, trigger size and trigger pattern). Compared with the existing defense work (STRIP), the proposed method has better detection performance on all the three datasets, and is more efficient than STRIP.
","
","arXiv
DBLP"
Defending against Backdoor Attack on Deep Neural Networks,"Kaidi Xu, Sijia Liu, Pin-Yu Chen, Pu Zhao, Xue Lin","arXiv
arXiv","2021-06-21
2020-02","<a href=""arXiv (2021-06-21) : Defending against Backdoor Attack on Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2002.12162v2]</a>
<a href=""DBLP (2020-02) : Defending against Backdoor Attack on Deep Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2002.12162]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2002.12162]</a>","Although deep neural networks (DNNs) have achieved a great success in various computer vision tasks, it is recently found that they are vulnerable to adversarial attacks. In this paper, we focus on the so-called \textit{backdoor attack}, which injects a backdoor trigger to a small portion of training data (also known as data poisoning) such that the trained DNN induces misclassification while facing examples with this trigger. To be specific, we carefully study the effect of both real and synthetic backdoor attacks on the internal response of vanilla and backdoored DNNs through the lens of Gard-CAM. Moreover, we show that the backdoor attack induces a significant bias in neuron activation in terms of the $\ell_\infty$ norm of an activation map compared to its $\ell_1$ and $\ell_2$ norm. Spurred by our results, we propose the \textit{$\ell_\infty$-based neuron pruning} to remove the backdoor from the backdoored DNN. Experiments show that our method could effectively decrease the attack success rate, and also hold a high classification accuracy for clean images.
","
","arXiv
DBLP"
FederatedReverse: A Detection and Defense Method against Backdoor Attacks in Federated Learning,Zhao C.,IH and MMSec 2021 - Proceedings of the 2021 ACM Workshop on Information Hiding and Multimedia Security,2021-06-17,"<a href=""ScienceDirect (2021-06-17) : FederatedReverse: A Detection and Defense Method against Backdoor Attacks in Federated Learning"" target=""_blank"">[https://doi.org/10.1145/3437880.3460403]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3437880.3460403]</a>",,,ScienceDirect
On the Robustness of Backdoor-based Watermarking in Deep Neural Networks,Shafieinejad M.,IH and MMSec 2021 - Proceedings of the 2021 ACM Workshop on Information Hiding and Multimedia Security,2021-06-17,"<a href=""ScienceDirect (2021-06-17) : On the Robustness of Backdoor-based Watermarking in Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1145/3437880.3460401]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3437880.3460401]</a>",,,ScienceDirect
Machine Learning with Electronic Health Records is vulnerable to Backdoor Trigger Attacks,"Byunggill Joe, Akshay Mehra, Insik Shin, Jihun Hamm","arXiv
arXiv","2021-06-15
2021-06","<a href=""arXiv (2021-06-15) : Machine Learning with Electronic Health Records is vulnerable to Backdoor Trigger Attacks"" target=""_blank"">[http://arxiv.org/abs/2106.07925v1]</a>
<a href=""DBLP (2021-06) : Machine Learning with Electronic Health Records is vulnerable to Backdoor Trigger Attacks"" target=""_blank"">[https://arxiv.org/abs/2106.07925]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2106.07925]</a>","Electronic Health Records (EHRs) provide a wealth of information for machine learning algorithms to predict the patient outcome from the data including diagnostic information, vital signals, lab tests, drug administration, and demographic information. Machine learning models can be built, for example, to evaluate patients based on their predicted mortality or morbidity and to predict required resources for efficient resource management in hospitals. In this paper, we demonstrate that an attacker can manipulate the machine learning predictions with EHRs easily and selectively at test time by backdoor attacks with the poisoned training data. Furthermore, the poison we create has statistically similar features to the original data making it hard to detect, and can also attack multiple machine learning models without any knowledge of the models. With less than 5% of the raw EHR data poisoned, we achieve average attack success rates of 97% on mortality prediction tasks with MIMIC-III database against Logistic Regression, Multilayer Perceptron, and Long Short-term Memory models simultaneously.
","
","arXiv
DBLP"
Survey on backdoor attacks targeted on neural network,Tan Q.,Chinese Journal of Network and Information Security,2021-06-15,"<a href=""ScienceDirect (2021-06-15) : Survey on backdoor attacks targeted on neural network"" target=""_blank"">[https://doi.org/10.11959/j.issn.2096-109x.2021053]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.11959/j.issn.2096-109x.2021053]</a>",,,ScienceDirect
HackerScope: the dynamics of a massive hacker online ecosystem,"Risul Islam, Md Omar Faruk Rokon, ... Michalis Faloutsos",Social Network Analysis and Mining,2021-06-14,"<a href=""Springer (2021-06-14) : HackerScope: the dynamics of a massive hacker online ecosystem"" target=""_blank"">[https://link.springer.com/article/10.1007/s13278-021-00758-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13278-021-00758-8]</a>","One would have thought that hackers would be striving to hide from public view, but we find that this is not the case: they have a public online...",,Springer
Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution,"Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, Maosong Sun","arXiv
ACL/IJCNLP
arXiv","2021-06-11
2021
2021-06","<a href=""arXiv (2021-06-11) : Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution"" target=""_blank"">[http://arxiv.org/abs/2106.06361v1]</a>
<a href=""DBLP (2021) : Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution"" target=""_blank"">[https://doi.org/10.18653/v1/2021.acl-long.377]</a>
<a href=""DBLP (2021-06) : Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution"" target=""_blank"">[https://arxiv.org/abs/2106.06361]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2021.acl-long.377]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2106.06361]</a>","Recent studies show that neural natural language processing (NLP) models are vulnerable to backdoor attacks. Injected with backdoors, models perform normally on benign examples but produce attacker-specified predictions when the backdoor is activated, presenting serious security threats to real-world applications. Since existing textual backdoor attacks pay little attention to the invisibility of backdoors, they can be easily detected and blocked. In this work, we present invisible backdoors that are activated by a learnable combination of word substitution. We show that NLP models can be injected with backdoors that lead to a nearly 100% attack success rate, whereas being highly invisible to existing defense strategies and even human inspections. The results raise a serious alarm to the security of NLP models, which requires further research to be resolved. All the data and code of this paper are released at https://github.com/thunlp/BkdAtk-LWS.

","<a href=""arXiv"" target=""_blank"">[https://github.com/thunlp/BkdAtk-LWS]</a>

","arXiv
DBLP
DBLP"
Backdoor attacks to graph neural networks,Zhang Z.,"Proceedings of ACM Symposium on Access Control Models and Technologies, SACMAT",2021-06-11,"<a href=""ScienceDirect (2021-06-11) : Backdoor attacks to graph neural networks"" target=""_blank"">[https://doi.org/10.1145/3450569.3463560]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3450569.3463560]</a>",,,ScienceDirect
Intrinsic Security and Self-Adaptive Cooperative Protection Enabling Cloud Native Network Slicing,W. Qiang W. Chunming Y. Xincheng C. Qiumei,IEEE Transactions on Network and Service Management,2021-06-10,"<a href=""IEEE (2021-06-10) : Intrinsic Security and Self-Adaptive Cooperative Protection Enabling Cloud Native Network Slicing"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9399165]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TNSM.2021.3071774]</a>","With the emergence of cloud native technology, the network slicing enables automatic service orchestration, flexible network scheduling and scalable network resource allocation, which profoundly affects the traditional security solution. Security is regarded as a technology independent of the cloud native architecture in the initial design, traditional passive defense such as “reinforced” and “stacked” is relied on to achieve system security protection. The lack of intrinsic security mechanisms makes the system capability insufficient when faces the uncertain threat brought by vulnerabilities and backdoors under the ecosystem of opening-up and sharing. The static nature of existing networks and computing systems makes them easy to be compromised and hard to defend, and thus it is urgent to provide intrinsic security and proactive protection against the unpredictable attacks. To this end, this paper proposes a novel paradigm named intrinsic cloud security (iCS) from the perspective of dynamic defense. The dynamic defense provides component-level security, and has complementary and consistency with the cloud native environment. In particular, iCS introduces mimic defense and moving target defense (MTD), and makes full use of the new features introduced by cloud native to implement an intrinsic and proactive defense mechanism with acceptable costs and efficiency. The iCS paradigm achieves seamless integration and symbiosis evolution between security and cloud native. We implement a trial of iCS based on 5GC commercial system and evaluate its performance on costs, efficiency and attack success. The result shows that the iCS enhanced mode always can provide a better and more stable defense effects.",,IEEE
Securing data in transit using data-in-transit defender architecture for cloud communication,"Keerthana Nandakumar, Viji Vinod, ... Sudhakar Sengan",Soft Computing,2021-06-10,"<a href=""Springer (2021-06-10) : Securing data in transit using data-in-transit defender architecture for cloud communication"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-021-05928-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-021-05928-6]</a>",The advent of cloud infrastructure in which third-party cloud services may retain sensitive consumer and company data in storage environments...,,Springer
Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger,"Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, Maosong Sun","arXiv
ACL/IJCNLP
arXiv","2021-06-03
2021
2021-05","<a href=""arXiv (2021-06-03) : Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger"" target=""_blank"">[http://arxiv.org/abs/2105.12400v2]</a>
<a href=""DBLP (2021) : Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger"" target=""_blank"">[https://doi.org/10.18653/v1/2021.acl-long.37]</a>
<a href=""DBLP (2021-05) : Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger"" target=""_blank"">[https://arxiv.org/abs/2105.12400]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2021.acl-long.37]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2105.12400]</a>","Backdoor attacks are a kind of insidious security threat against machine learning models. After being injected with a backdoor in training, the victim model will produce adversary-specified outputs on the inputs embedded with predesigned triggers but behave properly on normal inputs during inference. As a sort of emergent attack, backdoor attacks in natural language processing (NLP) are investigated insufficiently. As far as we know, almost all existing textual backdoor attack methods insert additional contents into normal samples as triggers, which causes the trigger-embedded samples to be detected and the backdoor attacks to be blocked without much effort. In this paper, we propose to use the syntactic structure as the trigger in textual backdoor attacks. We conduct extensive experiments to demonstrate that the syntactic trigger-based attack method can achieve comparable attack performance (almost 100% success rate) to the insertion-based methods but possesses much higher invisibility and stronger resistance to defenses. These results also reveal the significant insidiousness and harmfulness of textual backdoor attacks. All the code and data of this paper can be obtained at https://github.com/thunlp/HiddenKiller.

","<a href=""arXiv"" target=""_blank"">[https://github.com/thunlp/HiddenKiller]</a>

","arXiv
DBLP
DBLP"
Evaluation indicators for open-source software: a review,"Yuhang Zhao, Ruigang Liang, ... Jing Zou",Cybersecurity,2021-06-02,"<a href=""Springer (2021-06-02) : Evaluation indicators for open-source software: a review"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-021-00084-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-021-00084-8]</a>","In recent years, the widespread applications of open-source software (OSS) have brought great convenience for software developers. However, it is...",,Springer
A novel mechanism to handle address spoofing attacks in SDN based IoT,"Hamza Aldabbas, Rashid Amin",Cluster Computing,2021-06-01,"<a href=""Springer (2021-06-01) : A novel mechanism to handle address spoofing attacks in SDN based IoT"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-021-03309-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-021-03309-0]</a>","The Internet of Things (IoT) is a network of devices (servers, sensors, nodes, and so on) used to conduct tasks like health monitoring, production...",,Springer
Explainability-based Backdoor Attacks Against Graph Neural Networks,"Jing Xu, Minhui (Jason) Xue, Stjepan Picek","WiseML '21: Proceedings of the 3rd ACM Workshop on Wireless Security and Machine Learning
arXiv
WiseML@WiSec
arXiv","2021-06
2021-07-13
2021
2021-04","<a href=""ACM (2021-06) : Explainability-based Backdoor Attacks Against Graph Neural Networks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3468218.3469046]</a>
<a href=""arXiv (2021-07-13) : Explainability-based Backdoor Attacks Against Graph Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2104.03674v2]</a>
<a href=""DBLP (2021) : Explainability-based Backdoor Attacks Against Graph Neural Networks"" target=""_blank"">[https://doi.org/10.1145/3468218.3469046]</a>
<a href=""DBLP (2021-04) : Explainability-based Backdoor Attacks Against Graph Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2104.03674]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3468218.3469046]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3468218.3469046]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2104.03674]</a>","Backdoor attacks represent a serious threat to neural network models. A backdoored model will misclassify the trigger-embedded inputs into an attacker-chosen target label while performing normally on other benign inputs. There are already numerous works ...
Backdoor attacks represent a serious threat to neural network models. A backdoored model will misclassify the trigger-embedded inputs into an attacker-chosen target label while performing normally on other benign inputs. There are already numerous works on backdoor attacks on neural networks, but only a few works consider graph neural networks (GNNs). As such, there is no intensive research on explaining the impact of trigger injecting position on the performance of backdoor attacks on GNNs. To bridge this gap, we conduct an experimental investigation on the performance of backdoor attacks on GNNs. We apply two powerful GNN explainability approaches to select the optimal trigger injecting position to achieve two attacker objectives -- high attack success rate and low clean accuracy drop. Our empirical results on benchmark datasets and state-of-the-art neural network models demonstrate the proposed method's effectiveness in selecting trigger injecting position for backdoor attacks on GNNs. For instance, on the node classification task, the backdoor attack with trigger injecting position selected by GraphLIME reaches over $84 \%$ attack success rate with less than $2.5 \%$ accuracy drop

","


","ACM
arXiv
DBLP
DBLP"
FederatedReverse: A Detection and Defense Method Against Backdoor Attacks in Federated Learning,"Chen Zhao, Yu Wen, Shuailou Li, Fucheng Liu, Dan Meng","IH&MMSec '21: Proceedings of the 2021 ACM Workshop on Information Hiding and Multimedia Security
IH&amp,MMSec","2021-06
2021","<a href=""ACM (2021-06) : FederatedReverse: A Detection and Defense Method Against Backdoor Attacks in Federated Learning"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3437880.3460403]</a>
<a href=""DBLP (2021) : FederatedReverse: A Detection and Defense Method Against Backdoor Attacks in Federated Learning"" target=""_blank"">[https://doi.org/10.1145/3437880.3460403]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3437880.3460403]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3437880.3460403]</a>","Federated learning is a secure machine learning technology proposed to protect data privacy and security in machine learning model training. However, recent studies show that federated learning is vulnerable to backdoor attacks, such as model ...
","
","ACM
DBLP"
Inaudible Manipulation of Voice-Enabled Devices Through BackDoor Using Robust Adversarial Audio Attacks: Invited Paper,"Morriel Kasher, Michael Zhao, Aryeh Greenberg, Devin Gulati, Silvija Kokalj-Filipovic, Predrag Spasojevic","WiseML '21: Proceedings of the 3rd ACM Workshop on Wireless Security and Machine Learning
WiseML@WiSec","2021-06
2021","<a href=""ACM (2021-06) : Inaudible Manipulation of Voice-Enabled Devices Through BackDoor Using Robust Adversarial Audio Attacks: Invited Paper"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3468218.3469048]</a>
<a href=""DBLP (2021) : Inaudible Manipulation of Voice-Enabled Devices Through BackDoor Using Robust Adversarial Audio Attacks: Invited Paper"" target=""_blank"">[https://doi.org/10.1145/3468218.3469048]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3468218.3469048]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3468218.3469048]</a>","The BackDoor system provides a method for inaudibly transmitting messages that are recorded by unmodified receiver microphones as if they were transmitted audibly. Adversarial Audio attacks allow for an audio sample to sound like one message but be ...
","
","ACM
DBLP"
Backdoor Attacks to Graph Neural Networks,"Zaixi Zhang, Jinyuan Jia, Binghui Wang, Neil Zhenqiang Gong","SACMAT '21: Proceedings of the 26th ACM Symposium on Access Control Models and Technologies

arXiv
SACMAT
arXiv","2021-06

2021-12-17
2021
2020-06","<a href=""ACM (2021-06) : Backdoor Attacks to Graph Neural Networks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3450569.3463560]</a>
<a href=""OpenReview () : Backdoor Attacks to Graph Neural Networks"" target=""_blank"">[https://openreview.net/pdf/e6320840daebea8db9e09c0248ab2cc99966dc5d.pdf]</a>
<a href=""arXiv (2021-12-17) : Backdoor Attacks to Graph Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2006.11165v4]</a>
<a href=""DBLP (2021) : Backdoor Attacks to Graph Neural Networks"" target=""_blank"">[https://doi.org/10.1145/3450569.3463560]</a>
<a href=""DBLP (2020-06) : Backdoor Attacks to Graph Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2006.11165]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3450569.3463560]</a>
<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/e6320840daebea8db9e09c0248ab2cc99966dc5d.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3450569.3463560]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2006.11165]</a>","In this work, we propose the first backdoor attack to graph neural networks (GNN). Specifically, we propose a subgraph based backdoor attack to GNN for graph classification. In our backdoor attack, a GNN classifier predicts an attacker-chosen target ...
In this work, we propose the first backdoor attack to graph neural networks (GNN). Specifically, we propose a \emph{subgraph based backdoor attack} to GNN for graph classification. In our backdoor attack, a GNN classifier predicts an attacker-chosen target label for a testing graph once a predefined subgraph is injected to the testing graph. Our empirical results on three real-world graph datasets show that our backdoor attacks are effective with a small impact on a GNN's prediction accuracy for clean testing graphs.
In this work, we propose the first backdoor attack to graph neural networks (GNN). Specifically, we propose a \emph{subgraph based backdoor attack} to GNN for graph classification. In our backdoor attack, a GNN classifier predicts an attacker-chosen target label for a testing graph once a predefined subgraph is injected to the testing graph. Our empirical results on three real-world graph datasets show that our backdoor attacks are effective with a small impact on a GNN's prediction accuracy for clean testing graphs. Moreover, we generalize a randomized smoothing based certified defense to defend against our backdoor attacks. Our empirical results show that the defense is effective in some cases but ineffective in other cases, highlighting the needs of new defenses for our backdoor attacks.

","



","ACM
OpenReview
arXiv
DBLP
DBLP"
Graph Embedding for Recommendation against Attribute Inference Attacks,"Shijie Zhang, Hongzhi Yin, Tong Chen, Zi Huang, Lizhen Cui, Xiangliang Zhang",WWW '21: Proceedings of the Web Conference 2021,2021-06,"<a href=""ACM (2021-06) : Graph Embedding for Recommendation against Attribute Inference Attacks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3442381.3449813]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3442381.3449813]</a>","In recent years, recommender systems play a pivotal role in helping users identify the most suitable items that satisfy personal preferences. As user-item interactions can be naturally modelled as graph-structured data, variants of graph convolutional ...",,ACM
A Novel Trojan Attack against Co-learning Based ASR DNN System,M. Li X. Wang D. Huo H. Wang C. Liu Y. Wang Y. Wang Z. Xu,2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD),2021-05-28,"<a href=""IEEE (2021-05-28) : A Novel Trojan Attack against Co-learning Based ASR DNN System"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9437669]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/CSCWD49262.2021.9437669]</a>","ASR (Automatic Speech Recognition) technology is a key technology for human-computer interaction. Especially the DNN models of wake-up-word speech recognition, which enables the smart device to recognize wake-up words spoken by users when they are in the sleep or lock screen state, allowing the device to directly enter the wait command state, and start the first step of voice interaction. ASR technology no wonder provides great convenience for people's daily life, but it's security problem has always been a hot topic of further research. The rapid development of ASR models has made these models very vulnerable which seriously affect their performance in real scenarios. This paper proposes a new backdoor attack method TNN (Trojan Neural Network) for deep learning models in voice wake-up scenarios. The attacker leaves backdoor in the deep learning model of the ASR. Besides specific wakeup words, attackers can also use other words to force the device to wake up and get the IoT smart device's highest privileges. And when the smart device is in the awake state, any audio containing a particular vocabulary will be recognized as a specific command and executed. In this papaer, we propose a novel method to attack ASR model, and without affecting the performance of clean samples, by applying the attack method, the success rate in compulsory recognition of specific words can be up to 100%. The experimental results prove that our attack method is very effective and poses a great security problem of the voice-interactive IoT device.",,IEEE
A robust intelligent zero-day cyber-attack detection technique,"Vikash Kumar, Ditipriya Sinha",Complex & Intelligent Systems,2021-05-28,"<a href=""Springer (2021-05-28) : A robust intelligent zero-day cyber-attack detection technique"" target=""_blank"">[https://link.springer.com/article/10.1007/s40747-021-00396-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s40747-021-00396-9]</a>","With the introduction of the Internet to the mainstream like e-commerce, online banking, health system and other day-to-day essentials, risk of being...",,Springer
Multi-layer perceptron for network intrusion detection,"Arnaud Rosay, Kévin Riou, ... Pascal Leroux",Annals of Telecommunications,2021-05-28,"<a href=""Springer (2021-05-28) : Multi-layer perceptron for network intrusion detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-021-00852-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-021-00852-0]</a>","The Internet connection is becoming ubiquitous in embedded systems, making them potential victims of intrusion. Although gaining popularity in recent...",,Springer
An improved text classification modelling approach to identify security messages in heterogeneous projects,"Tosin Daniel Oyetoyan, Patrick Morrison",Software Quality Journal,2021-05-27,"<a href=""Springer (2021-05-27) : An improved text classification modelling approach to identify security messages in heterogeneous projects"" target=""_blank"">[https://link.springer.com/article/10.1007/s11219-020-09546-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11219-020-09546-7]</a>","Security remains under-addressed in many organisations, illustrated by the number of large-scale software security breaches. Preventing breaches can...",,Springer
Hybrid semantic deep learning architecture and optimal advanced encryption standard key management scheme for secure cloud storage and intrusion detection,"Varun Prabhakaran, Ashokkumar Kulandasamy",Neural Computing and Applications,2021-05-27,"<a href=""Springer (2021-05-27) : Hybrid semantic deep learning architecture and optimal advanced encryption standard key management scheme for secure cloud storage and intrusion detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-021-06085-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-021-06085-5]</a>","Cloud computing helps users to store and retrieve their data in the cloud online on an as-per-pay basis anytime, anywhere in the world. As a...",,Springer
An effective NIDS framework based on a comprehensive survey of feature optimization and classification techniques,"Pankaj Kumar Keserwani, Mahesh Chandra Govil, Emmanuel S. Pilli",Neural Computing and Applications,2021-05-22,"<a href=""Springer (2021-05-22) : An effective NIDS framework based on a comprehensive survey of feature optimization and classification techniques"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-021-06093-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-021-06093-5]</a>",The technological advancement leads to an increase in the usage of the Internet with many applications and connected devices. This increased network...,,Springer
Prioritizing refactorings for security-critical code,"Chaima Abid, Vahid Alizadeh, ... Rick Kazman",Automated Software Engineering,2021-05-18,"<a href=""Springer (2021-05-18) : Prioritizing refactorings for security-critical code"" target=""_blank"">[https://link.springer.com/article/10.1007/s10515-021-00281-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10515-021-00281-2]</a>",It is vitally important to fix quality issues in security-critical code as they may be sources of vulnerabilities in the future. These quality issues...,,Springer
DeepMal: maliciousness-Preserving adversarial instruction learning against static malware detection,"Chun Yang, Jinghui Xu, ... Dan Meng",Cybersecurity,2021-05-14,"<a href=""Springer (2021-05-14) : DeepMal: maliciousness-Preserving adversarial instruction learning against static malware detection"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-021-00079-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-021-00079-5]</a>","Outside the explosive successful applications of deep learning (DL) in natural language processing, computer vision, and information retrieval, there...",,Springer
Optimum-path forest stacking-based ensemble for intrusion detection,"Mateus A. Bertoni, Gustavo H. de Rosa, Jose R. F. Brega",Evolutionary Intelligence,2021-05-12,"<a href=""Springer (2021-05-12) : Optimum-path forest stacking-based ensemble for intrusion detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s12065-021-00609-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12065-021-00609-7]</a>","Machine learning techniques have been extensively researched in the last years, mainly due to their effectiveness when dealing with recognition or...",,Springer
Poisoning MorphNet for Clean-Label Backdoor Attack to Point Clouds,"Guiyu Tian, Wenhao Jiang, Wei Liu, Yadong Mu","arXiv
arXiv","2021-05-11
2021-05","<a href=""arXiv (2021-05-11) : Poisoning MorphNet for Clean-Label Backdoor Attack to Point Clouds"" target=""_blank"">[http://arxiv.org/abs/2105.04839v1]</a>
<a href=""DBLP (2021-05) : Poisoning MorphNet for Clean-Label Backdoor Attack to Point Clouds"" target=""_blank"">[https://arxiv.org/abs/2105.04839]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2105.04839]</a>","This paper presents Poisoning MorphNet, the first backdoor attack method on point clouds. Conventional adversarial attack takes place in the inference stage, often fooling a model by perturbing samples. In contrast, backdoor attack aims to implant triggers into a model during the training stage, such that the victim model acts normally on the clean data unless a trigger is present in a sample. This work follows a typical setting of clean-label backdoor attack, where a few poisoned samples (with their content tampered yet labels unchanged) are injected into the training set. The unique contributions of MorphNet are two-fold. First, it is key to ensure the implanted triggers both visually imperceptible to humans and lead to high attack success rate on the point clouds. To this end, MorphNet jointly optimizes two objectives for sample-adaptive poisoning: a reconstruction loss that preserves the visual similarity between benign / poisoned point clouds, and a classification loss that enforces a modern recognition model of point clouds tends to mis-classify the poisoned sample to a pre-specified target category. This implicitly conducts spectral separation over point clouds, hiding sample-adaptive triggers in fine-grained high-frequency details. Secondly, existing backdoor attack methods are mainly designed for image data, easily defended by some point cloud specific operations (such as denoising). We propose a third loss in MorphNet for suppressing isolated points, leading to improved resistance to denoising-based defense. Comprehensive evaluations are conducted on ModelNet40 and ShapeNetcorev2. Our proposed Poisoning MorphNet outstrips all previous methods with clear margins.
","
","arXiv
DBLP"
Invisible poison: A blackbox clean label backdoor attack to deep neural networks,Ning R.,Proceedings - IEEE INFOCOM,2021-05-10,"<a href=""ScienceDirect (2021-05-10) : Invisible poison: A blackbox clean label backdoor attack to deep neural networks"" target=""_blank"">[https://doi.org/10.1109/INFOCOM42981.2021.9488902]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/INFOCOM42981.2021.9488902]</a>",,,ScienceDirect
A novel method for malware detection based on hardware events using deep neural networks,"Hadis Ghanei, Farnoush Manavi, Ali Hamzeh",Journal of Computer Virology and Hacking Techniques,2021-05-08,"<a href=""Springer (2021-05-08) : A novel method for malware detection based on hardware events using deep neural networks"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-021-00386-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-021-00386-y]</a>","With the increasing availability of internet access, the number of malware is growing dramatically. So, defence against malware is an important issue...",,Springer
Research on the Impact of Performance Compensation Commitment on Backdoor Listing Companies Earnings Management,Menghan Z.,E3S Web of Conferences,2021-05-06,"<a href=""ScienceDirect (2021-05-06) : Research on the Impact of Performance Compensation Commitment on Backdoor Listing Companies Earnings Management"" target=""_blank"">[https://doi.org/10.1051/e3sconf/202125302066]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1051/e3sconf/202125302066]</a>",,,ScienceDirect
A Master Key Backdoor for Universal Impersonation Attack against DNN-based Face Verification,"Wei Guo, Benedetta Tondi, Mauro Barni","arXiv
arXiv","2021-05-01
2021-05","<a href=""arXiv (2021-05-01) : A Master Key Backdoor for Universal Impersonation Attack against DNN-based Face Verification"" target=""_blank"">[http://arxiv.org/abs/2105.00249v1]</a>
<a href=""DBLP (2021-05) : A Master Key Backdoor for Universal Impersonation Attack against DNN-based Face Verification"" target=""_blank"">[https://arxiv.org/abs/2105.00249]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1016/j.patrec.2021.01.009]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2105.00249]</a>","We introduce a new attack against face verification systems based on Deep Neural Networks (DNN). The attack relies on the introduction into the network of a hidden backdoor, whose activation at test time induces a verification error allowing the attacker to impersonate any user. The new attack, named Master Key backdoor attack, operates by interfering with the training phase, so to instruct the DNN to always output a positive verification answer when the face of the attacker is presented at its input. With respect to existing attacks, the new backdoor attack offers much more flexibility, since the attacker does not need to know the identity of the victim beforehand. In this way, he can deploy a Universal Impersonation attack in an open-set framework, allowing him to impersonate any enrolled users, even those that were not yet enrolled in the system when the attack was conceived. We present a practical implementation of the attack targeting a Siamese-DNN face verification system, and show its effectiveness when the system is trained on VGGFace2 dataset and tested on LFW and YTF datasets. According to our experiments, the Master Key backdoor attack provides a high attack success rate even when the ratio of poisoned training data is as small as 0.01, thus raising a new alarm regarding the use of DNN-based face verification systems in security-critical applications.
","
","arXiv
DBLP"
Backdoors hidden in facial features: a novel invisible backdoor attack against face recognition systems,Xue M.,Peer-to-Peer Networking and Applications,2021-05-01,"<a href=""ScienceDirect (2021-05-01) : Backdoors hidden in facial features: a novel invisible backdoor attack against face recognition systems"" target=""_blank"">[https://doi.org/10.1007/s12083-020-01031-z]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/s12083-020-01031-z]</a>",,,ScienceDirect
DeepPayload: Black-box backdoor attack on deep learning models through neural payload injection,Li Y.,Proceedings - International Conference on Software Engineering,2021-05-01,"<a href=""ScienceDirect (2021-05-01) : DeepPayload: Black-box backdoor attack on deep learning models through neural payload injection"" target=""_blank"">[https://doi.org/10.1109/ICSE43902.2021.00035]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICSE43902.2021.00035]</a>",,,ScienceDirect
Research and Challenge of Distributed Deep Learning Privacy and Security Attack,Zhou C.,Jisuanji Yanjiu yu Fazhan/Computer Research and Development,2021-05-01,"<a href=""ScienceDirect (2021-05-01) : Research and Challenge of Distributed Deep Learning Privacy and Security Attack"" target=""_blank"">[https://doi.org/10.7544/issn1000-1239.2021.20200966]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.7544/issn1000-1239.2021.20200966]</a>",,,ScienceDirect
Stealthy Backdoors as Compression Artifacts,"Yulong Tian, Fnu Suya, Fengyuan Xu, David Evans","arXiv
IEEE Trans. Inf. Forensics Secur.
arXiv","2021-04-30
2022
2021-04","<a href=""arXiv (2021-04-30) : Stealthy Backdoors as Compression Artifacts"" target=""_blank"">[http://arxiv.org/abs/2104.15129v1]</a>
<a href=""DBLP (2022) : Stealthy Backdoors as Compression Artifacts"" target=""_blank"">[https://doi.org/10.1109/TIFS.2022.3160359]</a>
<a href=""DBLP (2021-04) : Stealthy Backdoors as Compression Artifacts"" target=""_blank"">[https://arxiv.org/abs/2104.15129]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2022.3160359]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2104.15129]</a>","In a backdoor attack on a machine learning model, an adversary produces a model that performs well on normal inputs but outputs targeted misclassifications on inputs containing a small trigger pattern. Model compression is a widely-used approach for reducing the size of deep learning models without much accuracy loss, enabling resource-hungry models to be compressed for use on resource-constrained devices. In this paper, we study the risk that model compression could provide an opportunity for adversaries to inject stealthy backdoors. We design stealthy backdoor attacks such that the full-sized model released by adversaries appears to be free from backdoors (even when tested using state-of-the-art techniques), but when the model is compressed it exhibits highly effective backdoors. We show this can be done for two common model compression techniques -- model pruning and model quantization. Our findings demonstrate how an adversary may be able to hide a backdoor as a compression artifact, and show the importance of performing security tests on the models that will actually be deployed not their precompressed version.

","

","arXiv
DBLP
DBLP"
APAE: an IoT intrusion detection system using asymmetric parallel auto-encoder,"Amir Basati, Mohammad Mehdi Faghih",Neural Computing and Applications,2021-04-29,"<a href=""Springer (2021-04-29) : APAE: an IoT intrusion detection system using asymmetric parallel auto-encoder"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-021-06011-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-021-06011-9]</a>","In recent years, the world has dramatically moved toward using the internet of things (IoT), and the IoT has become a hot research field. Among...",,Springer
Backdoor Attack in the Physical World,"Yiming Li, Tongqing Zhai, Yong Jiang, Zhifeng Li, Shu-Tao Xia","arXiv
arXiv","2021-04-24
2021-04","<a href=""arXiv (2021-04-24) : Backdoor Attack in the Physical World"" target=""_blank"">[http://arxiv.org/abs/2104.02361v2]</a>
<a href=""DBLP (2021-04) : Backdoor Attack in the Physical World"" target=""_blank"">[https://arxiv.org/abs/2104.02361]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2104.02361]</a>","Backdoor attack intends to inject hidden backdoor into the deep neural networks (DNNs), such that the prediction of infected models will be maliciously changed if the hidden backdoor is activated by the attacker-defined trigger. Currently, most existing backdoor attacks adopted the setting of static trigger, $i.e.,$ triggers across the training and testing images follow the same appearance and are located in the same area. In this paper, we revisit this attack paradigm by analyzing trigger characteristics. We demonstrate that this attack paradigm is vulnerable when the trigger in testing images is not consistent with the one used for training. As such, those attacks are far less effective in the physical world, where the location and appearance of the trigger in the digitized image may be different from that of the one used for training. Moreover, we also discuss how to alleviate such vulnerability. We hope that this work could inspire more explorations on backdoor properties, to help the design of more advanced backdoor attack and defense methods.
","
","arXiv
DBLP"
Automatic Permission Optimization Framework for Privacy Enhancement of Mobile Applications,Y. Qu S. Du S. Li Y. Meng L. Zhang H. Zhu,IEEE Internet of Things Journal,2021-04-23,"<a href=""IEEE (2021-04-23) : Automatic Permission Optimization Framework for Privacy Enhancement of Mobile Applications"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9270036]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/JIOT.2020.3039472]</a>","Mobile applications play a crucial role in the IoT system, which is experiencing unprecedented growth. However, users possessing little knowledge of permission configurations often accept app permission requests without reading them, which opens a backdoor for the potential adversaries to launch the future attacks. Proposing an automatic permission management scheme is an attractive solution to solve this issue, but since users have varying attitudes toward privacy, such a scheme would be neither straightforward nor user friendly. In this study, an automatic permission optimization framework, Permizer, is proposed to recommend different app permission configurations to users with different privacy preferences. Permizer estimates the permission risks and builds the permission-functionality mapping to each app, then regulates the relationship between permission and app functionality. Permizer is the first module to achieve a balance between privacy protection and app functionality under the personal privacy preference condition. Finally, we develop Permizer as a one-button service on the real-world Android OS with 58 apps. Case studies conducted on TikTok and Amazon Alexa also demonstrate its practicability and effectiveness.",,IEEE
SPECTRE: Defending Against Backdoor Attacks Using Robust Statistics,"Jonathan Hayase, Weihao Kong, Raghav Somani, Sewoong Oh","arXiv
arXiv","2021-04-22
2021-04","<a href=""arXiv (2021-04-22) : SPECTRE: Defending Against Backdoor Attacks Using Robust Statistics"" target=""_blank"">[http://arxiv.org/abs/2104.11315v1]</a>
<a href=""DBLP (2021-04) : SPECTRE: Defending Against Backdoor Attacks Using Robust Statistics"" target=""_blank"">[https://arxiv.org/abs/2104.11315]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2104.11315]</a>","Modern machine learning increasingly requires training on a large collection of data from multiple sources, not all of which can be trusted. A particularly concerning scenario is when a small fraction of poisoned data changes the behavior of the trained model when triggered by an attacker-specified watermark. Such a compromised model will be deployed unnoticed as the model is accurate otherwise. There have been promising attempts to use the intermediate representations of such a model to separate corrupted examples from clean ones. However, these defenses work only when a certain spectral signature of the poisoned examples is large enough for detection. There is a wide range of attacks that cannot be protected against by the existing defenses. We propose a novel defense algorithm using robust covariance estimation to amplify the spectral signature of corrupted data. This defense provides a clean model, completely removing the backdoor, even in regimes where previous methods have no hope of detecting the poisoned examples. Code and pre-trained models are available at https://github.com/SewoongLab/spectre-defense .
","<a href=""arXiv"" target=""_blank"">[https://github.com/SewoongLab/spectre-defense]</a>
","arXiv
DBLP"
Graph embedding for recommendation against attribute inference attacks,Zhang S.,"The Web Conference 2021 - Proceedings of the World Wide Web Conference, WWW 2021",2021-04-19,"<a href=""ScienceDirect (2021-04-19) : Graph embedding for recommendation against attribute inference attacks"" target=""_blank"">[https://doi.org/10.1145/3442381.3449813]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3442381.3449813]</a>",,,ScienceDirect
BaFFLe: Backdoor detection via Feedback-based Federated Learning,"Sebastien Andreina, Giorgia Azzurra Marson, Helen Möllering, Ghassan Karame","arXiv
ICDCS
arXiv","2021-04-18
2021
2020-11","<a href=""arXiv (2021-04-18) : BaFFLe: Backdoor detection via Feedback-based Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2011.02167v2]</a>
<a href=""DBLP (2021) : BaFFLe: Backdoor Detection via Feedback-based Federated Learning"" target=""_blank"">[https://doi.org/10.1109/ICDCS51616.2021.00086]</a>
<a href=""DBLP (2020-11) : BaFFLe: Backdoor detection via Feedback-based Federated Learning"" target=""_blank"">[https://arxiv.org/abs/2011.02167]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICDCS51616.2021.00086]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2011.02167]</a>","Recent studies have shown that federated learning (FL) is vulnerable to poisoning attacks that inject a backdoor into the global model. These attacks are effective even when performed by a single client, and undetectable by most existing defensive techniques. In this paper, we propose Backdoor detection via Feedback-based Federated Learning (BAFFLE), a novel defense to secure FL against backdoor attacks. The core idea behind BAFFLE is to leverage data of multiple clients not only for training but also for uncovering model poisoning. We exploit the availability of diverse datasets at the various clients by incorporating a feedback loop into the FL process, to integrate the views of those clients when deciding whether a given model update is genuine or not. We show that this powerful construct can achieve very high detection rates against state-of-the-art backdoor attacks, even when relying on straightforward methods to validate the model. Through empirical evaluation using the CIFAR-10 and FEMNIST datasets, we show that by combining the feedback loop with a method that suspects poisoning attempts by assessing the per-class classification performance of the updated model, BAFFLE reliably detects state-of-the-art backdoor attacks with a detection accuracy of 100% and a false-positive rate below 5%. Moreover, we show that our solution can detect adaptive attacks aimed at bypassing the defense.

","

","arXiv
DBLP
DBLP"
Detecting scene-plausible perceptible backdoors in trained dnns without access to the training set,Xiang Z.,Neural Computation,2021-04-13,"<a href=""ScienceDirect (2021-04-13) : Detecting scene-plausible perceptible backdoors in trained dnns without access to the training set"" target=""_blank"">[https://doi.org/10.1162/neco_a_01376]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1162/neco_a_01376]</a>",,,ScienceDirect
A Backdoor Attack against 3D Point Cloud Classifiers,"Zhen Xiang, David J. Miller, Siheng Chen, Xi Li, George Kesidis","arXiv
ICCV
arXiv","2021-04-12
2021
2021-04","<a href=""arXiv (2021-04-12) : A Backdoor Attack against 3D Point Cloud Classifiers"" target=""_blank"">[http://arxiv.org/abs/2104.05808v1]</a>
<a href=""DBLP (2021) : A Backdoor Attack against 3D Point Cloud Classifiers"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.00750]</a>
<a href=""DBLP (2021-04) : A Backdoor Attack against 3D Point Cloud Classifiers"" target=""_blank"">[https://arxiv.org/abs/2104.05808]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.00750]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2104.05808]</a>","Vulnerability of 3D point cloud (PC) classifiers has become a grave concern due to the popularity of 3D sensors in safety-critical applications. Existing adversarial attacks against 3D PC classifiers are all test-time evasion (TTE) attacks that aim to induce test-time misclassifications using knowledge of the classifier. But since the victim classifier is usually not accessible to the attacker, the threat is largely diminished in practice, as PC TTEs typically have poor transferability. Here, we propose the first backdoor attack (BA) against PC classifiers. Originally proposed for images, BAs poison the victim classifier's training set so that the classifier learns to decide to the attacker's target class whenever the attacker's backdoor pattern is present in a given input sample. Significantly, BAs do not require knowledge of the victim classifier. Different from image BAs, we propose to insert a cluster of points into a PC as a robust backdoor pattern customized for 3D PCs. Such clusters are also consistent with a physical attack (i.e., with a captured object in a scene). We optimize the cluster's location using an independently trained surrogate classifier and choose the cluster's local geometry to evade possible PC preprocessing and PC anomaly detectors (ADs). Experimentally, our BA achieves a uniformly high success rate (> 87%) and shows evasiveness against state-of-the-art PC ADs.

","

","arXiv
DBLP
DBLP"
DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations,"Eitan Borgnia, Jonas Geiping, Valeriia Cherepanova, Liam Fowl, Arjun Gupta, Amin Ghiasi, Furong Huang, Micah Goldblum, Tom Goldstein","arXiv
arXiv","2021-04-08
2021-03","<a href=""arXiv (2021-04-08) : DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations"" target=""_blank"">[http://arxiv.org/abs/2103.02079v2]</a>
<a href=""DBLP (2021-03) : DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations"" target=""_blank"">[https://arxiv.org/abs/2103.02079]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2103.02079]</a>","Data poisoning and backdoor attacks manipulate training data to induce security breaches in a victim model. These attacks can be provably deflected using differentially private (DP) training methods, although this comes with a sharp decrease in model performance. The InstaHide method has recently been proposed as an alternative to DP training that leverages supposed privacy properties of the mixup augmentation, although without rigorous guarantees. In this work, we show that strong data augmentations, such as mixup and random additive noise, nullify poison attacks while enduring only a small accuracy trade-off. To explain these finding, we propose a training method, DP-InstaHide, which combines the mixup regularizer with additive noise. A rigorous analysis of DP-InstaHide shows that mixup does indeed have privacy advantages, and that training with k-way mixup provably yields at least k times stronger DP guarantees than a naive DP mechanism. Because mixup (as opposed to noise) is beneficial to model performance, DP-InstaHide provides a mechanism for achieving stronger empirical performance against poisoning attacks than other known DP methods.
","
","arXiv
DBLP"
Hidden Backdoor Attack against Semantic Segmentation Models,"Yiming Li, Yanjie Li, Yalei Lv, Yong Jiang, Shu-Tao Xia","arXiv
arXiv","2021-04-03
2021-03","<a href=""arXiv (2021-04-03) : Hidden Backdoor Attack against Semantic Segmentation Models"" target=""_blank"">[http://arxiv.org/abs/2103.04038v3]</a>
<a href=""DBLP (2021-03) : Hidden Backdoor Attack against Semantic Segmentation Models"" target=""_blank"">[https://arxiv.org/abs/2103.04038]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2103.04038]</a>","Deep neural networks (DNNs) are vulnerable to the \emph{backdoor attack}, which intends to embed hidden backdoors in DNNs by poisoning training data. The attacked model behaves normally on benign samples, whereas its prediction will be changed to a particular target label if hidden backdoors are activated. So far, backdoor research has mostly been conducted towards classification tasks. In this paper, we reveal that this threat could also happen in semantic segmentation, which may further endanger many mission-critical applications ($e.g.$, autonomous driving). Except for extending the existing attack paradigm to maliciously manipulate the segmentation models from the image-level, we propose a novel attack paradigm, the \emph{fine-grained attack}, where we treat the target label ($i.e.$, annotation) from the object-level instead of the image-level to achieve more sophisticated manipulation. In the annotation of poisoned samples generated by the fine-grained attack, only pixels of specific objects will be labeled with the attacker-specified target class while others are still with their ground-truth ones. Experiments show that the proposed methods can successfully attack semantic segmentation models by poisoning only a small proportion of training data. Our method not only provides a new perspective for designing novel attacks but also serves as a strong baseline for improving the robustness of semantic segmentation methods.
","
","arXiv
DBLP"
Research on Deep Learning-Powered Malware Attack and Defense Techniques,Ji T.T.,Jisuanji Xuebao/Chinese Journal of Computers,2021-04-01,"<a href=""ScienceDirect (2021-04-01) : Research on Deep Learning-Powered Malware Attack and Defense Techniques"" target=""_blank"">[https://doi.org/10.11897/SP.J.1016.2021.00669]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.11897/SP.J.1016.2021.00669]</a>",,,ScienceDirect
The Design and Development of a Game to Study Backdoor Poisoning Attacks: The Backdoor Game,"Zahra Ashktorab, Casey Dugan, James Johnson, Aabhas Sharma, Dustin Ramsey Torres, Ingrid Lange, Benjamin Hoover, Heiko Ludwig, Bryant Chen, Nathalie Baracaldo, Werner Geyer, Qian Pan","IUI '21: Proceedings of the 26th International Conference on Intelligent User Interfaces
IUI","2021-04
2021","<a href=""ACM (2021-04) : The Design and Development of a Game to Study Backdoor Poisoning Attacks: The Backdoor Game"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3397481.3450647]</a>
<a href=""DBLP (2021) : The Design and Development of a Game to Study Backdoor Poisoning Attacks: The Backdoor Game"" target=""_blank"">[https://doi.org/10.1145/3397481.3450647]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3397481.3450647]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3397481.3450647]</a>","AI Security researchers have identified a new way crowdsourced data can be intentionally compromised. Backdoor attacks are a process through which an adversary creates a vulnerability in a machine learning model by ?poisoning?’ the training set by ...
","
","ACM
DBLP"
RABA: A Robust Avatar Backdoor Attack on Deep Neural Network,"Ying He, Zhili Shen, Chang Xia, Jingyu Hua, Wei Tong, Sheng Zhong",arXiv,2021-04,"<a href=""DBLP (2021-04) : RABA: A Robust Avatar Backdoor Attack on Deep Neural Network"" target=""_blank"">[https://arxiv.org/abs/2104.01026]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2104.01026]</a>",,,DBLP
"Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses","Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander Madry, Bo Li, Tom Goldstein","arXiv
IEEE Trans. Pattern Anal. Mach. Intell.
arXiv","2021-03-31
2023
2020-12","<a href=""arXiv (2021-03-31) : Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses"" target=""_blank"">[http://arxiv.org/abs/2012.10544v4]</a>
<a href=""DBLP (2023) : Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses"" target=""_blank"">[https://doi.org/10.1109/TPAMI.2022.3162397]</a>
<a href=""DBLP (2020-12) : Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses"" target=""_blank"">[https://arxiv.org/abs/2012.10544]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TPAMI.2022.3162397]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2012.10544]</a>","As machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance. The absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities, training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space. In addition to describing various poisoning and backdoor threat models and the relationships among them, we develop their unified taxonomy.

","

","arXiv
DBLP
DBLP"
MCIDS-Multi Classifier Intrusion Detection system for IoT Cyber Attack using Deep Learning algorithm,S. Singh S. V. Fernandes V. Padmanabha P. Rubini,2021 Third International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV),2021-03-31,"<a href=""IEEE (2021-03-31) : MCIDS-Multi Classifier Intrusion Detection system for IoT Cyber Attack using Deep Learning algorithm"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9388579]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICICV50876.2021.9388579]</a>","The massive increase in development, deployment and usage of IoT has given rise to smart cities. These smart city devices have the ability to perform communications on their own which require free flow of data. The interconnectivity has also resulted in exponential growth of data being processed, making it susceptible to intrusion attacks. Traditional IDS systems are not designed to work efficiently in an IoT network as these devices have restricted resources and sparse functionality. To tackle the cyber security threats in IoT, MCIDS (Multi Classifier Intrusion Detection system) has been proposed which is based on deep learning algorithm. The UNSW-NB15 dataset is utilised to train and test the model. Proposed Solution can effectively detect and alert Reconnaissance, Backdoors, Analysis, DoS, Fuzzers, Generic, Worms and Shellcodes and achieve high accuracy with low false positives.",,IEEE
Stratification of Hardware Attacks: Side Channel Attacks and Fault Injection Techniques,"Shaminder Kaur, Balwinder Singh, Harsimranjit Kaur",SN Computer Science,2021-03-31,"<a href=""Springer (2021-03-31) : Stratification of Hardware Attacks: Side Channel Attacks and Fault Injection Techniques"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-021-00562-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-021-00562-3]</a>",Cryptographic devices have many encrypted and secured solutions to protect them against hardware attacks. Hardware designers spent huge amount of...,,Springer
Advances in privacy-preserving computing,"Kaiping Xue, Zhe Liu, ... David S. L. Wei",Peer-to-Peer Networking and Applications,2021-03-30,"<a href=""Springer (2021-03-30) : Advances in privacy-preserving computing"" target=""_blank"">[https://link.springer.com/article/10.1007/s12083-021-01110-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12083-021-01110-9]</a>",,,Springer
Analysis of error-based machine learning algorithms in network anomaly detection and categorization,"Samuel A. Ajila, Chung-Horng Lung, Anurag Das",Annals of Telecommunications,2021-03-30,"<a href=""Springer (2021-03-30) : Analysis of error-based machine learning algorithms in network anomaly detection and categorization"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-021-00836-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-021-00836-0]</a>",Intrusion and anomaly detection are particularly important to protect computer networks and communication vulnerability. This research aims to...,,Springer
"AI-Driven Cybersecurity: An Overview, Security Intelligence Modeling and Research Directions","Iqbal H. Sarker, Md Hasan Furhad, Raza Nowrozy",SN Computer Science,2021-03-26,"<a href=""Springer (2021-03-26) : AI-Driven Cybersecurity: An Overview, Security Intelligence Modeling and Research Directions"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-021-00557-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-021-00557-0]</a>","Artificial intelligence (AI) is one of the key technologies of the Fourth Industrial Revolution (or Industry 4.0), which can be used for the...",,Springer
Toward Hybrid Static-Dynamic Detection of Vulnerabilities in IoT Firmware,D. He H. Gu T. Li Y. Du X. Wang S. Zhu N. Guizani,IEEE Network,2021-03-26,"<a href=""IEEE (2021-03-26) : Toward Hybrid Static-Dynamic Detection of Vulnerabilities in IoT Firmware"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9246617]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/MNET.011.2000450]</a>","IoT devices are becoming increasingly ubiquitous because they have greatly simplified many aspects of our daily life and our work. However, most firmware in these embedded devices carry various security vulnerabilities, such as hard-cod-ed passwords, cryptographic keys, insecure configurations and backdoors. Recent large-scale attacks have demonstrated that the security vulnerabilities in IoT firmware have posed a severe threat to the Internet infrastructure. In this work, we design a hybrid platform to detect vulnerabilities in IoT firmware, which integrates both offline static detection and online dynamic detection. Our evaluation on real IoT devices shows that the proposed platform can effectively identify various security weaknesses and risks in firmware, such as dangerous processes, exploitable vulnerabilities, and other attack surfaces.",,IEEE
Black-box Detection of Backdoor Attacks with Limited Information and Data,"Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, Jun Zhu","arXiv
ICCV
arXiv","2021-03-24
2021
2021-03","<a href=""arXiv (2021-03-24) : Black-box Detection of Backdoor Attacks with Limited Information and Data"" target=""_blank"">[http://arxiv.org/abs/2103.13127v1]</a>
<a href=""DBLP (2021) : Black-box Detection of Backdoor Attacks with Limited Information and Data"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01617]</a>
<a href=""DBLP (2021-03) : Black-box Detection of Backdoor Attacks with Limited Information and Data"" target=""_blank"">[https://arxiv.org/abs/2103.13127]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01617]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2103.13127]</a>","Although deep neural networks (DNNs) have made rapid progress in recent years, they are vulnerable in adversarial environments. A malicious backdoor could be embedded in a model by poisoning the training dataset, whose intention is to make the infected model give wrong predictions during inference when the specific trigger appears. To mitigate the potential threats of backdoor attacks, various backdoor detection and defense methods have been proposed. However, the existing techniques usually require the poisoned training data or access to the white-box model, which is commonly unavailable in practice. In this paper, we propose a black-box backdoor detection (B3D) method to identify backdoor attacks with only query access to the model. We introduce a gradient-free optimization algorithm to reverse-engineer the potential trigger for each class, which helps to reveal the existence of backdoor attacks. In addition to backdoor detection, we also propose a simple strategy for reliable predictions using the identified backdoored models. Extensive experiments on hundreds of DNN models trained on several datasets corroborate the effectiveness of our method under the black-box setting against various backdoor attacks.

","

","arXiv
DBLP
DBLP"
"Robotics cyber security: vulnerabilities, attacks, countermeasures, and recommendations","Jean-Paul A. Yaacoub, Hassan N. Noura, ... Ali Chehab",International Journal of Information Security,2021-03-19,"<a href=""Springer (2021-03-19) : Robotics cyber security: vulnerabilities, attacks, countermeasures, and recommendations"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-021-00545-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-021-00545-8]</a>","The recent digital revolution led robots to become integrated more than ever into different domains such as agricultural, medical, industrial,...",,Springer
TOP: Backdoor Detection in Neural Networks via Transferability of Perturbation,"Todd Huster, Emmanuel Ekwedike","arXiv
arXiv","2021-03-18
2021-03","<a href=""arXiv (2021-03-18) : TOP: Backdoor Detection in Neural Networks via Transferability of Perturbation"" target=""_blank"">[http://arxiv.org/abs/2103.10274v1]</a>
<a href=""DBLP (2021-03) : TOP: Backdoor Detection in Neural Networks via Transferability of Perturbation"" target=""_blank"">[https://arxiv.org/abs/2103.10274]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2103.10274]</a>","Deep neural networks (DNNs) are vulnerable to ""backdoor"" poisoning attacks, in which an adversary implants a secret trigger into an otherwise normally functioning model. Detection of backdoors in trained models without access to the training data or example triggers is an important open problem. In this paper, we identify an interesting property of these models: adversarial perturbations transfer from image to image more readily in poisoned models than in clean models. This holds for a variety of model and trigger types, including triggers that are not linearly separable from clean data. We use this feature to detect poisoned models in the TrojAI benchmark, as well as additional models.
","
","arXiv
DBLP"
EX-RAY: Distinguishing Injected Backdoor from Natural Features in Neural Networks by Examining Differential Feature Symmetry,"Yingqi Liu, Guangyu Shen, Guanhong Tao, Zhenting Wang, Shiqing Ma, Xiangyu Zhang","arXiv
arXiv","2021-03-17
2021-03","<a href=""arXiv (2021-03-17) : EX-RAY: Distinguishing Injected Backdoor from Natural Features in Neural Networks by Examining Differential Feature Symmetry"" target=""_blank"">[http://arxiv.org/abs/2103.08820v2]</a>
<a href=""DBLP (2021-03) : EX-RAY: Distinguishing Injected Backdoor from Natural Features in Neural Networks by Examining Differential Feature Symmetry"" target=""_blank"">[https://arxiv.org/abs/2103.08820]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2103.08820]</a>","Backdoor attack injects malicious behavior to models such that inputs embedded with triggers are misclassified to a target label desired by the attacker. However, natural features may behave like triggers, causing misclassification once embedded. While they are inevitable, mis-recognizing them as injected triggers causes false warnings in backdoor scanning. A prominent challenge is hence to distinguish natural features and injected backdoors. We develop a novel symmetric feature differencing method that identifies a smallest set of features separating two classes. A backdoor is considered injected if the corresponding trigger consists of features different from the set of features distinguishing the victim and target classes. We evaluate the technique on thousands of models, including both clean and trojaned models, from the TrojAI rounds 2-4 competitions and a number of models on ImageNet. Existing backdoor scanning techniques may produce hundreds of false positives (i.e., clean models recognized as trojaned). Our technique removes 78-100% of the false positives (by a state-of-the-art scanner ABS) with a small increase of false negatives by 0-30%, achieving 17-41% overall accuracy improvement, and facilitates achieving top performance on the leaderboard. It also boosts performance of other scanners. It outperforms false positive removal methods using L2 distance and attribution techniques. We also demonstrate its potential in detecting a number of semantic backdoor attacks.
","
","arXiv
DBLP"
Mitigating backdoor attacks in LSTM-based Text Classification Systems by Backdoor Keyword Identification,"Chuanshuai Chen, Jiazhu Dai","arXiv
arXiv","2021-03-15
2020-07","<a href=""arXiv (2021-03-15) : Mitigating backdoor attacks in LSTM-based Text Classification Systems by Backdoor Keyword Identification"" target=""_blank"">[http://arxiv.org/abs/2007.12070v3]</a>
<a href=""DBLP (2020-07) : Mitigating backdoor attacks in LSTM-based Text Classification Systems by Backdoor Keyword Identification"" target=""_blank"">[https://arxiv.org/abs/2007.12070]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2007.12070]</a>","It has been proved that deep neural networks are facing a new threat called backdoor attacks, where the adversary can inject backdoors into the neural network model through poisoning the training dataset. When the input containing some special pattern called the backdoor trigger, the model with backdoor will carry out malicious task such as misclassification specified by adversaries. In text classification systems, backdoors inserted in the models can cause spam or malicious speech to escape detection. Previous work mainly focused on the defense of backdoor attacks in computer vision, little attention has been paid to defense method for RNN backdoor attacks regarding text classification. In this paper, through analyzing the changes in inner LSTM neurons, we proposed a defense method called Backdoor Keyword Identification (BKI) to mitigate backdoor attacks which the adversary performs against LSTM-based text classification by data poisoning. This method can identify and exclude poisoning samples crafted to insert backdoor into the model from training data without a verified and trusted dataset. We evaluate our method on four different text classification datset: IMDB, DBpedia ontology, 20 newsgroups and Reuters-21578 dataset. It all achieves good performance regardless of the trigger sentences.
","
","arXiv
DBLP"
"Artificial intelligence in cyber security: research advances, challenges, and opportunities","Zhimin Zhang, Huansheng Ning, ... Kim-Kwang Raymond Choo",Artificial Intelligence Review,2021-03-13,"<a href=""Springer (2021-03-13) : Artificial intelligence in cyber security: research advances, challenges, and opportunities"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-021-09976-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10462-021-09976-0]</a>","In recent times, there have been attempts to leverage artificial intelligence (AI) techniques in a broad range of cyber security applications....",,Springer
Enhanced DNNs for malware classification with GAN-based adversarial training,"Yunchun Zhang, Haorui Li, ... Jiaqi Jiang",Journal of Computer Virology and Hacking Techniques,2021-03-10,"<a href=""Springer (2021-03-10) : Enhanced DNNs for malware classification with GAN-based adversarial training"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-021-00378-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-021-00378-y]</a>","Deep learning based malware classification gains momentum recently. However, deep learning models are vulnerable to adversarial perturbation attacks...",,Springer
Research on Intrusion Detection Based on Particle Swarm Optimization in IoT,J. Liu D. Yang M. Lian M. Li,IEEE Access,2021-03-10,"<a href=""IEEE (2021-03-10) : Research on Intrusion Detection Based on Particle Swarm Optimization in IoT"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9367134]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2021.3063671]</a>","With the advent of the “Internet plus” era, the Internet of Things (IoT) is gradually penetrating into various fields, and the scale of its equipment is also showing an explosive growth trend. The age of the “Internet of Everything” is coming. The integration and diversification of IoT terminals and applications make IoT more vulnerable to various intrusion attacks. Therefore, it is particularly important to design an intrusion detection model that guarantees the security, integrity and reliability of the IoT. Traditional intrusion detection technology has the disadvantages of low detection rate and poor scalability, which cannot adapt to the complex and changeable IoT environment. In this paper, we propose a particle swarm optimization-based gradient descent (PSO-LightGBM) for the intrusion detection. In this method, PSO-LightGBM is used to extract the features of the data and inputs it into one-class SVM (OCSVM) to discover and identify malicious data. The UNSW-NB15 dataset is applied to verify the intrusion detection model. The experimental results show that the model we propose is very robust in detecting either normal or various malicious data, especially small sample data such as Backdoor, Shellcode and Worms.",,IEEE
P-STORE: Extension of STORE Methodology to Elicit Privacy Requirements,"Md Tarique Jamal Ansari, Abdullah Baz, ... Raees Ahmad Khan",Arabian Journal for Science and Engineering,2021-03-07,"<a href=""Springer (2021-03-07) : P-STORE: Extension of STORE Methodology to Elicit Privacy Requirements"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-021-05476-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-021-05476-z]</a>",Implementing security and privacy requirements at every level of the software development cycle is imperative for ensuring optimum usability as well...,,Springer
Novel image encryption algorithm using fractional chaos and cellular neural network,"Farhan Musanna, Deepak Dangwal, Sanjeev Kumar",Journal of Ambient Intelligence and Humanized Computing,2021-03-06,"<a href=""Springer (2021-03-06) : Novel image encryption algorithm using fractional chaos and cellular neural network"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-021-02982-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-021-02982-8]</a>",The work presented in this article gives a novel digital image encryption algorithm using a fractional-order chaotic system and cellular neural...,,Springer
A privacy-conserving framework based intrusion detection method for detecting and recognizing malicious behaviours in cyber-physical power networks,"Izhar Ahmed Khan, Dechang Pi, ... Farman Ali",Applied Intelligence,2021-03-05,"<a href=""Springer (2021-03-05) : A privacy-conserving framework based intrusion detection method for detecting and recognizing malicious behaviours in cyber-physical power networks"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-021-02222-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-021-02222-8]</a>",Contemporary Smart Power Systems (SPNs) depend on Cyber-Physical Systems (CPSs) to connect physical devices and control tools. Developing a robust...,,Springer
WaNet -- Imperceptible Warping-based Backdoor Attack,"Anh Nguyen, Anh Tran",arXiv,2021-03-04,"<a href=""arXiv (2021-03-04) : WaNet -- Imperceptible Warping-based Backdoor Attack"" target=""_blank"">[http://arxiv.org/abs/2102.10369v4]</a>","<a href=""arXiv"" target=""_blank"">[]</a>","With the thriving of deep learning and the widespread practice of using pre-trained networks, backdoor attacks have become an increasing security threat drawing many research interests in recent years. A third-party model can be poisoned in training to work well in normal conditions but behave maliciously when a trigger pattern appears. However, the existing backdoor attacks are all built on noise perturbation triggers, making them noticeable to humans. In this paper, we instead propose using warping-based triggers. The proposed backdoor outperforms the previous methods in a human inspection test by a wide margin, proving its stealthiness. To make such models undetectable by machine defenders, we propose a novel training mode, called the ``noise mode. The trained networks successfully attack and bypass the state-of-the-art defense methods on standard classification datasets, including MNIST, CIFAR-10, GTSRB, and CelebA. Behavior analyses show that our backdoors are transparent to network inspection, further proving this novel attack mechanism's efficiency.",,arXiv
Embedding asymmetric backdoors into the RSA key generator,Markelova A.V.,Journal of Computer Virology and Hacking Techniques,2021-03-01,"<a href=""ScienceDirect (2021-03-01) : Embedding asymmetric backdoors into the RSA key generator"" target=""_blank"">[https://doi.org/10.1007/s11416-020-00363-x]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/s11416-020-00363-x]</a>",,,ScienceDirect
Gateways or backdoors to development? Filtering mechanisms and territorial embeddedness in the Chilean copper GPN’s urban system,Atienza M.,Growth and Change,2021-03-01,"<a href=""ScienceDirect (2021-03-01) : Gateways or backdoors to development? Filtering mechanisms and territorial embeddedness in the Chilean copper GPN’s urban system"" target=""_blank"">[https://doi.org/10.1111/grow.12447]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1111/grow.12447]</a>",,,ScienceDirect
PoisonGAN: Generative Poisoning Attacks against Federated Learning in Edge Computing Systems,Zhang J.,IEEE Internet of Things Journal,2021-03-01,"<a href=""ScienceDirect (2021-03-01) : PoisonGAN: Generative Poisoning Attacks against Federated Learning in Edge Computing Systems"" target=""_blank"">[https://doi.org/10.1109/JIOT.2020.3023126]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/JIOT.2020.3023126]</a>",,,ScienceDirect
Stability-Based Analysis and Defense against Backdoor Attacks on Edge Computing Services,Zhao Y.,IEEE Network,2021-03-01,"<a href=""ScienceDirect (2021-03-01) : Stability-Based Analysis and Defense against Backdoor Attacks on Edge Computing Services"" target=""_blank"">[https://doi.org/10.1109/MNET.011.2000265]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/MNET.011.2000265]</a>",,,ScienceDirect
APT backdoor for block cipher software and its countermeasures,Wang A.,Journal of Cryptologic Research,2021-02-25,"<a href=""ScienceDirect (2021-02-25) : APT backdoor for block cipher software and its countermeasures"" target=""_blank"">[https://doi.org/10.13868/j.cnki.jcr.000420]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.13868/j.cnki.jcr.000420]</a>",,,ScienceDirect
Detection and Location for Network Hidden Threat Information Based on Improved MSCKF Algorithm,"Jie Zhang, Jinguang Sun, Hua He",Wireless Personal Communications,2021-02-24,"<a href=""Springer (2021-02-24) : Detection and Location for Network Hidden Threat Information Based on Improved MSCKF Algorithm"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-021-08270-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-021-08270-0]</a>","In order to accurately detect and locate network hidden threat information and improve the detection and location effect, a network hidden threat...",,Springer
Role of Artificial Intelligence in the Internet of Things (IoT) cybersecurity,"Murat Kuzlu, Corinne Fair, Ozgur Guler",Discover Internet of Things,2021-02-24,"<a href=""Springer (2021-02-24) : Role of Artificial Intelligence in the Internet of Things (IoT) cybersecurity"" target=""_blank"">[https://link.springer.com/article/10.1007/s43926-020-00001-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s43926-020-00001-4]</a>","In recent years, the use of the Internet of Things (IoT) has increased exponentially, and cybersecurity concerns have increased along with it. On the...",,Springer
Blind Backdoors in Deep Learning Models,"Eugene Bagdasaryan, Vitaly Shmatikov","arXiv
USENIX Security Symposium
arXiv","2021-02-19
2021
2020-05","<a href=""arXiv (2021-02-19) : Blind Backdoors in Deep Learning Models"" target=""_blank"">[http://arxiv.org/abs/2005.03823v4]</a>
<a href=""DBLP (2021) : Blind Backdoors in Deep Learning Models"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity21/presentation/bagdasaryan]</a>
<a href=""DBLP (2020-05) : Blind Backdoors in Deep Learning Models"" target=""_blank"">[https://arxiv.org/abs/2005.03823]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity21/presentation/bagdasaryan]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2005.03823]</a>","We investigate a new method for injecting backdoors into machine learning models, based on compromising the loss-value computation in the model-training code. We use it to demonstrate new classes of backdoors strictly more powerful than those in the prior literature: single-pixel and physical backdoors in ImageNet models, backdoors that switch the model to a covert, privacy-violating task, and backdoors that do not require inference-time input modifications. Our attack is blind: the attacker cannot modify the training data, nor observe the execution of his code, nor access the resulting model. The attack code creates poisoned training inputs ""on the fly,"" as the model is training, and uses multi-objective optimization to achieve high accuracy on both the main and backdoor tasks. We show how a blind attack can evade any known defense and propose new ones.

","

","arXiv
DBLP
DBLP"
PoisonGAN: Generative Poisoning Attacks Against Federated Learning in Edge Computing Systems,J. Zhang B. Chen X. Cheng H. T. T. Binh S. Yu,IEEE Internet of Things Journal,2021-02-18,"<a href=""IEEE (2021-02-18) : PoisonGAN: Generative Poisoning Attacks Against Federated Learning in Edge Computing Systems"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9194010]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/JIOT.2020.3023126]</a>","Edge computing is a key-enabling technology that meets continuously increasing requirements for the intelligent Internet-of-Things (IoT) applications. To cope with the increasing privacy leakages of machine learning while benefiting from unbalanced data distributions, federated learning has been wildly adopted as a novel intelligent edge computing framework with a localized training mechanism. However, recent studies found that the federated learning framework exhibits inherent vulnerabilities on active attacks, and poisoning attack is one of the most powerful and secluded attacks where the functionalities of the global model could be damaged through attacker's well-crafted local updates. In this article, we give a comprehensive exploration of the poisoning attack mechanisms in the context of federated learning. We first present a poison data generation method, named Data_Gen, based on the generative adversarial networks (GANs). This method mainly relies upon the iteratively updated global model parameters to regenerate samples of interested victims. Second, we further propose a novel generative poisoning attack model, named PoisonGAN, against the federated learning framework. This model utilizes the designed Data_Gen method to efficiently reduce the attack assumptions and make attacks feasible in practice. We finally evaluate our data generation and attack models by implementing two types of typical poisoning attack strategies, label flipping and backdoor, on a federated learning prototype. The experimental results demonstrate that these two attack models are effective in federated learning.",,IEEE
Performance Evaluation of Machine Learning Based Face Recognition Techniques,"Sahil Sharma, Vijay Kumar",Wireless Personal Communications,2021-02-16,"<a href=""Springer (2021-02-16) : Performance Evaluation of Machine Learning Based Face Recognition Techniques"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-021-08186-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-021-08186-9]</a>",The robustness of machine-learning model-based face recognition techniques to image processing attacks using the quantization of extracted features...,,Springer
Security Threats and Defensive Approaches in Machine Learning System Under Big Data Environment,"Chen Hongsong, Zhang Yongpeng, ... Bharat Bhargava",Wireless Personal Communications,2021-02-13,"<a href=""Springer (2021-02-13) : Security Threats and Defensive Approaches in Machine Learning System Under Big Data Environment"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-021-08284-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-021-08284-8]</a>","Under big data environment, machine learning has been rapidly developed and widely used. It has been successfully applied in computer vision, natural...",,Springer
Detecting and mitigating cyberattacks using software defined networks for integrated clinical environments,"Alberto Huertas Celdrán, Kallol Krishna Karmakar, ... Vijay Varadharajan",Peer-to-Peer Networking and Applications,2021-02-10,"<a href=""Springer (2021-02-10) : Detecting and mitigating cyberattacks using software defined networks for integrated clinical environments"" target=""_blank"">[https://link.springer.com/article/10.1007/s12083-021-01082-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12083-021-01082-w]</a>",The evolution of integrated clinical environments (ICE) and the future generations of mobile networks brings to reality the hospitals of the future...,,Springer
Recursive Backdoors for SAT,"Nikolas Mählmann, Sebastian Siebertz, Alexandre Vigny","arXiv
MFCS
arXiv","2021-02-09
2021
2021-02","<a href=""arXiv (2021-02-09) : Recursive Backdoors for SAT"" target=""_blank"">[http://arxiv.org/abs/2102.04707v1]</a>
<a href=""DBLP (2021) : Recursive Backdoors for SAT"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.MFCS.2021.73]</a>
<a href=""DBLP (2021-02) : Recursive Backdoors for SAT"" target=""_blank"">[https://arxiv.org/abs/2102.04707]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.MFCS.2021.73]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2102.04707]</a>","A strong backdoor in a formula $\phi$ of propositional logic to a tractable class $\mathcal{C}$ of formulas is a set $B$ of variables of $\phi$ such that every assignment of the variables in $B$ results in a formula from $\mathcal{C}$. Strong backdoors of small size or with a good structure, e.g. with small backdoor treewidth, lead to efficient solutions for the propositional satisfiability problem SAT. In this paper we propose the new notion of recursive backdoors, which is inspired by the observation that in order to solve SAT we can independently recurse into the components that are created by partial assignments of variables. The quality of a recursive backdoor is measured by its recursive backdoor depth. Similar to the concept of backdoor treewidth, recursive backdoors of bounded depth include backdoors of unbounded size that have a certain treelike structure. However, the two concepts are incomparable and our results yield new tractability results for SAT.

","

","arXiv
DBLP
DBLP"
Active DNN IP Protection: A Novel User Fingerprint Management and DNN Authorization Control Technique,M. Xue Z. Wu C. He J. Wang W. Liu,"2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)",2021-02-09,"<a href=""IEEE (2021-02-09) : Active DNN IP Protection: A Novel User Fingerprint Management and DNN Authorization Control Technique"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9343023]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TrustCom50675.2020.00130]</a>","The training process of deep learning model is costly. As such, deep learning model can be treated as an intellectual property (IP) of the model creator. However, a pirate can illegally copy, redistribute or abuse the model without permission. In recent years, a few Deep Neural Networks (DNN) IP protection works have been proposed. However, most of existing works passively verify the copyright of the model after the piracy occurs, and lack of user identity management, thus cannot provide commercial copyright management functions. In this paper, a novel user fingerprint management and DNN authorization control technique based on backdoor is proposed to provide active DNN IP protection. The proposed method can not only verify the ownership of the model, but can also authenticate and manage the user's unique identity, so as to provide a commercially applicable DNN IP management mechanism. Experimental results on CIFAR-10, CIFAR-100 and Fashion-MNIST datasets show that the proposed method can achieve high detection rate for user authentication (up to 100% in the three datasets). Illegal users with forged fingerprints cannot pass authentication as the detection rates are all 0% in the three datasets. Model owner can verify his ownership since he can trigger the backdoor with a high confidence. In addition, the accuracy drops are only 0.52%, 1.61% and -0.65% on CIFAR-10, CIFAR-100 and Fashion-MNIST, respectively, which indicate that the proposed method will not affect the performance of the DNN models. The proposed method is also robust to model fine-tuning and pruning attacks. The detection rates for owner verification on CIFAR-10, CIFAR-100 and Fashion-MNIST are all 100% after model pruning attack, and are 90%, 83% and 93% respectively after model fine-tuning attack, on the premise that the attacker wants to preserve the accuracy of the model.",,IEEE
IoT-based botnet attacks systematic mapping study of literature,"Habiba Hamid, Rafidah Md Noor, ... Emran Mohd Tamil",Scientometrics,2021-02-09,"<a href=""Springer (2021-02-09) : IoT-based botnet attacks systematic mapping study of literature"" target=""_blank"">[https://link.springer.com/article/10.1007/s11192-020-03819-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11192-020-03819-5]</a>",The rapid escalation in the usage of the Internet of Things (IoT) devices is threatened by botnets. The expected increase in botnet attacks has seen...,,Springer
Deep Learning Backdoors,"Shaofeng Li, Shiqing Ma, Minhui Xue, Benjamin Zi Hao Zhao","arXiv
Security and Artificial Intelligence
arXiv","2021-02-06
2022
2020-07","<a href=""arXiv (2021-02-06) : Deep Learning Backdoors"" target=""_blank"">[http://arxiv.org/abs/2007.08273v2]</a>
<a href=""DBLP (2022) : Deep Learning Backdoors"" target=""_blank"">[https://doi.org/10.1007/978-3-030-98795-4_13]</a>
<a href=""DBLP (2020-07) : Deep Learning Backdoors"" target=""_blank"">[https://arxiv.org/abs/2007.08273]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-030-98795-4_13]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2007.08273]</a>","Intuitively, a backdoor attack against Deep Neural Networks (DNNs) is to inject hidden malicious behaviors into DNNs such that the backdoor model behaves legitimately for benign inputs, yet invokes a predefined malicious behavior when its input contains a malicious trigger. The trigger can take a plethora of forms, including a special object present in the image (e.g., a yellow pad), a shape filled with custom textures (e.g., logos with particular colors) or even image-wide stylizations with special filters (e.g., images altered by Nashville or Gotham filters). These filters can be applied to the original image by replacing or perturbing a set of image pixels.

","

","arXiv
DBLP
DBLP"
MCIDS-Multi classifier intrusion detection system for IoT cyber attack using deep learning algorithm,Singh S.,"Proceedings of the 3rd International Conference on Intelligent Communication Technologies and Virtual Mobile Networks, ICICV 2021",2021-02-04,"<a href=""ScienceDirect (2021-02-04) : MCIDS-Multi classifier intrusion detection system for IoT cyber attack using deep learning algorithm"" target=""_blank"">[https://doi.org/10.1109/ICICV50876.2021.9388579]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICICV50876.2021.9388579]</a>",,,ScienceDirect
Proposing Innovative Intruder Detection System for Host Machines in Cloud Computing,A. K. Chaturvedi P. Kumar K. Sharma,2020 9th International Conference System Modeling and Advancement in Research Trends (SMART),2021-02-04,"<a href=""IEEE (2021-02-04) : Proposing Innovative Intruder Detection System for Host Machines in Cloud Computing"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9337078]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SMART50582.2020.9337078]</a>","There is very significant role of Virtualization in cloud computing. The physical hardware in the cloud computing reside with the host machine and the virtualization software runs on it. The virtualization allows virtual machines to exist. The host machine shares its physical components such as memory, storage, and processor ultimately to handle the needs of the virtual machines. If an attacker effectively compromises one VM, it could outbreak others on the same host on the network over long periods of time. This is an gradually more popular method for cross-virtual-machine attacks, since traffic between VMs cannot be examined by standard IDS/IPS software programs. As we know that the cloud environment is distributed in nature and hence more susceptible to various types of intrusion attacks which include installing malicious software and generating backdoors. In a cloud environment, where organizations have hosted important and critical data, the security of underlying technologies becomes critical. To alleviate the hazard to cloud environments, Intrusion Detection Systems (IDS) are a cover of defense. In this paper, we are proposing an innovative model for Intrusion Detection System for securing Host machines in cloud infrastructure. This proposed IDS has two important features: (1) signature based and (2) prompt alert system.",,IEEE
Backdoor Attack against Speaker Verification,"Tongqing Zhai, Yiming Li, Ziqi Zhang, Baoyuan Wu, Yong Jiang, Shu-Tao Xia","arXiv
arXiv","2021-02-03
2020-10","<a href=""arXiv (2021-02-03) : Backdoor Attack against Speaker Verification"" target=""_blank"">[http://arxiv.org/abs/2010.11607v3]</a>
<a href=""DBLP (2020-10) : Backdoor Attack against Speaker Verification"" target=""_blank"">[https://arxiv.org/abs/2010.11607]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2010.11607]</a>","Speaker verification has been widely and successfully adopted in many mission-critical areas for user identification. The training of speaker verification requires a large amount of data, therefore users usually need to adopt third-party data ($e.g.$, data from the Internet or third-party data company). This raises the question of whether adopting untrusted third-party data can pose a security threat. In this paper, we demonstrate that it is possible to inject the hidden backdoor for infecting speaker verification models by poisoning the training data. Specifically, we design a clustering-based attack scheme where poisoned samples from different clusters will contain different triggers ($i.e.$, pre-defined utterances), based on our understanding of verification tasks. The infected models behave normally on benign samples, while attacker-specified unenrolled triggers will successfully pass the verification even if the attacker has no information about the enrolled speaker. We also demonstrate that existing backdoor attacks cannot be directly adopted in attacking speaker verification. Our approach not only provides a new perspective for designing novel attacks, but also serves as a strong baseline for improving the robustness of verification methods. The code for reproducing main results is available at \url{https://github.com/zhaitongqing233/Backdoor-attack-against-speaker-verification}.
","<a href=""arXiv"" target=""_blank"">[https://github.com/zhaitongqing233/Backdoor-attack-against-speaker-verification}]</a>
","arXiv
DBLP"
Compromising device security via NVM controller vulnerability,S. Skorobogatov,2020 IEEE Physical Assurance and Inspection of Electronics (PAINE),2021-02-03,"<a href=""IEEE (2021-02-03) : Compromising device security via NVM controller vulnerability"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9337736]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/PAINE49178.2020.9337736]</a>","This paper introduces a new vulnerability found in a low-cost secure authentication IC that stores its security settings in non-volatile memory (NVM). Such devices are widely used for prevention of counterfeiting in consumable products and accessories (printer cartridges, batteries etc.) and for aftermarket control. The particular device targeted here uses hardwired application logic and lacks a microprocessor, however, more sophisticated security devices used in medical and banking applications could similarly be vulnerable. The newly discovered self-induced fault attacks exploit implications of the use of error-correction codes inside modern embedded NVM blocks and their associated control logic, which can leave the application vulnerable to early termination of NVM write operation. This could potentially be used to change the security settings of a device in a way that bypasses the intended state machine controlling access and allow reverting the stored hardware security level back to the factory test/debug mode. The paper also outlines some measures that could substantially reduce the chances of a successful attack.",,IEEE
"What Doesn&apos,t Kill You Makes You Robust(er): Adversarial Training against Poisons and Backdoors","Jonas Geiping, Liam Fowl, Gowthami Somepalli, Micah Goldblum, Michael Moeller, Tom Goldstein",arXiv,2021-02,"<a href=""DBLP (2021-02) : What Doesn&apos,t Kill You Makes You Robust(er): Adversarial Training against Poisons and Backdoors"" target=""_blank"">[https://arxiv.org/abs/2102.13624]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2102.13624]</a>",,,DBLP
Defense against neural trojan attacks: A survey,Kaviani S.,Neurocomputing,2021-01-29,"<a href=""ScienceDirect (2021-01-29) : Defense against neural trojan attacks: A survey"" target=""_blank"">[https://doi.org/10.1016/j.neucom.2020.07.133]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.neucom.2020.07.133]</a>",,,ScienceDirect
Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks,"Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma","arXiv
ICLR
arXiv","2021-01-27
2021
2021-01","<a href=""arXiv (2021-01-27) : Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2101.05930v2]</a>
<a href=""DBLP (2021) : Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks"" target=""_blank"">[https://openreview.net/forum?id=9l0K4OM-oXE]</a>
<a href=""DBLP (2021-01) : Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2101.05930]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/forum?id=9l0K4OM-oXE]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2101.05930]</a>","Deep neural networks (DNNs) are known vulnerable to backdoor attacks, a training time attack that injects a trigger pattern into a small proportion of training data so as to control the model's prediction at the test time. Backdoor attacks are notably dangerous since they do not affect the model's performance on clean examples, yet can fool the model to make incorrect prediction whenever the trigger pattern appears during testing. In this paper, we propose a novel defense framework Neural Attention Distillation (NAD) to erase backdoor triggers from backdoored DNNs. NAD utilizes a teacher network to guide the finetuning of the backdoored student network on a small clean subset of data such that the intermediate-layer attention of the student network aligns with that of the teacher network. The teacher network can be obtained by an independent finetuning process on the same clean subset. We empirically show, against 6 state-of-the-art backdoor attacks, NAD can effectively erase the backdoor triggers using only 5\% clean training data without causing obvious performance degradation on clean examples. Code is available in https://github.com/bboylyg/NAD.

","<a href=""arXiv"" target=""_blank"">[https://github.com/bboylyg/NAD]</a>

","arXiv
DBLP
DBLP"
Towards the design of real-time autonomous IoT NIDS,"Alaa Alhowaide, Izzat Alsmadi, Jian Tang",Cluster Computing,2021-01-22,"<a href=""Springer (2021-01-22) : Towards the design of real-time autonomous IoT NIDS"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-021-03231-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-021-03231-5]</a>","Classic security methods become less effective against the Internet of Things (IoT) cyber-attacks, such as cryptography. An urgent need for real-time...",,Springer
Backdoor Decomposable Monotone Circuits and their Propagation Complete Encodings,"Petr Kučera, Petr Savický","arXiv
AAAI
arXiv","2021-01-21
2021
2018-11","<a href=""arXiv (2021-01-21) : Backdoor Decomposable Monotone Circuits and their Propagation Complete Encodings"" target=""_blank"">[http://arxiv.org/abs/1811.09435v4]</a>
<a href=""DBLP (2021) : Backdoor Decomposable Monotone Circuits and Propagation Complete Encodings"" target=""_blank"">[https://doi.org/10.1609/aaai.v35i5.16501]</a>
<a href=""DBLP (2018-11) : Backdoor Decomposable Monotone Circuits and their Propagation Complete Encodings"" target=""_blank"">[http://arxiv.org/abs/1811.09435]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v35i5.16501]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1811.09435]</a>","We describe a compilation language of backdoor decomposable monotone circuits (BDMCs) which generalizes several concepts appearing in the literature, e.g. DNNFs and backdoor trees. A $\mathcal{C}$-BDMC sentence is a monotone circuit which satisfies decomposability property (such as in DNNF) in which the inputs (or leaves) are associated with CNF encodings from a given base class $\mathcal{C}$. We consider the class of propagation complete (PC) encodings as a base class and we show that PC-BDMCs are polynomially equivalent to PC encodings. Additionally, we use this to determine the properties of PC-BDMCs and PC encodings with respect to the knowledge compilation map including the list of efficient operations on the languages.

","

","arXiv
DBLP
DBLP"
Intrusion detection in internet of things using supervised machine learning based on application and transport layer features using UNSW-NB15 data-set,"Muhammad Ahmad, Qaiser Riaz, ... Muhammad Safeer Khan",EURASIP Journal on Wireless Communications and Networking,2021-01-21,"<a href=""Springer (2021-01-21) : Intrusion detection in internet of things using supervised machine learning based on application and transport layer features using UNSW-NB15 data-set"" target=""_blank"">[https://link.springer.com/article/10.1186/s13638-021-01893-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13638-021-01893-8]</a>","Internet of Things (IoT) devices are well-connected, they generate and consume data which involves transmission of data back and forth among various...",,Springer
On Provable Backdoor Defense in Collaborative Learning,"Ximing Qiao, Yuhua Bai, Siping Hu, Ang Li, Yiran Chen, Hai Li","arXiv
arXiv","2021-01-19
2021-01","<a href=""arXiv (2021-01-19) : On Provable Backdoor Defense in Collaborative Learning"" target=""_blank"">[http://arxiv.org/abs/2101.08177v1]</a>
<a href=""DBLP (2021-01) : On Provable Backdoor Defense in Collaborative Learning"" target=""_blank"">[https://arxiv.org/abs/2101.08177]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2101.08177]</a>","As collaborative learning allows joint training of a model using multiple sources of data, the security problem has been a central concern. Malicious users can upload poisoned data to prevent the model's convergence or inject hidden backdoors. The so-called backdoor attacks are especially difficult to detect since the model behaves normally on standard test data but gives wrong outputs when triggered by certain backdoor keys. Although Byzantine-tolerant training algorithms provide convergence guarantee, provable defense against backdoor attacks remains largely unsolved. Methods based on randomized smoothing can only correct a small number of corrupted pixels or labels, methods based on subset aggregation cause a severe drop in classification accuracy due to low data utilization. We propose a novel framework that generalizes existing subset aggregation methods. The framework shows that the subset selection process, a deciding factor for subset aggregation methods, can be viewed as a code design problem. We derive the theoretical bound of data utilization ratio and provide optimal code construction. Experiments on non-IID versions of MNIST and CIFAR-10 show that our method with optimal codes significantly outperforms baselines using non-overlapping partition and random selection. Additionally, integration with existing coding theory results shows that special codes can track the location of the attackers. Such capability provides new countermeasures to backdoor attacks.
","
","arXiv
DBLP"
Continuous improvement process (CIP)-based privacy-preserving framework for smart connected toys,Benjamin Yankson,International Journal of Information Security,2021-01-19,"<a href=""Springer (2021-01-19) : Continuous improvement process (CIP)-based privacy-preserving framework for smart connected toys"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-020-00535-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-020-00535-2]</a>","Advances within the toy industry and interconnectedness have resulted in the rapid and pervasive development of smart connected toys (SCTs), built...",,Springer
Exploring adversarial attacks against malware classifiers in the backdoor poisoning attack,Reddy G.S.,IOP Conference Series: Materials Science and Engineering,2021-01-18,"<a href=""ScienceDirect (2021-01-18) : Exploring adversarial attacks against malware classifiers in the backdoor poisoning attack"" target=""_blank"">[https://doi.org/10.1088/1757-899X/1022/1/012037]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1088/1757-899X/1022/1/012037]</a>",,,ScienceDirect
Natural Backdoor Attack on Text Data,Lichao Sun,"arXiv
arXiv","2021-01-15
2020-06","<a href=""arXiv (2021-01-15) : Natural Backdoor Attack on Text Data"" target=""_blank"">[http://arxiv.org/abs/2006.16176v4]</a>
<a href=""DBLP (2020-06) : Natural Backdoor Attack on Text Data"" target=""_blank"">[https://arxiv.org/abs/2006.16176]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2006.16176]</a>","Recently, advanced NLP models have seen a surge in the usage of various applications. This raises the security threats of the released models. In addition to the clean models' unintentional weaknesses, {\em i.e.,} adversarial attacks, the poisoned models with malicious intentions are much more dangerous in real life. However, most existing works currently focus on the adversarial attacks on NLP models instead of positioning attacks, also named \textit{backdoor attacks}. In this paper, we first propose the \textit{natural backdoor attacks} on NLP models. Moreover, we exploit the various attack strategies to generate trigger on text data and investigate different types of triggers based on modification scope, human recognition, and special cases. Last, we evaluate the backdoor attacks, and the results show the excellent performance of with 100\% backdoor attacks success rate and sacrificing of 0.83\% on the text classification task.
","
","arXiv
DBLP"
Mitigating Backdoor Attacks in Federated Learning,"Chen Wu, Xian Yang, Sencun Zhu, Prasenjit Mitra","arXiv
arXiv","2021-01-14
2020-11","<a href=""arXiv (2021-01-14) : Mitigating Backdoor Attacks in Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2011.01767v2]</a>
<a href=""DBLP (2020-11) : Mitigating Backdoor Attacks in Federated Learning"" target=""_blank"">[https://arxiv.org/abs/2011.01767]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2011.01767]</a>","Malicious clients can attack federated learning systems using malicious data, including backdoor samples, during the training phase. The compromised global model will perform well on the validation dataset designed for the task, but a small subset of data with backdoor patterns may trigger the model to make a wrong prediction. There has been an arms race between attackers who tried to conceal attacks and defenders who tried to detect attacks during the aggregation stage of training on the server-side. In this work, we propose a new and effective method to mitigate backdoor attacks after the training phase. Specifically, we design a federated pruning method to remove redundant neurons in the network and then adjust the model's extreme weight values. Our experiments conducted on distributed Fashion-MNIST show that our method can reduce the average attack success rate from 99.7% to 1.9% with a 5.5% loss of test accuracy on the validation dataset. To minimize the pruning influence on test accuracy, we can fine-tune after pruning, and the attack success rate drops to 6.4%, with only a 1.7% loss of test accuracy. Further experiments under Distributed Backdoor Attacks on CIFAR-10 also show promising results that the average attack success rate drops more than 70% with less than 2% loss of test accuracy on the validation dataset.
","
","arXiv
DBLP"
FSDroid:- A feature selection technique to detect malware from Android using Machine Learning Techniques,"Arvind Mahindru, A.L. Sangal",Multimedia Tools and Applications,2021-01-14,"<a href=""Springer (2021-01-14) : FSDroid:- A feature selection technique to detect malware from Android using Machine Learning Techniques"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-020-10367-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-020-10367-w]</a>","With the recognition of free apps, Android has become the most widely used smartphone operating system these days and it naturally invited...",,Springer
Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers,"Giorgio Severi, Jim Meyer, Scott Coull, Alina Oprea","arXiv
USENIX Security Symposium","2021-01-11
2021","<a href=""arXiv (2021-01-11) : Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers"" target=""_blank"">[http://arxiv.org/abs/2003.01031v3]</a>
<a href=""DBLP (2021) : Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity21/presentation/severi]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity21/presentation/severi]</a>","Training pipelines for machine learning (ML) based malware classification often rely on crowdsourced threat feeds, exposing a natural attack injection point. In this paper, we study the susceptibility of feature-based ML malware classifiers to backdoor poisoning attacks, specifically focusing on challenging ""clean label"" attacks where attackers do not control the sample labeling process. We propose the use of techniques from explainable machine learning to guide the selection of relevant features and values to create effective backdoor triggers in a model-agnostic fashion. Using multiple reference datasets for malware classification, including Windows PE files, PDFs, and Android applications, we demonstrate effective attacks against a diverse set of machine learning models and evaluate the effect of various constraints imposed on the attacker. To demonstrate the feasibility of our backdoor attacks in practice, we create a watermarking utility for Windows PE files that preserves the binary's functionality, and we leverage similar behavior-preserving alteration methodologies for Android and PDF files. Finally, we experiment with potential defensive strategies and show the difficulties of completely defending against these attacks, especially when the attacks blend in with the legitimate sample distribution.
","
","arXiv
DBLP"
Toward Design of an Intelligent Cyber Attack Detection System using Hybrid Feature Reduced Approach for IoT Networks,"Prabhat Kumar, Govind P. Gupta, Rakesh Tripathi",Arabian Journal for Science and Engineering,2021-01-11,"<a href=""Springer (2021-01-11) : Toward Design of an Intelligent Cyber Attack Detection System using Hybrid Feature Reduced Approach for IoT Networks"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-020-05181-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-020-05181-3]</a>","With simple connectivity and fast-growing demand of smart devices and networks, IoT has become more prone to cyber attacks. In order to detect and...",,Springer
EdgeKeeper: a trusted edge computing framework for ubiquitous power Internet of Things,"Weiyong Yang, Wei Liu, ... Longyun Qi",Frontiers of Information Technology & Electronic Engineering,2021-01-08,"<a href=""Springer (2021-01-08) : EdgeKeeper: a trusted edge computing framework for ubiquitous power Internet of Things"" target=""_blank"">[https://link.springer.com/article/10.1631/FITEE.1900636]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1631/FITEE.1900636]</a>","Ubiquitous power Internet of Things (IoT) is a smart service system oriented to all aspects of the power system, and has the characteristics of...",,Springer
Enabling isolation and recovery in PLC redundancy framework of metro train systems,"Edwin Franco Myloth Josephlal, Sridhar Adepu, ... Jianying Zhou",International Journal of Information Security,2021-01-04,"<a href=""Springer (2021-01-04) : Enabling isolation and recovery in PLC redundancy framework of metro train systems"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-020-00529-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-020-00529-0]</a>",The current train systems are heavily computerized which makes them highly prone to cyberattacks. Many functions in the trains are controlled by...,,Springer
From federated learning to federated neural architecture search: a survey,"Hangyu Zhu, Haoyu Zhang, Yaochu Jin",Complex & Intelligent Systems,2021-01-04,"<a href=""Springer (2021-01-04) : From federated learning to federated neural architecture search: a survey"" target=""_blank"">[https://link.springer.com/article/10.1007/s40747-020-00247-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s40747-020-00247-z]</a>","Federated learning is a recently proposed distributed machine learning paradigm for privacy preservation, which has found a wide range of...",,Springer
EEG-Based Brain-Computer Interfaces Are Vulnerable to Backdoor Attacks,"Lubin Meng, Jian Huang, Zhigang Zeng, Xue Jiang, Shan Yu, Tzyy-Ping Jung, Chin-Teng Lin, Ricardo Chavarriaga, Dongrui Wu","arXiv
arXiv","2021-01-02
2020-11","<a href=""arXiv (2021-01-02) : EEG-Based Brain-Computer Interfaces Are Vulnerable to Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/2011.00101v2]</a>
<a href=""DBLP (2020-11) : EEG-Based Brain-Computer Interfaces Are Vulnerable to Backdoor Attacks"" target=""_blank"">[https://arxiv.org/abs/2011.00101]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2011.00101]</a>","Research and development of electroencephalogram (EEG) based brain-computer interfaces (BCIs) have advanced rapidly, partly due to deeper understanding of the brain and wide adoption of sophisticated machine learning approaches for decoding the EEG signals. However, recent studies have shown that machine learning algorithms are vulnerable to adversarial attacks. This article proposes to use narrow period pulse for poisoning attack of EEG-based BCIs, which is implementable in practice and has never been considered before. One can create dangerous backdoors in the machine learning model by injecting poisoning samples into the training set. Test samples with the backdoor key will then be classified into the target class specified by the attacker. What most distinguishes our approach from previous ones is that the backdoor key does not need to be synchronized with the EEG trials, making it very easy to implement. The effectiveness and robustness of the backdoor attack approach is demonstrated, highlighting a critical security concern for EEG-based BCIs and calling for urgent attention to address it.
","
","arXiv
DBLP"
Blacksite: human-in-the-loop artificial immune system for intrusion detection in internet of things,"James Brown, Mohd Anwar",Human-Intelligent Systems Integration,2021-01-02,"<a href=""Springer (2021-01-02) : Blacksite: human-in-the-loop artificial immune system for intrusion detection in internet of things"" target=""_blank"">[https://link.springer.com/article/10.1007/s42454-020-00017-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42454-020-00017-9]</a>",The Internet of Things (IoT) has rapidly changed information systems and networks in a significant way. Traditional networks are experiencing...,,Springer
Learning Pseudo-Backdoors for Mixed Integer Programs,Ferber A.,"14th International Symposium on Combinatorial Search, SoCS 2021
Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","2021-01-01
2022-01-01","<a href=""ScienceDirect (2021-01-01) : Learning Pseudo-Backdoors for Mixed Integer Programs"" target=""_blank"">[]</a>
<a href=""ScienceDirect (2022-01-01) : Learning Pseudo-Backdoors for Mixed Integer Programs"" target=""_blank"">[https://doi.org/10.1007/978-3-031-08011-1_8]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>
<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-031-08011-1_8]</a>","
","
","ScienceDirect
ScienceDirect"
A Mimic Cloud Ruling Method for Defending Against Time-Delayed Covert Channel Attacks,Zeng W.,"Proceedings - 2021 International Conference on Information Science, Parallel and Distributed Systems, ISPDS 2021",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : A Mimic Cloud Ruling Method for Defending Against Time-Delayed Covert Channel Attacks"" target=""_blank"">[https://doi.org/10.1109/ISPDS54097.2021.00020]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ISPDS54097.2021.00020]</a>",,,ScienceDirect
A Synergetic Attack against Neural Network Classifiers combining Backdoor and Adversarial Examples,Liu G.,"Proceedings - 2021 IEEE International Conference on Big Data, Big Data 2021",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : A Synergetic Attack against Neural Network Classifiers combining Backdoor and Adversarial Examples"" target=""_blank"">[https://doi.org/10.1109/BigData52589.2021.9671964]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/BigData52589.2021.9671964]</a>",,,ScienceDirect
A Textual Clean-Label Backdoor Attack Strategy against Spam Detection,Yerlikaya F.A.,"Proceedings - 2021 14th International Conference on Security of Information and Networks, SIN 2021",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : A Textual Clean-Label Backdoor Attack Strategy against Spam Detection"" target=""_blank"">[https://doi.org/10.1109/SIN54109.2021.9699173]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SIN54109.2021.9699173]</a>",,,ScienceDirect
A panoramic view of cyber attack detection and prevention using machine learning and deep learning approaches,Daniel E.,Applied Learning Algorithms for Intelligent IoT,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : A panoramic view of cyber attack detection and prevention using machine learning and deep learning approaches"" target=""_blank"">[https://doi.org/10.1201/9781003119838-4]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1201/9781003119838-4]</a>",,,ScienceDirect
BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning,Wang L.,IJCAI International Joint Conference on Artificial Intelligence,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : BACKDOORL: Backdoor Attack against Competitive Reinforcement Learning"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Backdoor Attacks Against Deep Learning Systems in the Physical World,Wenger E.,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Backdoor Attacks Against Deep Learning Systems in the Physical World"" target=""_blank"">[https://doi.org/10.1109/CVPR46437.2021.00614]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CVPR46437.2021.00614]</a>",,,ScienceDirect
Backdoor Scanning for Deep Neural Networks through K-Arm Optimization,Shen G.,Proceedings of Machine Learning Research,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Backdoor Scanning for Deep Neural Networks through K-Arm Optimization"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Backdoor attack against speaker verification,Zhai T.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Backdoor attack against speaker verification"" target=""_blank"">[https://doi.org/10.1109/ICASSP39728.2021.9413468]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP39728.2021.9413468]</a>",,,ScienceDirect
Blind backdoors in deep learning models,Bagdasaryan E.,Proceedings of the 30th USENIX Security Symposium,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Blind backdoors in deep learning models"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
CLEAR: Clean-up Sample-Targeted Backdoor in Neural Networks,Zhu L.,Proceedings of the IEEE International Conference on Computer Vision,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : CLEAR: Clean-up Sample-Targeted Backdoor in Neural Networks"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01614]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01614]</a>",,,ScienceDirect
CRFL: Certifiably Robust Federated Learning against Backdoor Attacks,Xie C.,Proceedings of Machine Learning Research,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : CRFL: Certifiably Robust Federated Learning against Backdoor Attacks"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Can a Differential Attack Work for an Arbitrarily Large Number of Rounds?,Courtois N.T.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Can a Differential Attack Work for an Arbitrarily Large Number of Rounds?"" target=""_blank"">[https://doi.org/10.1007/978-3-030-68890-5_9]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-030-68890-5_9]</a>",,,ScienceDirect
Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification,Cheng S.,"35th AAAI Conference on Artificial Intelligence, AAAI 2021",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Defend Against Poisoning Attacks in Federated Learning,Zhu C.,Communications in Computer and Information Science,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Defend Against Poisoning Attacks in Federated Learning"" target=""_blank"">[https://doi.org/10.1007/978-981-16-7502-7_26]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-981-16-7502-7_26]</a>",,,ScienceDirect
Defending against Backdoors in Federated Learning with Robust Learning Rate,Ozdayi M.S.,"35th AAAI Conference on Artificial Intelligence, AAAI 2021",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Defending against Backdoors in Federated Learning with Robust Learning Rate"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Demon in the Variant: Statistical analysis of DNNs for robust backdoor contamination detection,Di T.,Proceedings of the 30th USENIX Security Symposium,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Demon in the Variant: Statistical analysis of DNNs for robust backdoor contamination detection"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Explanation-guided backdoor poisoning attacks against malware classifiers,Severi G.,Proceedings of the 30th USENIX Security Symposium,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Explanation-guided backdoor poisoning attacks against malware classifiers"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Graph backdoor,Xi Z.,Proceedings of the 30th USENIX Security Symposium,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Graph backdoor"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Hidden killer: Invisible textual backdoor attacks with syntactic trigger,Qi F.,"ACL-IJCNLP 2021 - 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the Conference",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Hidden killer: Invisible textual backdoor attacks with syntactic trigger"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Honeypots Vulnerabilities to Backdoor Attack,Javid F.,"14th International Conference on Information Security and Cryptology, ISCTURKEY 2021 - Proceedings",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Honeypots Vulnerabilities to Backdoor Attack"" target=""_blank"">[https://doi.org/10.1109/ISCTURKEY53027.2021.9654401]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ISCTURKEY53027.2021.9654401]</a>",,,ScienceDirect
Identifying and blocking the backdoors in Linux,Ylli E.,CEUR Workshop Proceedings,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Identifying and blocking the backdoors in Linux"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Improved Torsion-Point Attacks on SIDH Variants,de Quehen V.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Improved Torsion-Point Attacks on SIDH Variants"" target=""_blank"">[https://doi.org/10.1007/978-3-030-84252-9_15]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-030-84252-9_15]</a>",,,ScienceDirect
Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks,Schwarzschild A.,Proceedings of Machine Learning Research,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
L-Red: Efficient post-training detection of imperceptible backdoor attacks without access to the training set,Xiang Z.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : L-Red: Efficient post-training detection of imperceptible backdoor attacks without access to the training set"" target=""_blank"">[https://doi.org/10.1109/ICASSP39728.2021.9414562]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP39728.2021.9414562]</a>",,,ScienceDirect
Manipulating SGD with Data Ordering Attacks,Shumailov I.,Advances in Neural Information Processing Systems,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Manipulating SGD with Data Ordering Attacks"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer,Qi F.,"EMNLP 2021 - 2021 Conference on Empirical Methods in Natural Language Processing, Proceedings",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Moat: Model Agnostic Defense against Targeted Poisoning Attacks in Federated Learning,Manna A.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Moat: Model Agnostic Defense against Targeted Poisoning Attacks in Federated Learning"" target=""_blank"">[https://doi.org/10.1007/978-3-030-86890-1_3]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-030-86890-1_3]</a>",,,ScienceDirect
NEURAL ATTENTION DISTILLATION: ERASING BACKDOOR TRIGGERS FROM DEEP NEURAL NETWORKS,Li Y.,ICLR 2021 - 9th International Conference on Learning Representations,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : NEURAL ATTENTION DISTILLATION: ERASING BACKDOOR TRIGGERS FROM DEEP NEURAL NETWORKS"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
ONION: A Simple and Effective Defense Against Textual Backdoor Attacks,Qi F.,"EMNLP 2021 - 2021 Conference on Empirical Methods in Natural Language Processing, Proceedings",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : ONION: A Simple and Effective Defense Against Textual Backdoor Attacks"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Protecting Deep Cerebrospinal Fluid Cell Image Processing Models with Backdoor and Semi-Distillation,Li F.Q.,DICTA 2021 - 2021 International Conference on Digital Image Computing: Techniques and Applications,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Protecting Deep Cerebrospinal Fluid Cell Image Processing Models with Backdoor and Semi-Distillation"" target=""_blank"">[https://doi.org/10.1109/DICTA52665.2021.9647115]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/DICTA52665.2021.9647115]</a>",,,ScienceDirect
RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models,Yang W.,"EMNLP 2021 - 2021 Conference on Empirical Methods in Natural Language Processing, Proceedings",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Research on the Investment Decision of Backdoor-listed Companies Based on the Intrinsic Value of Stocks,Sun Y.,"Proceedings of the 33rd Chinese Control and Decision Conference, CCDC 2021",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Research on the Investment Decision of Backdoor-listed Companies Based on the Intrinsic Value of Stocks"" target=""_blank"">[https://doi.org/10.1109/CCDC52312.2021.9601993]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CCDC52312.2021.9601993]</a>",,,ScienceDirect
Rethinking stealthiness of backdoor attack against NLP models,Yang W.,"ACL-IJCNLP 2021 - 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the Conference",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Rethinking stealthiness of backdoor attack against NLP models"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective,Zeng Y.,Proceedings of the IEEE International Conference on Computer Vision,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01616]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01616]</a>",,,ScienceDirect
Robust Backdoor Attacks against Deep Neural Networks in Real Physical World,Xue M.,"Proceedings - 2021 IEEE 20th International Conference on Trust, Security and Privacy in Computing and Communications, TrustCom 2021",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Robust Backdoor Attacks against Deep Neural Networks in Real Physical World"" target=""_blank"">[https://doi.org/10.1109/TrustCom53373.2021.00093]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TrustCom53373.2021.00093]</a>",,,ScienceDirect
SIMTROJAN: STEALTHY BACKDOOR ATTACK,Ren Y.,"Proceedings - International Conference on Image Processing, ICIP",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : SIMTROJAN: STEALTHY BACKDOOR ATTACK"" target=""_blank"">[https://doi.org/10.1109/ICIP42928.2021.9506313]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICIP42928.2021.9506313]</a>",,,ScienceDirect
SnakeGX: A Sneaky Attack Against SGX Enclaves,Toffalini F.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021-01-01,"<a href=""ScienceDirect (2021-01-01) : SnakeGX: A Sneaky Attack Against SGX Enclaves"" target=""_blank"">[https://doi.org/10.1007/978-3-030-78372-3_13]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-030-78372-3_13]</a>",,,ScienceDirect
Strong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff,Borgnia E.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Strong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff"" target=""_blank"">[https://doi.org/10.1109/ICASSP39728.2021.9414862]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP39728.2021.9414862]</a>",,,ScienceDirect
T-Miner: A generative approach to defend against trojan attacks on DNN-based Text Classification,Azizi A.,Proceedings of the 30th USENIX Security Symposium,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : T-Miner: A generative approach to defend against trojan attacks on DNN-based Text Classification"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
TARGETED ATTACK AGAINST DEEP NEURAL NETWORKS VIA FLIPPING LIMITED WEIGHT BITS,Bai J.,ICLR 2021 - 9th International Conference on Learning Representations,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : TARGETED ATTACK AGAINST DEEP NEURAL NETWORKS VIA FLIPPING LIMITED WEIGHT BITS"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Textual Backdoor Attack for the Text Classification System,Kwon H.,Security and Communication Networks,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Textual Backdoor Attack for the Text Classification System"" target=""_blank"">[https://doi.org/10.1155/2021/2938386]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1155/2021/2938386]</a>",,,ScienceDirect
TridentShell: a Covert and Scalable Backdoor Injection Attack on Web Applications,Yu X.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021-01-01,"<a href=""ScienceDirect (2021-01-01) : TridentShell: a Covert and Scalable Backdoor Injection Attack on Web Applications"" target=""_blank"">[https://doi.org/10.1007/978-3-030-91356-4_10]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-030-91356-4_10]</a>",,,ScienceDirect
Turn the combination lock: Learnable textual backdoor attacks via word substitution,Qi F.,"ACL-IJCNLP 2021 - 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the Conference",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Turn the combination lock: Learnable textual backdoor attacks via word substitution"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Using Deep learning for network traffic prediction to secure Software networks against DDoS attacks,Sulaga D.T.,"CITISIA 2021 - IEEE Conference on Innovative Technologies in Intelligent System and Industrial Application, Proceedings",2021-01-01,"<a href=""ScienceDirect (2021-01-01) : Using Deep learning for network traffic prediction to secure Software networks against DDoS attacks"" target=""_blank"">[https://doi.org/10.1109/CITISIA53721.2021.9719978]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CITISIA53721.2021.9719978]</a>",,,ScienceDirect
VarDefense: Variance-Based Defense against Poison Attack,Fan M.,Wireless Communications and Mobile Computing,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : VarDefense: Variance-Based Defense against Poison Attack"" target=""_blank"">[https://doi.org/10.1155/2021/1974822]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1155/2021/1974822]</a>",,,ScienceDirect
WADS: A Webshell Attack Defender Assisted by Software-Defined Networks,Yu B.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021-01-01,"<a href=""ScienceDirect (2021-01-01) : WADS: A Webshell Attack Defender Assisted by Software-Defined Networks"" target=""_blank"">[https://doi.org/10.1007/978-3-030-93206-0_13]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-030-93206-0_13]</a>",,,ScienceDirect
WANET - IMPERCEPTIBLE WARPING-BASED BACKDOOR ATTACK,Nguyen A.T.,ICLR 2021 - 9th International Conference on Learning Representations,2021-01-01,"<a href=""ScienceDirect (2021-01-01) : WANET - IMPERCEPTIBLE WARPING-BASED BACKDOOR ATTACK"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
WaNet - Imperceptible Warping-based Backdoor Attack,"Tuan Anh Nguyen, Anh Tuan Tran","ICLR
arXiv","2021
2021-02","<a href=""DBLP (2021) : WaNet - Imperceptible Warping-based Backdoor Attack"" target=""_blank"">[https://openreview.net/forum?id=eEn8KTtJOx]</a>
<a href=""DBLP (2021-02) : WaNet - Imperceptible Warping-based Backdoor Attack"" target=""_blank"">[https://arxiv.org/abs/2102.10369]</a>","<a href=""DBLP"" target=""_blank"">[https://openreview.net/forum?id=eEn8KTtJOx]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2102.10369]</a>","
","
","DBLP
DBLP"
Defending Deep Neural Networks against Backdoor Attack by Using De-trigger Autoencoder,H. Kwon,"IEEE Access
2021 12th International Conference on Computing Communication and Networking Technologies, ICCCNT 2021","2021
2021-01-01","<a href=""IEEE (2021) : Defending Deep Neural Networks against Backdoor Attack by Using De-trigger Autoencoder"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9579062]</a>
<a href=""ScienceDirect (2021-01-01) : Defending Deep Neural Networks against Backdoor Attack by Using De-trigger Autoencoder"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2021.3086529]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2021.3086529]</a>
<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2021.3086529]</a>","A backdoor attack is a method that causes misrecognition in a deep neural network by training it on additional data that have a specific trigger. The network will correctly recognize normal samples (which lack the specific trigger) as their proper classes but will misrecognize backdoor samples (which contain the trigger) as target classes. In this paper, I propose a method of defense against backdoor attacks that uses a de-trigger autoencoder. In the proposed scheme, the trigger in the backdoor sample is removed using the de-trigger autoencoder, and the backdoor sample is detected from the change in the classification result. Experiments were conducted using MNIST, Fashion-MNIST, and CIFAR-10 as the experimental datasets and TensorFlow as the machine learning library. For MNIST, Fashion-MNIST, and CIFAR-10, respectively, the proposed method detected 91.5%, 82.3%, and 90.9% of the backdoor samples and had 96.1%, 89.6%, and 91.2% accuracy on legitimate samples.
","
","IEEE
ScienceDirect"
Backdoor Attack of Graph Neural Networks Based on Subgraph Trigger,"Yu Sheng, Rong Chen, ... Li Kuang","Collaborative Computing: Networking, Applications and Worksharing
CollaborateCom","2021
2021","<a href=""Springer (2021) : Backdoor Attack of Graph Neural Networks Based on Subgraph Trigger"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92638-0_17]</a>
<a href=""DBLP (2021) : Backdoor Attack of Graph Neural Networks Based on Subgraph Trigger"" target=""_blank"">[https://doi.org/10.1007/978-3-030-92638-0_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92638-0_17]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-030-92638-0_17]</a>","Graph Neural Networks (GNN) is a kind of deep learning model to process structural and semantic features of graph data. They are widely used in node...
","
","Springer
DBLP"
A Characterisation of Smart Grid DoS Attacks,"Dilara Acarali, Muttukrishnan Rajarajan, ... Mark Ginzburg",Security and Privacy in New Computing Environments,2021,"<a href=""Springer (2021) : A Characterisation of Smart Grid DoS Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-66922-5_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-66922-5_1]</a>",Traditional power grids are evolving to keep pace with the demands of the modern age. Smart grids contain integrated IT systems for better management...,,Springer
A Collaborative Intrusion Detection System Using Deep Blockchain Framework for Securing Cloud Networks,"Osama Alkadi, Nour Moustafa, Benjamin Turnbull",Intelligent Systems and Applications,2021,"<a href=""Springer (2021) : A Collaborative Intrusion Detection System Using Deep Blockchain Framework for Securing Cloud Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-55180-3_41]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-55180-3_41]</a>","Security solutions, especially intrusion detection and blockchain, have been individually employed in the cloud for detecting cyber threats and...",,Springer
"A Comparative Study of Techniques, Datasets and Performances for Intrusion Detection Systems in IoT","Arathi Boyanapalli, A. Shanthini",Artificial Intelligence Techniques for Advanced Computing Applications,2021,"<a href=""Springer (2021) : A Comparative Study of Techniques, Datasets and Performances for Intrusion Detection Systems in IoT"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-5329-5_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-5329-5_22]</a>",IoT Security is the area concerned with safeguarding connected systems. IoT involves the set-up of various integrated devices. Devices are identified...,,Springer
A Master Key backdoor for universal impersonation attack against DNN-based face verification,"Wei Guo, Benedetta Tondi, Mauro Barni",Pattern Recognit. Lett.,2021,"<a href=""DBLP (2021) : A Master Key backdoor for universal impersonation attack against DNN-based face verification"" target=""_blank"">[https://doi.org/10.1016/j.patrec.2021.01.009]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.patrec.2021.01.009]</a>",,,DBLP
A Multi Class Classification for Detection of IoT Botnet Malware,"Hrushikesh Chunduri, T. Gireesh Kumar, P. V Sai Charan","Computing Science, Communication and Security",2021,"<a href=""Springer (2021) : A Multi Class Classification for Detection of IoT Botnet Malware"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-76776-1_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-76776-1_2]</a>",Botnets are one of the most prevailing threats for cyber-physical devices around the world. The evolution of botnet attacks has been rampant and...,,Springer
A Random Multi-target Backdooring Attack on Deep Neural Networks,"Xinrui Liu, Xiao Yu, Zhibin Zhang, Quanxin Zhang, Yuanzhang Li, Yu-an Tan",DMBD,2021,"<a href=""DBLP (2021) : A Random Multi-target Backdooring Attack on Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1007/978-981-16-7502-7_5]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-981-16-7502-7_5]</a>",,,DBLP
A Recent Research on Malware Detection Using Machine Learning Algorithm: Current Challenges and Future Works,"Nor Zakiah Gorment, Ali Selamat, Ondrej Krejcar",Advances in Visual Informatics,2021,"<a href=""Springer (2021) : A Recent Research on Malware Detection Using Machine Learning Algorithm: Current Challenges and Future Works"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90235-3_41]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90235-3_41]</a>","Each year, malware issues remain one of the cybersecurity concerns since malware’s complexity is constantly changing as the innovation rapidly grows....",,Springer
A Short Review: Issues and Threats Pertaining the Security of SCADA Systems,"Qais Saif Qassim, Norziana Jamil, ... Lariyah Mohd Sidek",Advances in Cyber Security,2021,"<a href=""Springer (2021) : A Short Review: Issues and Threats Pertaining the Security of SCADA Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-8059-5_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-8059-5_14]</a>","SCADA systems are commonly used to track and manage utilities in vital national infrastructures including electricity generation and delivery,...",,Springer
A Study on IDS Based CMAC Neuron Network to Improve the Attack Detection Rate,"Trong-Minh Hoang, Trang-Linh Le Thi",Industrial Networks and Intelligent Systems,2021,"<a href=""Springer (2021) : A Study on IDS Based CMAC Neuron Network to Improve the Attack Detection Rate"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-77424-0_39]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-77424-0_39]</a>",The massive growth of the Internet of Things has brought a lot of attractive benefits because it is going to have a positive impact on life and work...,,Springer
A Survey of Energy Theft Detection Approaches in Smart Meters,"Divam Lehri, Arjun Choudhary",Intelligent Energy Management Technologies,2021,"<a href=""Springer (2021) : A Survey of Energy Theft Detection Approaches in Smart Meters"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-8820-4_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-8820-4_2]</a>",Stealing of electricity through meter tampering has always been a major cause not only for loss of revenue to the governments but also for irregular...,,Springer
A Survey of Machine Learning Techniques for IoT Security,Cao Tien Thanh,"Future Data and Security Engineering. Big Data, Security and Privacy, Smart City and Industry 4.0 Applications",2021,"<a href=""Springer (2021) : A Survey of Machine Learning Techniques for IoT Security"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-8062-5_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-8062-5_9]</a>",The Internet of Things (IoT) has begun to reform and alter our lives due to its rapid expansion. The internet-connected deployment of a significant...,,Springer
A Survey on Common Threats in npm and PyPi Registries,"Berkay Kaplan, Jingyu Qian",Deployable Machine Learning for Security Defense,2021,"<a href=""Springer (2021) : A Survey on Common Threats in npm and PyPi Registries"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-87839-9_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-87839-9_6]</a>","Software engineers regularly use JavaScript and Python for both front-end and back-end automation tasks. On top of JavaScript and Python, there are...",,Springer
A Trigger Exploration Method for Backdoor Attacks on Deep Learning-Based Traffic Control Systems,"Yue Wang, Michail Maniatakos, Saif Eddin Jabari",CDC,2021,"<a href=""DBLP (2021) : A Trigger Exploration Method for Backdoor Attacks on Deep Learning-Based Traffic Control Systems"" target=""_blank"">[https://doi.org/10.1109/CDC45484.2021.9683577]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CDC45484.2021.9683577]</a>",,,DBLP
A Two-Level Hybrid Intrusion Detection Learning Method,"K. Gayatri, B. Premamayudu, M. Srikanth Yadav",Machine Intelligence and Soft Computing,2021,"<a href=""Springer (2021) : A Two-Level Hybrid Intrusion Detection Learning Method"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-9516-5_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-9516-5_21]</a>",The goal of this analysis is to identify intrusions on CSE-CIC-IDS2018 datasets. The methods used are split into two sections as a one-tier and...,,Springer
AI-Based Voice Assistants Technology Comparison in Term of Conversational and Response Time,"Yusuph J. Koni, Mohammed Abdulhakim Al-Absi, ... Hoon Jae Lee",Intelligent Human Computer Interaction,2021,"<a href=""Springer (2021) : AI-Based Voice Assistants Technology Comparison in Term of Conversational and Response Time"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-68452-5_39]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-68452-5_39]</a>","With the rapid development of artificial intelligence (AI) technology, the field of mobile phone technology is also developing in the direction of...",,Springer
ARCSECURE: Centralized Hub for Securing a Network of IoT Devices,"Kavinga Yapa Abeywardena, A. M. I. S. Abeykoon, ... C. N. Samarasekara",Intelligent Computing,2021,"<a href=""Springer (2021) : ARCSECURE: Centralized Hub for Securing a Network of IoT Devices"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-80129-8_70]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-80129-8_70]</a>","As far as it is considered, IoT has been a game changer in the advancement of technology. In the current context, the major issue that users face is...",,Springer
Abuse Resistant Law Enforcement Access Systems,"Matthew Green, Gabriel Kaptchuk, Gijs Van Laer",Advances in Cryptology – EUROCRYPT 2021,2021,"<a href=""Springer (2021) : Abuse Resistant Law Enforcement Access Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-77883-5_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-77883-5_19]</a>",The increasing deployment of end-to-end encrypted communications services has ignited a debate between technology firms and law enforcement agencies...,,Springer
Adversaries Strike Hard: Adversarial Attacks Against Malware Classifiers Using Dynamic API Calls as Features,"Hariom, Anand Handa, ... Sandeep Kumar Shukla",Cyber Security Cryptography and Machine Learning,2021,"<a href=""Springer (2021) : Adversaries Strike Hard: Adversarial Attacks Against Malware Classifiers Using Dynamic API Calls as Features"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-78086-9_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-78086-9_2]</a>","Malware designers have become increasingly sophisticated over time, crafting polymorphic and metamorphic malware employing obfuscation tricks such as...",,Springer
Adware Attack Detection on IoT Devices Using Deep Logistic Regression SVM (DL-SVM-IoT),"E. Arul, A. Punidha",Congress on Intelligent Systems,2021,"<a href=""Springer (2021) : Adware Attack Detection on IoT Devices Using Deep Logistic Regression SVM (DL-SVM-IoT)"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-33-6981-8_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-33-6981-8_14]</a>",Malware represents an imminent threat to businesses and users every day. It is a fact that protection systems cannot compete today whether it is...,,Springer
Algebraic Adversaries in the Universal Composability Framework,"Michel Abdalla, Manuel Barbosa, ... Jiayu Xu",Advances in Cryptology – ASIACRYPT 2021,2021,"<a href=""Springer (2021) : Algebraic Adversaries in the Universal Composability Framework"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92078-4_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92078-4_11]</a>","The algebraic-group model (AGM), which lies between the generic group model and the standard model of computation, provides a means by which to...",,Springer
An Adaptive Algorithm Based on Adaboost for Mimicry Multimode Decisions,"Feng Wang, Dingde Jiang, ... Yingchun Chen",Simulation Tools and Techniques,2021,"<a href=""Springer (2021) : An Adaptive Algorithm Based on Adaboost for Mimicry Multimode Decisions"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-72792-5_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-72792-5_12]</a>",The traditional information security protection cannot prevent the malicious and directed intrusion of the network. To discover potential risks...,,Springer
An Analysis of Remotely Triggered Malware Exploits in Content Management System-Based Web Applications,"C. Kavithamani, R. S. Sankara Subramanian, ... Gayatri Iyer",Intelligence in Big Data Technologies—Beyond the Hype,2021,"<a href=""Springer (2021) : An Analysis of Remotely Triggered Malware Exploits in Content Management System-Based Web Applications"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-5285-4_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-5285-4_15]</a>","Web content management systems, referred here as CMS, is a customizable software platform on which websites can be easily built. According to Web...",,Springer
An Assessment of Obfuscated Ransomware Detection and Prevention Methods,"Ahmad Ghafarian, Deniz Keskin, Graham Helton",Advances in Information and Communication,2021,"<a href=""Springer (2021) : An Assessment of Obfuscated Ransomware Detection and Prevention Methods"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-73100-7_56]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-73100-7_56]</a>","Ransomware is a special type of malware, which infects a system and limits user’s access to the system and its resources until a ransom is paid. It...",,Springer
An Introduction to Network Security Attacks,Mayank Srivastava,Inventive Systems and Control,2021,"<a href=""Springer (2021) : An Introduction to Network Security Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-1395-1_37]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-1395-1_37]</a>",The computer network and its computing resources are rapidly growing along with the advancement in Internet technology. Apart from using computing...,,Springer
Anomaly Intrusion Detection Systems in IoT Using Deep Learning Techniques: A Survey,"Muaadh. A. Alsoufi, Shukor Razak, ... Salah Abdo",Innovative Systems for Intelligent Health Informatics,2021,"<a href=""Springer (2021) : Anomaly Intrusion Detection Systems in IoT Using Deep Learning Techniques: A Survey"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-70713-2_60]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-70713-2_60]</a>",Security has a major role to play in the utilization and operations of the internet of things (IoT). Several studies have explored anomaly intrusion...,,Springer
Application of Neural Networks in Intrusion Monitoring Systems for Wireless Sensor Networks,"Olexander Belej, Kostiantyn Kolesnyk, Orest Polotai",Advances in Intelligent Systems and Computing V,2021,"<a href=""Springer (2021) : Application of Neural Networks in Intrusion Monitoring Systems for Wireless Sensor Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-63270-0_74]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-63270-0_74]</a>",This article has shown the current state of the intrusion detection area and the main areas of research. Network attack detection is currently one of...,,Springer
Artificial Empathy for Clinical Companion Robots with Privacy-By-Design,"Miguel Vargas Martin, Eduardo Pérez Valle, Sheri Horsburgh",Wireless Mobile Communication and Healthcare,2021,"<a href=""Springer (2021) : Artificial Empathy for Clinical Companion Robots with Privacy-By-Design"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-70569-5_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-70569-5_23]</a>",We present a prototype whereby we enabled a humanoid robot to be used to assist mental health patients and their families. Our approach removes the...,,Springer
Association Attacks in IEEE 802.11: Exploiting WiFi Usability Features,"George Chatzisofroniou, Panayiotis Kotzanikolaou",Socio-Technical Aspects in Security and Trust,2021,"<a href=""Springer (2021) : Association Attacks in IEEE 802.11: Exploiting WiFi Usability Features"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-55958-8_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-55958-8_6]</a>","Association attacks in IEEE 802.11 aim to manipulate wireless clients into associating with a malicious access point, usually by exploiting usability...",,Springer
Backdoor Attack Against Speaker Verification,"Tongqing Zhai, Yiming Li, Ziqi Zhang, Baoyuan Wu, Yong Jiang, Shu-Tao Xia",ICASSP,2021,"<a href=""DBLP (2021) : Backdoor Attack Against Speaker Verification"" target=""_blank"">[https://doi.org/10.1109/ICASSP39728.2021.9413468]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICASSP39728.2021.9413468]</a>",,,DBLP
Backdoor Attack with Imperceptible Input and Latent Modification,"Khoa D. Doan, Yingjie Lao, Ping Li",NeurIPS,2021,"<a href=""DBLP (2021) : Backdoor Attack with Imperceptible Input and Latent Modification"" target=""_blank"">[https://proceedings.neurips.cc/paper/2021/hash/9d99197e2ebf03fc388d09f1e94af89b-Abstract.html]</a>","<a href=""DBLP"" target=""_blank"">[https://proceedings.neurips.cc/paper/2021/hash/9d99197e2ebf03fc388d09f1e94af89b-Abstract.html]</a>",,,DBLP
Backdoor Filter: Mitigating Visible Backdoor Triggers in Dataset,"Ziqi Wei, Junjian Shi, Yihe Duan, Ranyang Liu, Ye Han, Zheli Liu",DTPI,2021,"<a href=""DBLP (2021) : Backdoor Filter: Mitigating Visible Backdoor Triggers in Dataset"" target=""_blank"">[https://doi.org/10.1109/DTPI52967.2021.9540109]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/DTPI52967.2021.9540109]</a>",,,DBLP
Bias Busters: Robustifying DL-Based Lithographic Hotspot Detectors Against Backdooring Attacks,"Kang Liu, Benjamin Tan, Gaurav Rajavendra Reddy, Siddharth Garg, Yiorgos Makris, Ramesh Karri",IEEE Trans. Comput. Aided Des. Integr. Circuits Syst.,2021,"<a href=""DBLP (2021) : Bias Busters: Robustifying DL-Based Lithographic Hotspot Detectors Against Backdooring Attacks"" target=""_blank"">[https://doi.org/10.1109/TCAD.2020.3033749]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TCAD.2020.3033749]</a>",,,DBLP
Bifurcated Signatures: Folding the Accountability vs. Anonymity Dilemma into a Single Private Signing Scheme,"Benoît Libert, Khoa Nguyen, ... Moti Yung",Advances in Cryptology – EUROCRYPT 2021,2021,"<a href=""Springer (2021) : Bifurcated Signatures: Folding the Accountability vs. Anonymity Dilemma into a Single Private Signing Scheme"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-77883-5_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-77883-5_18]</a>","Over the development of modern cryptography, often, alternative cryptographic schemes are developed to achieve goals that in some important respect...",,Springer
Breaking and Fixing Third-Party Payment Service for Mobile Apps,"Shangcheng Shi, Xianbo Wang, Wing Cheong Lau",Applied Cryptography and Network Security,2021,"<a href=""Springer (2021) : Breaking and Fixing Third-Party Payment Service for Mobile Apps"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-78375-4_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-78375-4_1]</a>","Riding on the widespread user adoption of mobile payment, a growing number of mobile apps have integrated the service from third-party payment...",,Springer
CONTRA: Defending Against Poisoning Attacks in Federated Learning,"Sana Awan, Bo Luo, Fengjun Li",Computer Security – ESORICS 2021,2021,"<a href=""Springer (2021) : CONTRA: Defending Against Poisoning Attacks in Federated Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-88418-5_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-88418-5_22]</a>","Federated learning (FL) is an emerging machine learning paradigm. With FL, distributed data owners aggregate their model updates to train a shared...",,Springer
Chain-AAFL: Chained Adversarial-Aware Federated Learning Framework,"Lina Ge, Xin He, ... Junyang Yu",Web Information Systems and Applications,2021,"<a href=""Springer (2021) : Chain-AAFL: Chained Adversarial-Aware Federated Learning Framework"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-87571-8_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-87571-8_21]</a>","Federated learning (FL) distributes model training among multiple agents, who perform training locally but only exchange gradients due to privacy...",,Springer
Cloud and Its Security Impacts on Managing a Workforce Remotely: A Reflection to Cover Remote Working Challenges,"Usman Javed Butt, William Richardson, ... Faisal Hashmi","Cybersecurity, Privacy and Freedom Protection in the Connected World",2021,"<a href=""Springer (2021) : Cloud and Its Security Impacts on Managing a Workforce Remotely: A Reflection to Cover Remote Working Challenges"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-68534-8_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-68534-8_18]</a>",Attacks against remote workers who are working from home due to the global pandemic has significantly increased. Cyber criminals have realised this...,,Springer
Collaborative Deanonymization,"Patrik Keller, Martin Florian, Rainer Böhme",Financial Cryptography and Data Security. FC 2021 International Workshops,2021,"<a href=""Springer (2021) : Collaborative Deanonymization"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-63958-0_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-63958-0_3]</a>",Privacy-seeking cryptocurrency users rely on anonymization techniques like CoinJoin and ring transactions. By using such technologies benign users...,,Springer
Combating Against Potentially Harmful Mobile Apps,"Muhammad Suleman, Tariq Rahim Soomro, ... Muhammad Alshurideh",Proceedings of the International Conference on Artificial Intelligence and Computer Vision (AICV2021),2021,"<a href=""Springer (2021) : Combating Against Potentially Harmful Mobile Apps"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-76346-6_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-76346-6_15]</a>",Mobile apps make our life much easier and simple in other aspects number of potentially harmful applications are making Smartphone users un-secure....,,Springer
Combining Oversampling with Recurrent Neural Networks for Intrusion Detection,"Jenq-Haur Wang, Tri Wanda Septian",Database Systems for Advanced Applications. DASFAA 2021 International Workshops,2021,"<a href=""Springer (2021) : Combining Oversampling with Recurrent Neural Networks for Intrusion Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-73216-5_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-73216-5_21]</a>","Previous studies on intrusion detection focus on analyzing features from existing datasets. With various types of fast-changing attacks, we need to...",,Springer
Controlling Network Traffic Microstructures for Machine-Learning Model Probing,"Henry Clausen, Robert Flood, David Aspinall",Security and Privacy in Communication Networks,2021,"<a href=""Springer (2021) : Controlling Network Traffic Microstructures for Machine-Learning Model Probing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90019-9_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90019-9_23]</a>",Network intrusion detection (NID) models increasingly rely on learning traffic microstructures that consist of pattern sequences in features such as...,,Springer
Countermeasures Against Backdoor Attacks Towards Malware Detectors,"Shintaro Narisada, Yuki Matsumoto, Seira Hidano, Toshihiro Uchibayashi, Takuo Suganuma, Masahiro Hiji, Shinsaku Kiyomoto",CANS,2021,"<a href=""DBLP (2021) : Countermeasures Against Backdoor Attacks Towards Malware Detectors"" target=""_blank"">[https://doi.org/10.1007/978-3-030-92548-2_16]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-030-92548-2_16]</a>",,,DBLP
Covid-19 digital Contact-tracing: a doorway to well-being or a backdoor to security vulnerabilities?,"Nishit Patel, David Cancel, Moitrayee Chatterjee, Md Shahinoor Rahman",IEEE BigData,2021,"<a href=""DBLP (2021) : Covid-19 digital Contact-tracing: a doorway to well-being or a backdoor to security vulnerabilities?"" target=""_blank"">[https://doi.org/10.1109/BigData52589.2021.9671880]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/BigData52589.2021.9671880]</a>",,,DBLP
Cryptanalysis of Full LowMC and LowMC-M with Algebraic Techniques,"Fukang Liu, Takanori Isobe, Willi Meier",Advances in Cryptology – CRYPTO 2021,2021,"<a href=""Springer (2021) : Cryptanalysis of Full LowMC and LowMC-M with Algebraic Techniques"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-84252-9_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-84252-9_13]</a>","In this paper, we revisit the difference enumeration technique for LowMC and develop new algebraic techniques to achieve efficient key-recovery...",,Springer
Cryptanalysis of an Oblivious PRF from Supersingular Isogenies,"Andrea Basso, Péter Kutas, ... Antonio Sanso",Advances in Cryptology – ASIACRYPT 2021,2021,"<a href=""Springer (2021) : Cryptanalysis of an Oblivious PRF from Supersingular Isogenies"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92062-3_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92062-3_6]</a>","We cryptanalyse the SIDH-based oblivious pseudorandom function from supersingular isogenies proposed at Asiacrypt’20 by Boneh, Kogan and Woo. To this...",,Springer
Cyber Attack Detection Framework for Cloud Computing,"Suryakant Badde, Vikash Kumar, ... Ditipriya Sinha",Intelligent Data Engineering and Analytics,2021,"<a href=""Springer (2021) : Cyber Attack Detection Framework for Cloud Computing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-5679-1_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-5679-1_23]</a>","To prevent cyber-attacks, cloud-based systems mainly depend upon different types of intrusion detection systems (IDS). Most of the approaches have...",,Springer
Cyber Threats of Modern Era,"Shaurya, Moutushi Singh","Proceedings of International Conference on Computational Intelligence, Data Science and Cloud Computing",2021,"<a href=""Springer (2021) : Cyber Threats of Modern Era"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-33-4968-1_51]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-33-4968-1_51]</a>",Todays world is called the age of information technology where it is impossible to circumvent computers. Information is now floating everywhere. But...,,Springer
Cybercrime in the Context of COVID-19,Mohamed Chawki,Intelligent Computing,2021,"<a href=""Springer (2021) : Cybercrime in the Context of COVID-19"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-80129-8_65]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-80129-8_65]</a>",The ongoing COVID-19 pandemic acts as a major cause of the attention of the whole world. All the aspects of life in our current world have been...,,Springer
Cybersecurity Attacks: Analysis of “WannaCry” Attack and Proposing Methods for Reducing or Preventing Such Attacks in Future,Sumaiah Algarni,ICT Systems and Sustainability,2021,"<a href=""Springer (2021) : Cybersecurity Attacks: Analysis of “WannaCry” Attack and Proposing Methods for Reducing or Preventing Such Attacks in Future"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-8289-9_73]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-8289-9_73]</a>",The technological revolution in the last few years has erased the communicational barriers and created a global network of users and data. At the...,,Springer
DeHiB: Deep Hidden Backdoor Attack on Semi-supervised Learning via Adversarial Perturbation,"Zhicong Yan, Gaolei Li, Yuan Tian, Jun Wu, Shenghong Li, Mingzhe Chen, H. Vincent Poor",AAAI,2021,"<a href=""DBLP (2021) : DeHiB: Deep Hidden Backdoor Attack on Semi-supervised Learning via Adversarial Perturbation"" target=""_blank"">[https://doi.org/10.1609/aaai.v35i12.17266]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v35i12.17266]</a>",,,DBLP
Deep Learning Approach-Based Network Intrusion Detection System for Fog-Assisted IoT,"Nausheen Sahar, Ratnesh Mishra, Sidra Kalam","Proceedings of International Conference on Big Data, Machine Learning and their Applications",2021,"<a href=""Springer (2021) : Deep Learning Approach-Based Network Intrusion Detection System for Fog-Assisted IoT"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-8377-3_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-8377-3_4]</a>","In an Internet of Things (IoT) network, miscellaneous devices exchange their resources as per their requirement via the Internet. The data aggregated...",,Springer
Deep Learning-Based Attack Detection in the Internet of Things,"Parushi Malhotra, Yashwant Singh","Proceedings of Second International Conference on Computing, Communications, and Cyber-Security",2021,"<a href=""Springer (2021) : Deep Learning-Based Attack Detection in the Internet of Things"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-0733-2_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-0733-2_22]</a>",The exponential growth of the Internet of Things in various domains has escalated the rise of concern in this digital era. The concern is primarily...,,Springer
Deep Neural Backdoor in Semi-Supervised Learning: Threats and Countermeasures,"Zhicong Yan, Jun Wu, Gaolei Li, Shenghong Li, Mohsen Guizani",IEEE Trans. Inf. Forensics Secur.,2021,"<a href=""DBLP (2021) : Deep Neural Backdoor in Semi-Supervised Learning: Threats and Countermeasures"" target=""_blank"">[https://doi.org/10.1109/TIFS.2021.3116431]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2021.3116431]</a>",,,DBLP
Defense against backdoor attacks via robust covariance estimation,"Jonathan Hayase, Weihao Kong, Raghav Somani, Sewoong Oh",ICML,2021,"<a href=""DBLP (2021) : Defense against backdoor attacks via robust covariance estimation"" target=""_blank"">[http://proceedings.mlr.press/v139/hayase21a.html]</a>","<a href=""DBLP"" target=""_blank"">[http://proceedings.mlr.press/v139/hayase21a.html]</a>",,,DBLP
Defense-Resistant Backdoor Attacks Against Deep Neural Networks in Outsourced Cloud Environment,"Xueluan Gong, Yanjiao Chen, Qian Wang, Huayang Huang, Lingshuo Meng, Chao Shen, Qian Zhang",IEEE J. Sel. Areas Commun.,2021,"<a href=""DBLP (2021) : Defense-Resistant Backdoor Attacks Against Deep Neural Networks in Outsourced Cloud Environment"" target=""_blank"">[https://doi.org/10.1109/JSAC.2021.3087237]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/JSAC.2021.3087237]</a>",,,DBLP
Detect and Remove Watermark in Deep Neural Networks via Generative Adversarial Networks,"Shichang Sun, Haoqi Wang, ... Weiqiang Liu",Information Security,2021,"<a href=""Springer (2021) : Detect and Remove Watermark in Deep Neural Networks via Generative Adversarial Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-91356-4_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-91356-4_18]</a>","Deep neural networks (DNN) have achieved remarkable performance in various fields. However, training a DNN model from scratch requires expensive...",,Springer
Detecting Scene-Plausible Perceptible Backdoors in Trained DNNs Without Access to the Training Set,"Zhen Xiang, David J. Miller, Hang Wang, George Kesidis",Neural Comput.,2021,"<a href=""DBLP (2021) : Detecting Scene-Plausible Perceptible Backdoors in Trained DNNs Without Access to the Training Set"" target=""_blank"">[https://doi.org/10.1162/neco_a_01376]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1162/neco_a_01376]</a>",,,DBLP
Detection and Prevention of Attacks on Active Directory Using SIEM,"S. Muthuraj, M. Sethumadhavan, ... R. Santhya",Information and Communication Technology for Intelligent Systems,2021,"<a href=""Springer (2021) : Detection and Prevention of Attacks on Active Directory Using SIEM"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-7062-9_53]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-7062-9_53]</a>",Active Directory is widely used in organizations to administer windows user accounts and related IT resources. It acts as centralized management to...,,Springer
"Digital Transformation for Sustainable Development Goals (SDGs) - A Security, Safety and Privacy Perspective on AI","Andreas Holzinger, Edgar Weippl, ... Peter Kieseberg",Machine Learning and Knowledge Extraction,2021,"<a href=""Springer (2021) : Digital Transformation for Sustainable Development Goals (SDGs) - A Security, Safety and Privacy Perspective on AI"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-84060-0_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-84060-0_1]</a>",The main driver of the digital transformation currently underway is undoubtedly artificial intelligence (AI). The potential of AI to benefit humanity...,,Springer
Drone Authentication Using ID-Based Signcryption in LoRaWAN Network,"Sana Benzarti, Bayrem Triki, Ouajdi Korbaa",Intelligent Systems Design and Applications,2021,"<a href=""Springer (2021) : Drone Authentication Using ID-Based Signcryption in LoRaWAN Network"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-49342-4_20]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-49342-4_20]</a>","The security of the Internet of Things (IoT) is now at the embryonic stage. In fact, unrelenting innovation has slowed down the implementation of...",,Springer
Echo-Guard: Acoustic-Based Anomaly Detection System for Smart Manufacturing Environments,"Chang-Bae Seo, Gyuseop Lee, ... Seung-Hyun Seo",Information Security Applications,2021,"<a href=""Springer (2021) : Echo-Guard: Acoustic-Based Anomaly Detection System for Smart Manufacturing Environments"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-89432-0_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-89432-0_6]</a>",The Industrial Internet of Things (IIoT) provides intelligence to industrial systems by linking sensors and devices with computer systems and...,,Springer
Evading Static and Dynamic Android Malware Detection Mechanisms,"Teenu S. John, Tony Thomas",Security in Computing and Communications,2021,"<a href=""Springer (2021) : Evading Static and Dynamic Android Malware Detection Mechanisms"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-0422-5_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-0422-5_3]</a>","With the widespread usage of Android mobile devices, malware developers are increasingly targeting Android applications for carrying out their...",,Springer
Existence versus exploitation: the opacity of backdoors and backbones,"Lane A. Hemaspaandra, David E. Narváez",Prog. Artif. Intell.,2021,"<a href=""DBLP (2021) : Existence versus exploitation: the opacity of backdoors and backbones"" target=""_blank"">[https://doi.org/10.1007/s13748-021-00234-6]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/s13748-021-00234-6]</a>",,,DBLP
Factoring Primes to Factor Moduli: Backdooring and Distributed Generation of Semiprimes,Giuseppe Vitto,IACR Cryptol. ePrint Arch.,2021,"<a href=""DBLP (2021) : Factoring Primes to Factor Moduli: Backdooring and Distributed Generation of Semiprimes"" target=""_blank"">[https://eprint.iacr.org/2021/1610]</a>","<a href=""DBLP"" target=""_blank"">[https://eprint.iacr.org/2021/1610]</a>",,,DBLP
Federated Learning-Based Intrusion Detection in the Context of IIoT Networks: Poisoning Attack and Defense,"Nguyen Chi Vy, Nguyen Huu Quyen, ... Van-Hau Pham",Network and System Security,2021,"<a href=""Springer (2021) : Federated Learning-Based Intrusion Detection in the Context of IIoT Networks: Poisoning Attack and Defense"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92708-0_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92708-0_8]</a>",The emerging of Federated Learning (FL) paradigm in training has been drawn much attention from research community because of the demand of privacy...,,Springer
Federated Learning: Issues in Medical Application,"Joo Hun Yoo, Hyejun Jeong, ... Tai-Myoung Chung",Future Data and Security Engineering,2021,"<a href=""Springer (2021) : Federated Learning: Issues in Medical Application"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-91387-8_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-91387-8_1]</a>","Since the federated learning, which makes AI learning possible without moving local data around, was introduced by google in 2017 it has been...",,Springer
Firewall for Intranet Security,"Premchand Ambhore, Archana Wankhade",International Conference on Mobile Computing and Sustainable Informatics,2021,"<a href=""Springer (2021) : Firewall for Intranet Security"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-49795-8_62]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-49795-8_62]</a>","At present, there are many commercial and noncommercial firewalls available in the market. Some of them are Squid Firewall for Unix environment,...",,Springer
Generalized Proofs of Knowledge with Fully Dynamic Setup,"Christian Badertscher, Daniel Jost, Ueli Maurer",Theory of Cryptography,2021,"<a href=""Springer (2021) : Generalized Proofs of Knowledge with Fully Dynamic Setup"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90459-3_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90459-3_17]</a>",Proofs of knowledge (PoK) are one of the most fundamental notions in cryptography. The appeal of this notion is that it provides a general template...,,Springer
Hardware Trojan Detection in Heterogeneous Systems on Chip,"Billel Guechi, Mohammed Redjimi",Innovations in Smart Cities Applications Volume 4,2021,"<a href=""Springer (2021) : Hardware Trojan Detection in Heterogeneous Systems on Chip"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-66840-2_84]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-66840-2_84]</a>","Nowadays micro-electronic chips are nearly everywhere. Since the use of modern technologies in many applications (Smart cities, Military, Aviation …)...",,Springer
How to (Legally) Keep Secrets from Mobile Operators,"Ghada Arfaoui, Olivier Blazy, ... Cristina Onete",Computer Security – ESORICS 2021,2021,"<a href=""Springer (2021) : How to (Legally) Keep Secrets from Mobile Operators"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-88418-5_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-88418-5_2]</a>","Secure-channel establishment allows two endpoints to communicate confidentially and authentically. Since they hide all data sent across them, good or...",,Springer
How to Backdoor a Cipher,"Raluca Posteuca, Tomer Ashur",IACR Cryptol. ePrint Arch.,2021,"<a href=""DBLP (2021) : How to Backdoor a Cipher"" target=""_blank"">[https://eprint.iacr.org/2021/442]</a>","<a href=""DBLP"" target=""_blank"">[https://eprint.iacr.org/2021/442]</a>",,,DBLP
How to Make Smart Contract Smarter,"Shanxuan Chen, Jia Zhu, ... Yong Tang",Computer Supported Cooperative Work and Social Computing,2021,"<a href=""Springer (2021) : How to Make Smart Contract Smarter"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-2540-4_54]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-2540-4_54]</a>","The smart contract is a self-executing code that is managed by blockchain nodes, providing coordination and enforcement framework for agreements...",,Springer
Identifying Physically Realizable Triggers for Backdoored Face Recognition Networks,"Ankita Raj, Ambar Pal, Chetan Arora",ICIP,2021,"<a href=""DBLP (2021) : Identifying Physically Realizable Triggers for Backdoored Face Recognition Networks"" target=""_blank"">[https://doi.org/10.1109/ICIP42928.2021.9506564]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICIP42928.2021.9506564]</a>",,,DBLP
Improving Convolutional Neural Network-Based Webshell Detection Through Reinforcement Learning,"Yalun Wu, Minglu Song, ... Jiqiang Liu",Information and Communications Security,2021,"<a href=""Springer (2021) : Improving Convolutional Neural Network-Based Webshell Detection Through Reinforcement Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-86890-1_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-86890-1_21]</a>","Webshell detection is highly important for network security protection. Conventional methods are based on keywords matching, which heavily relies on...",,Springer
Industrial IoT: Challenges and Mitigation Policies,"Pankaj Kumar, Amit Singh, Aritro Sengupta","Computer Networks, Big Data and IoT",2021,"<a href=""Springer (2021) : Industrial IoT: Challenges and Mitigation Policies"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-0965-7_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-0965-7_13]</a>","In today’s world, with the innovations of various technologies and the subsequent growth and competition in various sectors, technologies such as IoT...",,Springer
Intelligent Machine Learning Approach for CIDS—Cloud Intrusion Detection System,"T. Sowmya, G. Muneeswari","Computer Networks, Big Data and IoT",2021,"<a href=""Springer (2021) : Intelligent Machine Learning Approach for CIDS—Cloud Intrusion Detection System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-0965-7_67]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-0965-7_67]</a>","In this new era of information technology world, security in cloud computing has gained more importance because of the flexible nature of the cloud....",,Springer
Intelligent Strategies for Cloud Computing Risk Management and Testing,"Vinita Malik, Sukhdip Singh",Computational Methods and Data Engineering,2021,"<a href=""Springer (2021) : Intelligent Strategies for Cloud Computing Risk Management and Testing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-7907-3_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-7907-3_8]</a>",The cloud computing uses dispersed models with access whenever requested with computing devices requiring high configuration and minimum management...,,Springer
"Internet of Things backdoors: Resource management issues, security challenges, and detection methods","Soheil Hashemi, Mani Zarei",Trans. Emerg. Telecommun. Technol.,2021,"<a href=""DBLP (2021) : Internet of Things backdoors: Resource management issues, security challenges, and detection methods"" target=""_blank"">[https://doi.org/10.1002/ett.4142]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1002/ett.4142]</a>",,,DBLP
Intrusion Detection Systems in Fog Computing – A Review,"Fadi Abu Zwayed, Mohammed Anbar, ... Selvakumar Manickam",Advances in Cyber Security,2021,"<a href=""Springer (2021) : Intrusion Detection Systems in Fog Computing – A Review"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-8059-5_30]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-8059-5_30]</a>","With the growing volume of network throughput, packet transmission and security threats and attacks in Fog computing, the study of Intrusion...",,Springer
Intrusion Detection Using Deep Neural Network with AntiRectifier Layer,"Ritika Lohiya, Ankit Thakkar",Applied Soft Computing and Communication Networks,2021,"<a href=""Springer (2021) : Intrusion Detection Using Deep Neural Network with AntiRectifier Layer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-33-6173-7_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-33-6173-7_7]</a>",Data security is regarded to be one of the crucial challenges in this fast-growing internet world. Data generated through internet is exposed to...,,Springer
L-Red: Efficient Post-Training Detection of Imperceptible Backdoor Attacks Without Access to the Training Set,"Zhen Xiang, David J. Miller, George Kesidis",ICASSP,2021,"<a href=""DBLP (2021) : L-Red: Efficient Post-Training Detection of Imperceptible Backdoor Attacks Without Access to the Training Set"" target=""_blank"">[https://doi.org/10.1109/ICASSP39728.2021.9414562]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICASSP39728.2021.9414562]</a>",,,DBLP
MORTON: Detection of Malicious Routines in Large-Scale DNS Traffic,"Yael Daihes, Hen Tzaban, ... Asaf Shabtai",Computer Security – ESORICS 2021,2021,"<a href=""Springer (2021) : MORTON: Detection of Malicious Routines in Large-Scale DNS Traffic"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-88418-5_35]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-88418-5_35]</a>","We present MORTON, a method that identifies compromised devices in enterprise networks based on the existence of routine DNS communication between...",,Springer
Malware Detection in Word Documents Using Machine Learning,"Riya Khan, Nitesh Kumar, ... Sandeep K. Shukla",Advances in Cyber Security,2021,"<a href=""Springer (2021) : Malware Detection in Word Documents Using Machine Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-33-6835-4_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-33-6835-4_22]</a>",Word documents are one of the most widely used types of documents and are used every day by millions of people to share information over the...,,Springer
Mapping of Security Issues and Concerns in Cloud Computing with Compromised Security Attributes,"Alok Raj, Nitin Jain, Surendra Singh Chauhan",Cybersecurity in Emerging Digital Era,2021,"<a href=""Springer (2021) : Mapping of Security Issues and Concerns in Cloud Computing with Compromised Security Attributes"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-84842-2_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-84842-2_2]</a>","In the ongoing years with the headway of innovation, cloud computing has gotten colossal mainstream among organizations, aggressors just as people...",,Springer
Mining Trojan Detection Based on Multi-dimensional Static Features,"Zixian Tang, Qiang Wang, ... Wen Wang",Science of Cyber Security,2021,"<a href=""Springer (2021) : Mining Trojan Detection Based on Multi-dimensional Static Features"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-89137-4_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-89137-4_4]</a>",The developing technic and the variety of Mining Trojan is increasingly threatening the computational resources from the weak-defend systems. Mining...,,Springer
Mining for Privacy: How to Bootstrap a Snarky Blockchain,"Thomas Kerber, Aggelos Kiayias, Markulf Kohlweiss",Financial Cryptography and Data Security,2021,"<a href=""Springer (2021) : Mining for Privacy: How to Bootstrap a Snarky Blockchain"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-64322-8_24]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-64322-8_24]</a>","Non-interactive zero-knowledge proofs, and more specifically succinct non-interactive zero-knowledge arguments (zk-SNARKs), have been proven to be...",,Springer
Mitigating backdoor attacks in LSTM-based text classification systems by Backdoor Keyword Identification,"Chuanshuai Chen, Jiazhu Dai",Neurocomputing,2021,"<a href=""DBLP (2021) : Mitigating backdoor attacks in LSTM-based text classification systems by Backdoor Keyword Identification"" target=""_blank"">[https://doi.org/10.1016/j.neucom.2021.04.105]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.neucom.2021.04.105]</a>",,,DBLP
Models for Forming Knowledge Databases for Decision Support Systems for Recognizing Cyberattacks,"Valery Lakhno, Bakhytzhan Akhmetov, ... Karyna Khorolska",Intelligent Computing and Optimization,2021,"<a href=""Springer (2021) : Models for Forming Knowledge Databases for Decision Support Systems for Recognizing Cyberattacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-68154-8_42]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-68154-8_42]</a>",Patterns of Bayesian networks have been developed for the computing core of the decision support system in the course of threats prediction and...,,Springer
Multi-classification of UNSW-NB15 Dataset for Network Anomaly Detection System,"Aditi Roy, Khundrakpam Johnson Singh",Proceedings of International Conference on Communication and Computational Technologies,2021,"<a href=""Springer (2021) : Multi-classification of UNSW-NB15 Dataset for Network Anomaly Detection System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-5077-5_40]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-5077-5_40]</a>","In this paper, after comparing with other classification algorithms and performing feature selection on our chosen UNSW-NB15 (University of New South...",,Springer
NNrepair: Constraint-Based Repair of Neural Network Classifiers,"Muhammad Usman, Divya Gopinath, ... Corina S. Păsăreanu",Computer Aided Verification,2021,"<a href=""Springer (2021) : NNrepair: Constraint-Based Repair of Neural Network Classifiers"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-81685-8_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-81685-8_1]</a>","We present NNrepair, a constraint-based technique for repairing neural network classifiers. The technique aims to fix the logic of the network at an...",,Springer
NetFlow Datasets for Machine Learning-Based Network Intrusion Detection Systems,"Mohanad Sarhan, Siamak Layeghy, ... Marius Portmann",Big Data Technologies and Applications,2021,"<a href=""Springer (2021) : NetFlow Datasets for Machine Learning-Based Network Intrusion Detection Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-72802-1_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-72802-1_9]</a>",Machine Learning (ML)-based Network Intrusion Detection Systems (NIDSs) have become a promising tool to protect networks against cyberattacks. A wide...,,Springer
Network Information Security Technology in the Era of Big Data,"Zhenzhi Gong, Fuan Zhang",Cyber Security Intelligence and Analytics,2021,"<a href=""Springer (2021) : Network Information Security Technology in the Era of Big Data"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-70042-3_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-70042-3_13]</a>","Although network technology has brought a lot of convenience to our life and work, in the process of practical application of this technology, data...",,Springer
Neural network laundering: Removing black-box backdoor watermarks from deep neural networks,"William Aiken, Hyoungshick Kim, Simon S. Woo, Jungwoo Ryoo",Comput. Secur.,2021,"<a href=""DBLP (2021) : Neural network laundering: Removing black-box backdoor watermarks from deep neural networks"" target=""_blank"">[https://doi.org/10.1016/j.cose.2021.102277]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.cose.2021.102277]</a>",,,DBLP
New Wrapper Feature Selection Algorithm for Anomaly-Based Intrusion Detection Systems,"Meriem Kherbache, David Espes, Kamal Amroun",Foundations and Practice of Security,2021,"<a href=""Springer (2021) : New Wrapper Feature Selection Algorithm for Anomaly-Based Intrusion Detection Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-70881-8_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-70881-8_1]</a>","With advanced persistent and zero-days threats, the threat landscape is constantly evolving. Signature-based defense is ineffective against these new...",,Springer
Non-exhaustive Learning Using Gaussian Mixture Generative Adversarial Networks,"Jun Zhuang, Mohammad Al Hasan",Machine Learning and Knowledge Discovery in Databases. Research Track,2021,"<a href=""Springer (2021) : Non-exhaustive Learning Using Gaussian Mixture Generative Adversarial Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-86520-7_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-86520-7_1]</a>","Supervised learning, while deployed in real-life scenarios, often encounters instances of unknown classes. Conventional algorithms for training a...",,Springer
"On the (Ir)Replaceability of Global Setups, or How (Not) to Use a Global Ledger","Christian Badertscher, Julia Hesse, Vassilis Zikas",Theory of Cryptography,2021,"<a href=""Springer (2021) : On the (Ir)Replaceability of Global Setups, or How (Not) to Use a Global Ledger"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90453-1_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90453-1_22]</a>","In universally composable (UC) security, a global setup is intended to capture the ideal behavior of a primitive which is accessible by multiple...",,Springer
On the Effectiveness of Time Travel to Inject COVID-19 Alerts,"Vincenzo Iovino, Serge Vaudenay, Martin Vuagnoux",Topics in Cryptology – CT-RSA 2021,2021,"<a href=""Springer (2021) : On the Effectiveness of Time Travel to Inject COVID-19 Alerts"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-75539-3_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-75539-3_18]</a>",Digital contact tracing apps allow to alert people who have been in contact with people who may be contagious. The Google/Apple Exposure Notification...,,Springer
On-line Functional Testing of Memristor-mapped Deep Neural Networks using Backdoored Checksums,"Ching-Yuan Chen, Krishnendu Chakrabarty",ITC,2021,"<a href=""DBLP (2021) : On-line Functional Testing of Memristor-mapped Deep Neural Networks using Backdoored Checksums"" target=""_blank"">[https://doi.org/10.1109/ITC50571.2021.00016]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ITC50571.2021.00016]</a>",,,DBLP
P2P Bot Detection Based on Host Behavior and Big Data Technology,"P. Sai Teja, P. Hema Sirija, ... S. Saravanan",Applied Soft Computing and Communication Networks,2021,"<a href=""Springer (2021) : P2P Bot Detection Based on Host Behavior and Big Data Technology"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-33-6173-7_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-33-6173-7_11]</a>","A botnet is a herd of malware compromised devices, known as bots, connected through the Internet to perform malicious activities. The botnet can be...",,Springer
PACE with Mutual Authentication – Towards an Upgraded eID in Europe,"Patryk Kozieł, Przemysław Kubiak, Mirosław Kutyłowski",Computer Security – ESORICS 2021,2021,"<a href=""Springer (2021) : PACE with Mutual Authentication – Towards an Upgraded eID in Europe"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-88428-4_25]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-88428-4_25]</a>",In this paper we present modifications to the protocols PACE (Password Authenticated Connection Establishment) and PACE CAM (PACE with Chip...,,Springer
PBDT: Python Backdoor Detection Model Based on Combined Features,"Yong Fang, Mingyu Xie, Cheng Huang",Secur. Commun. Networks,2021,"<a href=""DBLP (2021) : PBDT: Python Backdoor Detection Model Based on Combined Features"" target=""_blank"">[https://doi.org/10.1155/2021/9923234]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1155/2021/9923234]</a>",,,DBLP
Performance Evaluation of Adversarial Attacks on Whole-Graph Embedding Models,"Mario Manzo, Maurizio Giordano, ... Mario R. Guarracino",Learning and Intelligent Optimization,2021,"<a href=""Springer (2021) : Performance Evaluation of Adversarial Attacks on Whole-Graph Embedding Models"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92121-7_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92121-7_19]</a>",Graph embedding techniques are becoming increasingly common in many fields ranging from scientific computing to biomedical applications and finance....,,Springer
Pixdoor: A Pixel-space Backdoor Attack on Deep Learning Models,"Iram Arshad, Mamoona Naveed Asghar, Yuansong Qiao, Brian Lee, Yuhang Ye",EUSIPCO,2021,"<a href=""DBLP (2021) : Pixdoor: A Pixel-space Backdoor Attack on Deep Learning Models"" target=""_blank"">[https://doi.org/10.23919/EUSIPCO54536.2021.9616118]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.23919/EUSIPCO54536.2021.9616118]</a>",,,DBLP
Policy-Compliant Signatures,"Christian Badertscher, Christian Matt, Hendrik Waldner",Theory of Cryptography,2021,"<a href=""Springer (2021) : Policy-Compliant Signatures"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90456-2_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90456-2_12]</a>",We introduce policy-compliant signatures (PCS). A PCS scheme can be used in a setting where a central authority determines a global policy and...,,Springer
Privacy Protection Framework for Credit Data in AI,"Congdong Lv, Xiaodong Zhang, Zhoubao Sun","Wireless Algorithms, Systems, and Applications",2021,"<a href=""Springer (2021) : Privacy Protection Framework for Credit Data in AI"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-86130-8_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-86130-8_23]</a>","Rich and fine personal and enterprise data are being collected and recorded, so as to provide big data support for personal and enterprise credit...",,Springer
Privacy and Security in Brain-Computer Interfaces,Sebastian Słaby,"Control, Computer Engineering and Neuroscience",2021,"<a href=""Springer (2021) : Privacy and Security in Brain-Computer Interfaces"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-72254-8_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-72254-8_19]</a>","Brain-computer interfaces (BCIs) have significantly improved the quality of life of many people through, but not limited to, helping with various...",,Springer
Protecting IoT Devices with Software-Defined Networks,Filip Holik,Intelligent Technologies and Applications,2021,"<a href=""Springer (2021) : Protecting IoT Devices with Software-Defined Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-71711-7_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-71711-7_4]</a>","The use of IoT devices is becoming more frequent in modern networks, such as Smart Grids, Smart Cities, and Smart Homes. These deployments are...",,Springer
Random Parameter Normalization Technique for Mimic Defense Based on Multi-queue Architecture,"Shunbin Li, Kun Zhang, ... Shaoyong Wu",Artificial Intelligence and Security,2021,"<a href=""Springer (2021) : Random Parameter Normalization Technique for Mimic Defense Based on Multi-queue Architecture"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-78612-0_26]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-78612-0_26]</a>","As a revolutionary and subversive theory and technology, the mimic defense system based on dynamic heterogeneous redundancy has played an essential...",,Springer
RansomLens: Understanding Ransomware via Causality Analysis on System Provenance Graph,"Rui Mei, Han-Bing Yan, Zhi-Hui Han",Science of Cyber Security,2021,"<a href=""Springer (2021) : RansomLens: Understanding Ransomware via Causality Analysis on System Provenance Graph"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-89137-4_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-89137-4_18]</a>",Malware analysis technology has been one of the most important research topics of cyber security. The recent surge in adoption of ransomware is...,,Springer
Reasoning Short Cuts in Infinite Domain Constraint Satisfaction: Algorithms and Lower Bounds for Backdoors,"Peter Jonsson, Victor Lagerkvist, Sebastian Ordyniak",CP,2021,"<a href=""DBLP (2021) : Reasoning Short Cuts in Infinite Domain Constraint Satisfaction: Algorithms and Lower Bounds for Backdoors"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.CP.2021.32]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.CP.2021.32]</a>",,,DBLP
Regulatory Considerations on Centralized Aspects of DeFi Managed by DAOs,"Ryosuke Ushida, James Angel",Financial Cryptography and Data Security. FC 2021 International Workshops,2021,"<a href=""Springer (2021) : Regulatory Considerations on Centralized Aspects of DeFi Managed by DAOs"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-63958-0_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-63958-0_2]</a>",This paper focuses on the centralized governance mechanisms of decentralized finance (DeFi) projects managed by Distributed Autonomous Organizations...,,Springer
Reinforcement Learning for Data Poisoning on Graph Neural Networks,"Jacob Dineen, A. S. M. Ahsan-Ul Haque, Matthew Bielskas","Social, Cultural, and Behavioral Modeling",2021,"<a href=""Springer (2021) : Reinforcement Learning for Data Poisoning on Graph Neural Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-80387-2_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-80387-2_14]</a>",Adversarial Machine Learning has emerged as a substantial subfield of Computer Science due to a lack of robustness in the models we train along with...,,Springer
Relationships Between Quantum IND-CPA Notions,"Tore Vincent Carstens, Ehsan Ebrahimi, ... Dominique Unruh",Theory of Cryptography,2021,"<a href=""Springer (2021) : Relationships Between Quantum IND-CPA Notions"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90459-3_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90459-3_9]</a>",An encryption scheme is called indistinguishable under chosen plaintext attack (short IND-CPA) if an attacker cannot distinguish the encryptions of...,,Springer
Remote Automated Vulnerability Assessment and Mitigation in an Organization LAN,"Nishant Sharma, H. Parveen Sultana, ... Shriniwas Patil",Advances in Distributed Computing and Machine Learning,2021,"<a href=""Springer (2021) : Remote Automated Vulnerability Assessment and Mitigation in an Organization LAN"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-4218-3_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-4218-3_22]</a>","The process of makingSharma, Nishant the network of an organization (an enterprise or an institution) fool-proof and secure holds great importance...",,Springer
Research on IoT Device Vulnerability Mining Technology Based on Static Preprocessing and Coloring Analysis,"Min Yao, Baojiang Cui, Chen Chen",Innovative Mobile and Internet Services in Ubiquitous Computing,2021,"<a href=""Springer (2021) : Research on IoT Device Vulnerability Mining Technology Based on Static Preprocessing and Coloring Analysis"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-50399-4_25]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-50399-4_25]</a>","IoT devices are playing an increasingly important role in people’s lives, and large-scale attacks on IoT devices will have serious consequences. Due...",,Springer
Research on Off-Path Exploits of Network Protocols,"Falin Hou, Xiao Yu, ... Yuanzhang Li",Data Mining and Big Data,2021,"<a href=""Springer (2021) : Research on Off-Path Exploits of Network Protocols"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-7476-1_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-7476-1_7]</a>","With the rapid development of the Internet industry, from a global perspective, the risks brought by network security are becoming increasingly...",,Springer
Research on the Application of Virtual Network Technology in Computer Network Security,"Zhou Yun, Lu Rui",The 2020 International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy,2021,"<a href=""Springer (2021) : Research on the Application of Virtual Network Technology in Computer Network Security"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-62746-1_29]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-62746-1_29]</a>","With the continuous progress and development of society, China has gradually entered the era of information society. Nowadays, the popularity of...",,Springer
Resisting Distributed Backdoor Attacks in Federated Learning: A Dynamic Norm Clipping Approach,"Yifan Guo, Qianlong Wang, Tianxi Ji, Xufei Wang, Pan Li",IEEE BigData,2021,"<a href=""DBLP (2021) : Resisting Distributed Backdoor Attacks in Federated Learning: A Dynamic Norm Clipping Approach"" target=""_blank"">[https://doi.org/10.1109/BigData52589.2021.9671910]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/BigData52589.2021.9671910]</a>",,,DBLP
Rethinking Stealthiness of Backdoor Attack against NLP Models,"Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, Xu Sun",ACL/IJCNLP,2021,"<a href=""DBLP (2021) : Rethinking Stealthiness of Backdoor Attack against NLP Models"" target=""_blank"">[https://doi.org/10.18653/v1/2021.acl-long.431]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.18653/v1/2021.acl-long.431]</a>",,,DBLP
Reverse Firewalls for Adaptively Secure MPC Without Setup,"Suvradip Chakraborty, Chaya Ganesh, ... Pratik Sarkar",Advances in Cryptology – ASIACRYPT 2021,2021,"<a href=""Springer (2021) : Reverse Firewalls for Adaptively Secure MPC Without Setup"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92075-3_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92075-3_12]</a>","We study Multi-party computation (MPC) in the setting of subversion, where the adversary tampers with the machines of honest parties. Our goal is to...",,Springer
Reverse engineering imperceptible backdoor attacks on deep neural networks for detection and training set cleansing,"Zhen Xiang, David J. Miller, George Kesidis",Comput. Secur.,2021,"<a href=""DBLP (2021) : Reverse engineering imperceptible backdoor attacks on deep neural networks for detection and training set cleansing"" target=""_blank"">[https://doi.org/10.1016/j.cose.2021.102280]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.cose.2021.102280]</a>",,,DBLP
Review of Machine Learning and Data Mining Methods to Predict Different Cyberattacks,"Narendrakumar Mangilal Chayal, Nimisha P. Patel",Data Science and Intelligent Applications,2021,"<a href=""Springer (2021) : Review of Machine Learning and Data Mining Methods to Predict Different Cyberattacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-4474-3_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-4474-3_5]</a>","Cybersecurity deals with various types of cybercrimes, but it is essential to identify the similarities in existing cybercrimes using data mining and...",,Springer
Round-Optimal Blind Signatures in the Plain Model from Classical and Quantum Standard Assumptions,"Shuichi Katsumata, Ryo Nishimaki, ... Takashi Yamakawa",Advances in Cryptology – EUROCRYPT 2021,2021,"<a href=""Springer (2021) : Round-Optimal Blind Signatures in the Plain Model from Classical and Quantum Standard Assumptions"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-77870-5_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-77870-5_15]</a>","Blind signatures, introduced by Chaum (Crypto’82), allows a user to obtain a signature on a message without revealing the message itself to the...",,Springer
Secure Federated Learning Model Verification: A Client-side Backdoor Triggered Watermarking Scheme,"Xiyao Liu, Shuo Shao, Yue Yang, Kangming Wu, Wenyuan Yang, Hui Fang",SMC,2021,"<a href=""DBLP (2021) : Secure Federated Learning Model Verification: A Client-side Backdoor Triggered Watermarking Scheme"" target=""_blank"">[https://doi.org/10.1109/SMC52423.2021.9658998]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SMC52423.2021.9658998]</a>",,,DBLP
Securing Software Defined Networking Using Intrusion Detection System - A Review,"Noor Al-Mi’ani, Mohammed Anbar, ... Shankar Karuppayah",Advances in Cyber Security,2021,"<a href=""Springer (2021) : Securing Software Defined Networking Using Intrusion Detection System - A Review"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-8059-5_26]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-8059-5_26]</a>","For the time being, the advances of the Internet technologies in respect of a wide-spread development and the fixed nature of traditional networks...",,Springer
Security Analysis of End-to-End Encryption for Zoom Meetings,"Takanori Isobe, Ryoma Ito",Information Security and Privacy,2021,"<a href=""Springer (2021) : Security Analysis of End-to-End Encryption for Zoom Meetings"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90567-5_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90567-5_12]</a>","In the wake of the global COVID-19 pandemic, video conference systems have become essential for not only business purposes, but also private,...",,Springer
Security Risks and Challenges in IoT-Based Applications,"Ashwin Perti, Alka Singh, ... Prabhat Kr. Srivastava","Proceedings of International Conference on Big Data, Machine Learning and their Applications",2021,"<a href=""Springer (2021) : Security Risks and Challenges in IoT-Based Applications"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-8377-3_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-8377-3_9]</a>","Internet of things (IoT) is a network having very large scale of heterogeneous devices, sensors and equipment connected together with the...",,Springer
Security and Privacy in E-learning,"Weigong Feng, Wennan Wang, Fengling Wang",Advances in Web-Based Learning – ICWL 2021,2021,"<a href=""Springer (2021) : Security and Privacy in E-learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90785-3_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90785-3_11]</a>","E-learning has the advantages of flexible and diverse learning methods, and the high-quality teaching resources can be shared without limitation by...",,Springer
Security of DBMSs,Suhair Amer,"Advances in Security, Networks, and Internet of Things",2021,"<a href=""Springer (2021) : Security of DBMSs"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-71017-0_32]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-71017-0_32]</a>",Database management systems utilize various security measures but still have some known weaknesses. A security risk of ADABAS occurs from direct...,,Springer
Short Paper: Terrorist Fraud in Distance Bounding: Getting Around the Models,David Gerault,Financial Cryptography and Data Security,2021,"<a href=""Springer (2021) : Short Paper: Terrorist Fraud in Distance Bounding: Getting Around the Models"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-64322-8_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-64322-8_17]</a>","Terrorist fraud is an attack against distance bounding protocols, whereby a malicious prover allows an adversary to authenticate on their behalf...",,Springer
SideLine: How Delay-Lines (May) Leak Secrets from Your SoC,"Joseph Gravellier, Jean-Max Dutertre, ... Philippe Loubet Moundi",Constructive Side-Channel Analysis and Secure Design,2021,"<a href=""Springer (2021) : SideLine: How Delay-Lines (May) Leak Secrets from Your SoC"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-89915-8_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-89915-8_1]</a>","To meet the ever-growing need for performance in silicon devices, SoC providers have been increasingly relying on software-hardware cooperation. By...",,Springer
Simtrojan: Stealthy Backdoor Attack,"Yankun Ren, Longfei Li, Jun Zhou",ICIP,2021,"<a href=""DBLP (2021) : Simtrojan: Stealthy Backdoor Attack"" target=""_blank"">[https://doi.org/10.1109/ICIP42928.2021.9506313]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICIP42928.2021.9506313]</a>",,,DBLP
SoK: Securing Email—A Stakeholder-Based Analysis,"Jeremy Clark, P. C. van Oorschot, ... Daniel Zappala",Financial Cryptography and Data Security,2021,"<a href=""Springer (2021) : SoK: Securing Email—A Stakeholder-Based Analysis"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-64322-8_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-64322-8_18]</a>","While email is the most ubiquitous and interoperable form of online communication today, it was not conceived with strong security guarantees, and...",,Springer
Stand-in Backdoor: A Stealthy and Powerful Backdoor Attack,"Shuang Li, Hongwei Li, Hanxiao Chen",GLOBECOM,2021,"<a href=""DBLP (2021) : Stand-in Backdoor: A Stealthy and Powerful Backdoor Attack"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM46510.2021.9685762]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/GLOBECOM46510.2021.9685762]</a>",,,DBLP
Static Analysis for Software Reliability and Security,"Hongjun Choi, Dayoung Kang, Jin-Young Choi","Advances in Security, Networks, and Internet of Things",2021,"<a href=""Springer (2021) : Static Analysis for Software Reliability and Security"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-71017-0_33]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-71017-0_33]</a>","Because people make software, they cannot avoid software errors. So, developers often use static analysis in the implementation phase to diagnose and...",,Springer
Steel: Composable Hardware-Based Stateful and Randomised Functional Encryption,"Pramod Bhatotia, Markulf Kohlweiss, ... Yiannis Tselekounis",Public-Key Cryptography – PKC 2021,2021,"<a href=""Springer (2021) : Steel: Composable Hardware-Based Stateful and Randomised Functional Encryption"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-75248-4_25]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-75248-4_25]</a>",Trusted execution environments (TEEs) enable secure execution of programs on untrusted hosts and cryptographically attest the correctness of outputs....,,Springer
Strategic Remote Attestation: Testbed for Internet-of-Things Devices and Stackelberg Security Game for Optimal Strategies,"Shanto Roy, Salah Uddin Kadir, ... Aron Laszka",Decision and Game Theory for Security,2021,"<a href=""Springer (2021) : Strategic Remote Attestation: Testbed for Internet-of-Things Devices and Stackelberg Security Game for Optimal Strategies"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90370-1_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90370-1_15]</a>","Internet of Things (IoT) devices and applications can have significant vulnerabilities, which may be exploited by adversaries to cause considerable...",,Springer
Study of scale-free structures in feed-forward neural networks against backdoor attacks,"Sara Kaviani, Insoo Sohn",ICT Express,2021,"<a href=""DBLP (2021) : Study of scale-free structures in feed-forward neural networks against backdoor attacks"" target=""_blank"">[https://doi.org/10.1016/j.icte.2020.11.004]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.icte.2020.11.004]</a>",,,DBLP
Séta: Supersingular Encryption from Torsion Attacks,"Luca De Feo, Cyprien Delpech de Saint Guilhem, ... Benjamin Wesolowski",Advances in Cryptology – ASIACRYPT 2021,2021,"<a href=""Springer (2021) : Séta: Supersingular Encryption from Torsion Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92068-5_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92068-5_9]</a>","We present Séta (To be pronounced [ʃe:tɒ] meaning “walk” in Hungarian.), a new family of public-key encryption schemes with post-quantum security...",,Springer
Text Backdoor Detection Using an Interpretable RNN Abstract Model,"Ming Fan, Ziliang Si, Xiaofei Xie, Yang Liu, Ting Liu",IEEE Trans. Inf. Forensics Secur.,2021,"<a href=""DBLP (2021) : Text Backdoor Detection Using an Interpretable RNN Abstract Model"" target=""_blank"">[https://doi.org/10.1109/TIFS.2021.3103064]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TIFS.2021.3103064]</a>",,,DBLP
The Deployment of Autonomous Drones During the COVID-19 Pandemic,"Usman Javed Butt, William Richardson, ... Caleb Eghan","Cybersecurity, Privacy and Freedom Protection in the Connected World",2021,"<a href=""Springer (2021) : The Deployment of Autonomous Drones During the COVID-19 Pandemic"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-68534-8_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-68534-8_13]</a>",Drones are being utilised in diverse domains all over the world. The latest employment is the utilisation of drones during the global pandemic for...,,Springer
Threats on Machine Learning Technique by Data Poisoning Attack: A Survey,"Ibrahim M. Ahmed, Manar Younis Kashmoola",Advances in Cyber Security,2021,"<a href=""Springer (2021) : Threats on Machine Learning Technique by Data Poisoning Attack: A Survey"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-8059-5_36]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-16-8059-5_36]</a>","With the huge services provided by machine learning systems in our daily life, the attacks on these services are increasing every day. The attackers...",,Springer
Towards Automated Assessment of Vulnerability Exposures in Security Operations,"Philip Huff, Qinghua Li",Security and Privacy in Communication Networks,2021,"<a href=""Springer (2021) : Towards Automated Assessment of Vulnerability Exposures in Security Operations"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90019-9_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90019-9_4]</a>",Current approaches for risk analysis of software vulnerabilities using manual assessment and numeric scoring do not complete fast enough to keep pace...,,Springer
Towards Open World Traffic Classification,"Zhu Liu, Lijun Cai, ... Dan Meng",Information and Communications Security,2021,"<a href=""Springer (2021) : Towards Open World Traffic Classification"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-86890-1_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-86890-1_19]</a>","Due to the dynamic evolution of network traffic, open world traffic classification has become a vital problem. Traditional traffic classification...",,Springer
Towards a Black-Box Security Evaluation Framework,"Mosabbah Mushir Ahmed, Youssef Souissi, ... Sofiane Takarabt",Security and Privacy,2021,"<a href=""Springer (2021) : Towards a Black-Box Security Evaluation Framework"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90553-8_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-90553-8_6]</a>",Injection of faults has been studied in various research works since last decades. Several hardware targets have been studied with respect to the...,,Springer
Training Data Poisoning in ML-CAD: Backdooring DL-Based Lithographic Hotspot Detectors,"Kang Liu, Benjamin Tan, Ramesh Karri, Siddharth Garg",IEEE Trans. Comput. Aided Des. Integr. Circuits Syst.,2021,"<a href=""DBLP (2021) : Training Data Poisoning in ML-CAD: Backdooring DL-Based Lithographic Hotspot Detectors"" target=""_blank"">[https://doi.org/10.1109/TCAD.2020.3024780]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TCAD.2020.3024780]</a>",,,DBLP
Trapdoor DDH Groups from Pairings and Isogenies,"Péter Kutas, Christophe Petit, Javier Silva",Selected Areas in Cryptography,2021,"<a href=""Springer (2021) : Trapdoor DDH Groups from Pairings and Isogenies"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-81652-0_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-81652-0_17]</a>","Trapdoor DDH groups are an appealing cryptographic primitive introduced by Dent–Galbraith (ANTS 2006), where DDH instances are hard to solve unless...",,Springer
"Two-Round Adaptively Secure MPC from Isogenies, LPN, or CDH","Navid Alamati, Hart Montgomery, ... Pratik Sarkar",Advances in Cryptology – ASIACRYPT 2021,2021,"<a href=""Springer (2021) : Two-Round Adaptively Secure MPC from Isogenies, LPN, or CDH"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92075-3_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-92075-3_11]</a>",We present a new framework for building round-optimal (two-round) adaptively secure MPC. We show that a relatively weak notion of OT that we call...,,Springer
UC-Secure Cryptographic Reverse Firewall–Guarding Corrupted Systems with the Minimum Trusted Module,"Geng Li, Jianwei Liu, ... Yanting Zhang",Information Security and Cryptology,2021,"<a href=""Springer (2021) : UC-Secure Cryptographic Reverse Firewall–Guarding Corrupted Systems with the Minimum Trusted Module"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-88323-2_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-88323-2_5]</a>","Nowadays, mass-surveillance is becoming an increasingly severe threat to the public’s privacy. The PRISM and a series of other events showed that...",,Springer
Use Procedural Noise to Achieve Backdoor Attack,"Xuan Chen, Yuena Ma, Shiwei Lu",IEEE Access,2021,"<a href=""DBLP (2021) : Use Procedural Noise to Achieve Backdoor Attack"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2021.3110239]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2021.3110239]</a>",,,DBLP
Using Natural Language Processing for Phishing Detection,"Richard Adolph Aires Jonker, Roshan Poudel, ... Rui Pedro Lopes","Optimization, Learning Algorithms and Applications",2021,"<a href=""Springer (2021) : Using Natural Language Processing for Phishing Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-91885-9_40]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-91885-9_40]</a>","We live in a world where computers are constantly changing the way we do things. People spend many hours on their phones or computers, whether it be...",,Springer
Variance Fractal Dimension Feature Selection for Detection of Cyber Security Attacks,"Samilat Kaiser, Ken Ferens",Advances in Artificial Intelligence and Applied Cognitive Computing,2021,"<a href=""Springer (2021) : Variance Fractal Dimension Feature Selection for Detection of Cyber Security Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-70296-0_82]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-70296-0_82]</a>","In an era where machine learning algorithms are widely used in order to improve the performance of network intrusion detection system, the complexity...",,Springer
What Is the Risk?,G. C. Rasner,Cybersecurity and Third-Party Risk: Third Party Threat Hunting,2021,"<a href=""IEEE (2021) : What Is the Risk?"" target=""_blank"">[https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9946829.pdf&bkn=9946654&pdfType=chapter]</a>","<a href=""IEEE"" target=""_blank"">[]</a>","This chapter explains that HyperbBro is commonly attributed to the cybercriminal group named “LuckyMouse,” a Chinese‐speaking threat actor known for highly targeted cyberattacks. Primarily active in South East and Central Asia, many of their attacks have a political aim. Tmanger is attributed to TA428, also a Chinese advanced persistent threat group. A <i>hardcoded backdoor root account</i> is one that cannot be underestimated in how critical the security flaw is. When an account is built within the code of a product, it cannot be removed unless the code itself is changed or updated by the manufacturer. While third‐party risk management (TRPM) organizations struggle to keep up with the level of breaches and incidents with vendors, evidence shows most cybersecurity organizations are not taking a lead in this domain, and that TPRM groups do not have the expertise to address this gap.",,IEEE
Why is Your Trojan NOT Responding? A Quantitative Analysis of Failures in Backdoor Attacks of Neural Networks,"Xingbo Hu, Yibing Lan, Ruimin Gao, Guozhu Meng, Kai Chen",ICA3PP,2021,"<a href=""DBLP (2021) : Why is Your Trojan NOT Responding? A Quantitative Analysis of Failures in Backdoor Attacks of Neural Networks"" target=""_blank"">[https://doi.org/10.1007/978-3-030-95391-1_47]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-030-95391-1_47]</a>",,,DBLP
YOSO: You Only Speak Once,"Craig Gentry, Shai Halevi, ... Sophia Yakoubov",Advances in Cryptology – CRYPTO 2021,2021,"<a href=""Springer (2021) : YOSO: You Only Speak Once"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-84245-1_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-84245-1_3]</a>","The inherent difficulty of maintaining stateful environments over long periods of time gave rise to the paradigm of serverless computing, where...",,Springer
Explainability Matters: Backdoor Attacks on Medical Imaging,"Munachiso Nwadike, Takumi Miyawaki, Esha Sarkar, Michail Maniatakos, Farah Shamout","arXiv
arXiv","2020-12-30
2021-01","<a href=""arXiv (2020-12-30) : Explainability Matters: Backdoor Attacks on Medical Imaging"" target=""_blank"">[http://arxiv.org/abs/2101.00008v1]</a>
<a href=""DBLP (2021-01) : Explainability Matters: Backdoor Attacks on Medical Imaging"" target=""_blank"">[https://arxiv.org/abs/2101.00008]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2101.00008]</a>","Deep neural networks have been shown to be vulnerable to backdoor attacks, which could be easily introduced to the training set prior to model training. Recent work has focused on investigating backdoor attacks on natural images or toy datasets. Consequently, the exact impact of backdoors is not yet fully understood in complex real-world applications, such as in medical imaging where misdiagnosis can be very costly. In this paper, we explore the impact of backdoor attacks on a multi-label disease classification task using chest radiography, with the assumption that the attacker can manipulate the training dataset to execute the attack. Extensive evaluation of a state-of-the-art architecture demonstrates that by introducing images with few-pixel perturbations into the training set, an attacker can execute the backdoor successfully without having to be involved with the training procedure. A simple 3$\times$3 pixel trigger can achieve up to 1.00 Area Under the Receiver Operating Characteristic (AUROC) curve on the set of infected images. In the set of clean images, the backdoored neural network could still achieve up to 0.85 AUROC, highlighting the stealthiness of the attack. As the use of deep learning based diagnostic systems proliferates in clinical practice, we also show how explainability is indispensable in this context, as it can identify spatially localized backdoors in inference time.
","
","arXiv
DBLP"
"Toward a Trustable, Self-Hosting Computer System",G. L. Somlo,2020 IEEE Security and Privacy Workshops (SPW),2020-12-18,"<a href=""IEEE (2020-12-18) : Toward a Trustable, Self-Hosting Computer System"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283874]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SPW50608.2020.00039]</a>","Due to the extremely rapid growth of the computing and IT technology market, commercial hardware made for the civilian, consumer sector is increasingly (and inevitably) deployed in security-sensitive environments. With the growing threat of hardware Trojans and backdoors, an adversary could perpetrate a full system compromise, or privilege escalation attack, even if the software is presumed to be perfectly secure. We propose a method of field stripping a computer system by empirically proving an equivalence between the trustability of the fielded system on one hand, and its comprehensive set of sources (including those of all toolchains used in its construction) on the other. In the long run, we hope to facilitate comprehensive verification and validation of fielded computer systems from fully self-contained hard-ware+software sources, as a way of mitigating against the lack of control over (and visibility into) the hardware supply chain.",,IEEE
Backdoor Attacks on Federated Meta-Learning,"Chien-Lun Chen, Leana Golubchik, Marco Paolieri","arXiv
arXiv","2020-12-16
2020-06","<a href=""arXiv (2020-12-16) : Backdoor Attacks on Federated Meta-Learning"" target=""_blank"">[http://arxiv.org/abs/2006.07026v2]</a>
<a href=""DBLP (2020-06) : Backdoor Attacks on Federated Meta-Learning"" target=""_blank"">[https://arxiv.org/abs/2006.07026]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2006.07026]</a>","Federated learning allows multiple users to collaboratively train a shared classification model while preserving data privacy. This approach, where model updates are aggregated by a central server, was shown to be vulnerable to poisoning backdoor attacks: a malicious user can alter the shared model to arbitrarily classify specific inputs from a given class. In this paper, we analyze the effects of backdoor attacks on federated meta-learning, where users train a model that can be adapted to different sets of output classes using only a few examples. While the ability to adapt could, in principle, make federated learning frameworks more robust to backdoor attacks (when new training examples are benign), we find that even 1-shot~attacks can be very successful and persist after additional training. To address these vulnerabilities, we propose a defense mechanism inspired by matching networks, where the class of an input is predicted from the similarity of its features with a support set of labeled examples. By removing the decision logic from the model shared with the federation, success and persistence of backdoor attacks are greatly reduced.
","
","arXiv
DBLP"
HaS-Nets: A Heal and Select Mechanism to Defend DNNs Against Backdoor Attacks for Data Collection Scenarios,"Hassan Ali, Surya Nepal, Salil S. Kanhere, Sanjay Jha","arXiv
arXiv","2020-12-14
2020-12","<a href=""arXiv (2020-12-14) : HaS-Nets: A Heal and Select Mechanism to Defend DNNs Against Backdoor Attacks for Data Collection Scenarios"" target=""_blank"">[http://arxiv.org/abs/2012.07474v1]</a>
<a href=""DBLP (2020-12) : HaS-Nets: A Heal and Select Mechanism to Defend DNNs Against Backdoor Attacks for Data Collection Scenarios"" target=""_blank"">[https://arxiv.org/abs/2012.07474]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2012.07474]</a>","We have witnessed the continuing arms race between backdoor attacks and the corresponding defense strategies on Deep Neural Networks (DNNs). Most state-of-the-art defenses rely on the statistical sanitization of the ""inputs"" or ""latent DNN representations"" to capture trojan behaviour. In this paper, we first challenge the robustness of such recently reported defenses by introducing a novel variant of targeted backdoor attack, called ""low-confidence backdoor attack"". We also propose a novel defense technique, called ""HaS-Nets"". ""Low-confidence backdoor attack"" exploits the confidence labels assigned to poisoned training samples by giving low values to hide their presence from the defender, both during training and inference. We evaluate the attack against four state-of-the-art defense methods, viz., STRIP, Gradient-Shaping, Februus and ULP-defense, and achieve Attack Success Rate (ASR) of 99%, 63.73%, 91.2% and 80%, respectively. We next present ""HaS-Nets"" to resist backdoor insertion in the network during training, using a reasonably small healing dataset, approximately 2% to 15% of full training data, to heal the network at each iteration. We evaluate it for different datasets - Fashion-MNIST, CIFAR-10, Consumer Complaint and Urban Sound - and network architectures - MLPs, 2D-CNNs, 1D-CNNs. Our experiments show that ""HaS-Nets"" can decrease ASRs from over 90% to less than 15%, independent of the dataset, attack configuration and network architecture.
","
","arXiv
DBLP"
Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection,"Di Tang, XiaoFeng Wang, Haixu Tang, Kehuan Zhang","arXiv
USENIX Security Symposium
arXiv","2020-12-10
2021
2019-08","<a href=""arXiv (2020-12-10) : Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection"" target=""_blank"">[http://arxiv.org/abs/1908.00686v2]</a>
<a href=""DBLP (2021) : Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity21/presentation/tang-di]</a>
<a href=""DBLP (2019-08) : Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection"" target=""_blank"">[http://arxiv.org/abs/1908.00686]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity21/presentation/tang-di]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1908.00686]</a>","A security threat to deep neural networks (DNN) is backdoor contamination, in which an adversary poisons the training data of a target model to inject a Trojan so that images carrying a specific trigger will always be classified into a specific label. Prior research on this problem assumes the dominance of the trigger in an image's representation, which causes any image with the trigger to be recognized as a member in the target class. Such a trigger also exhibits unique features in the representation space and can therefore be easily separated from legitimate images. Our research, however, shows that simple target contamination can cause the representation of an attack image to be less distinguishable from that of legitimate ones, thereby evading existing defenses against the backdoor infection. In our research, we show that such a contamination attack actually subtly changes the representation distribution for the target class, which can be captured by a statistic analysis. More specifically, we leverage an EM algorithm to decompose an image into its identity part (e.g., person, traffic sign) and variation part within a class (e.g., lighting, poses). Then we analyze the distribution in each class, identifying those more likely to be characterized by a mixture model resulted from adding attack samples to the legitimate image pool. Our research shows that this new technique effectively detects data contamination attacks, including the new one we propose, and is also robust against the evasion attempts made by a knowledgeable adversary.

","

","arXiv
DBLP
DBLP"
Cybersecurity of Smart Electric Vehicle Charging: A Power Grid Perspective,S. Acharya Y. Dvorkin H. Pandžić R. Karri,IEEE Access,2020-12-08,"<a href=""IEEE (2020-12-08) : Cybersecurity of Smart Electric Vehicle Charging: A Power Grid Perspective"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272723]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2020.3041074]</a>","With the roll-out of electric vehicles (EVs), the automobile industry is transitioning away from conventional gasoline-fueled vehicles. As a result, the EV charging demand is continuously growing and to meet this growing demand, various types of electric vehicle charging stations (EVCSs) are being deployed for commercial and residential use. This nexus of EVs, EVCSs, and power grids creates complex cyber-physical interdependencies that can be maliciously exploited to damage each of these components. This paper describes and analyzes cyber vulnerabilities that arise at this nexus and points to the current and emerging gaps in the security of the EV charging ecosystem. These vulnerabilities must be addressed as the number of EVs continue to grow worldwide and their impact on the power grid becomes more viable. The purpose of this paper is to list and characterize all backdoors that can be exploited to seriously harm either EV and EVCS equipments, or power grid, or both. The presented issues and challenges intend to ignite research efforts on cybersecurity of smart EV charging and enhancing power grid resiliency against such demand-side cyberattacks in general.",,IEEE
Februus: Input Purification Defense against Trojan Attacks on Deep Neural Network Systems,Doan B.G.,ACM International Conference Proceeding Series,2020-12-07,"<a href=""ScienceDirect (2020-12-07) : Februus: Input Purification Defense against Trojan Attacks on Deep Neural Network Systems"" target=""_blank"">[https://doi.org/10.1145/3427228.3427264]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3427228.3427264]</a>",,,ScienceDirect
Financial performance analysis of backdoor listed companies,Yuhong H.,E3S Web of Conferences,2020-12-07,"<a href=""ScienceDirect (2020-12-07) : Financial performance analysis of backdoor listed companies"" target=""_blank"">[https://doi.org/10.1051/e3sconf/202021402032]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1051/e3sconf/202021402032]</a>",,,ScienceDirect
Analysis of USB Based Spying Method Using Arduino and Metasploit Framework in Windows Operating System,Ferryansa A. Budiono A. Almaarif,2020 3rd International Conference on Computer and Informatics Engineering (IC2IE),2020-12-04,"<a href=""IEEE (2020-12-04) : Analysis of USB Based Spying Method Using Arduino and Metasploit Framework in Windows Operating System"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274643]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IC2IE50715.2020.9274643]</a>","The use of a very wide windows operating system is undeniably also followed by increasing attacks on the operating system. Universal Serial Bus (USB) is one of the mechanisms used by many people with plug and play functionality that is very easy to use, making data transfers fast and easy compared to other hardware. Some research shows that the Windows operating system has weaknesses so that it is often exploited by using various attacks and malware. There are various methods used to exploit the Windows operating system, one of them by using a USB device. By using a USB device, a criminal can plant a backdoor reverse shell to exploit the victim's computer just by connecting the USB device to the victim's computer without being noticed. This research was conducted by planting a reverse shell backdoor through a USB device to exploit the victim's device, especially the webcam and microphone device on the target computer. From 35 experiments that have been carried out, it was found that 83% of spying attacks using USB devices on the Windows operating system were successfully carried out.",,IEEE
Countermeasure Based on Smart Contracts and AI against DoS/DDoS Attack in 5G Circumstances,L. Fang B. Zhao Y. Li Z. Liu C. Ge W. Meng,IEEE Network,2020-12-02,"<a href=""IEEE (2020-12-02) : Countermeasure Based on Smart Contracts and AI against DoS/DDoS Attack in 5G Circumstances"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9277902]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/MNET.021.1900614]</a>","The development of 5G has substantially increased the destructiveness of DoS/DDoS attacks because the data processing capability of computers has not been accordingly enhanced, and this contradiction creates a vulnerability for attackers to compromise a server by sending a massive data flow. in practical 5G circumstances, it is difficult to extract distinct features between malicious and benign massive data flows. This amplifies the difficulties of DoS/DDoS detection. Thus, precautions against DoS/DDoS attack in 5G are of great importance. in this article, we present a solution based on smart contracts and machine learning as a countermeasure against DoS/DDoS attacks in the 5G background by hiding a protected server in a blockchain network and flexibly restricting the scale of DoS/DDoS via transaction fees. We also leverage non-repudiation of smart contracts, analyzing users' malicious behavior of communication and executing punishment via smart contracts. Our scheme could effectively mitigate massive DoS/DDoS attacks in advance and dynamically punitively charge DoS/DDoS attacks. Compared to existing DoS/DDoS defense in 4G, our scheme offers numerous benefits, including making benign communication always dominate rational users and countering DoS/DDoS attacks before they are launched. Moreover, compared to common DoS/DDoS detection based on Ai, we consider the source trustworthiness of training samples and take measures to avoid backdoors where model trainers may compromise Ai models to launch DoS/DDoS attacks.",,IEEE
Segmentation based backdoor attack detection,Kees N.,Proceedings - International Conference on Machine Learning and Cybernetics,2020-12-02,"<a href=""ScienceDirect (2020-12-02) : Segmentation based backdoor attack detection"" target=""_blank"">[https://doi.org/10.1109/ICMLC51923.2020.9469037]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICMLC51923.2020.9469037]</a>",,,ScienceDirect
Lower Bounds for Adversarially Robust PAC Learning under Evasion and Hybrid Attacks,Diochnos D.I.,"Proceedings - 19th IEEE International Conference on Machine Learning and Applications, ICMLA 2020",2020-12-01,"<a href=""ScienceDirect (2020-12-01) : Lower Bounds for Adversarially Robust PAC Learning under Evasion and Hybrid Attacks"" target=""_blank"">[https://doi.org/10.1109/ICMLA51294.2020.00117]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICMLA51294.2020.00117]</a>",,,ScienceDirect
No Backdoors: Investigating the Dutch Standpoint on Encryption,Veen J.,Policy and Internet,2020-12-01,"<a href=""ScienceDirect (2020-12-01) : No Backdoors: Investigating the Dutch Standpoint on Encryption"" target=""_blank"">[https://doi.org/10.1002/poi3.233]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1002/poi3.233]</a>",,,ScienceDirect
Reduction of the Number of Analyzed Parameters in Network Attack Detection Systems,"E. A. Popova, V. V. Platonov",Automatic Control and Computer Sciences,2020-12-01,"<a href=""Springer (2020-12-01) : Reduction of the Number of Analyzed Parameters in Network Attack Detection Systems"" target=""_blank"">[https://link.springer.com/article/10.3103/S0146411620080295]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.3103/S0146411620080295]</a>",Abstract— Methods to reduce the number of network traffic parameters are analyzed. A prototype of the network attack detection system with a module...,,Springer
Trembling triggers: exploring the sensitivity of backdoors in DNN-based face recognition,Pasquini C.,Eurasip Journal on Information Security,2020-12-01,"<a href=""ScienceDirect (2020-12-01) : Trembling triggers: exploring the sensitivity of backdoors in DNN-based face recognition"" target=""_blank"">[https://doi.org/10.1186/s13635-020-00104-z]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1186/s13635-020-00104-z]</a>",,,ScienceDirect
Backdoor Attack with Sample-Specific Triggers,"Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, Siwei Lyu",arXiv,2020-12,"<a href=""DBLP (2020-12) : Backdoor Attack with Sample-Specific Triggers"" target=""_blank"">[https://arxiv.org/abs/2012.03816]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2012.03816]</a>",,,DBLP
DeepSweep: An Evaluation Framework for Mitigating DNN Backdoor Attacks using Data Augmentation,"Yi Zeng, Han Qiu, Shangwei Guo, Tianwei Zhang, Meikang Qiu, Bhavani Thuraisingham",arXiv,2020-12,"<a href=""DBLP (2020-12) : DeepSweep: An Evaluation Framework for Mitigating DNN Backdoor Attacks using Data Augmentation"" target=""_blank"">[https://arxiv.org/abs/2012.07006]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2012.07006]</a>",,,DBLP
Februus: Input Purification Defense Against Trojan Attacks on Deep Neural Network Systems,"Bao Gia Doan, Ehsan Abbasnejad, Damith C. Ranasinghe",ACSAC '20: Proceedings of the 36th Annual Computer Security Applications Conference,2020-12,"<a href=""ACM (2020-12) : Februus: Input Purification Defense Against Trojan Attacks on Deep Neural Network Systems"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3427228.3427264]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3427228.3427264]</a>","We propose Februus, a new idea to neutralize highly potent and insidious Trojan attacks on Deep Neural Network (DNN) systems at run-time. In Trojan attacks, an adversary activates a backdoor crafted in a deep neural network model using a secret trigger, ...",,ACM
TROJANZOO: Everything you ever wanted to know about neural backdoors (but were afraid to ask),"Ren Pang, Zheng Zhang, Xiangshan Gao, Zhaohan Xi, Shouling Ji, Peng Cheng, Ting Wang",arXiv,2020-12,"<a href=""DBLP (2020-12) : TROJANZOO: Everything you ever wanted to know about neural backdoors (but were afraid to ask)"" target=""_blank"">[https://arxiv.org/abs/2012.09302]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2012.09302]</a>",,,DBLP
Effect of backdoor attacks over the complexity of the latent space distribution,"Henry D. Chacon, Paul Rad","arXiv
arXiv","2020-11-29
2020-12","<a href=""arXiv (2020-11-29) : Effect of backdoor attacks over the complexity of the latent space distribution"" target=""_blank"">[http://arxiv.org/abs/2012.01931v1]</a>
<a href=""DBLP (2020-12) : Effect of backdoor attacks over the complexity of the latent space distribution"" target=""_blank"">[https://arxiv.org/abs/2012.01931]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2012.01931]</a>","The input space complexity determines the model's capabilities to extract their knowledge and translate the space of attributes into a function which is assumed in general, as a concatenation of non-linear functions between layers. In the presence of backdoor attacks, the space complexity changes, and induces similarities between classes that directly affect the model's training. As a consequence, the model tends to overfit the input set. In this research, we suggest the D-vine Copula Auto-Encoder (VCAE) as a tool to estimate the latent space distribution under the presence of backdoor triggers. Since no assumptions are made on the distribution estimation, like in Variational Autoencoders (VAE). It is possible to observe the backdoor stamp in non-attacked categories randomly generated. We exhibit the differences between a clean model (baseline) and the attacked one (backdoor) in a pairwise representation of the distribution. The idea is to illustrate the dependency structure change in the input space induced by backdoor features. Finally, we quantify the entropy's changes and the Kullback-Leibler divergence between models. In our results, we found the entropy in the latent space increases by around 27\% due to the backdoor trigger added to the input
","
","arXiv
DBLP"
A distributed ensemble design based intrusion detection system using fog computing to protect the internet of things networks,"Prabhat Kumar, Govind P. Gupta, Rakesh Tripathi",Journal of Ambient Intelligence and Humanized Computing,2020-11-27,"<a href=""Springer (2020-11-27) : A distributed ensemble design based intrusion detection system using fog computing to protect the internet of things networks"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-020-02696-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-020-02696-3]</a>","With the development of internet of things (IoT), capabilities of computing, networking infrastructure, storage of data and management have come very...",,Springer
Cyber intrusion detection through association rule mining on multi-source logs,"Ping Lou, Guantong Lu, ... Junwei Yan",Applied Intelligence,2020-11-26,"<a href=""Springer (2020-11-26) : Cyber intrusion detection through association rule mining on multi-source logs"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-020-02007-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-020-02007-5]</a>","Security logs in cloud environment like intrusion detection system (IDS) logs, firewall logs, and system logs provide historical information...",,Springer
CleaNN: Accelerated Trojan Shield for Embedded Neural Networks,M. Javaheripi M. Samragh G. Fields T. Javidi F. Koushanfar,2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD),2020-11-25,"<a href=""IEEE (2020-11-25) : CleaNN: Accelerated Trojan Shield for Embedded Neural Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9256811]</a>","<a href=""IEEE"" target=""_blank"">[]</a>","We propose Cleann, the first end-to-end framework that enables online mitigation of Trojans for embedded Deep Neural Network (DNN) applications. A Trojan attack works by injecting a backdoor in the DNN while training during inference, the Trojan can be activated by the specific backdoor trigger. What differentiates Cleann from the prior work is its lightweight methodology which recovers the ground-truth class of Trojan samples without the need for labeled data, model retraining, or prior assumptions on the trigger or the attack. We leverage dictionary learning and sparse approximation to characterize the statistical behavior of benign data and identify Trojan triggers. Cleann is devised based on algorithm/hardware co-design and is equipped with specialized hardware to enable efficient real-time execution on resource-constrained embedded platforms. Proof of concept evaluations on Cleann for the state-of-the-art Neural Trojan attacks on visual benchmarks demonstrate its competitive advantage in terms of attack resiliency and execution overhead.",,IEEE
Performance Analysis of Intrusion Detection Systems Using a Feature Selection Method on the UNSW-NB15 Dataset,"Sydney M. Kasongo, Yanxia Sun",Journal of Big Data,2020-11-25,"<a href=""Springer (2020-11-25) : Performance Analysis of Intrusion Detection Systems Using a Feature Selection Method on the UNSW-NB15 Dataset"" target=""_blank"">[https://link.springer.com/article/10.1186/s40537-020-00379-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s40537-020-00379-6]</a>",Computer networks intrusion detection systems (IDSs) and intrusion prevention systems (IPSs) are critical aspects that contribute to the success of...,,Springer
Removable weak keys for discrete logarithm-based cryptography,"Michael John Jacobson Jr., Prabhat Kushwaha",Journal of Cryptographic Engineering,2020-11-24,"<a href=""Springer (2020-11-24) : Removable weak keys for discrete logarithm-based cryptography"" target=""_blank"">[https://link.springer.com/article/10.1007/s13389-020-00250-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13389-020-00250-7]</a>",We describe a novel type of weak cryptographic private key that can exist in any discrete logarithm-based public-key cryptosystem set in a group of...,,Springer
The Nooscope manifested: AI as instrument of knowledge extractivism,"Matteo Pasquinelli, Vladan Joler",AI & SOCIETY,2020-11-21,"<a href=""Springer (2020-11-21) : The Nooscope manifested: AI as instrument of knowledge extractivism"" target=""_blank"">[https://link.springer.com/article/10.1007/s00146-020-01097-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00146-020-01097-6]</a>","Some enlightenment regarding the project to mechanise reason. The assembly line of machine learning: data, algorithm, model. The training dataset:...",,Springer
"Blockchain-enabled supply chain: analysis, challenges, and future directions","Sohail Jabbar, Huw Lloyd, ... Umar Raza",Multimedia Systems,2020-11-20,"<a href=""Springer (2020-11-20) : Blockchain-enabled supply chain: analysis, challenges, and future directions"" target=""_blank"">[https://link.springer.com/article/10.1007/s00530-020-00687-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00530-020-00687-0]</a>",Managing the integrity of products and processes in a multi-stakeholder supply chain environment is a significant challenge. Many current solutions...,,Springer
Open-sourced Dataset Protection via Backdoor Watermarking,"Yiming Li, Ziqi Zhang, Jiawang Bai, Baoyuan Wu, Yong Jiang, Shu-Tao Xia","arXiv
arXiv","2020-11-19
2020-10","<a href=""arXiv (2020-11-19) : Open-sourced Dataset Protection via Backdoor Watermarking"" target=""_blank"">[http://arxiv.org/abs/2010.05821v3]</a>
<a href=""DBLP (2020-10) : Open-sourced Dataset Protection via Backdoor Watermarking"" target=""_blank"">[https://arxiv.org/abs/2010.05821]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2010.05821]</a>","The rapid development of deep learning has benefited from the release of some high-quality open-sourced datasets ($e.g.$, ImageNet), which allows researchers to easily verify the effectiveness of their algorithms. Almost all existing open-sourced datasets require that they can only be adopted for academic or educational purposes rather than commercial purposes, whereas there is still no good way to protect them. In this paper, we propose a \emph{backdoor embedding based dataset watermarking} method to protect an open-sourced image-classification dataset by verifying whether it is used for training a third-party model. Specifically, the proposed method contains two main processes, including \emph{dataset watermarking} and \emph{dataset verification}. We adopt classical poisoning-based backdoor attacks ($e.g.$, BadNets) for dataset watermarking, ie, generating some poisoned samples by adding a certain trigger ($e.g.$, a local patch) onto some benign samples, labeled with a pre-defined target class. Based on the proposed backdoor-based watermarking, we use a hypothesis test guided method for dataset verification based on the posterior probability generated by the suspicious third-party model of the benign samples and their correspondingly watermarked samples ($i.e.$, images with trigger) on the target class. Experiments on some benchmark datasets are conducted, which verify the effectiveness of the proposed method.
","
","arXiv
DBLP"
Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff,"Eitan Borgnia, Valeriia Cherepanova, Liam Fowl, Amin Ghiasi, Jonas Geiping, Micah Goldblum, Tom Goldstein, Arjun Gupta","arXiv
ICASSP
arXiv","2020-11-18
2021
2020-11","<a href=""arXiv (2020-11-18) : Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff"" target=""_blank"">[http://arxiv.org/abs/2011.09527v1]</a>
<a href=""DBLP (2021) : Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff"" target=""_blank"">[https://doi.org/10.1109/ICASSP39728.2021.9414862]</a>
<a href=""DBLP (2020-11) : Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff"" target=""_blank"">[https://arxiv.org/abs/2011.09527]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICASSP39728.2021.9414862]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2011.09527]</a>","Data poisoning and backdoor attacks manipulate victim models by maliciously modifying training data. In light of this growing threat, a recent survey of industry professionals revealed heightened fear in the private sector regarding data poisoning. Many previous defenses against poisoning either fail in the face of increasingly strong attacks, or they significantly degrade performance. However, we find that strong data augmentations, such as mixup and CutMix, can significantly diminish the threat of poisoning and backdoor attacks without trading off performance. We further verify the effectiveness of this simple defense against adaptive poisoning methods, and we compare to baselines including the popular differentially private SGD (DP-SGD) defense. In the context of backdoors, CutMix greatly mitigates the attack while simultaneously increasing validation accuracy by 9%.

","

","arXiv
DBLP
DBLP"
Dynamic backdoor attacks against federated learning,Anbu Huang,"arXiv
arXiv","2020-11-15
2020-11","<a href=""arXiv (2020-11-15) : Dynamic backdoor attacks against federated learning"" target=""_blank"">[http://arxiv.org/abs/2011.07429v1]</a>
<a href=""DBLP (2020-11) : Dynamic backdoor attacks against federated learning"" target=""_blank"">[https://arxiv.org/abs/2011.07429]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2011.07429]</a>","Federated Learning (FL) is a new machine learning framework, which enables millions of participants to collaboratively train machine learning model without compromising data privacy and security. Due to the independence and confidentiality of each client, FL does not guarantee that all clients are honest by design, which makes it vulnerable to adversarial attack naturally. In this paper, we focus on dynamic backdoor attacks under FL setting, where the goal of the adversary is to reduce the performance of the model on targeted tasks while maintaining a good performance on the main task, current existing studies are mainly focused on static backdoor attacks, that is the poison pattern injected is unchanged, however, FL is an online learning framework, and adversarial targets can be changed dynamically by attacker, traditional algorithms require learning a new targeted task from scratch, which could be computationally expensive and require a large number of adversarial training examples, to avoid this, we bridge meta-learning and backdoor attacks under FL setting, in which case we can learn a versatile model from previous experiences, and fast adapting to new adversarial tasks with a few of examples. We evaluate our algorithm on different datasets, and demonstrate that our algorithm can achieve good results with respect to dynamic backdoor attacks. To the best of our knowledge, this is the first paper that focus on dynamic backdoor attacks research under FL setting.
","
","arXiv
DBLP"
Disabling Backdoor and Identifying Poison Data by using Knowledge Distillation in Backdoor Attacks on Deep Neural Networks,Yoshida K.,AISec 2020 - Proceedings of the 13th ACM Workshop on Artificial Intelligence and Security,2020-11-13,"<a href=""ScienceDirect (2020-11-13) : Disabling Backdoor and Identifying Poison Data by using Knowledge Distillation in Backdoor Attacks on Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1145/3411508.3421375]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3411508.3421375]</a>",,,ScienceDirect
The Multi-Watermarks Attack of DNN Watermarking,Li D.,ACM International Conference Proceeding Series,2020-11-13,"<a href=""ScienceDirect (2020-11-13) : The Multi-Watermarks Attack of DNN Watermarking"" target=""_blank"">[https://doi.org/10.1145/3441250.3441279]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3441250.3441279]</a>",,,ScienceDirect
Defending Deep Learning Based Anomaly Detection Systems against White-Box Adversarial Examples and Backdoor Attacks,Alrawashdeh K.,"International Symposium on Technology and Society, Proceedings",2020-11-12,"<a href=""ScienceDirect (2020-11-12) : Defending Deep Learning Based Anomaly Detection Systems against White-Box Adversarial Examples and Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1109/ISTAS50296.2020.9462227]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ISTAS50296.2020.9462227]</a>",,,ScienceDirect
SOMDROID: android malware detection by artificial neural network trained using unsupervised learning,"Arvind Mahindru, A. L. Sangal",Evolutionary Intelligence,2020-11-12,"<a href=""Springer (2020-11-12) : SOMDROID: android malware detection by artificial neural network trained using unsupervised learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s12065-020-00518-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12065-020-00518-1]</a>",Android has gained its popularity due to its open-source and number of freely available apps in its official play store. Appropriate functioning of...,,Springer
Towards Designing a Secure RISC-V System-on-Chip: ITUS,"Vinay B. Y. Kumar, Suman Deb, ... Avi Mendelson",Journal of Hardware and Systems Security,2020-11-10,"<a href=""Springer (2020-11-10) : Towards Designing a Secure RISC-V System-on-Chip: ITUS"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-020-00108-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-020-00108-8]</a>","A rising tide of exploits, in the recent years, following a steady discovery of the many vulnerabilities pervasive in modern computing systems has...",,Springer
Detecting Backdoors in Neural Networks Using Novel Feature-Based Anomaly Detection,"Hao Fu, Akshaj Kumar Veldanda, Prashanth Krishnamurthy, Siddharth Garg, Farshad Khorrami","arXiv
arXiv","2020-11-04
2020-11","<a href=""arXiv (2020-11-04) : Detecting Backdoors in Neural Networks Using Novel Feature-Based Anomaly Detection"" target=""_blank"">[http://arxiv.org/abs/2011.02526v1]</a>
<a href=""DBLP (2020-11) : Detecting Backdoors in Neural Networks Using Novel Feature-Based Anomaly Detection"" target=""_blank"">[https://arxiv.org/abs/2011.02526]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2022.3141077]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2011.02526]</a>","This paper proposes a new defense against neural network backdooring attacks that are maliciously trained to mispredict in the presence of attacker-chosen triggers. Our defense is based on the intuition that the feature extraction layers of a backdoored network embed new features to detect the presence of a trigger and the subsequent classification layers learn to mispredict when triggers are detected. Therefore, to detect backdoors, the proposed defense uses two synergistic anomaly detectors trained on clean validation data: the first is a novelty detector that checks for anomalous features, while the second detects anomalous mappings from features to outputs by comparing with a separate classifier trained on validation data. The approach is evaluated on a wide range of backdoored networks (with multiple variations of triggers) that successfully evade state-of-the-art defenses. Additionally, we evaluate the robustness of our approach on imperceptible perturbations, scalability on large-scale datasets, and effectiveness under domain shift. This paper also shows that the defense can be further improved using data augmentation.
","
","arXiv
DBLP"
Towards inspecting and eliminating trojan backdoors in deep neural networks,Guo W.,"Proceedings - IEEE International Conference on Data Mining, ICDM",2020-11-01,"<a href=""ScienceDirect (2020-11-01) : Towards inspecting and eliminating trojan backdoors in deep neural networks"" target=""_blank"">[https://doi.org/10.1109/ICDM50108.2020.00025]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICDM50108.2020.00025]</a>",,,ScienceDirect
TrojDRL: evaluation of backdoor attacks on deep reinforcement learning,"Panagiota Kiourti, Kacper Wardega, Susmit Jha, Wenchao Li",DAC '20: Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference,2020-11,"<a href=""ACM (2020-11) : TrojDRL: evaluation of backdoor attacks on deep reinforcement learning"" target=""_blank"">[https://dl.acm.org/doi/10.5555/3437539.3437570]</a>","<a href=""ACM"" target=""_blank"">[]</a>","We present TrojDRL, a tool for exploring and evaluating backdoor attacks on deep reinforcement learning agents. TrojDRL exploits the sequential nature of deep reinforcement learning (DRL) and considers different gradations of threat models. We show that ...",,ACM
"Privacy for the weak, transparency for the powerful: the cypherpunk ethics of Julian Assange",Patrick D. Anderson,Ethics and Information Technology,2020-10-31,"<a href=""Springer (2020-10-31) : Privacy for the weak, transparency for the powerful: the cypherpunk ethics of Julian Assange"" target=""_blank"">[https://link.springer.com/article/10.1007/s10676-020-09571-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10676-020-09571-x]</a>","WikiLeaks is among the most controversial institutions of the last decade, and this essay contributes to an understanding of WikiLeaks by revealing...",,Springer
Artificial intelligence-based antivirus in order to detect malware preventively,"Sidney M. L. de Lima, Heverton K. de L. Silva, ... Alisson M. da Silva",Progress in Artificial Intelligence,2020-10-30,"<a href=""Springer (2020-10-30) : Artificial intelligence-based antivirus in order to detect malware preventively"" target=""_blank"">[https://link.springer.com/article/10.1007/s13748-020-00220-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13748-020-00220-4]</a>",The proposed paper investigates commercial antiviruses. About 17% of the antiviruses did not recognize the existence of the malicious samples...,,Springer
Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features,Lin J.,Proceedings of the ACM Conference on Computer and Communications Security,2020-10-30,"<a href=""ScienceDirect (2020-10-30) : Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features"" target=""_blank"">[https://doi.org/10.1145/3372297.3423362]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3372297.3423362]</a>",,,ScienceDirect
Research on identification method and device with active immune attack,Wang Q.,Journal of Physics: Conference Series,2020-10-30,"<a href=""ScienceDirect (2020-10-30) : Research on identification method and device with active immune attack"" target=""_blank"">[https://doi.org/10.1088/1742-6596/1646/1/012152]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1088/1742-6596/1646/1/012152]</a>",,,ScienceDirect
Federated Mimic Learning for Privacy Preserving Intrusion Detection,N. A. Al-Athba Al-Marri B. S. Ciftler M. M. Abdallah,2020 IEEE International Black Sea Conference on Communications and Networking (BlackSeaCom),2020-10-27,"<a href=""IEEE (2020-10-27) : Federated Mimic Learning for Privacy Preserving Intrusion Detection"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9234959]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/BlackSeaCom48709.2020.9234959]</a>","Internet of things (IoT) devices are prone to attacks due to the limitation of their privacy and security components. These attacks vary from exploiting backdoors to disrupting the communication network of the devices. Intrusion Detection Systems (IDS) play an essential role in ensuring information privacy and security of IoT devices against these attacks. Recently, deep learning-based IDS techniques are becoming more prominent due to their high classification accuracy. However, conventional deep learning techniques jeopardize user privacy due to the transfer of user data to a centralized server. Federated learning (FL) is a popular privacy-preserving decentralized learning method. FL enables training models locally at the edge devices and transferring local models to a centralized server instead of transferring sensitive data. Nevertheless, FL can suffer from reverse engineering ML attacks that can learn information about the user's data from model. To overcome the problem of reverse engineering, mimic learning is another way to preserve the privacy of ML-based IDS. In mimic learning, a student model is trained with the public dataset, which is labeled with the teacher model that is trained by sensitive user data. In this work, we propose a novel approach that combines the advantages of FL and mimic learning, namely federated mimic learning to create a distributed IDS while minimizing the risk of jeopardizing users' privacy, and benchmark its performance compared to other ML-based IDS techniques using NSL-KDD dataset. Our results show that we can achieve 98.11% detection accuracy with federated mimic learning.",,IEEE
Unraveling robustness of deep face anti-spoofing models against pixel attacks,"Naima Bousnina, Lilei Zheng, ... Khalid Minaoui",Multimedia Tools and Applications,2020-10-27,"<a href=""Springer (2020-10-27) : Unraveling robustness of deep face anti-spoofing models against pixel attacks"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-020-10041-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-020-10041-1]</a>","In the last few decades, deep-learning-based face verification and recognition systems have had enormous success in solving complex security...",,Springer
Conditional Probability Voting Algorithm Based on Heterogeneity of Mimic Defense System,S. Wei H. Zhang W. Zhang H. Yu,IEEE Access,2020-10-26,"<a href=""IEEE (2020-10-26) : Conditional Probability Voting Algorithm Based on Heterogeneity of Mimic Defense System"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9224846]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2020.3031323]</a>","In recent years network attacks have been increasing rapidly, and it is difficult to defend against these attacks, especially attacks at unknown vulnerabilities or backdoors. As a novel method, Mimic defense architecture has been proposed to solve these cyberspace security problems by using heterogeneous redundant variants to perform the same task. How to choose appropriate variants and voting algorithm according to heterogeneities of these variants become the key issue of designing mimic defense architecture. Most of current researches are based on the 2-level similarity of variants, but the results are not accurate enough. This article presents an attack model based on mimic defense architecture, abstracts binary division vector and relevant indexes to describe the heterogeneity of these variants, and innovatively proposes conditional probability voting algorithm, which is different from classic majority voting algorithm. This article also analyzes the system failure probability and scalability of these voting algorithms, experiment results show that conditional probability voting algorithm is the best, both in system failure probability and scalability.",,IEEE
On Evaluating Neural Network Backdoor Defenses,"Akshaj Veldanda, Siddharth Garg","arXiv
arXiv","2020-10-23
2020-10","<a href=""arXiv (2020-10-23) : On Evaluating Neural Network Backdoor Defenses"" target=""_blank"">[http://arxiv.org/abs/2010.12186v1]</a>
<a href=""DBLP (2020-10) : On Evaluating Neural Network Backdoor Defenses"" target=""_blank"">[https://arxiv.org/abs/2010.12186]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2010.12186]</a>","Deep neural networks (DNNs) demonstrate superior performance in various fields, including scrutiny and security. However, recent studies have shown that DNNs are vulnerable to backdoor attacks. Several defenses were proposed in the past to defend DNNs against such backdoor attacks. In this work, we conduct a critical analysis and identify common pitfalls in these existing defenses, prepare a comprehensive database of backdoor attacks, conduct a side-by-side evaluation of existing defenses against this database. Finally, we layout some general guidelines to help researchers develop more robust defenses in the future and avoid common mistakes from the past.
","
","arXiv
DBLP"
L-RED: Efficient Post-Training Detection of Imperceptible Backdoor Attacks without Access to the Training Set,"Zhen Xiang, David J. Miller, George Kesidis","arXiv
arXiv","2020-10-21
2020-10","<a href=""arXiv (2020-10-21) : L-RED: Efficient Post-Training Detection of Imperceptible Backdoor Attacks without Access to the Training Set"" target=""_blank"">[http://arxiv.org/abs/2010.09987v2]</a>
<a href=""DBLP (2020-10) : L-RED: Efficient Post-Training Detection of Imperceptible Backdoor Attacks without Access to the Training Set"" target=""_blank"">[https://arxiv.org/abs/2010.09987]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2010.09987]</a>","Backdoor attacks (BAs) are an emerging form of adversarial attack typically against deep neural network image classifiers. The attacker aims to have the classifier learn to classify to a target class when test images from one or more source classes contain a backdoor pattern, while maintaining high accuracy on all clean test images. Reverse-Engineering-based Defenses (REDs) against BAs do not require access to the training set but only to an independent clean dataset. Unfortunately, most existing REDs rely on an unrealistic assumption that all classes except the target class are source classes of the attack. REDs that do not rely on this assumption often require a large set of clean images and heavy computation. In this paper, we propose a Lagrangian-based RED (L-RED) that does not require knowledge of the number of source classes (or whether an attack is present). Our defense requires very few clean images to effectively detect BAs and is computationally efficient. Notably, we detect 56 out of 60 BAs using only two clean images per class in our experiments on CIFAR-10.
","
","arXiv
DBLP"
Input-Aware Dynamic Backdoor Attack,"Anh Nguyen, Anh Tran","arXiv
arXiv
NeurIPS","2020-10-16
2020-10
2020","<a href=""arXiv (2020-10-16) : Input-Aware Dynamic Backdoor Attack"" target=""_blank"">[http://arxiv.org/abs/2010.08138v1]</a>
<a href=""DBLP (2020-10) : Input-Aware Dynamic Backdoor Attack"" target=""_blank"">[https://arxiv.org/abs/2010.08138]</a>
<a href=""DBLP (2020) : Input-Aware Dynamic Backdoor Attack"" target=""_blank"">[https://proceedings.neurips.cc/paper/2020/hash/234e691320c0ad5b45ee3c96d0d7b8f8-Abstract.html]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2010.08138]</a>
<a href=""DBLP"" target=""_blank"">[https://proceedings.neurips.cc/paper/2020/hash/234e691320c0ad5b45ee3c96d0d7b8f8-Abstract.html]</a>","In recent years, neural backdoor attack has been considered to be a potential security threat to deep learning systems. Such systems, while achieving the state-of-the-art performance on clean data, perform abnormally on inputs with predefined triggers. Current backdoor techniques, however, rely on uniform trigger patterns, which are easily detected and mitigated by current defense methods. In this work, we propose a novel backdoor attack technique in which the triggers vary from input to input. To achieve this goal, we implement an input-aware trigger generator driven by diversity loss. A novel cross-trigger test is applied to enforce trigger nonreusablity, making backdoor verification impossible. Experiments show that our method is efficient in various attack scenarios as well as multiple datasets. We further demonstrate that our backdoor can bypass the state of the art defense methods. An analysis with a famous neural network inspector again proves the stealthiness of the proposed attack. Our code is publicly available at https://github.com/VinAIResearch/input-aware-backdoor-attack-release.

","<a href=""arXiv"" target=""_blank"">[https://github.com/VinAIResearch/input-aware-backdoor-attack-release]</a>

","arXiv
DBLP
DBLP"
Multi-level Gaussian mixture modeling for detection of malicious network traffic,"Radhika Chapaneri, Seema Shah",The Journal of Supercomputing,2020-10-16,"<a href=""Springer (2020-10-16) : Multi-level Gaussian mixture modeling for detection of malicious network traffic"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-020-03447-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-020-03447-z]</a>","Along with the growing network connectivity across the world, there is a substantial increase in malicious network traffic to exploit the...",,Springer
Reverse Engineering Imperceptible Backdoor Attacks on Deep Neural Networks for Detection and Training Set Cleansing,"Zhen Xiang, David J. Miller, George Kesidis","arXiv
arXiv","2020-10-15
2020-10","<a href=""arXiv (2020-10-15) : Reverse Engineering Imperceptible Backdoor Attacks on Deep Neural Networks for Detection and Training Set Cleansing"" target=""_blank"">[http://arxiv.org/abs/2010.07489v1]</a>
<a href=""DBLP (2020-10) : Reverse Engineering Imperceptible Backdoor Attacks on Deep Neural Networks for Detection and Training Set Cleansing"" target=""_blank"">[https://arxiv.org/abs/2010.07489]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2010.07489]</a>","Backdoor data poisoning is an emerging form of adversarial attack usually against deep neural network image classifiers. The attacker poisons the training set with a relatively small set of images from one (or several) source class(es), embedded with a backdoor pattern and labeled to a target class. For a successful attack, during operation, the trained classifier will: 1) misclassify a test image from the source class(es) to the target class whenever the same backdoor pattern is present, 2) maintain a high classification accuracy for backdoor-free test images. In this paper, we make a break-through in defending backdoor attacks with imperceptible backdoor patterns (e.g. watermarks) before/during the training phase. This is a challenging problem because it is a priori unknown which subset (if any) of the training set has been poisoned. We propose an optimization-based reverse-engineering defense, that jointly: 1) detects whether the training set is poisoned, 2) if so, identifies the target class and the training images with the backdoor pattern embedded, and 3) additionally, reversely engineers an estimate of the backdoor pattern used by the attacker. In benchmark experiments on CIFAR-10, for a large variety of attacks, our defense achieves a new state-of-the-art by reducing the attack success rate to no more than 4.9% after removing detected suspicious training images.
","
","arXiv
DBLP"
Malware Injection in Operational Technology Networks,M. Khadpe P. Binnar F. Kazi,"2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",2020-10-15,"<a href=""IEEE (2020-10-15) : Malware Injection in Operational Technology Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9225382]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICCCNT49239.2020.9225382]</a>","Security Issues of Industrial Control System (ICS) increasing day by day because of this, it gets more and more attention. Hackers finding different methods or approaches to finding vulnerability of Operational Technology (OT) Networks. Some of the methods used by Attacker are injection of malware and backdoor. Programmable logic controller (PLC) and Supervisory Control and Data Acquisition (SCADA) both are important part of industry. Usually, Industrial Control System network is completely isolated from external network, but the administrative computer which is connected to the network are vulnerable, one of the reasons will be use of internet are increasing day by day. Stuxnet Attack is example of this type of vulnerability. The proposed work consist of are performing a Denial of service (DoS) attack on Allen Bradley PLC which is used to control Waste-Water Treatment Plant (WTP). To performing this attack we first, inject a malware into an administrative computer, which is connected to Waste-Water Treatment Plant network. The malware we inject into the network is capable of collecting all information of network which includes all IP address, its vendor information, MAC address etc. The data is stored into a text file which will downloaded from Victim Computer into Hacker computer by using Backdoor. And after finding target IP address, we will perform DoS attack. Also, this paper discussed about process of creating malware and backdoor as well as it includes analysis of network traffic before and after performing an attack.",,IEEE
Optimizing deep learning based intrusion detection systems defense against white-box and backdoor adversarial attacks through a genetic algorithm,Alrawashdeh K.,Proceedings - Applied Imagery Pattern Recognition Workshop,2020-10-13,"<a href=""ScienceDirect (2020-10-13) : Optimizing deep learning based intrusion detection systems defense against white-box and backdoor adversarial attacks through a genetic algorithm"" target=""_blank"">[https://doi.org/10.1109/AIPR50011.2020.9425293]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/AIPR50011.2020.9425293]</a>",,,ScienceDirect
GangSweep: Sweep out Neural Backdoors by GAN,Zhu L.,MM 2020 - Proceedings of the 28th ACM International Conference on Multimedia,2020-10-12,"<a href=""ScienceDirect (2020-10-12) : GangSweep: Sweep out Neural Backdoors by GAN"" target=""_blank"">[https://doi.org/10.1145/3394171.3413546]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3394171.3413546]</a>",,,ScienceDirect
Unified Architectural Support for Secure and Robust Deep Learning,M. Javaheripi H. Chen F. Koushanfar,2020 57th ACM/IEEE Design Automation Conference (DAC),2020-10-09,"<a href=""IEEE (2020-10-09) : Unified Architectural Support for Secure and Robust Deep Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9218575]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/DAC18072.2020.9218575]</a>","Recent advances in Deep Learning (DL) have enabled a paradigm shift to include machine intelligence in a wide range of autonomous tasks. As a result, a largely unexplored surface has opened up for attacks jeopardizing the integrity of DL models and hindering the success of autonomous systems. To enable ubiquitous deployment of DL approaches across various intelligent applications, we propose to develop architectural support for hardware implementation of secure and robust DL. Towards this goal, we leverage hardware/software co-design to develop a DL execution engine that supports algorithms specifically designed to defend against various attacks. The proposed framework is enhanced with two real-time defense mechanisms, securing both DL training and execution stages. In particular, we enable model-level Trojan detection to mitigate backdoor attacks and malicious behaviors induced on the DL model during training. We further realize real-time adversarial attack detection to avert malicious behavior during execution. The proposed execution engine is equipped with hardware-level IP protection and usage control mechanism to attest the legitimacy of the DL model mapped to the device. Our design is modular and can be tuned to task-specific demands, e.g., power, throughput, and memory bandwidth, by means of a customized hardware compiler. We further provide an accompanying API to reduce the nonrecurring engineering cost and ensure automated adaptation to various domains and applications.",,IEEE
Risk assessment of cyber-attacks on telemetry-enabled cardiac implantable electronic devices (CIED),"Mikaëla Ngamboé, Paul Berthier, ... José M. Fernandez",International Journal of Information Security,2020-10-07,"<a href=""Springer (2020-10-07) : Risk assessment of cyber-attacks on telemetry-enabled cardiac implantable electronic devices (CIED)"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-020-00522-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-020-00522-7]</a>","Cardiac implantable electronic devices (CIED) are vulnerable to radio frequency (RF) cyber-attacks. Besides, CIED communicate with medical equipment...",,Springer
A survey on boosting IoT security and privacy through blockchain,"Omar Alfandi, Salam Khanji, ... Asad Khattak",Cluster Computing,2020-10-06,"<a href=""Springer (2020-10-06) : A survey on boosting IoT security and privacy through blockchain"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-020-03137-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-020-03137-8]</a>",The constant development of interrelated computing devices and the emergence of new network technologies have caused a dramatic growth in the number...,,Springer
Firmware Attack Detection on Gadgets Using Kohonen's Self Organizing Feature Maps (KSOFM),E. Arul A. Punidha,2020 Third International Conference on Smart Systems and Inventive Technology (ICSSIT),2020-10-06,"<a href=""IEEE (2020-10-06) : Firmware Attack Detection on Gadgets Using Kohonen's Self Organizing Feature Maps (KSOFM)"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9214115]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICSSIT48917.2020.9214115]</a>","According to the NIST International Security risk report, throughout the last several years, firmware exploits has almost quintupled. Public network wireless and community users can be particularly compromised, as the non-commercial users. However, conventional extremely pro-virus software, operating procedures and vulnerability device prototypes are not detectable for such firmware hacks. To avoid the high risk, companies among all sorts must, along with server and application security, render the safety of edge computer equipment and firmware AS their primary concern. The proposed Firmware Kohonen's Self Organizing Feature Maps (KSOFM) helps to describe such a firmware assault on gadgets. With the firmware KSOM and information from a non-regular source space, the model will choose a collection of better characteristics to approximate the intrinsic dissemination. The characteristic map represents changes in the source density figures: the reference area through which test learning segments are derived when compared to the bigger target fields and thus higher than the test area areas through which testing dimensions are derived with lower probabilities. The result shows that the truth positive rating is 98.28 percent, and that the adware attack is 0.02 percent positive.",,IEEE
Can Adversarial Weight Perturbations Inject Neural Backdoors,"Siddhant Garg, Adarsh Kumar, Vibhor Goel, Yingyu Liang","CIKM '20: Proceedings of the 29th ACM International Conference on Information & Knowledge Management
arXiv
CIKM
arXiv","2020-10
2020-09-21
2020
2020-08","<a href=""ACM (2020-10) : Can Adversarial Weight Perturbations Inject Neural Backdoors"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3340531.3412130]</a>
<a href=""arXiv (2020-09-21) : Can Adversarial Weight Perturbations Inject Neural Backdoors?"" target=""_blank"">[http://arxiv.org/abs/2008.01761v2]</a>
<a href=""DBLP (2020) : Can Adversarial Weight Perturbations Inject Neural Backdoors"" target=""_blank"">[https://doi.org/10.1145/3340531.3412130]</a>
<a href=""DBLP (2020-08) : Can Adversarial Weight Perturbations Inject Neural Backdoors?"" target=""_blank"">[https://arxiv.org/abs/2008.01761]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3340531.3412130]</a>
<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1145/3340531.3412130]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3340531.3412130]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2008.01761]</a>","Adversarial machine learning has exposed several security hazards of neural models. Thus far, the concept of an ""adversarial perturbation"" has exclusively been used with reference to the input space referring to a small, imperceptible change which can ...
Adversarial machine learning has exposed several security hazards of neural models and has become an important research topic in recent times. Thus far, the concept of an ""adversarial perturbation"" has exclusively been used with reference to the input space referring to a small, imperceptible change which can cause a ML model to err. In this work we extend the idea of ""adversarial perturbations"" to the space of model weights, specifically to inject backdoors in trained DNNs, which exposes a security risk of using publicly available trained models. Here, injecting a backdoor refers to obtaining a desired outcome from the model when a trigger pattern is added to the input, while retaining the original model predictions on a non-triggered input. From the perspective of an adversary, we characterize these adversarial perturbations to be constrained within an $\ell_{\infty}$ norm around the original model weights. We introduce adversarial perturbations in the model weights using a composite loss on the predictions of the original model and the desired trigger through projected gradient descent. We empirically show that these adversarial weight perturbations exist universally across several computer vision and natural language processing tasks. Our results show that backdoors can be successfully injected with a very small average relative change in model weight values for several applications.

","


","ACM
arXiv
DBLP
DBLP"
Embedding Backdoors as the Facial Features: Invisible Backdoor Attacks Against Face Recognition Systems,"Can He, Mingfu Xue, Jian Wang, Weiqiang Liu","ACM TURC '20: Proceedings of the ACM Turing Celebration Conference - China
ACM TUR-C","2020-10
2020","<a href=""ACM (2020-10) : Embedding Backdoors as the Facial Features: Invisible Backdoor Attacks Against Face Recognition Systems"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3393527.3393567]</a>
<a href=""DBLP (2020) : Embedding Backdoors as the Facial Features: Invisible Backdoor Attacks Against Face Recognition Systems"" target=""_blank"">[https://doi.org/10.1145/3393527.3393567]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3393527.3393567]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3393527.3393567]</a>","Deep neural network (DNN) based face recognition systems have been widely applied in various identity authentication scenarios. However, recent studies show that the DNN models are vulnerable to backdoor attacks. An attacker can embed backdoors into the ...
","
","ACM
DBLP"
"Don&apos,t Trigger Me! A Triggerless Backdoor Attack Against Deep Neural Networks","Ahmed Salem, Michael Backes, Yang Zhang",arXiv,2020-10,"<a href=""DBLP (2020-10) : Don&apos,t Trigger Me! A Triggerless Backdoor Attack Against Deep Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2010.03282]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2010.03282]</a>",,,DBLP
Mixed-Models Method Based on Machine Learning in Detecting WebShell Attack,"Lu Jinping, Tang Zhi, Mao Jian, Gu Zhiling, Zhang Jiemin","CIPAE 2020: Proceedings of the 2020 International Conference on Computers, Information Processing and Advanced Education",2020-10,"<a href=""ACM (2020-10) : Mixed-Models Method Based on Machine Learning in Detecting WebShell Attack"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3419635.3419716]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3419635.3419716]</a>","WebShell is a command execution environment in the form of web files and also a remote administration tool in web containers. However, it is also a web page backdoor for attackers. Malicious WebShell endangers safety of the Web services. Traditional ...",,ACM
NeuroAttack: Undermining Spiking Neural Networks Security through Externally Triggered Bit-Flips,V. Venceslai A. Marchisio I. Alouani M. Martina M. Shafique,2020 International Joint Conference on Neural Networks (IJCNN),2020-09-28,"<a href=""IEEE (2020-09-28) : NeuroAttack: Undermining Spiking Neural Networks Security through Externally Triggered Bit-Flips"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207351]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IJCNN48605.2020.9207351]</a>","Due to their proven efficiency, machine-learning systems are deployed in a wide range of complex real-life problems. More specifically, Spiking Neural Networks (SNNs) emerged as a promising solution to the accuracy, resource-utilization, and energy-efficiency challenges in machine-learning systems. While these systems are going mainstream, they have inherent security and reliability issues. In this paper, we propose NeuroAttack, a cross-layer attack that threatens the SNNs integrity by exploiting low-level reliability issues through a high-level attack. Particularly, we trigger a fault-injection based sneaky hardware backdoor through a carefully crafted adversarial input noise. Our results on Deep Neural Networks (DNNs) and SNNs show a serious integrity threat to state-of-the art machine-learning techniques.",,IEEE
Biometric Backdoors: A Poisoning Attack Against Unsupervised Template Updating,"Giulio Lovisotto, Simon Eberz, Ivan Martinovic","arXiv
EuroS&amp,P
arXiv","2020-09-27
2020
2019-05","<a href=""arXiv (2020-09-27) : Biometric Backdoors: A Poisoning Attack Against Unsupervised Template Updating"" target=""_blank"">[http://arxiv.org/abs/1905.09162v2]</a>
<a href=""DBLP (2020) : Biometric Backdoors: A Poisoning Attack Against Unsupervised Template Updating"" target=""_blank"">[https://doi.org/10.1109/EuroSP48549.2020.00020]</a>
<a href=""DBLP (2019-05) : Biometric Backdoors: A Poisoning Attack Against Unsupervised Template Updating"" target=""_blank"">[http://arxiv.org/abs/1905.09162]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1109/EuroSP48549.2020.00020]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/EuroSP48549.2020.00020]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1905.09162]</a>","In this work, we investigate the concept of biometric backdoors: a template poisoning attack on biometric systems that allows adversaries to stealthily and effortlessly impersonate users in the long-term by exploiting the template update procedure. We show that such attacks can be carried out even by attackers with physical limitations (no digital access to the sensor) and zero knowledge of training data (they know neither decision boundaries nor user template). Based on the adversaries' own templates, they craft several intermediate samples that incrementally bridge the distance between their own template and the legitimate user's. As these adversarial samples are added to the template, the attacker is eventually accepted alongside the legitimate user. To avoid detection, we design the attack to minimize the number of rejected samples. We design our method to cope with the weak assumptions for the attacker and we evaluate the effectiveness of this approach on state-of-the-art face recognition pipelines based on deep neural networks. We find that in scenarios where the deep network is known, adversaries can successfully carry out the attack over 70% of cases with less than ten injection attempts. Even in black-box scenarios, we find that exploiting the transferability of adversarial samples from surrogate models can lead to successful attacks in around 15% of cases. Finally, we design a poisoning detection technique that leverages the consistent directionality of template updates in feature space to discriminate between legitimate and malicious updates. We evaluate such a countermeasure with a set of intra-user variability factors which may present the same directionality characteristics, obtaining equal error rates for the detection between 7-14% and leading to over 99% of attacks being detected after only two sample injections.

","

","arXiv
DBLP
DBLP"
An Efficient Hybrid Evolutionary Approach for Identification of Zero-Day Attacks on Wired/Wireless Network System,Alok Kumar Shukla,Wireless Personal Communications,2020-09-23,"<a href=""Springer (2020-09-23) : An Efficient Hybrid Evolutionary Approach for Identification of Zero-Day Attacks on Wired/Wireless Network System"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-020-07808-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-020-07808-y]</a>",Attacks from network applications establish considerable security threats for computer networks and end users. Existing cyber-attack detection...,,Springer
What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors,"Yi-Shan Lin, Wen-Chuan Lee, Z. Berkay Celik","arXiv
arXiv","2020-09-22
2020-09","<a href=""arXiv (2020-09-22) : What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors"" target=""_blank"">[http://arxiv.org/abs/2009.10639v1]</a>
<a href=""DBLP (2020-09) : What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors"" target=""_blank"">[https://arxiv.org/abs/2009.10639]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2009.10639]</a>","EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the parts of the inputs deemed important to arrive a decision at a specific target. However, it remains challenging to quantify correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns--hidden malicious functionalities that cause misclassification--to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for systematic evaluation of explanations that an XAI method generates and evaluate seven state-of-the-art model-free and model-specific posthoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. We discovered six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region.
","
","arXiv
DBLP"
Light Can Hack Your Face! Black-box Backdoor Attack on Face Recognition Systems,"Haoliang Li, Yufei Wang, Xiaofei Xie, Yang Liu, Shiqi Wang, Renjie Wan, Lap-Pui Chau, Alex C. Kot","arXiv
arXiv","2020-09-15
2020-09","<a href=""arXiv (2020-09-15) : Light Can Hack Your Face! Black-box Backdoor Attack on Face Recognition Systems"" target=""_blank"">[http://arxiv.org/abs/2009.06996v1]</a>
<a href=""DBLP (2020-09) : Light Can Hack Your Face! Black-box Backdoor Attack on Face Recognition Systems"" target=""_blank"">[https://arxiv.org/abs/2009.06996]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2009.06996]</a>","Deep neural networks (DNN) have shown great success in many computer vision applications. However, they are also known to be susceptible to backdoor attacks. When conducting backdoor attacks, most of the existing approaches assume that the targeted DNN is always available, and an attacker can always inject a specific pattern to the training data to further fine-tune the DNN model. However, in practice, such attack may not be feasible as the DNN model is encrypted and only available to the secure enclave. In this paper, we propose a novel black-box backdoor attack technique on face recognition systems, which can be conducted without the knowledge of the targeted DNN model. To be specific, we propose a backdoor attack with a novel color stripe pattern trigger, which can be generated by modulating LED in a specialized waveform. We also use an evolutionary computing strategy to optimize the waveform for backdoor attack. Our backdoor attack can be conducted in a very mild condition: 1) the adversary cannot manipulate the input in an unnatural way (e.g., injecting adversarial noise), 2) the adversary cannot access the training database, 3) the adversary has no knowledge of the training model as well as the training set used by the victim party. We show that the backdoor trigger can be quite effective, where the attack success rate can be up to $88\%$ based on our simulation study and up to $40\%$ based on our physical-domain study by considering the task of face recognition and verification based on at most three-time attempts during authentication. Finally, we evaluate several state-of-the-art potential defenses towards backdoor attacks, and find that our attack can still be effective. We highlight that our study revealed a new physical backdoor attack, which calls for the attention of the security issue of the existing face recognition/verification techniques.
","
","arXiv
DBLP"
"Naive Bayes: applications, variations and vulnerabilities: a review of literature with code snippets for implementation","Indika Wickramasinghe, Harsha Kalutarage",Soft Computing,2020-09-09,"<a href=""Springer (2020-09-09) : Naive Bayes: applications, variations and vulnerabilities: a review of literature with code snippets for implementation"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-020-05297-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-020-05297-6]</a>",Naïve Bayes (NB) is a well-known probabilistic classification algorithm. It is a simple but efficient algorithm with a wide variety of real-world...,,Springer
Secure neural network watermarking protocol against forging attack,"Renjie Zhu, Xinpeng Zhang, ... Zhenjun Tang",EURASIP Journal on Image and Video Processing,2020-09-04,"<a href=""Springer (2020-09-04) : Secure neural network watermarking protocol against forging attack"" target=""_blank"">[https://link.springer.com/article/10.1186/s13640-020-00527-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13640-020-00527-1]</a>","In order to protect the intellectual property of neural network, an owner may select a set of trigger samples and their corresponding labels to train...",,Springer
A Review on the Effectiveness of Machine Learning and Deep Learning Algorithms for Cyber Security,"R. Geetha, T. Thilagam",Archives of Computational Methods in Engineering,2020-09-02,"<a href=""Springer (2020-09-02) : A Review on the Effectiveness of Machine Learning and Deep Learning Algorithms for Cyber Security"" target=""_blank"">[https://link.springer.com/article/10.1007/s11831-020-09478-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11831-020-09478-2]</a>",In recent years there exists a wide variety of cyber attacks with the drastic development of the internet technology. Detection of these attacks is...,,Springer
TempoCode-IoT: temporal codebook-based encoding of flow features for intrusion detection in Internet of Things,"Abdul Jabbar Siddiqui, Azzedine Boukerche",Cluster Computing,2020-09-02,"<a href=""Springer (2020-09-02) : TempoCode-IoT: temporal codebook-based encoding of flow features for intrusion detection in Internet of Things"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-020-03153-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-020-03153-8]</a>","In the recent years, the Internet of Things has been becoming a vulnerable target of intrusion attacks. As the academia and industry move towards...",,Springer
Biometric Backdoors: A Poisoning Attack against Unsupervised Template Updating,Lovisotto G.,"Proceedings - 5th IEEE European Symposium on Security and Privacy, Euro S and P 2020",2020-09-01,"<a href=""ScienceDirect (2020-09-01) : Biometric Backdoors: A Poisoning Attack against Unsupervised Template Updating"" target=""_blank"">[https://doi.org/10.1109/EuroSP48549.2020.00020]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/EuroSP48549.2020.00020]</a>",,,ScienceDirect
Comparative study between backdoor and conventional thyroidectomy in non-recurrent goiter,El–sayed Y.A.,Egyptian Journal of Hospital Medicine,2020-09-01,"<a href=""ScienceDirect (2020-09-01) : Comparative study between backdoor and conventional thyroidectomy in non-recurrent goiter"" target=""_blank"">[https://doi.org/10.21608/ejhm.2020.121921]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.21608/ejhm.2020.121921]</a>",,,ScienceDirect
On Optical Attacks Making Logic Obfuscation Fragile,Lavdas L.,"Proceedings - 2020 IEEE International Test Conference in Asia, ITC-Asia 2020",2020-09-01,"<a href=""ScienceDirect (2020-09-01) : On Optical Attacks Making Logic Obfuscation Fragile"" target=""_blank"">[https://doi.org/10.1109/ITC-Asia51099.2020.00024]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ITC-Asia51099.2020.00024]</a>",,,ScienceDirect
"Revealing perceptible backdoors in DNNs, without the training set, via the maximum achievable misclassification fraction statistic",Xiang Z.,"IEEE International Workshop on Machine Learning for Signal Processing, MLSP",2020-09-01,"<a href=""ScienceDirect (2020-09-01) : Revealing perceptible backdoors in DNNs, without the training set, via the maximum achievable misclassification fraction statistic"" target=""_blank"">[https://doi.org/10.1109/MLSP49062.2020.9231861]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/MLSP49062.2020.9231861]</a>",,,ScienceDirect
WebShell attack detection based on a deep super learner,Ai Z.,Symmetry,2020-09-01,"<a href=""ScienceDirect (2020-09-01) : WebShell attack detection based on a deep super learner"" target=""_blank"">[https://doi.org/10.3390/SYM12091406]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/SYM12091406]</a>",,,ScienceDirect
Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization,"Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, Xinpeng Zhang","arXiv
IEEE Trans. Dependable Secur. Comput.","2020-08-31
2021","<a href=""arXiv (2020-08-31) : Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization"" target=""_blank"">[http://arxiv.org/abs/1909.02742v3]</a>
<a href=""DBLP (2021) : Invisible Backdoor Attacks on Deep Neural Networks Via Steganography and Regularization"" target=""_blank"">[https://doi.org/10.1109/TDSC.2020.3021407]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TDSC.2020.3021407]</a>","Deep neural networks (DNNs) have been proven vulnerable to backdoor attacks, where hidden features (patterns) trained to a normal model, which is only activated by some specific input (called triggers), trick the model into producing unexpected behavior. In this paper, we create covert and scattered triggers for backdoor attacks, invisible backdoors, where triggers can fool both DNN models and human inspection. We apply our invisible backdoors through two state-of-the-art methods of embedding triggers for backdoor attacks. The first approach on Badnets embeds the trigger into DNNs through steganography. The second approach of a trojan attack uses two types of additional regularization terms to generate the triggers with irregular shape and size. We use the Attack Success Rate and Functionality to measure the performance of our attacks. We introduce two novel definitions of invisibility for human perception, one is conceptualized by the Perceptual Adversarial Similarity Score (PASS) and the other is Learned Perceptual Image Patch Similarity (LPIPS). We show that the proposed invisible backdoors can be fairly effective across various DNN models as well as four datasets MNIST, CIFAR-10, CIFAR-100, and GTSRB, by measuring their attack success rates for the adversary, functionality for the normal users, and invisibility scores for the administrators. We finally argue that the proposed invisible backdoor attacks can effectively thwart the state-of-the-art trojan backdoor detection approaches, such as Neural Cleanse and TABOR.
","
","arXiv
DBLP"
Towards a Backdoorless Network Architecture Based on Remote Attestation and Backdoor Inspection,"Takayuki Sasaki, Yusuke Shimada","arXiv
arXiv","2020-08-20
2020-07","<a href=""arXiv (2020-08-20) : Towards a Backdoorless Network Architecture Based on Remote Attestation and Backdoor Inspection"" target=""_blank"">[http://arxiv.org/abs/2007.14748v2]</a>
<a href=""DBLP (2020-07) : Towards a Backdoorless Network Architecture Based on Remote Attestation and Backdoor Inspection"" target=""_blank"">[https://arxiv.org/abs/2007.14748]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2007.14748]</a>","To keep a system secure, all devices in the system need to be benign. To avoid malicious and/or compromised devices, network access control such as authentication using a credential and remote attestation based on trusted hardware has been used. These techniques ensure the authenticity and integrity of the devices, but do not mitigate risks of a backdoor embedded in the devices by the developer. To tackle this problem, we propose a novel architecture that integrates remote attestation and backdoor inspection. Specifically, the backdoor inspection result is stored in a server and the verifier retrieves and checks the backdoor inspection result when the remote attestation is performed. Moreover, we discuss issues to deploy the proposed architecture to the real world.
","
","arXiv
DBLP"
Detection of Backdoors in Trained Classifiers Without Access to the Training Set,"Zhen Xiang, David J. Miller, George Kesidis","arXiv
IEEE Trans. Neural Networks Learn. Syst.","2020-08-19
2022","<a href=""arXiv (2020-08-19) : Detection of Backdoors in Trained Classifiers Without Access to the Training Set"" target=""_blank"">[http://arxiv.org/abs/1908.10498v3]</a>
<a href=""DBLP (2022) : Detection of Backdoors in Trained Classifiers Without Access to the Training Set"" target=""_blank"">[https://doi.org/10.1109/TNNLS.2020.3041202]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/TNNLS.2020.3041202]</a>","Recently, a special type of data poisoning (DP) attack targeting Deep Neural Network (DNN) classifiers, known as a backdoor, was proposed. These attacks do not seek to degrade classification accuracy, but rather to have the classifier learn to classify to a target class whenever the backdoor pattern is present in a test example. Launching backdoor attacks does not require knowledge of the classifier or its training process - it only needs the ability to poison the training set with (a sufficient number of) exemplars containing a sufficiently strong backdoor pattern (labeled with the target class). Here we address post-training detection of backdoor attacks in DNN image classifiers, seldom considered in existing works, wherein the defender does not have access to the poisoned training set, but only to the trained classifier itself, as well as to clean examples from the classification domain. This is an important scenario because a trained classifier may be the basis of e.g. a phone app that will be shared with many users. Detecting backdoors post-training may thus reveal a widespread attack. We propose a purely unsupervised anomaly detection (AD) defense against imperceptible backdoor attacks that: i) detects whether the trained DNN has been backdoor-attacked, ii) infers the source and target classes involved in a detected attack, iii) we even demonstrate it is possible to accurately estimate the backdoor pattern. We test our AD approach, in comparison with alternative defenses, for several backdoor patterns, data sets, and attack settings and demonstrate its favorability. Our defense essentially requires setting a single hyperparameter (the detection threshold), which can e.g. be chosen to fix the system's false positive rate.
","
","arXiv
DBLP"
Machine Learning Based Detection and Categorization of Anomalous Behavior in Enterprise Network Traffic,T. Teik-Toe Y. E. Jaddoo N. Y. Yen,2019 IEEE 14th International Conference on Intelligent Systems and Knowledge Engineering (ISKE),2020-08-18,"<a href=""IEEE (2020-08-18) : Machine Learning Based Detection and Categorization of Anomalous Behavior in Enterprise Network Traffic"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9170421]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ISKE47853.2019.9170421]</a>","Network anomaly detection is a dynamic research area and numerous methods have been proposed in literatures. In this study, we investigate both detecting and categorizing anomalies on top of detecting network events. Recent advances in machine learning techniques have demonstrated their efficiency in different areas including intrusion detection. In this paper, we first generate a new dataset which covers a good variety of attacks which are up to date such as DOS, Bruteforce, Backdoor & Infiltration, Injection, Cross Site scripting, Phishing and Probe. The dataset is labelled and contains a comprehensive set of around 80 features generated using a publicly available tool called “Flowmeter” which extracts and calculates features from the net-work captured files. Next, we analyze the generated dataset to select the best feature set to detect different attacks as well as evaluate our dataset through the execution of 4 common machine learning algorithms, namely decision tree, Naive Bayesian, Support Vector Machine and Multi-Layer Perceptron. Lastly, we investigate the feasibility of distinguishing between different attacks rather than just detecting anomalous traffic.",,IEEE
MCKC: a modified cyber kill chain model for cognitive APTs analysis within Enterprise multimedia network,"Ankang Ju, Yuanbo Guo, Tao Li",Multimedia Tools and Applications,2020-08-13,"<a href=""Springer (2020-08-13) : MCKC: a modified cyber kill chain model for cognitive APTs analysis within Enterprise multimedia network"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-020-09444-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-020-09444-x]</a>",The emerging cyber security threats pose many challenges to security analysts of enterprise multimedia environments when analysts attempting to...,,Springer
"Cloud of Things: architecture, applications and challenges","Fahd Alhaidari, Atta Rahman, Rachid Zagrouba",Journal of Ambient Intelligence and Humanized Computing,2020-08-10,"<a href=""Springer (2020-08-10) : Cloud of Things: architecture, applications and challenges"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-020-02448-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-020-02448-3]</a>","Nowadays, Cloud Computing and Internet of Things (IoT) are amongst the most emerging internet technologies playing a vital role in our daily lives....",,Springer
Removing Backdoor-Based Watermarks in Neural Networks with Limited Data,"Xuankai Liu, Fengting Li, Bihan Wen, Qi Li","arXiv
ICPR
arXiv","2020-08-08
2020
2020-08","<a href=""arXiv (2020-08-08) : Removing Backdoor-Based Watermarks in Neural Networks with Limited Data"" target=""_blank"">[http://arxiv.org/abs/2008.00407v2]</a>
<a href=""DBLP (2020) : Removing Backdoor-Based Watermarks in Neural Networks with Limited Data"" target=""_blank"">[https://doi.org/10.1109/ICPR48806.2021.9412684]</a>
<a href=""DBLP (2020-08) : Removing Backdoor-Based Watermarks in Neural Networks with Limited Data"" target=""_blank"">[https://arxiv.org/abs/2008.00407]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICPR48806.2021.9412684]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2008.00407]</a>","Deep neural networks have been widely applied and achieved great success in various fields. As training deep models usually consumes massive data and computational resources, trading the trained deep models is highly demanded and lucrative nowadays. Unfortunately, the naive trading schemes typically involves potential risks related to copyright and trustworthiness issues, e.g., a sold model can be illegally resold to others without further authorization to reap huge profits. To tackle this problem, various watermarking techniques are proposed to protect the model intellectual property, amongst which the backdoor-based watermarking is the most commonly-used one. However, the robustness of these watermarking approaches is not well evaluated under realistic settings, such as limited in-distribution data availability and agnostic of watermarking patterns. In this paper, we benchmark the robustness of watermarking, and propose a novel backdoor-based watermark removal framework using limited data, dubbed WILD. The proposed WILD removes the watermarks of deep models with only a small portion of training data, and the output model can perform the same as models trained from scratch without watermarks injected. In particular, a novel data augmentation method is utilized to mimic the behavior of watermark triggers. Combining with the distribution alignment between the normal and perturbed (e.g., occluded) data in the feature space, our approach generalizes well on all typical types of trigger contents. The experimental results demonstrate that our approach can effectively remove the watermarks without compromising the deep model performance for the original task with the limited access to training data.

","

","arXiv
DBLP
DBLP"
Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review,"Yansong Gao, Bao Gia Doan, Zhi Zhang, Siqi Ma, Jiliang Zhang, Anmin Fu, Surya Nepal, Hyoungshick Kim","arXiv
arXiv","2020-08-02
2020-07","<a href=""arXiv (2020-08-02) : Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review"" target=""_blank"">[http://arxiv.org/abs/2007.10760v3]</a>
<a href=""DBLP (2020-07) : Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review"" target=""_blank"">[https://arxiv.org/abs/2007.10760]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2007.10760]</a>","This work provides the community with a timely comprehensive review of backdoor attacks and countermeasures on deep learning. According to the attacker's capability and affected stage of the machine learning pipeline, the attack surfaces are recognized to be wide and then formalized into six categorizations: code poisoning, outsourcing, pretrained, data collection, collaborative learning and post-deployment. Accordingly, attacks under each categorization are combed. The countermeasures are categorized into four general classes: blind backdoor removal, offline backdoor inspection, online backdoor inspection, and post backdoor removal. Accordingly, we review countermeasures, and compare and analyze their advantages and disadvantages. We have also reviewed the flip side of backdoor attacks, which are explored for i) protecting intellectual property of deep learning models, ii) acting as a honeypot to catch adversarial example attacks, and iii) verifying data deletion requested by the data contributor.Overall, the research on defense is far behind the attack, and there is no single defense that can prevent all types of backdoor attacks. In some cases, an attacker can intelligently bypass existing defenses with an adaptive attack. Drawing the insights from the systematic review, we also present key areas for future research on the backdoor, such as empirical security evaluations from physical trigger attacks, and in particular, more efficient and practical countermeasures are solicited.
","
","arXiv
DBLP"
Defense Method of Ruby Code Injection Attack Based on Instruction Set Randomization,"Jiang Wang, Yuan Yao, Gaofei Zhang, Fangyun Li",ICCCM '20: Proceedings of the 8th International Conference on Computer and Communications Management,2020-08,"<a href=""ACM (2020-08) : Defense Method of Ruby Code Injection Attack Based on Instruction Set Randomization"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3411174.3411199]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3411174.3411199]</a>","Code injection attack is a major security threat to applications, especially web applications. This type of attack stems from the attacker's ability to use the vulnerability/backdoor of the application to inject a malicious program into the server and ...",,ACM
Noise-response Analysis for Rapid Detection of Backdoors in Deep Neural Networks,"N. Benjamin Erichson, Dane Taylor, Qixuan Wu, Michael W. Mahoney",arXiv,2020-08,"<a href=""DBLP (2020-08) : Noise-response Analysis for Rapid Detection of Backdoors in Deep Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2008.00123]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2008.00123]</a>",,,DBLP
LimonDroid: a system coupling three signature-based schemes for profiling Android malware,"Franklin Tchakounté, Roger Corneille Ndjeumou Ngassi, ... Kalum Priyanath Udagepola",Iran Journal of Computer Science,2020-07-30,"<a href=""Springer (2020-07-30) : LimonDroid: a system coupling three signature-based schemes for profiling Android malware"" target=""_blank"">[https://link.springer.com/article/10.1007/s42044-020-00068-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42044-020-00068-w]</a>",Android remains an interesting target to attackers due to its openness. A contribution in the literature consists of providing similarity measurement...,,Springer
Verifiable Edge Computing for Indoor Positioning,S. Liu Z. Yan,ICC 2020 - 2020 IEEE International Conference on Communications (ICC),2020-07-27,"<a href=""IEEE (2020-07-27) : Verifiable Edge Computing for Indoor Positioning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9148819]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICC40277.2020.9148819]</a>","Edge computing has been widely adopted in many systems, thanks for its advantages to offer low latency and alleviate heavy request loads from end users. Its integration with indoor positioning is one of promising research topics. Different from a traditional positioning system where a user normally query remotely deployed positioning services provided by a Location Information Service Provider (LIS), LIS will outsource its service to an edge device, and the user can obtain the service by directly accessing the edge device in an edge computing-based system. Though the benefits from edge computing, there is still some open issues for service outsourcing. One of them is how to ensure that the outsourced service is executed honestly by the edge device. However, the current literature has not yet seriously studied this issue with a feasible solution. In this paper, we design a verification scheme to solve this open problem for indoor positioning based on edge computing. By injecting some specially designed dataset into a trained machine learning based positioning model, the functionality of outsourced model on edge devices can be verified through this dataset with regard to its prediction accuracy from outsourced model. The verification is successful only when the prediction accuracy can pass a threshold. In experiments, we provide extensive empirical evidence using state-of-the-art positioning models based on real-world datasets to prove the effectiveness of our proposed scheme and meanwhile investigate the effects caused by different factors.",,IEEE
Exploit remote attack test in operating system using arduino micro,Wahanani H.E.,Journal of Physics: Conference Series,2020-07-23,"<a href=""ScienceDirect (2020-07-23) : Exploit remote attack test in operating system using arduino micro"" target=""_blank"">[https://doi.org/10.1088/1742-6596/1569/2/022038]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1088/1742-6596/1569/2/022038]</a>",,,ScienceDirect
A Study on Various Attacks and Detection Methodologies in Software Defined Networks,"Sukhvinder Singh, S. K. V. Jayakumar",Wireless Personal Communications,2020-07-22,"<a href=""Springer (2020-07-22) : A Study on Various Attacks and Detection Methodologies in Software Defined Networks"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-020-07387-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-020-07387-y]</a>",The Software Defined Networks (SDN) is widely used in many industrial and enterprise networking applications due to its flexibility and gaining...,,Springer
On Certifying Robustness against Backdoor Attacks via Randomized Smoothing,"Binghui Wang, Xiaoyu Cao, Jinyuan jia, Neil Zhenqiang Gong","arXiv
arXiv","2020-07-20
2020-02","<a href=""arXiv (2020-07-20) : On Certifying Robustness against Backdoor Attacks via Randomized Smoothing"" target=""_blank"">[http://arxiv.org/abs/2002.11750v4]</a>
<a href=""DBLP (2020-02) : On Certifying Robustness against Backdoor Attacks via Randomized Smoothing"" target=""_blank"">[https://arxiv.org/abs/2002.11750]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2002.11750]</a>","Backdoor attack is a severe security threat to deep neural networks (DNNs). We envision that, like adversarial examples, there will be a cat-and-mouse game for backdoor attacks, i.e., new empirical defenses are developed to defend against backdoor attacks but they are soon broken by strong adaptive backdoor attacks. To prevent such cat-and-mouse game, we take the first step towards certified defenses against backdoor attacks. Specifically, in this work, we study the feasibility and effectiveness of certifying robustness against backdoor attacks using a recent technique called randomized smoothing. Randomized smoothing was originally developed to certify robustness against adversarial examples. We generalize randomized smoothing to defend against backdoor attacks. Our results show the theoretical feasibility of using randomized smoothing to certify robustness against backdoor attacks. However, we also find that existing randomized smoothing methods have limited effectiveness at defending against backdoor attacks, which highlight the needs of new theory and methods to certify robustness against backdoor attacks.
","
","arXiv
DBLP"
Defense method of ruby code injection attack based on instruction set randomization,Wang J.,ACM International Conference Proceeding Series,2020-07-17,"<a href=""ScienceDirect (2020-07-17) : Defense method of ruby code injection attack based on instruction set randomization"" target=""_blank"">[https://doi.org/10.1145/3411174.3411199]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3411174.3411199]</a>",,,ScienceDirect
Causal inference and counterfactual prediction in machine learning for actionable healthcare,"Mattia Prosperi, Yi Guo, ... Jiang Bian",Nature Machine Intelligence,2020-07-13,"<a href=""Springer (2020-07-13) : Causal inference and counterfactual prediction in machine learning for actionable healthcare"" target=""_blank"">[https://link.springer.com/article/10.1038/s42256-020-0197-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1038/s42256-020-0197-y]</a>","Big data, high-performance computing, and (deep) machine learning are increasingly becoming key to precision medicine—from identifying disease risks...",,Springer
Backdoor attacks and defenses in feature-partitioned collaborative learning,"Yang Liu, Zhihao Yi, Tianjian Chen","arXiv
arXiv","2020-07-07
2020-07","<a href=""arXiv (2020-07-07) : Backdoor attacks and defenses in feature-partitioned collaborative learning"" target=""_blank"">[http://arxiv.org/abs/2007.03608v1]</a>
<a href=""DBLP (2020-07) : Backdoor attacks and defenses in feature-partitioned collaborative learning"" target=""_blank"">[https://arxiv.org/abs/2007.03608]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2007.03608]</a>","Since there are multiple parties in collaborative learning, malicious parties might manipulate the learning process for their own purposes through backdoor attacks. However, most of existing works only consider the federated learning scenario where data are partitioned by samples. The feature-partitioned learning can be another important scenario since in many real world applications, features are often distributed across different parties. Attacks and defenses in such scenario are especially challenging when the attackers have no labels and the defenders are not able to access the data and model parameters of other participants. In this paper, we show that even parties with no access to labels can successfully inject backdoor attacks, achieving high accuracy on both main and backdoor tasks. Next, we introduce several defense techniques, demonstrating that the backdoor can be successfully blocked by a combination of these techniques without hurting main task accuracy. To the best of our knowledge, this is the first systematical study to deal with backdoor attacks in the feature-partitioned collaborative learning framework.
","
","arXiv
DBLP"
A secure architecture for TCP/UDP-based cloud communications,"Abu Faisal, Mohammad Zulkernine",International Journal of Information Security,2020-07-07,"<a href=""Springer (2020-07-07) : A secure architecture for TCP/UDP-based cloud communications"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-020-00511-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-020-00511-w]</a>",Cloud communication is an intrinsic aspect of cloud architecture. It is an internet-based communication that enables access to millions of cloud...,,Springer
Non-classic disorder of adrenal steroidogenesis and clinical dilemmas in 21-hydroxylase deficiency combined with backdoor androgen pathway. Mini-review and case report,Sumińska M.,International Journal of Molecular Sciences,2020-07-01,"<a href=""ScienceDirect (2020-07-01) : Non-classic disorder of adrenal steroidogenesis and clinical dilemmas in 21-hydroxylase deficiency combined with backdoor androgen pathway. Mini-review and case report"" target=""_blank"">[https://doi.org/10.3390/ijms21134622]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/ijms21134622]</a>",,,ScienceDirect
Targeted Forgetting and False Memory Formation in Continual Learners through Adversarial Backdoor Attacks,Umer M.,Proceedings of the International Joint Conference on Neural Networks,2020-07-01,"<a href=""ScienceDirect (2020-07-01) : Targeted Forgetting and False Memory Formation in Continual Learners through Adversarial Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1109/IJCNN48605.2020.9206809]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/IJCNN48605.2020.9206809]</a>",,,ScienceDirect
The Defense Method for Code-Injection Attacks Based on Instruction Set Randomization,Ma B.,Journal of Cyber Security,2020-07-01,"<a href=""ScienceDirect (2020-07-01) : The Defense Method for Code-Injection Attacks Based on Instruction Set Randomization"" target=""_blank"">[https://doi.org/10.19363/J.cnki.cn10-1380/tn.2020.07.03]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.19363/J.cnki.cn10-1380/tn.2020.07.03]</a>",,,ScienceDirect
TrojDRL: Evaluation of backdoor attacks on deep reinforcement learning,Kiourti P.,Proceedings - Design Automation Conference,2020-07-01,"<a href=""ScienceDirect (2020-07-01) : TrojDRL: Evaluation of backdoor attacks on deep reinforcement learning"" target=""_blank"">[https://doi.org/10.1109/DAC18072.2020.9218663]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/DAC18072.2020.9218663]</a>",,,ScienceDirect
Detecting acoustic backdoor transmission of inaudible messages using deep learning,"Silvija Kokalj-Filipovic, Morriel Kasher, Michael Zhao, Predrag Spasojevic","WiseML '20: Proceedings of the 2nd ACM Workshop on Wireless Security and Machine Learning
WiseML@WiSec","2020-07
2020","<a href=""ACM (2020-07) : Detecting acoustic backdoor transmission of inaudible messages using deep learning"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3395352.3402629]</a>
<a href=""DBLP (2020) : Detecting acoustic backdoor transmission of inaudible messages using deep learning"" target=""_blank"">[https://doi.org/10.1145/3395352.3402629]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3395352.3402629]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3395352.3402629]</a>","The novel secret inaudible acoustic communication channel [11], referred to as the BackDoor channel, is a method of embedding inaudible signals in acoustic data that is likely to be processed by a trained deep neural net. In this paper we perform ...
","
","ACM
DBLP"
Defending Against Backdoors in Federated Learning with Robust Learning Rate,"Mustafa Safa Özdayi, Murat Kantarcioglu, Yulia R. Gel",arXiv,2020-07,"<a href=""DBLP (2020-07) : Defending Against Backdoors in Federated Learning with Robust Learning Rate"" target=""_blank"">[https://arxiv.org/abs/2007.03767]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2007.03767]</a>",,,DBLP
Combined software and hardware fault injection vulnerability detection,"Thomas Given-Wilson, Nisrine Jafri, Axel Legay",Innovations in Systems and Software Engineering,2020-06-27,"<a href=""Springer (2020-06-27) : Combined software and hardware fault injection vulnerability detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s11334-020-00364-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11334-020-00364-5]</a>",Fault injection is a well-known method to test the robustness and security vulnerabilities of software. Software-based and hardware-based approaches...,,Springer
A Causally Formulated Hazard Ratio Estimation through Backdoor Adjustment on Structural Causal Model,"Riddhiman Adib, Paul Griffin, Sheikh Iqbal Ahamed, Mohammad Adibuzzaman","arXiv
MLHC
arXiv","2020-06-22
2020
2020-06","<a href=""arXiv (2020-06-22) : A Causally Formulated Hazard Ratio Estimation through Backdoor Adjustment on Structural Causal Model"" target=""_blank"">[http://arxiv.org/abs/2006.12573v1]</a>
<a href=""DBLP (2020) : A Causally Formulated Hazard Ratio Estimation through Backdoor Adjustment on Structural Causal Model"" target=""_blank"">[http://proceedings.mlr.press/v126/adib20a.html]</a>
<a href=""DBLP (2020-06) : A Causally Formulated Hazard Ratio Estimation through Backdoor Adjustment on Structural Causal Model"" target=""_blank"">[https://arxiv.org/abs/2006.12573]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://proceedings.mlr.press/v126/adib20a.html]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2006.12573]</a>","Identifying causal relationships for a treatment intervention is a fundamental problem in health sciences. Randomized controlled trials (RCTs) are considered the gold standard for identifying causal relationships. However, recent advancements in the theory of causal inference based on the foundations of structural causal models (SCMs) have allowed the identification of causal relationships from observational data, under certain assumptions. Survival analysis provides standard measures, such as the hazard ratio, to quantify the effects of an intervention. While hazard ratios are widely used in clinical and epidemiological studies for RCTs, a principled approach does not exist to compute hazard ratios for observational studies with SCMs. In this work, we review existing approaches to compute hazard ratios as well as their causal interpretation, if it exists. We also propose a novel approach to compute hazard ratios from observational studies using backdoor adjustment through SCMs and do-calculus. Finally, we evaluate the approach using experimental data for Ewing's sarcoma.

","

","arXiv
DBLP
DBLP"
Improving accuracy of HPC-based malware classification for embedded platforms using gradient descent optimization,"Manaar Alam, Debdeep Mukhopadhyay, ... Thambipillai Srikanthan",Journal of Cryptographic Engineering,2020-06-22,"<a href=""Springer (2020-06-22) : Improving accuracy of HPC-based malware classification for embedded platforms using gradient descent optimization"" target=""_blank"">[https://link.springer.com/article/10.1007/s13389-020-00232-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13389-020-00232-9]</a>",Malware detection is still one of the difficult problems in computer security because of the daily occurrences of newer varieties of malware...,,Springer
FaceHack: Triggering backdoored facial recognition systems using facial characteristics,"Esha Sarkar, Hadjer Benkraouda, Michail Maniatakos","arXiv
arXiv","2020-06-20
2020-06","<a href=""arXiv (2020-06-20) : FaceHack: Triggering backdoored facial recognition systems using facial characteristics"" target=""_blank"">[http://arxiv.org/abs/2006.11623v1]</a>
<a href=""DBLP (2020-06) : FaceHack: Triggering backdoored facial recognition systems using facial characteristics"" target=""_blank"">[https://arxiv.org/abs/2006.11623]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2006.11623]</a>","Recent advances in Machine Learning (ML) have opened up new avenues for its extensive use in real-world applications. Facial recognition, specifically, is used from simple friend suggestions in social-media platforms to critical security applications for biometric validation in automated immigration at airports. Considering these scenarios, security vulnerabilities to such ML algorithms pose serious threats with severe outcomes. Recent work demonstrated that Deep Neural Networks (DNNs), typically used in facial recognition systems, are susceptible to backdoor attacks, in other words,the DNNs turn malicious in the presence of a unique trigger. Adhering to common characteristics for being unnoticeable, an ideal trigger is small, localized, and typically not a part of the main im-age. Therefore, detection mechanisms have focused on detecting these distinct trigger-based outliers statistically or through their reconstruction. In this work, we demonstrate that specific changes to facial characteristics may also be used to trigger malicious behavior in an ML model. The changes in the facial attributes maybe embedded artificially using social-media filters or introduced naturally using movements in facial muscles. By construction, our triggers are large, adaptive to the input, and spread over the entire image. We evaluate the success of the attack and validate that it does not interfere with the performance criteria of the model. We also substantiate the undetectability of our triggers by exhaustively testing them with state-of-the-art defenses.
","
","arXiv
DBLP"
Conceptualisation of Cyberattack prediction with deep learning,"Ayei E. Ibor, Florence A. Oladeji, ... Obeten O. Ekabua",Cybersecurity,2020-06-17,"<a href=""Springer (2020-06-17) : Conceptualisation of Cyberattack prediction with deep learning"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-020-00053-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-020-00053-7]</a>",The state of the cyberspace portends uncertainty for the future Internet and its accelerated number of users. New paradigms add more concerns with...,,Springer
A novel scalable intrusion detection system based on deep learning,"Soosan Naderi Mighan, Mohsen Kahani",International Journal of Information Security,2020-06-15,"<a href=""Springer (2020-06-15) : A novel scalable intrusion detection system based on deep learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-020-00508-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-020-00508-5]</a>",This paper successfully tackles the problem of processing a vast amount of security related data for the task of network intrusion detection. It...,,Springer
Poisoning the (Data) Well in ML-Based CAD: A Case Study of Hiding Lithographic Hotspots,K. Liu B. Tan R. Karri S. Garg,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",2020-06-15,"<a href=""IEEE (2020-06-15) : Poisoning the (Data) Well in ML-Based CAD: A Case Study of Hiding Lithographic Hotspots"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9116489]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.23919/DATE48585.2020.9116489]</a>","Machine learning (ML) provides state-of-the-art performance in many parts of computer-aided design (CAD) flows. However, deep neural networks (DNNs) are susceptible to various adversarial attacks, including data poisoning to compromise training to insert backdoors. Sensitivity to training data integrity presents a security vulnerability, especially in light of malicious insiders who want to cause targeted neural network misbehavior. In this study, we explore this threat in lithographic hotspot detection via training data poisoning, where hotspots in a layout clip can be ""hidden"" at inference time by including a trigger shape in the input. We show that training data poisoning attacks are feasible and stealthy, demonstrating a backdoored neural network that performs normally on clean inputs but misbehaves on inputs when a backdoor trigger is present. Furthermore, our results raise some fundamental questions about the robustness of ML-based systems in CAD.",,IEEE
A Class-Specific Intrusion Detection Model: Hierarchical Multi-class IDS Model,"Alper Sarıkaya, Banu Günel Kılıç",SN Computer Science,2020-06-12,"<a href=""Springer (2020-06-12) : A Class-Specific Intrusion Detection Model: Hierarchical Multi-class IDS Model"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-020-00213-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-020-00213-z]</a>","Nowadays, cyberattacks are occurring continuously. There are many kinds of attack types, which are malicious and harmful for our networks, resources...",,Springer
A practical key agreement scheme for videoconferencing,Cengiz Toğay,Multimedia Tools and Applications,2020-06-12,"<a href=""Springer (2020-06-12) : A practical key agreement scheme for videoconferencing"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-020-09136-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-020-09136-6]</a>","Recently, videoconferencing is becoming more and more pervasive as a consequence of new concerns about privacy and security. The media should be...",,Springer
Scalable Backdoor Detection in Neural Networks,"Haripriya Harikumar, Vuong Le, Santu Rana, Sourangshu Bhattacharya, Sunil Gupta, Svetha Venkatesh","arXiv
ECML/PKDD
arXiv","2020-06-10
2020
2020-06","<a href=""arXiv (2020-06-10) : Scalable Backdoor Detection in Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2006.05646v1]</a>
<a href=""DBLP (2020) : Scalable Backdoor Detection in Neural Networks"" target=""_blank"">[https://doi.org/10.1007/978-3-030-67661-2_18]</a>
<a href=""DBLP (2020-06) : Scalable Backdoor Detection in Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2006.05646]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-030-67661-2_18]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2006.05646]</a>","Recently, it has been shown that deep learning models are vulnerable to Trojan attacks, where an attacker can install a backdoor during training time to make the resultant model misidentify samples contaminated with a small trigger patch. Current backdoor detection methods fail to achieve good detection performance and are computationally expensive. In this paper, we propose a novel trigger reverse-engineering based approach whose computational complexity does not scale with the number of labels, and is based on a measure that is both interpretable and universal across different network and patch types. In experiments, we observe that our method achieves a perfect score in separating Trojaned models from pure models, which is an improvement over the current state-of-the art method.

","

","arXiv
DBLP
DBLP"
Cache-Zoomer: On-demand High-resolution Cache Monitoring for Security,"Hongyu Fang, Sai Santosh Dayapule, ... Guru Venkataramani",Journal of Hardware and Systems Security,2020-06-07,"<a href=""Springer (2020-06-07) : Cache-Zoomer: On-demand High-resolution Cache Monitoring for Security"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-020-00095-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-020-00095-w]</a>","Information leakage through timing channels is an increasing threat in most computer systems. Among various hardware components, the CPU caches...",,Springer
Bypassing Backdoor Detection Algorithms in Deep Learning,"Te Juin Lester Tan, Reza Shokri","arXiv
EuroS&amp,P
arXiv","2020-06-06
2020
2019-05","<a href=""arXiv (2020-06-06) : Bypassing Backdoor Detection Algorithms in Deep Learning"" target=""_blank"">[http://arxiv.org/abs/1905.13409v2]</a>
<a href=""DBLP (2020) : Bypassing Backdoor Detection Algorithms in Deep Learning"" target=""_blank"">[https://doi.org/10.1109/EuroSP48549.2020.00019]</a>
<a href=""DBLP (2019-05) : Bypassing Backdoor Detection Algorithms in Deep Learning"" target=""_blank"">[http://arxiv.org/abs/1905.13409]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/EuroSP48549.2020.00019]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1905.13409]</a>","Deep learning models are vulnerable to various adversarial manipulations of their training data, parameters, and input sample. In particular, an adversary can modify the training data and model parameters to embed backdoors into the model, so the model behaves according to the adversary's objective if the input contains the backdoor features, referred to as the backdoor trigger (e.g., a stamp on an image). The poisoned model's behavior on clean data, however, remains unchanged. Many detection algorithms are designed to detect backdoors on input samples or model parameters, through the statistical difference between the latent representations of adversarial and clean input samples in the poisoned model. In this paper, we design an adversarial backdoor embedding algorithm that can bypass the existing detection algorithms including the state-of-the-art techniques. We design an adaptive adversarial training algorithm that optimizes the original loss function of the model, and also maximizes the indistinguishability of the hidden representations of poisoned data and clean data. This work calls for designing adversary-aware defense mechanisms for backdoor detection.

","

","arXiv
DBLP
DBLP"
Hashing-based authentication for CAN bus and application to Denial-of-Service protection,O. CROS G. CHÊNEVERT,2019 3rd Cyber Security in Networking Conference (CSNet),2020-06-05,"<a href=""IEEE (2020-06-05) : Hashing-based authentication for CAN bus and application to Denial-of-Service protection"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9108978]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/CSNet47905.2019.9108978]</a>","The Controller Area Network (CAN) bus is an embedded network infrastructure especially used in automotive and aerospace contexts. It is well-known for its reliability but, due to historical and architectural reasons, authentication solutions were not integrated in the protocol itself. In particular, it does not integrate solutions of protection against external attackers. This can lead to data transitting through the network without any identity control and can quickly lead to backdoors allowing external attackers to take control of a vehicle, even while driving. Authentication solutions exist for CAN, but are often based on additional protocol layers or consume too much resources. The point of this paper is to propose an authentication solution in CAN based on a hashing mechanism that could be integrated in CAN standard protocol.",,IEEE
Sequence Triggered Hardware Trojan in Neural Network Accelerator,Z. Liu J. Ye X. Hu H. Li X. Li Y. Hu,2020 IEEE 38th VLSI Test Symposium (VTS),2020-06-04,"<a href=""IEEE (2020-06-04) : Sequence Triggered Hardware Trojan in Neural Network Accelerator"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9107582]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/VTS48691.2020.9107582]</a>","With the rapid development of deep learning techniques, the security issue for Neural Network (NN) systems has emerged as an urgent and severe problem. Hardware Trojan attack is one of the threatens, which provides attackers backdoors to control the prediction results of NN systems. This paper proposes a sequence triggered hardware Trojan. Normal images but with specific sequence are used to trigger the hardware Trojan and let attackers fully control the prediction results. This kind of trigger is not only robust to image pre-processing, but also unrecognizable by human beings. In comparison with existing hardware Trojan design, it is more practical and less hardware overhead. The experiments on MNIST, CIFAR100, and ISLVRC show that the proposed hardware Trojan is rarely triggered in normal working status while the hardware cost is reduced by 19X.",,IEEE
A Hybrid Nested Genetic-Fuzzy Algorithm Framework for Intrusion Detection and Attacks,R. Elhefnawy H. Abounaser A. Badr,IEEE Access,2020-06-03,"<a href=""IEEE (2020-06-03) : A Hybrid Nested Genetic-Fuzzy Algorithm Framework for Intrusion Detection and Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9097860]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2020.2996226]</a>","Intrusion Detection System (IDS) plays a very important role in security systems. Among its different types, Network Intrusion Detection System (NIDS) has an effective role in monitoring computer networks systems for malicious and illegal activities. In the literature, the detection of DoS and Probe attacks were with reasonable accuracy in most of the NIDS researches. However, the detection accuracy of other categories of attacks is still low, such as the R2L and U2R in KDDCUP99 dataset along with the Backdoors and Worms in UNSW-NB15 dataset. Computational Intelligence (CI) techniques have the characteristics to address such imprecision problem. In this research, a Hybrid Nested Genetic-Fuzzy Algorithm (HNGFA) framework has been developed to produce highly optimized outputs for security experts in classifying both major and minor categories of attacks. The adaptive model is evolved using two-nested Genetic-Fuzzy Algorithms (GFA). Each GFA consists of two-nested Genetic Algorithms (GA). The outer is to evolve fuzzy sets and the inner is to evolve fuzzy rules. The outer GFA assists the inner GFA in training phase, where the best individual in outer GFA interacts with the weak individual in inner GFA to generate new solutions that enhance the prediction of mutated attacks. Both GFA interact together to evolve the best rules for normal, major and minor categories of attacks through the optimization process. Several experiments have been conducted with different settings over different datasets. The obtained results show that the developed model has good accuracy and is more efficient compared with several state-of-the-art techniques.",,IEEE
Fuzzy logic and Fog based Secure Architecture for Internet of Things (FLFSIoT),"Syed Rameem Zahra, Mohammad Ahsan Chishti",Journal of Ambient Intelligence and Humanized Computing,2020-06-02,"<a href=""Springer (2020-06-02) : Fuzzy logic and Fog based Secure Architecture for Internet of Things (FLFSIoT)"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-020-02128-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-020-02128-2]</a>","Motivated by the recent explosion of interest around the Internet of Things (IoT), this paper is focused on the dynamics of its security. It is a...",,Springer
Key schedule against template attack-based simple power analysis on a single target,Won Y.S.,Applied Sciences (Switzerland),2020-06-01,"<a href=""ScienceDirect (2020-06-01) : Key schedule against template attack-based simple power analysis on a single target"" target=""_blank"">[https://doi.org/10.3390/app10113804]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/app10113804]</a>",,,ScienceDirect
On the Security of LWE Cryptosystem against Subversion Attacks,Z. Yang R. Chen C. Li L. Qu G. Yang,The Computer Journal,2020-06-01,"<a href=""IEEE (2020-06-01) : On the Security of LWE Cryptosystem against Subversion Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9104638]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1093/comjnl/bxz084]</a>","Subversion of cryptography has received wide attentions especially after the Snowden Revelations in 2013. Most of the currently proposed subversion attacks essentially rely on the freedom of randomness choosing in the cryptographic protocol to hide backdoors embedded in the cryptosystems. Despite the fact that significant progresses in this line of research have been made, most of them mainly considered the classical setting, while the research gap regarding subversion attacks against post-quantum cryptography remains tremendous. Inspired by this observation, we investigate a subversion attack against existing protocol that is proved post-quantum secure. Particularly, we show an efficient way to undetectably subvert the well-known lattice-based encryption scheme proposed by Regev (STOC 2005). Our subversion enables the subverted algorithm to stealthily leak arbitrary messages to the outsider who knows the backdoor. Through theoretical analysis and experimental observations, we demonstrate that the subversion attack against the LWE encryption scheme is feasible and practical.",,IEEE
Similarity Based Binary Backdoor Detection via Attributed Control Flow Graph,Zhang T.,"Proceedings of 2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference, ITNEC 2020",2020-06-01,"<a href=""ScienceDirect (2020-06-01) : Similarity Based Binary Backdoor Detection via Attributed Control Flow Graph"" target=""_blank"">[https://doi.org/10.1109/ITNEC48623.2020.9085069]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ITNEC48623.2020.9085069]</a>",,,ScienceDirect
Systematic evaluation of backdoor data poisoning attacks on image classifiers,Truong L.,IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops,2020-06-01,"<a href=""ScienceDirect (2020-06-01) : Systematic evaluation of backdoor data poisoning attacks on image classifiers"" target=""_blank"">[https://doi.org/10.1109/CVPRW50498.2020.00402]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CVPRW50498.2020.00402]</a>",,,ScienceDirect
A new measure for overfitting and its implications for backdooring of deep learning,"Kathrin Grosse, Taesung Lee, Youngja Park, Michael Backes, Ian M. Molloy",arXiv,2020-06,"<a href=""DBLP (2020-06) : A new measure for overfitting and its implications for backdooring of deep learning"" target=""_blank"">[https://arxiv.org/abs/2006.06721]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2006.06721]</a>",,,DBLP
Backdoor Attacks on Facial Recognition in the Physical World,"Emily Wenger, Josephine Passananti, Yuanshun Yao, Haitao Zheng, Ben Y. Zhao",arXiv,2020-06,"<a href=""DBLP (2020-06) : Backdoor Attacks on Facial Recognition in the Physical World"" target=""_blank"">[https://arxiv.org/abs/2006.14580]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2006.14580]</a>",,,DBLP
BadNL: Backdoor Attacks Against NLP Models,"Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, Yang Zhang",arXiv,2020-06,"<a href=""DBLP (2020-06) : BadNL: Backdoor Attacks Against NLP Models"" target=""_blank"">[https://arxiv.org/abs/2006.01043]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2006.01043]</a>",,,DBLP
Information Hiding in Industrial Control Systems: An OPC UA based Supply Chain Attack and its Detection,"Mario Hildebrandt, Kevin Lamshöft, Jana Dittmann, Tom Neubert, Claus Vielhauer",IH&MMSec '20: Proceedings of the 2020 ACM Workshop on Information Hiding and Multimedia Security,2020-06,"<a href=""ACM (2020-06) : Information Hiding in Industrial Control Systems: An OPC UA based Supply Chain Attack and its Detection"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3369412.3395068]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3369412.3395068]</a>",Industrial Control Systems (ICS) help to automate various cyber-physical systems in our world. The controlled processes range from rather simple traffic lights and elevators to complex networks of ICS in car manufacturing or controlling nuclear power ...,,ACM
Embedding backdoors as the facial features: Invisible backdoor attacks against face recognition systems,He C.,ACM International Conference Proceeding Series,2020-05-22,"<a href=""ScienceDirect (2020-05-22) : Embedding backdoors as the facial features: Invisible backdoor attacks against face recognition systems"" target=""_blank"">[https://doi.org/10.1145/3393527.3393567]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3393527.3393567]</a>",,,ScienceDirect
Circuit implementation of 3D chaotic self-exciting single-disk homopolar dynamo and its application in digital image confidentiality,"Noor Munir, Majid Khan, ... Iqtadar Hussain",Wireless Networks,2020-05-18,"<a href=""Springer (2020-05-18) : Circuit implementation of 3D chaotic self-exciting single-disk homopolar dynamo and its application in digital image confidentiality"" target=""_blank"">[https://link.springer.com/article/10.1007/s11276-020-02361-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11276-020-02361-9]</a>",Confidentiality of secret information is one of the mandatory issue of digitally modernized era of science and technology. Due to availability of...,,Springer
Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs,"Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, Heiko Hoffmann","arXiv
CVPR
arXiv","2020-05-15
2020
2019-06","<a href=""arXiv (2020-05-15) : Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs"" target=""_blank"">[http://arxiv.org/abs/1906.10842v2]</a>
<a href=""DBLP (2020) : Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs"" target=""_blank"">[https://openaccess.thecvf.com/content_CVPR_2020/html/Kolouri_Universal_Litmus_Patterns_Revealing_Backdoor_Attacks_in_CNNs_CVPR_2020_paper.html]</a>
<a href=""DBLP (2019-06) : Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs"" target=""_blank"">[http://arxiv.org/abs/1906.10842]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://openaccess.thecvf.com/content_CVPR_2020/html/Kolouri_Universal_Litmus_Patterns_Revealing_Backdoor_Attacks_in_CNNs_CVPR_2020_paper.html]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1906.10842]</a>","The unprecedented success of deep neural networks in many applications has made these networks a prime target for adversarial exploitation. In this paper, we introduce a benchmark technique for detecting backdoor attacks (aka Trojan attacks) on deep convolutional neural networks (CNNs). We introduce the concept of Universal Litmus Patterns (ULPs), which enable one to reveal backdoor attacks by feeding these universal patterns to the network and analyzing the output (i.e., classifying the network as `clean' or `corrupted'). This detection is fast because it requires only a few forward passes through a CNN. We demonstrate the effectiveness of ULPs for detecting backdoor attacks on thousands of networks with different architectures trained on four benchmark datasets, namely the German Traffic Sign Recognition Benchmark (GTSRB), MNIST, CIFAR10, and Tiny-ImageNet. The codes and train/test models for this paper can be found here https://umbcvision.github.io/Universal-Litmus-Patterns/.

","<a href=""arXiv"" target=""_blank"">[https://umbcvision.github.io/Universal-Litmus-Patterns/]</a>

","arXiv
DBLP
DBLP"
Deep nonlinear regression least squares polynomial fit to detect malicious attack on IoT devices,E. Arul,Journal of Ambient Intelligence and Humanized Computing,2020-05-14,"<a href=""Springer (2020-05-14) : Deep nonlinear regression least squares polynomial fit to detect malicious attack on IoT devices"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-020-02075-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-020-02075-y]</a>",The explosion of IoT gadgets which be able to more effortlessly conceded than PCs has prompted an expansion in the existence of IoT-dependent botnet...,,Springer
Cyber security in New Space,"M. Manulis, C. P. Bridges, ... A. Davis",International Journal of Information Security,2020-05-12,"<a href=""Springer (2020-05-12) : Cyber security in New Space"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-020-00503-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-020-00503-w]</a>","Developments in technologies, attitudes and investment are transforming the space environment, achieving greater accessibility for an increasing...",,Springer
A Comprehensive Study on Critical Security Issues and Challenges of the IoT World,"Shivani Panchiwala, Manan Shah","Journal of Data, Information and Management",2020-05-06,"<a href=""Springer (2020-05-06) : A Comprehensive Study on Critical Security Issues and Challenges of the IoT World"" target=""_blank"">[https://link.springer.com/article/10.1007/s42488-020-00030-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42488-020-00030-2]</a>","The Internet of Things (IoT) expected for infinite connectivity among various elements or “things”. It converges with interpersonal organizations,...",,Springer
A statistical class center based triangle area vector method for detection of denial of service attacks,"N. G. Bhuvaneswari Amma, S. Selvakumar",Cluster Computing,2020-05-06,"<a href=""Springer (2020-05-06) : A statistical class center based triangle area vector method for detection of denial of service attacks"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-020-03120-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-020-03120-3]</a>",Denial of service (DoS) attack is the menace to private cloud computing environment that denies services provided by cloud servers leading to huge...,,Springer
An intrusion detection approach based on improved deep belief network,"Qiuting Tian, Dezhi Han, ... Arcangelo Castiglione",Applied Intelligence,2020-05-06,"<a href=""Springer (2020-05-06) : An intrusion detection approach based on improved deep belief network"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-020-01694-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-020-01694-4]</a>","In today’s interconnected society, cyberattacks have become more frequent and sophisticated, and existing intrusion detection systems may not be...",,Springer
Automatic extraction of named entities of cyber threats using a deep Bi-LSTM-CRF network,"Gyeongmin Kim, Chanhee Lee, ... Heuiseok Lim",International Journal of Machine Learning and Cybernetics,2020-05-02,"<a href=""Springer (2020-05-02) : Automatic extraction of named entities of cyber threats using a deep Bi-LSTM-CRF network"" target=""_blank"">[https://link.springer.com/article/10.1007/s13042-020-01122-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13042-020-01122-6]</a>",Countless cyber threat intelligence (CTI) reports are used by companies around the world on a daily basis for security reasons. To secure critical...,,Springer
Backdooring and poisoning neural networks with image-scaling attacks,Quiring E.,"Proceedings - 2020 IEEE Symposium on Security and Privacy Workshops, SPW 2020",2020-05-01,"<a href=""ScienceDirect (2020-05-01) : Backdooring and poisoning neural networks with image-scaling attacks"" target=""_blank"">[https://doi.org/10.1109/SPW50608.2020.00024]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SPW50608.2020.00024]</a>",,,ScienceDirect
"CYP17A1 exhibits 17αhydroxylase/17,20-lyase activity towards 11β-hydroxyprogesterone and 11-ketoprogesterone metabolites in the C11-oxy backdoor pathway",van Rooyen D.,Journal of Steroid Biochemistry and Molecular Biology,2020-05-01,"<a href=""ScienceDirect (2020-05-01) : CYP17A1 exhibits 17αhydroxylase/17,20-lyase activity towards 11β-hydroxyprogesterone and 11-ketoprogesterone metabolites in the C11-oxy backdoor pathway"" target=""_blank"">[https://doi.org/10.1016/j.jsbmb.2020.105614]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.jsbmb.2020.105614]</a>",,,ScienceDirect
"Revealing backdoors, post-training, in DNN classifiers via novel inference on optimized perturbations inducing group misclassification",Xiang Z.,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2020-05-01,"<a href=""ScienceDirect (2020-05-01) : Revealing backdoors, post-training, in DNN classifiers via novel inference on optimized perturbations inducing group misclassification"" target=""_blank"">[https://doi.org/10.1109/ICASSP40776.2020.9054581]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICASSP40776.2020.9054581]</a>",,,ScienceDirect
A Philosophy of Security Architecture Design,Geir M. Køien,Wireless Personal Communications,2020-04-30,"<a href=""Springer (2020-04-30) : A Philosophy of Security Architecture Design"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-020-07310-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-020-07310-5]</a>","Digital systems are almost always vulnerable, yet we increasingly depend on these systems. There will be many threats towards these system. In a...",,Springer
DP-FL: a novel differentially private federated learning framework for the unbalanced data,"Xixi Huang, Ye Ding, ... Qing Liao",World Wide Web,2020-04-30,"<a href=""Springer (2020-04-30) : DP-FL: a novel differentially private federated learning framework for the unbalanced data"" target=""_blank"">[https://link.springer.com/article/10.1007/s11280-020-00780-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11280-020-00780-4]</a>","Security issues of artificial intelligence attract many attention in many research fields and industries, such as face recognition, medical care, and...",,Springer
"Machine Learning Security: Threats, Countermeasures, and Evaluations",M. Xue C. Yuan H. Wu Y. Zhang W. Liu,IEEE Access,2020-04-29,"<a href=""IEEE (2020-04-29) : Machine Learning Security: Threats, Countermeasures, and Evaluations"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9064510]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2020.2987435]</a>","Machine learning has been pervasively used in a wide range of applications due to its technical breakthroughs in recent years. It has demonstrated significant success in dealing with various complex problems, and shows capabilities close to humans or even beyond humans. However, recent studies show that machine learning models are vulnerable to various attacks, which will compromise the security of the models themselves and the application systems. Moreover, such attacks are stealthy due to the unexplained nature of the deep learning models. In this survey, we systematically analyze the security issues of machine learning, focusing on existing attacks on machine learning systems, corresponding defenses or secure learning techniques, and security evaluation methods. Instead of focusing on one stage or one type of attack, this paper covers all the aspects of machine learning security from the training phase to the test phase. First, the machine learning model in the presence of adversaries is presented, and the reasons why machine learning can be attacked are analyzed. Then, the machine learning security-related issues are classified into five categories: training set poisoning backdoors in the training set adversarial example attacks model theft recovery of sensitive training data. The threat models, attack approaches, and defense techniques are analyzed systematically. To demonstrate that these threats are real concerns in the physical world, we also reviewed the attacks in real-world conditions. Several suggestions on security evaluations of machine learning systems are also provided. Last, future directions for machine learning security are also presented.",,IEEE
Bias Busters: Robustifying DL-based Lithographic Hotspot Detectors Against Backdooring Attacks,"Kang Liu, Benjamin Tan, Gaurav Rajavendra Reddy, Siddharth Garg, Yiorgos Makris, Ramesh Karri","arXiv
arXiv","2020-04-26
2020-04","<a href=""arXiv (2020-04-26) : Bias Busters: Robustifying DL-based Lithographic Hotspot Detectors Against Backdooring Attacks"" target=""_blank"">[http://arxiv.org/abs/2004.12492v1]</a>
<a href=""DBLP (2020-04) : Bias Busters: Robustifying DL-based Lithographic Hotspot Detectors Against Backdooring Attacks"" target=""_blank"">[https://arxiv.org/abs/2004.12492]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2004.12492]</a>","Deep learning (DL) offers potential improvements throughout the CAD tool-flow, one promising application being lithographic hotspot detection. However, DL techniques have been shown to be especially vulnerable to inference and training time adversarial attacks. Recent work has demonstrated that a small fraction of malicious physical designers can stealthily ""backdoor"" a DL-based hotspot detector during its training phase such that it accurately classifies regular layout clips but predicts hotspots containing a specially crafted trigger shape as non-hotspots. We propose a novel training data augmentation strategy as a powerful defense against such backdooring attacks. The defense works by eliminating the intentional biases introduced in the training data but does not require knowledge of which training samples are poisoned or the nature of the backdoor trigger. Our results show that the defense can drastically reduce the attack success rate from 84% to ~0%.
","
","arXiv
DBLP"
Systematic Evaluation of Backdoor Data Poisoning Attacks on Image Classifiers,"Loc Truong, Chace Jones, Brian Hutchinson, Andrew August, Brenda Praggastis, Robert Jasper, Nicole Nichols, Aaron Tuor","arXiv
CVPR Workshops
arXiv","2020-04-24
2020
2020-04","<a href=""arXiv (2020-04-24) : Systematic Evaluation of Backdoor Data Poisoning Attacks on Image Classifiers"" target=""_blank"">[http://arxiv.org/abs/2004.11514v1]</a>
<a href=""DBLP (2020) : Systematic Evaluation of Backdoor Data Poisoning Attacks on Image Classifiers"" target=""_blank"">[https://openaccess.thecvf.com/content_CVPRW_2020/html/w47/Truong_Systematic_Evaluation_of_Backdoor_Data_Poisoning_Attacks_on_Image_Classifiers_CVPRW_2020_paper.html]</a>
<a href=""DBLP (2020-04) : Systematic Evaluation of Backdoor Data Poisoning Attacks on Image Classifiers"" target=""_blank"">[https://arxiv.org/abs/2004.11514]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://openaccess.thecvf.com/content_CVPRW_2020/html/w47/Truong_Systematic_Evaluation_of_Backdoor_Data_Poisoning_Attacks_on_Image_Classifiers_CVPRW_2020_paper.html]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2004.11514]</a>","Backdoor data poisoning attacks have recently been demonstrated in computer vision research as a potential safety risk for machine learning (ML) systems. Traditional data poisoning attacks manipulate training data to induce unreliability of an ML model, whereas backdoor data poisoning attacks maintain system performance unless the ML model is presented with an input containing an embedded ""trigger"" that provides a predetermined response advantageous to the adversary. Our work builds upon prior backdoor data-poisoning research for ML image classifiers and systematically assesses different experimental conditions including types of trigger patterns, persistence of trigger patterns during retraining, poisoning strategies, architectures (ResNet-50, NasNet, NasNet-Mobile), datasets (Flowers, CIFAR-10), and potential defensive regularization techniques (Contrastive Loss, Logit Squeezing, Manifold Mixup, Soft-Nearest-Neighbors Loss). Experiments yield four key findings. First, the success rate of backdoor poisoning attacks varies widely, depending on several factors, including model architecture, trigger pattern and regularization technique. Second, we find that poisoned models are hard to detect through performance inspection alone. Third, regularization typically reduces backdoor success rate, although it can have no effect or even slightly increase it, depending on the form of regularization. Finally, backdoors inserted through data poisoning can be rendered ineffective after just a few epochs of additional training on a small set of clean data without affecting the model's performance.

","

","arXiv
DBLP
DBLP"
Reverse Engineering and Backdooring Router Firmwares,A. Adithyan K. Nagendran R. Chethana G. Pandy D. G. Prashanth K.,2020 6th International Conference on Advanced Computing and Communication Systems (ICACCS),2020-04-23,"<a href=""IEEE (2020-04-23) : Reverse Engineering and Backdooring Router Firmwares"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9074317]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICACCS48705.2020.9074317]</a>","Recently, there has been a dramatic increase in cyber attacks around the globe. Hundreds of 0day vulnerabilities on different platforms are discovered by security researchers worldwide. The attack vectors are becoming more and more difficult to be discovered by any anti threat detection engine. Inorder to bypass these smart detection mechanisms, attackers now started carrying out attacks at extremely low level where no threat inspection units are present. This makes the attack more stealthy with increased success rate and almost zero detection rate. A best case example for this scenario would be attacks like Meltdown and Spectre that targeted the modern processors to steal information by exploiting out-of-order execution feature in modern processors. These types of attacks are incredibly hard to detect and patch. Even if a patch is released, a wide range of normal audience are unaware of this both the vulnerability and the patch. This paper describes one such low level attacks that involves the process of reverse engineering firmwares and manually backdooring them with several linux utilities. Also, compromising a real world WiFi router with the manually backdoored firmware and attaining reverse shell from the router is discussed. The WiFi routers are almost everywhere especially in public places. Firmwares are responsible for controlling the routers. If the attacker manipulates the firmware and gains control over the firmware installed in the router, then the attacker can get a hold of the network and perform various MITM attacks inside the network with the help of the router.",,IEEE
Neural Network Laundering: Removing Black-Box Backdoor Watermarks from Deep Neural Networks,"William Aiken, Hyoungshick Kim, Simon Woo","arXiv
arXiv","2020-04-22
2020-04","<a href=""arXiv (2020-04-22) : Neural Network Laundering: Removing Black-Box Backdoor Watermarks from Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2004.11368v1]</a>
<a href=""DBLP (2020-04) : Neural Network Laundering: Removing Black-Box Backdoor Watermarks from Deep Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2004.11368]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2004.11368]</a>","Creating a state-of-the-art deep-learning system requires vast amounts of data, expertise, and hardware, yet research into embedding copyright protection for neural networks has been limited. One of the main methods for achieving such protection involves relying on the susceptibility of neural networks to backdoor attacks, but the robustness of these tactics has been primarily evaluated against pruning, fine-tuning, and model inversion attacks. In this work, we propose a neural network ""laundering"" algorithm to remove black-box backdoor watermarks from neural networks even when the adversary has no prior knowledge of the structure of the watermark. We are able to effectively remove watermarks used for recent defense or copyright protection mechanisms while achieving test accuracies above 97% and 80% for both MNIST and CIFAR-10, respectively. For all backdoor watermarking methods addressed in this paper, we find that the robustness of the watermark is significantly weaker than the original claims. We also demonstrate the feasibility of our algorithm in more complex tasks as well as in more realistic scenarios where the adversary is able to carry out efficient laundering attacks using less than 1% of the original training set size, demonstrating that existing backdoor watermarks are not sufficient to reach their claims.
","
","arXiv
DBLP"
Review and insight on the behavioral aspects of cybersecurity,"Rachid Ait Maalem Lahcen, Bruce Caulkins, ... Manish Kumar",Cybersecurity,2020-04-21,"<a href=""Springer (2020-04-21) : Review and insight on the behavioral aspects of cybersecurity"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-020-00050-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-020-00050-w]</a>",Stories of cyber attacks are becoming a routine in which cyber attackers show new levels of intention by sophisticated attacks on networks....,,Springer
A Framework for Mimic Defense System in Cyberspace,"Guangsong Li, Wei Wang, ... Xueming Si",Journal of Signal Processing Systems,2020-04-15,"<a href=""Springer (2020-04-15) : A Framework for Mimic Defense System in Cyberspace"" target=""_blank"">[https://link.springer.com/article/10.1007/s11265-019-01473-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11265-019-01473-6]</a>",The long-term existence of various vulnerabilities and backdoors in software and hardware makes security threats of the cyberspace more and more...,,Springer
"Revealing Perceptible Backdoors, without the Training Set, via the Maximum Achievable Misclassification Fraction Statistic","Zhen Xiang, David J. Miller, Hang Wang, George Kesidis","arXiv
MLSP
arXiv","2020-04-06
2020
2019-11","<a href=""arXiv (2020-04-06) : Revealing Perceptible Backdoors, without the Training Set, via the Maximum Achievable Misclassification Fraction Statistic"" target=""_blank"">[http://arxiv.org/abs/1911.07970v2]</a>
<a href=""DBLP (2020) : Revealing Perceptible Backdoors in DNNs, Without the Training Set, via the Maximum Achievable Misclassification Fraction Statistic"" target=""_blank"">[https://doi.org/10.1109/MLSP49062.2020.9231861]</a>
<a href=""DBLP (2019-11) : Revealing Perceptible Backdoors, without the Training Set, via the Maximum Achievable Misclassification Fraction Statistic"" target=""_blank"">[http://arxiv.org/abs/1911.07970]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/MLSP49062.2020.9231861]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1911.07970]</a>","Recently, a backdoor data poisoning attack was proposed, which adds mislabeled examples to the training set, with an embedded backdoor pattern, aiming to have the classifier learn to classify to a target class whenever the backdoor pattern is present in a test sample. Here, we address post-training detection of innocuous perceptible backdoors in DNN image classifiers, wherein the defender does not have access to the poisoned training set, but only to the trained classifier, as well as unpoisoned examples. This problem is challenging because without the poisoned training set, we have no hint about the actual backdoor pattern used during training. This post-training scenario is also of great import because in many practical contexts the DNN user did not train the DNN and does not have access to the training data. We identify two important properties of perceptible backdoor patterns - spatial invariance and robustness - based upon which we propose a novel detector using the maximum achievable misclassification fraction (MAMF) statistic. We detect whether the trained DNN has been backdoor-attacked and infer the source and target classes. Our detector outperforms other existing detectors and, coupled with an imperceptible backdoor detector, helps achieve post-training detection of all evasive backdoors.

","

","arXiv
DBLP
DBLP"
Backdoor suppression in neural networks using input fuzzing and majority voting,Sarkar E.,IEEE Design and Test,2020-04-01,"<a href=""ScienceDirect (2020-04-01) : Backdoor suppression in neural networks using input fuzzing and majority voting"" target=""_blank"">[https://doi.org/10.1109/MDAT.2020.2968275]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/MDAT.2020.2968275]</a>",,,ScienceDirect
Helicopter Backdoor Drag Reduction Technology Based on Non-smooth Surface,Wang Z.,Nanjing Hangkong Hangtian Daxue Xuebao/Journal of Nanjing University of Aeronautics and Astronautics,2020-04-01,"<a href=""ScienceDirect (2020-04-01) : Helicopter Backdoor Drag Reduction Technology Based on Non-smooth Surface"" target=""_blank"">[https://doi.org/10.16356/j.1005-2615.2020.02.009]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.16356/j.1005-2615.2020.02.009]</a>",,,ScienceDirect
Preacher: Network Policy Checker for Adversarial Environments,K. Thimmaraju L. Schiff S. Schmid,"2019 38th Symposium on Reliable Distributed Systems (SRDS)
IEEE/ACM Transactions on Networking","2020-03-30
2021-10-14","<a href=""IEEE (2020-03-30) : Preacher: Network Policy Checker for Adversarial Environments"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9049554]</a>
<a href=""IEEE (2021-10-14) : Preacher: Network Policy Checker for Adversarial Environments"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9432841]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SRDS47363.2019.00014]</a>
<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TNET.2021.3078143]</a>","Private networks are typically assumed to be trusted as security mechanisms are usually deployed on hosts and the data plane is managed in-house. The increasing number of attacks on network devices, and recent reports on backdoors, forces us to revisit existing security assumptions and demands new approaches to detect malicious activity. This paper presents Preacher, a runtime network policy checker, which leverages a secure, redundant and adaptive sample distribution scheme that allows us to provably detect adversarial switches or routers trying to reroute, mirror, drop, inject, or modify packets (i.e., header and/or payload) even under collusion. Additionally, the analysis performed by Preacher is highly parallelizable. We show that emerging programmable networks provide an ideal vehicle to detect suspicious network activity. Furthermore, we analytically and empirically evaluate the effectiveness of our approach in different adversarial settings, report on a proof-of-concept implementation using ONOS, and provide insights into the resource and performance overheads of Preacher.
Private networks are typically assumed to be trusted as security mechanisms are usually deployed on hosts and the data plane is managed in-house. The increasing number of attacks on network devices, and recent reports on backdoors, forces us to revisit existing security assumptions and demands new approaches to detect malicious activity. This paper presents Preacher, a runtime network policy checker, which leverages a secure, redundant and adaptive sample distribution scheme that allows us to provably detect and localize adversarial switches or routers trying to reroute, mirror, drop, inject, or modify packets (i.e., header and/or payload) even under collusion. The analysis performed by Preacher is highly parallelizable. We show that emerging programmable networks provide an ideal vehicle to detect suspicious network activity. Furthermore, we analytically and empirically evaluate the effectiveness of our approach in different adversarial settings, report on a proof-of-concept implementation using ONOS, and provide insights into the resource and performance overheads of Preacher.","
","IEEE
IEEE"
Key-Dependent S-box Scheme for Enhancing the Security of Block Ciphers,A. Alasaad A. Alghafis,2019 2nd International Conference on Signal Processing and Information Security (ICSPIS),2020-03-26,"<a href=""IEEE (2020-03-26) : Key-Dependent S-box Scheme for Enhancing the Security of Block Ciphers"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9045900]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICSPIS48135.2019.9045900]</a>","The Advanced Encryption Standard (AES) is the current de facto standard block cipher for extremely confidential information. It is used by many large agencies, and popular for its computational efficiency in both software and hardware. However, the standard (Rijndael) S-box used of AES is a static and fixed matrix. Thus, a backdoor can be built into the cipher to exploit this feature. In this paper, we argue for a very simple key dependent S-box scheme which generates a dynamic S-box for each round of encryption. Our scheme uses some bits of the primary key (or expanded key) to directly manipulate the standard S-box in such a way that its content is changed but its cryptographic properties are preserved. We show using experimental tests that our proposed scheme strengthens the cipher against certain attacks, at the expense of a relatively modest one-time computational procedure during the set-up phase.",,IEEE
Under false flag: using technical artifacts for cyber attack attribution,"Florian Skopik, Timea Pahi",Cybersecurity,2020-03-20,"<a href=""Springer (2020-03-20) : Under false flag: using technical artifacts for cyber attack attribution"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-020-00048-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-020-00048-4]</a>",The attribution of cyber attacks is often neglected. The consensus still is that little can be done to prosecute the perpetrators – and...,,Springer
Backdooring and Poisoning Neural Networks with Image-Scaling Attacks,"Erwin Quiring, Konrad Rieck","arXiv
SP
arXiv","2020-03-19
2020
2020-03","<a href=""arXiv (2020-03-19) : Backdooring and Poisoning Neural Networks with Image-Scaling Attacks"" target=""_blank"">[http://arxiv.org/abs/2003.08633v1]</a>
<a href=""DBLP (2020) : Backdooring and Poisoning Neural Networks with Image-Scaling Attacks"" target=""_blank"">[https://doi.org/10.1109/SPW50608.2020.00024]</a>
<a href=""DBLP (2020-03) : Backdooring and Poisoning Neural Networks with Image-Scaling Attacks"" target=""_blank"">[https://arxiv.org/abs/2003.08633]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SPW50608.2020.00024]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2003.08633]</a>","Backdoors and poisoning attacks are a major threat to the security of machine-learning and vision systems. Often, however, these attacks leave visible artifacts in the images that can be visually detected and weaken the efficacy of the attacks. In this paper, we propose a novel strategy for hiding backdoor and poisoning attacks. Our approach builds on a recent class of attacks against image scaling. These attacks enable manipulating images such that they change their content when scaled to a specific resolution. By combining poisoning and image-scaling attacks, we can conceal the trigger of backdoors as well as hide the overlays of clean-label poisoning. Furthermore, we consider the detection of image-scaling attacks and derive an adaptive attack. In an empirical evaluation, we demonstrate the effectiveness of our strategy. First, we show that backdoors and poisoning work equally well when combined with image-scaling attacks. Second, we demonstrate that current detection defenses against image-scaling attacks are insufficient to uncover our manipulations. Overall, our work provides a novel means for hiding traces of manipulations, being applicable to different poisoning approaches.

","

","arXiv
DBLP
DBLP"
Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation,Zhong H.,CODASPY 2020 - Proceedings of the 10th ACM Conference on Data and Application Security and Privacy,2020-03-16,"<a href=""ScienceDirect (2020-03-16) : Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation"" target=""_blank"">[https://doi.org/10.1145/3374664.3375751]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3374664.3375751]</a>",,,ScienceDirect
Poisoning Attacks in Federated Learning: An Evaluation on Traffic Sign Classification,Nuding F.,CODASPY 2020 - Proceedings of the 10th ACM Conference on Data and Application Security and Privacy,2020-03-16,"<a href=""ScienceDirect (2020-03-16) : Poisoning Attacks in Federated Learning: An Evaluation on Traffic Sign Classification"" target=""_blank"">[https://doi.org/10.1145/3374664.3379534]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3374664.3379534]</a>",,,ScienceDirect
Backdoor Attacks against Transfer Learning with Pre-trained Deep Learning Models,"Shuo Wang, Surya Nepal, Carsten Rudolph, Marthie Grobler, Shangyu Chen, Tianle Chen","arXiv
arXiv","2020-03-12
2020-01","<a href=""arXiv (2020-03-12) : Backdoor Attacks against Transfer Learning with Pre-trained Deep Learning Models"" target=""_blank"">[http://arxiv.org/abs/2001.03274v2]</a>
<a href=""DBLP (2020-01) : Backdoor Attacks against Transfer Learning with Pre-trained Deep Learning Models"" target=""_blank"">[https://arxiv.org/abs/2001.03274]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1109/TSC.2020.3000900]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2001.03274]</a>","Transfer learning provides an effective solution for feasibly and fast customize accurate \textit{Student} models, by transferring the learned knowledge of pre-trained \textit{Teacher} models over large datasets via fine-tuning. Many pre-trained Teacher models used in transfer learning are publicly available and maintained by public platforms, increasing their vulnerability to backdoor attacks. In this paper, we demonstrate a backdoor threat to transfer learning tasks on both image and time-series data leveraging the knowledge of publicly accessible Teacher models, aimed at defeating three commonly-adopted defenses: \textit{pruning-based}, \textit{retraining-based} and \textit{input pre-processing-based defenses}. Specifically, (A) ranking-based selection mechanism to speed up the backdoor trigger generation and perturbation process while defeating \textit{pruning-based} and/or \textit{retraining-based defenses}. (B) autoencoder-powered trigger generation is proposed to produce a robust trigger that can defeat the \textit{input pre-processing-based defense}, while guaranteeing that selected neuron(s) can be significantly activated. (C) defense-aware retraining to generate the manipulated model using reverse-engineered model inputs. We launch effective misclassification attacks on Student models over real-world images, brain Magnetic Resonance Imaging (MRI) data and Electrocardiography (ECG) learning systems. The experiments reveal that our enhanced attack can maintain the $98.4\%$ and $97.2\%$ classification accuracy as the genuine model on clean image and time series inputs respectively while improving $27.9\%-100\%$ and $27.1\%-56.1\%$ attack success rate on trojaned image and time series inputs respectively in the presence of pruning-based and/or retraining-based defenses.
","
","arXiv
DBLP"
Adversarial Learning Targeting Deep Neural Network Classification: A Comprehensive Review of Defenses Against Attacks,D. J. Miller Z. Xiang G. Kesidis,Proceedings of the IEEE,2020-03-04,"<a href=""IEEE (2020-03-04) : Adversarial Learning Targeting Deep Neural Network Classification: A Comprehensive Review of Defenses Against Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9013065]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/JPROC.2020.2970615]</a>","With wide deployment of machine learning (ML)-based systems for a variety of applications including medical, military, automotive, genomic, multimedia, and social networking, there is great potential for damage from adversarial learning (AL) attacks. In this article, we provide a contemporary survey of AL, focused particularly on defenses against attacks on deep neural network classifiers. After introducing relevant terminology and the goals and range of possible knowledge of both attackers and defenders, we survey recent work on test-time evasion (TTE), data poisoning (DP), backdoor DP, and reverse engineering (RE) attacks and particularly defenses against the same. In so doing, we distinguish robust classification from anomaly detection (AD), unsupervised from supervised, and statistical hypothesis-based defenses from ones that do not have an explicit null (no attack) hypothesis. We also consider several scenarios for detecting backdoors. We provide a technical assessment for reviewed works, including identifying any issues/limitations, required hyperparameters, needed computational complexity, as well as the performance measures evaluated and the obtained quality. We then delve deeper, providing novel insights that challenge conventional AL wisdom and that target unresolved issues, including: robust classification versus AD as a defense strategy the belief that attack success increases with attack strength, which ignores susceptibility to AD small perturbations for TTE attacks: a fallacy or a requirement validity of the universal assumption that a TTE attacker knows the ground-truth class for the example to be attacked black, gray, or white-box attacks as the standard for defense evaluation and susceptibility of query-based RE to an AD defense. We also discuss attacks on the privacy of training data. We then present benchmark comparisons of several defenses against TTE, RE, and backdoor DP attacks on images. The article concludes with a discussion of continuing research directions, including the supreme challenge of detecting attacks whose goal is not to alter classification decisions, but rather simply to embed, without detection, “fake news” or other false content.",,IEEE
Detection of data integrity attacks by constructing an effective intrusion detection system,"R. B. Benisha, S. Raja Ratna",Journal of Ambient Intelligence and Humanized Computing,2020-03-04,"<a href=""Springer (2020-03-04) : Detection of data integrity attacks by constructing an effective intrusion detection system"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-020-01850-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-020-01850-1]</a>",Network users are heavily targeted by data integrity attacks that affect the development of new security techniques. The main challenge in network...,,Springer
Adversarial Learning Targeting Deep Neural Network Classification: A Comprehensive Review of Defenses against Attacks,Miller D.J.,Proceedings of the IEEE,2020-03-01,"<a href=""ScienceDirect (2020-03-01) : Adversarial Learning Targeting Deep Neural Network Classification: A Comprehensive Review of Defenses against Attacks"" target=""_blank"">[https://doi.org/10.1109/JPROC.2020.2970615]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/JPROC.2020.2970615]</a>",,,ScienceDirect
Backdoor purposivism,Krishnakumar A.S.,Duke Law Journal,2020-03-01,"<a href=""ScienceDirect (2020-03-01) : Backdoor purposivism"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
FriendNet Backdoor: Indentifying Backdoor Attack that is safe for Friendly Deep Neural Network,"Hyun Kwon, Hyunsoo Yoon, Ki-Woong Park","ICSIM '20: Proceedings of the 3rd International Conference on Software Engineering and Information Management
ICSIM","2020-03
2020","<a href=""ACM (2020-03) : FriendNet Backdoor: Indentifying Backdoor Attack that is safe for Friendly Deep Neural Network"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3378936.3378938]</a>
<a href=""DBLP (2020) : FriendNet Backdoor: Indentifying Backdoor Attack that is safe for Friendly Deep Neural Network"" target=""_blank"">[https://doi.org/10.1145/3378936.3378938]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3378936.3378938]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3378936.3378938]</a>","Deep neural networks (DNNs) provide good performance in image recognition, speech recognition and pattern analysis. However, DNNs are vulnerable to backdoor attacks. Backdoor attacks allow attackers to proactively access training data of DNNs to train ...
","
","ACM
DBLP"
Exploring Backdoor Poisoning Attacks Against Malware Classifiers,"Giorgio Severi, Jim Meyer, Scott E. Coull, Alina Oprea",arXiv,2020-03,"<a href=""DBLP (2020-03) : Exploring Backdoor Poisoning Attacks Against Malware Classifiers"" target=""_blank"">[https://arxiv.org/abs/2003.01031]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2003.01031]</a>",,,DBLP
Exposing Backdoors in Robust Machine Learning Models,"Ezekiel O. Soremekun, Sakshi Udeshi, Sudipta Chattopadhyay, Andreas Zeller",arXiv,2020-03,"<a href=""DBLP (2020-03) : Exposing Backdoors in Robust Machine Learning Models"" target=""_blank"">[https://arxiv.org/abs/2003.00865]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2003.00865]</a>",,,DBLP
Watch your back: Backdoor Attacks in Deep Reinforcement Learning-based Autonomous Vehicle Control Systems,"Yue Wang, Esha Sarkar, Michail Maniatakos, Saif Eddin Jabari",arXiv,2020-03,"<a href=""DBLP (2020-03) : Watch your back: Backdoor Attacks in Deep Reinforcement Learning-based Autonomous Vehicle Control Systems"" target=""_blank"">[https://arxiv.org/abs/2003.07859]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2003.07859]</a>",,,DBLP
"A survey on security challenges in cloud computing: issues, threats, and solutions","Hamed Tabrizchi, Marjan Kuchaki Rafsanjani",The Journal of Supercomputing,2020-02-28,"<a href=""Springer (2020-02-28) : A survey on security challenges in cloud computing: issues, threats, and solutions"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-020-03213-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-020-03213-1]</a>",Cloud computing has gained huge attention over the past decades because of continuously increasing demands. There are several advantages to...,,Springer
RERTL: Finite State Transducer Logic Recovery at Register Transfer Level,J. Portillo T. Meade J. Hacker S. Zhang Y. Jin,2019 Asian Hardware Oriented Security and Trust Symposium (AsianHOST),2020-02-24,"<a href=""IEEE (2020-02-24) : RERTL: Finite State Transducer Logic Recovery at Register Transfer Level"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9006699]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/AsianHOST47458.2019.9006699]</a>","Increasingly complex Intellectual Property (IP) design, coupled with shorter Time-To-Market (TTM), breeds flaws at various levels of the Integrated Circuit (IC) production. With access to IPs at all stages of production, design defects can easily be found and corrected, i.e., knowledge of the Register Transfer Level (RTL) code allows for the option of easy defect detection. However, third-party IPs are typically delivered as hard IPs or gate-level netlists, which complicates the defect detection process. The inaccessibility of source RTL code and the lack of RTL recovery tools make the task of finding high-level security flaws in logic intractable. Upon this request, in this paper, we present an RTL recovery tool suite named RERTL that leverages advanced graph algorithms including Lengauer-Tarjan's dominator tree and Euler tour tree technique to assist in netlist analysis. Supported by RERTL, logical states and their interactions are recovered from the initial design in the format of gate-level netlists. After the recovery of state interaction, RERTL further converts the full design into human-readable RTL. A series of netlist case studies were examined using RERTL covering benign logic structures, designs with accidental defects, and designs with deliberate backdoors. The experimental results show that all of our designs at various complexities were recoverable within seconds.",,IEEE
TargetNet Backdoor: Attack on Deep Neural Network with Use of Different Triggers,Kwon H.,ACM International Conference Proceeding Series,2020-02-19,"<a href=""ScienceDirect (2020-02-19) : TargetNet Backdoor: Attack on Deep Neural Network with Use of Different Triggers"" target=""_blank"">[https://doi.org/10.1145/3385209.3385216]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3385209.3385216]</a>",,,ScienceDirect
"Machine learning algorithms for improving security on touch screen devices: a survey, challenges and new perspectives","Auwal Ahmed Bello, Haruna Chiroma, ... Liyana Shuib",Neural Computing and Applications,2020-02-17,"<a href=""Springer (2020-02-17) : Machine learning algorithms for improving security on touch screen devices: a survey, challenges and new perspectives"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-020-04775-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-020-04775-0]</a>",Mobile phone touch screen devices are equipped with high processing power and high memory. This led to users not only storing photos or videos but...,,Springer
Strong leakage-resilient encryption: enhancing data confidentiality by hiding partial ciphertext,"Jia Xu, Jianying Zhou",International Journal of Information Security,2020-02-12,"<a href=""Springer (2020-02-12) : Strong leakage-resilient encryption: enhancing data confidentiality by hiding partial ciphertext"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-020-00487-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-020-00487-7]</a>","Leakage-resilient encryption is a powerful tool to protect data confidentiality against side channel attacks. In this work, we introduce a new and...",,Springer
Analysis of Dataset in Private Cloud for Cloud Forensics Using Eucalyptus and Hadoop,M. S. Patil B. Ainapure,2019 International Conference on Smart Systems and Inventive Technology (ICSSIT),2020-02-10,"<a href=""IEEE (2020-02-10) : Analysis of Dataset in Private Cloud for Cloud Forensics Using Eucalyptus and Hadoop"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8987923]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICSSIT46314.2019.8987923]</a>","At present, in most of the areas of research, development and daily usage, almost all platforms and service providers are dependent on cloud computing. To achieve scalability and sustainability, cloud architecture is considered as a result of technological amalgamation. If the design of the architecture is complex in nature, it could have adverse effects on data recoverability and analysis when the system gets compromised. This type of structure introduces issues like inherent architecture flaws, backdoors, code smelling which could lead to exploitable vulnerabilities for hackers and scammers. Likewise, its complex nature puts limits on forensic investigation methods. For such problems, digital forensic provides a solution. It uses the three-stage approach which consists of evidence collection eye-witnessed by the user, evidence preservation in an unaltered way, offline safeguarding of evidence for feigning its collection. Among its various methods like live forensics, timeline analysis, logging, sandboxing, logging is generalized and easy to use method. The proposed work is about digital forensic technique, log analysis, which is the most effective approach to override investigation issues in the cloud environment. Snort, Network Intrusion Detection System (NIDS), will work as a daemon on Eucalyptus private cloud to monitor and log intrusive attempts of network activities on it. Second, the proposed approach will fix the incapability of Eucalyptus to export logs to the rSyslog server. Third, the generated datasets, irrespective of location and format, would be analyzed by Hadoop, for improved analysis of a system.",,IEEE
NNoculation: Broad Spectrum and Targeted Treatment of Backdoored DNNs,"Akshaj Kumar Veldanda, Kang Liu, Benjamin Tan, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Brendan Dolan-Gavitt, Siddharth Garg",arXiv,2020-02,"<a href=""DBLP (2020-02) : NNoculation: Broad Spectrum and Targeted Treatment of Backdoored DNNs"" target=""_blank"">[https://arxiv.org/abs/2002.08313]</a>","<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2002.08313]</a>",,,DBLP
An authentication protocol based on chaos and zero knowledge proof,"Will Major, William J. Buchanan, Jawad Ahmad",Nonlinear Dynamics,2020-01-21,"<a href=""Springer (2020-01-21) : An authentication protocol based on chaos and zero knowledge proof"" target=""_blank"">[https://link.springer.com/article/10.1007/s11071-020-05463-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11071-020-05463-3]</a>","Port Knocking is a method for authenticating clients through a closed stance firewall, and authorising their requested actions, enabling severs to...",,Springer
Incorporating evolutionary computation for securing wireless network against cyberthreats,"Shubhra Dwivedi, Manu Vardhan, Sarsij Tripathi",The Journal of Supercomputing,2020-01-20,"<a href=""Springer (2020-01-20) : Incorporating evolutionary computation for securing wireless network against cyberthreats"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-020-03161-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-020-03161-w]</a>","Due to the rapid growth of internet services, the demand for protection and security of the network against sophisticated attacks is continuously...",,Springer
An emerging threat Fileless malware: a survey and research challenges,"Sudhakar, Sushil Kumar",Cybersecurity,2020-01-14,"<a href=""Springer (2020-01-14) : An emerging threat Fileless malware: a survey and research challenges"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-019-0043-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-019-0043-x]</a>","With the evolution of cybersecurity countermeasures, the threat landscape has also evolved, especially in malware from traditional file-based malware...",,Springer
FriendNet backdoor: Indentifying backdoor attack that is safe for friendly deep neural network,Kwon H.,ACM International Conference Proceeding Series,2020-01-12,"<a href=""ScienceDirect (2020-01-12) : FriendNet backdoor: Indentifying backdoor attack that is safe for friendly deep neural network"" target=""_blank"">[https://doi.org/10.1145/3378936.3378938]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3378936.3378938]</a>",,,ScienceDirect
Wireless sensor network intrusion detection system based on MK-ELM,"Wenjie Zhang, Dezhi Han, ... Francisco Isidro Massetto",Soft Computing,2020-01-10,"<a href=""Springer (2020-01-10) : Wireless sensor network intrusion detection system based on MK-ELM"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-020-04678-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-020-04678-1]</a>","Advances in digital electronics, wireless communications, and electro-mechanical systems technology have revolutionized the society and economy...",,Springer
Malware Detection Based on Multi-level and Dynamic Multi-feature Using Ensemble Learning at Hypervisor,"Jian Zhang, Cheng Gao, ... Wenzhen Li",Mobile Networks and Applications,2020-01-08,"<a href=""Springer (2020-01-08) : Malware Detection Based on Multi-level and Dynamic Multi-feature Using Ensemble Learning at Hypervisor"" target=""_blank"">[https://link.springer.com/article/10.1007/s11036-019-01503-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11036-019-01503-4]</a>","As more and more applications migrate to clouds, the type and amount of malware attack against virtualized environments are increasing, which is a...",,Springer
Scheduling Sequence Control Method Based on Sliding Window in Cyberspace Mimic Defense,W. Guo Z. Wu F. Zhang J. Wu,IEEE Access,2020-01-07,"<a href=""IEEE (2020-01-07) : Scheduling Sequence Control Method Based on Sliding Window in Cyberspace Mimic Defense"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938780]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2019.2961644]</a>","Cyberspace Mimic Defense (CMD) is a proactive defense theory proposed in recent years to deal with vulnerability and backdoor threats that are common in information systems. Different from moving target defense (MTD), CMD can obtain foundation by verifying multiple results from isolated, heterogeneous, and parallel running spaces, thus initiating a more targeted defensive action, such as scheduling and structure transformation. However, scheduling sequence control is a severe problem in this process, which needs to select a series of scheduling time and take into account security, efficiency, and robustness for variable attack situations. Inspired by the traffic and congestion control mechanism in computer networks, this paper proposed a sliding window-based scheduling sequence control method. By setting driver events to trigger the window “sliding,” the control parameters update and adapt to the current state accordingly. Then, considering internal resource constraints and external attack situations, a two-factor driver on variable period and exception threshold with their corresponding calculations are specified. Evaluations show that this method can maintain good performance under different scenarios, which proves to be an effective solution for scheduling sequence control in CMD.",,IEEE
Simulation on vertical microchannel evaporator for rack-backdoor cooling of data center,Zhan B.,Applied Thermal Engineering,2020-01-05,"<a href=""ScienceDirect (2020-01-05) : Simulation on vertical microchannel evaporator for rack-backdoor cooling of data center"" target=""_blank"">[https://doi.org/10.1016/j.applthermaleng.2019.114550]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.applthermaleng.2019.114550]</a>",,,ScienceDirect
Adversarial Fingerprinting of Cyber Attacks Based on Stateful Honeypots,A. Cantelli-Forti M. Colajanni,2018 International Conference on Computational Science and Computational Intelligence (CSCI),2020-01-02,"<a href=""IEEE (2020-01-02) : Adversarial Fingerprinting of Cyber Attacks Based on Stateful Honeypots"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8947682]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/CSCI46756.2018.00012]</a>","The cyber defenses of Critical Infrastructures require early detection of new threats and attacks. This includes defensive systems that are able to learn from novel attacks and to detect 0-day vulnerabilities as early as possible. Honeypots are not defensive systems based on prevention, but they still represent an effective way to gather information about attacks from the source. Nevertheless, most existing solutions operate in a stateless way. As a consequence, they are easily identified by expert attackers, and they are unable to track progress of individual attacks in large applications. We propose a novel approach that enables a so called stateful honeypot. The idea comes from the observation that a typical cyber attack to a Critical Infrastructure is carried out through multiple attempts and intrusions. Hence the main goal is to fingerprint each attacker by observing and registering his adopted methods, tools and actions. Once identified, the adversary is redirected to his specific environment that preserves the history of his previous operations including the installation of rootkits or backdoors. The proposed solution paves the way to a more effective generation of honeypots that are necessary to face the augmented complexity of cyber attacks.",,IEEE
A Methodology for Root-Causing In-field Attacks on Microfluidic Executions,Roy P.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,"<a href=""ScienceDirect (2020-01-01) : A Methodology for Root-Causing In-field Attacks on Microfluidic Executions"" target=""_blank"">[https://doi.org/10.1007/978-3-662-61092-3_7]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-662-61092-3_7]</a>",,,ScienceDirect
A hybrid nested genetic-fuzzy algorithm framework for intrusion detection and attacks,Elhefnawy R.,IEEE Access,2020-01-01,"<a href=""ScienceDirect (2020-01-01) : A hybrid nested genetic-fuzzy algorithm framework for intrusion detection and attacks"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2020.2996226]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2020.2996226]</a>",,,ScienceDirect
"Attack of the tails: Yes, you really can backdoor federated learning",Wang H.,Advances in Neural Information Processing Systems,2020-01-01,"<a href=""ScienceDirect (2020-01-01) : Attack of the tails: Yes, you really can backdoor federated learning"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Backdoor attacks in sequential decision-making agents,Yang Z.,CEUR Workshop Proceedings,2020-01-01,"<a href=""ScienceDirect (2020-01-01) : Backdoor attacks in sequential decision-making agents"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Clean-Label Backdoor Attacks on Video Recognition Models,Zhao S.,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,2020-01-01,"<a href=""ScienceDirect (2020-01-01) : Clean-Label Backdoor Attacks on Video Recognition Models"" target=""_blank"">[https://doi.org/10.1109/CVPR42600.2020.01445]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CVPR42600.2020.01445]</a>",,,ScienceDirect
Countermeasure against backdoor attacks using epistemic classifiers,Yang Z.,Proceedings of SPIE - The International Society for Optical Engineering,2020-01-01,"<a href=""ScienceDirect (2020-01-01) : Countermeasure against backdoor attacks using epistemic classifiers"" target=""_blank"">[https://doi.org/10.1117/12.2558255]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1117/12.2558255]</a>",,,ScienceDirect
DBA: DISTRIBUTED BACKDOOR ATTACKS AGAINST FEDERATED LEARNING,Xie C.,"8th International Conference on Learning Representations, ICLR 2020",2020-01-01,"<a href=""ScienceDirect (2020-01-01) : DBA: DISTRIBUTED BACKDOOR ATTACKS AGAINST FEDERATED LEARNING"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Detecting backdoor attacks via class difference in deep neural networks,Kwon H.,IEEE Access,2020-01-01,"<a href=""ScienceDirect (2020-01-01) : Detecting backdoor attacks via class difference in deep neural networks"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2020.3032411]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2020.3032411]</a>",,,ScienceDirect
Formal Description of Cyber Attacks,Wu J.,Wireless Networks(United Kingdom),2020-01-01,"<a href=""ScienceDirect (2020-01-01) : Formal Description of Cyber Attacks"" target=""_blank"">[https://doi.org/10.1007/978-3-030-29844-9_2]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-030-29844-9_2]</a>",,,ScienceDirect
Hidden trigger backdoor attacks,Saha A.,AAAI 2020 - 34th AAAI Conference on Artificial Intelligence,2020-01-01,"<a href=""ScienceDirect (2020-01-01) : Hidden trigger backdoor attacks"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Input-aware dynamic backdoor attack,Nguyen T.A.,Advances in Neural Information Processing Systems,2020-01-01,"<a href=""ScienceDirect (2020-01-01) : Input-aware dynamic backdoor attack"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Kleptographic Attack on Elliptic Curve Based Cryptographic Protocols,Sajjad A.,IEEE Access,2020-01-01,"<a href=""ScienceDirect (2020-01-01) : Kleptographic Attack on Elliptic Curve Based Cryptographic Protocols"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2020.3012823]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2020.3012823]</a>",,,ScienceDirect
Mitigating webshell attacks through machine learning techniques,Guo Y.,Future Internet,2020-01-01,"<a href=""ScienceDirect (2020-01-01) : Mitigating webshell attacks through machine learning techniques"" target=""_blank"">[https://doi.org/10.3390/fi12010012]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/fi12010012]</a>",,,ScienceDirect
Multi-targeted backdoor: Indentifying backdoor attack for multiple deep neural networks,Kwon H.,IEICE Transactions on Information and Systems,2020-01-01,"<a href=""ScienceDirect (2020-01-01) : Multi-targeted backdoor: Indentifying backdoor attack for multiple deep neural networks"" target=""_blank"">[https://doi.org/10.1587/transinf.2019EDL8170]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1587/transinf.2019EDL8170]</a>",,,ScienceDirect
On the trade-off between adversarial and backdoor robustness,Weng C.H.,Advances in Neural Information Processing Systems,2020-01-01,"<a href=""ScienceDirect (2020-01-01) : On the trade-off between adversarial and backdoor robustness"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Poison attacks against text datasets with conditional adversarially regularized autoencoder,Chan A.,Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020,2020-01-01,"<a href=""ScienceDirect (2020-01-01) : Poison attacks against text datasets with conditional adversarially regularized autoencoder"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
ROBUST ANOMALY DETECTION AND BACKDOOR ATTACK DETECTION VIA DIFFERENTIAL PRIVACY,Du M.,"8th International Conference on Learning Representations, ICLR 2020",2020-01-01,"<a href=""ScienceDirect (2020-01-01) : ROBUST ANOMALY DETECTION AND BACKDOOR ATTACK DETECTION VIA DIFFERENTIAL PRIVACY"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Removing backdoor-based watermarks in neural networks with limited data,Liu X.,Proceedings - International Conference on Pattern Recognition,2020-01-01,"<a href=""ScienceDirect (2020-01-01) : Removing backdoor-based watermarks in neural networks with limited data"" target=""_blank"">[https://doi.org/10.1109/ICPR48806.2021.9412684]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICPR48806.2021.9412684]</a>",,,ScienceDirect
Security Risks from Vulnerabilities and Backdoors,Wu J.,Wireless Networks(United Kingdom),2020-01-01,"<a href=""ScienceDirect (2020-01-01) : Security Risks from Vulnerabilities and Backdoors"" target=""_blank"">[https://doi.org/10.1007/978-3-030-29844-9_1]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-030-29844-9_1]</a>",,,ScienceDirect
The malicious framework: Embedding backdoors into tweakable block ciphers,Peyrin T.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,"<a href=""ScienceDirect (2020-01-01) : The malicious framework: Embedding backdoors into tweakable block ciphers"" target=""_blank"">[https://doi.org/10.1007/978-3-030-56877-1_9]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-030-56877-1_9]</a>",,,ScienceDirect
USB Devices with Hardware Backdoor,Kondratev M.I.,"Proceedings of the 2020 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering, EIConRus 2020",2020-01-01,"<a href=""ScienceDirect (2020-01-01) : USB Devices with Hardware Backdoor"" target=""_blank"">[https://doi.org/10.1109/EIConRus49466.2020.9039065]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/EIConRus49466.2020.9039065]</a>",,,ScienceDirect
Weight poisoning attacks on pre-trained models,Kurita K.,Proceedings of the Annual Meeting of the Association for Computational Linguistics,2020-01-01,"<a href=""ScienceDirect (2020-01-01) : Weight poisoning attacks on pre-trained models"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
One-Pixel Signature: Characterizing CNN Models for Backdoor Detection,"Shanjiaoyang Huang, Weiqi Peng, ... Zhuowen Tu","Computer Vision – ECCV 2020
arXiv
ECCV
arXiv","2020
2020-08-18
2020
2020-08","<a href=""Springer (2020) : One-Pixel Signature: Characterizing CNN Models for Backdoor Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-58583-9_20]</a>
<a href=""arXiv (2020-08-18) : One-pixel Signature: Characterizing CNN Models for Backdoor Detection"" target=""_blank"">[http://arxiv.org/abs/2008.07711v1]</a>
<a href=""DBLP (2020) : One-Pixel Signature: Characterizing CNN Models for Backdoor Detection"" target=""_blank"">[https://doi.org/10.1007/978-3-030-58583-9_20]</a>
<a href=""DBLP (2020-08) : One-pixel Signature: Characterizing CNN Models for Backdoor Detection"" target=""_blank"">[https://arxiv.org/abs/2008.07711]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-58583-9_20]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-030-58583-9_20]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2008.07711]</a>","We tackle the convolution neural networks (CNNs) backdoor detection problem by proposing a new representation called one-pixel signature. Our task is...
We tackle the convolution neural networks (CNNs) backdoor detection problem by proposing a new representation called one-pixel signature. Our task is to detect/classify if a CNN model has been maliciously inserted with an unknown Trojan trigger or not. Here, each CNN model is associated with a signature that is created by generating, pixel-by-pixel, an adversarial value that is the result of the largest change to the class prediction. The one-pixel signature is agnostic to the design choice of CNN architectures, and how they were trained. It can be computed efficiently for a black-box CNN model without accessing the network parameters. Our proposed one-pixel signature demonstrates a substantial improvement (by around 30% in the absolute detection accuracy) over the existing competing methods for backdoored CNN detection/classification. One-pixel signature is a general representation that can be used to characterize CNN models beyond backdoor detection.

","


","Springer
arXiv
DBLP
DBLP"
Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks,"Yunfei Liu, Xingjun Ma, ... Feng Lu","Computer Vision – ECCV 2020
arXiv
ECCV
arXiv","2020
2020-07-13
2020
2020-07","<a href=""Springer (2020) : Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-58607-2_11]</a>
<a href=""arXiv (2020-07-13) : Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2007.02343v2]</a>
<a href=""DBLP (2020) : Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1007/978-3-030-58607-2_11]</a>
<a href=""DBLP (2020-07) : Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks"" target=""_blank"">[https://arxiv.org/abs/2007.02343]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-58607-2_11]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-030-58607-2_11]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2007.02343]</a>","Recent studies have shown that DNNs can be compromised by backdoor attacks crafted at training time. A backdoor attack installs a backdoor into the...
Recent studies have shown that DNNs can be compromised by backdoor attacks crafted at training time. A backdoor attack installs a backdoor into the victim model by injecting a backdoor pattern into a small proportion of the training data. At test time, the victim model behaves normally on clean test data, yet consistently predicts a specific (likely incorrect) target class whenever the backdoor pattern is present in a test example. While existing backdoor attacks are effective, they are not stealthy. The modifications made on training data or labels are often suspicious and can be easily detected by simple data filtering or human inspection. In this paper, we present a new type of backdoor attack inspired by an important natural phenomenon: reflection. Using mathematical modeling of physical reflection models, we propose reflection backdoor (Refool) to plant reflections as backdoor into a victim model. We demonstrate on 3 computer vision tasks and 5 datasets that, Refool can attack state-of-the-art DNNs with high success rate, and is resistant to state-of-the-art backdoor defenses.

","


","Springer
arXiv
DBLP
DBLP"
Differentiable Causal Backdoor Discovery,"Limor Gultchin, Matt J. Kusner, Varun Kanade, Ricardo Silva","AISTATS
arXiv","2020
2020-03","<a href=""DBLP (2020) : Differentiable Causal Backdoor Discovery"" target=""_blank"">[http://proceedings.mlr.press/v108/gultchin20a.html]</a>
<a href=""DBLP (2020-03) : Differentiable Causal Backdoor Discovery"" target=""_blank"">[https://arxiv.org/abs/2003.01461]</a>","<a href=""DBLP"" target=""_blank"">[http://proceedings.mlr.press/v108/gultchin20a.html]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2003.01461]</a>","
","
","DBLP
DBLP"
The MALICIOUS Framework: Embedding Backdoors into Tweakable Block Ciphers,"Thomas Peyrin, Haoyang Wang","Advances in Cryptology – CRYPTO 2020
CRYPTO
IACR Cryptol. ePrint Arch.","2020
2020
2020","<a href=""Springer (2020) : The MALICIOUS Framework: Embedding Backdoors into Tweakable Block Ciphers"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-56877-1_9]</a>
<a href=""DBLP (2020) : The MALICIOUS Framework: Embedding Backdoors into Tweakable Block Ciphers"" target=""_blank"">[https://doi.org/10.1007/978-3-030-56877-1_9]</a>
<a href=""DBLP (2020) : The MALICIOUS Framework: Embedding Backdoors into Tweakable Block Ciphers"" target=""_blank"">[https://eprint.iacr.org/2020/986]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-56877-1_9]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-030-56877-1_9]</a>
<a href=""DBLP"" target=""_blank"">[https://eprint.iacr.org/2020/986]</a>","Inserting backdoors in encryption algorithms has long seemed like a very interesting, yet difficult problem. Most attempts have been unsuccessful for...

","

","Springer
DBLP
DBLP"
A Defence Against Input-Agnostic Backdoor Attacks on Deep Neural Networks,"Yansong Gao, Surya Nepal","Information Systems Security
ICISS","2020
2020","<a href=""Springer (2020) : A Defence Against Input-Agnostic Backdoor Attacks on Deep Neural Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-65610-2_4]</a>
<a href=""DBLP (2020) : A Defence Against Input-Agnostic Backdoor Attacks on Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1007/978-3-030-65610-2_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-65610-2_4]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-030-65610-2_4]</a>","Backdoor attacks insert hidden associations or triggers to the deep neural network (DNN) models to override correct inference such as classification....
","
","Springer
DBLP"
Escaping Backdoor Attack Detection of Deep Learning,"Yayuan Xiong, Fengyuan Xu, ... Qun Li","ICT Systems Security and Privacy Protection
SEC","2020
2020","<a href=""Springer (2020) : Escaping Backdoor Attack Detection of Deep Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-58201-2_29]</a>
<a href=""DBLP (2020) : Escaping Backdoor Attack Detection of Deep Learning"" target=""_blank"">[https://doi.org/10.1007/978-3-030-58201-2_29]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-58201-2_29]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-030-58201-2_29]</a>","Malicious attacks become a top concern in the field of deep learning (DL) because they have kept threatening the security and safety of applications...
","
","Springer
DBLP"
Towards Defeating Backdoored Random Oracles: Indifferentiability with Bounded Adaptivity,"Yevgeniy Dodis, Pooya Farshim, Sogol Mazaheri, Stefano Tessaro","IACR Cryptol. ePrint Arch.
TCC","2020
2020","<a href=""DBLP (2020) : Towards Defeating Backdoored Random Oracles: Indifferentiability with Bounded Adaptivity"" target=""_blank"">[https://eprint.iacr.org/2020/1199]</a>
<a href=""DBLP (2020) : Towards Defeating Backdoored Random Oracles: Indifferentiability with Bounded Adaptivity"" target=""_blank"">[https://doi.org/10.1007/978-3-030-64381-2_9]</a>","<a href=""DBLP"" target=""_blank"">[https://eprint.iacr.org/2020/1199]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-030-64381-2_9]</a>","
","
","DBLP
DBLP"
"Revealing Backdoors, Post-Training, in DNN Classifiers via Novel Inference on Optimized Perturbations Inducing Group Misclassification","Zhen Xiang, David J. Miller, George Kesidis","ICASSP
arXiv","2020
2019-08","<a href=""DBLP (2020) : Revealing Backdoors, Post-Training, in DNN Classifiers via Novel Inference on Optimized Perturbations Inducing Group Misclassification"" target=""_blank"">[https://doi.org/10.1109/ICASSP40776.2020.9054581]</a>
<a href=""DBLP (2019-08) : Revealing Backdoors, Post-Training, in DNN Classifiers via Novel Inference on Optimized Perturbations Inducing Group Misclassification"" target=""_blank"">[http://arxiv.org/abs/1908.10498]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICASSP40776.2020.9054581]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1908.10498]</a>","
","
","DBLP
DBLP"
A Data Augmentation-Based Defense Method Against Adversarial Attacks in Neural Networks,"Yi Zeng, Han Qiu, ... Meikang Qiu",Algorithms and Architectures for Parallel Processing,2020,"<a href=""Springer (2020) : A Data Augmentation-Based Defense Method Against Adversarial Attacks in Neural Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-60239-0_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-60239-0_19]</a>","Deep Neural Networks (DNNs) in Computer Vision (CV) are well-known to be vulnerable to Adversarial Examples (AEs), namely imperceptible perturbations...",,Springer
A Design for a Secure Malware Laboratory,"Xavier Riofrío, Fernando Salinas-Herrera, David Galindo",Information and Communication Technologies of Ecuador (TIC.EC),2020,"<a href=""Springer (2020) : A Design for a Secure Malware Laboratory"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-35740-5_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-35740-5_19]</a>","Malicious software teaching is based on theory, consequently, students do not experiment with real practice. Therefore, when they confront a rising...",,Springer
A Detailed Analysis of Intruders’ Activities in the Network Through the Real-Time Virtual Honeynet Experimentation,"Rajarajan Ganesarathinam, M. Amutha Prabakar, ... A. Leo Fernandez",Artificial Intelligence and Evolutionary Computations in Engineering Systems,2020,"<a href=""Springer (2020) : A Detailed Analysis of Intruders’ Activities in the Network Through the Real-Time Virtual Honeynet Experimentation"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0199-9_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0199-9_4]</a>",The menace of attackers over the network is unstoppable for the past two decades. The security practitioners and researchers are devising mechanisms...,,Springer
A Formal Technique for Concurrent Generation of Software’s Functional and Security Requirements in SOFL Specifications,"Busalire Emeka, Shaoying Liu",Structured Object-Oriented Formal Language and Method,2020,"<a href=""Springer (2020) : A Formal Technique for Concurrent Generation of Software’s Functional and Security Requirements in SOFL Specifications"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-41418-4_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-41418-4_2]</a>",Formal methods offer a precise and concise way of expressing system requirements which can be tested and validated with formal proofs techniques....,,Springer
A Poisoning Attack Against the Recognition Model Trained by the Data Augmentation Method,"Yunhao Yang, Long Li, ... Tianlong Gu",Machine Learning for Cyber Security,2020,"<a href=""Springer (2020) : A Poisoning Attack Against the Recognition Model Trained by the Data Augmentation Method"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-62460-6_49]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-62460-6_49]</a>","The training model often preprocesses the training set with the data augmentation method. Aiming at this kind of training mode, a poisoning attack...",,Springer
A Sequential Approach to Network Intrusion Detection,"Nicholas Lee, Shih Yin Ooi, Ying Han Pang",Computational Science and Technology,2020,"<a href=""Springer (2020) : A Sequential Approach to Network Intrusion Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0058-9_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0058-9_2]</a>","In this paper, we combine the sequential modeling capability of Recurrent Neural Network (RNN), and the robustness of Random Forest (RF) in detecting...",,Springer
A Study of Digital Banking: Security Issues and Challenges,"B. Vishnuvardhan, B. Manjula, R. Lakshman Naik",Proceedings of the Third International Conference on Computational Intelligence and Informatics,2020,"<a href=""Springer (2020) : A Study of Digital Banking: Security Issues and Challenges"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-1480-7_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-1480-7_14]</a>","In the extensive sense, mobile banking (M-Banking) is defined as the financial transaction execution through smart devices or electronic devices. The...",,Springer
A Survey on Cloud Computing Security Issues and Cryptographic Techniques,"Vidushi Agarwal, Ashish K. Kaushal, Lokesh Chouhan",Social Networking and Computational Intelligence,2020,"<a href=""Springer (2020) : A Survey on Cloud Computing Security Issues and Cryptographic Techniques"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-2071-6_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-2071-6_10]</a>","Cloud computing is an Internet-based computing model, having various resources used by distinct users in a concurrent manner. Apart from all of its...",,Springer
"A Survey on the Impact of DDoS Attacks in Cloud Computing: Prevention, Detection and Mitigation Techniques","Karthik Srinivasan, Azath Mubarakali, ... A. Dinesh Kumar",Intelligent Communication Technologies and Virtual Mobile Networks,2020,"<a href=""Springer (2020) : A Survey on the Impact of DDoS Attacks in Cloud Computing: Prevention, Detection and Mitigation Techniques"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-28364-3_24]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-28364-3_24]</a>","In recent years, cloud services are emerging popular among the public and business ventures. Most of companies are trusting on cloud computing...",,Springer
About the Security Assessment of Embedded Software in Automated Process Control System,"I. A. Korsakov, A. P. Durakovskiy",Advanced Technologies in Robotics and Intelligent Systems,2020,"<a href=""Springer (2020) : About the Security Assessment of Embedded Software in Automated Process Control System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-33491-8_46]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-33491-8_46]</a>",This work explores theoretical related to the assessment of the embedded software security of programmable logic controllers (PLC) of industrial...,,Springer
Adversarial Examples Attack and Countermeasure for Speech Recognition System: A Survey,"Donghua Wang, Rangding Wang, ... Yongkang Gong",Security and Privacy in Digital Economy,2020,"<a href=""Springer (2020) : Adversarial Examples Attack and Countermeasure for Speech Recognition System: A Survey"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-9129-7_31]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-9129-7_31]</a>",Speech recognition technology is affecting and changing the current human-computer interaction profoundly. Due to the remarkable progress of deep...,,Springer
An Attack Simulation Language for the IT Domain,"Sotirios Katsikeas, Simon Hacks, ... Per Eliasson",Graphical Models for Security,2020,"<a href=""Springer (2020) : An Attack Simulation Language for the IT Domain"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-62230-5_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-62230-5_4]</a>","Cyber-attacks on IT infrastructures can have disastrous consequences for individuals, regions, as well as whole nations. In order to respond to these...",,Springer
An Experimental Approach to Unravel Effects of Malware on System Network Interface,"Sikiru Olanrewaju Subairu, John Alhassan, ... Rytis Maskeliunas","Advances in Data Sciences, Security and Applications",2020,"<a href=""Springer (2020) : An Experimental Approach to Unravel Effects of Malware on System Network Interface"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0372-6_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0372-6_17]</a>",Malware is malicious code that tends to take control of the system remotely. The author of these codes drops their malicious payload on to the...,,Springer
An Exploration on Cloud Computing Security Strategies and Issues,"Amrita Raj, Rakesh Kumar",Inventive Communication and Computational Technologies,2020,"<a href=""Springer (2020) : An Exploration on Cloud Computing Security Strategies and Issues"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0146-3_52]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0146-3_52]</a>","Cloud computing is revolutionizing many ecosystems by providing organization with computing resources that feature easy connectivity, deployment,...",,Springer
Analysis and Review of Cloud Based Encryption Methods,"Vicky Yadav, Rejo Mathew","Proceeding of the International Conference on Computer Networks, Big Data and IoT (ICCBI - 2019)",2020,"<a href=""Springer (2020) : Analysis and Review of Cloud Based Encryption Methods"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-43192-1_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-43192-1_19]</a>",Data security plays a crucial role in cloud computing. Cloud storages are secured and managed centrally by cloud service providers. Data stored in...,,Springer
Analysis on Improving the Performance of Machine Learning Models Using Feature Selection Technique,"N. Maajid Khan, Nalina Madhav C, ... I. Sumaiya Thaseen",Intelligent Systems Design and Applications,2020,"<a href=""Springer (2020) : Analysis on Improving the Performance of Machine Learning Models Using Feature Selection Technique"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-16660-1_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-16660-1_7]</a>",Many organizations deploying computer networks are susceptible to different kinds of attacks in the current era. These attacks compromise the...,,Springer
Android Malware Detection,"Shymala Gowri Selvaganapathy, G. Sudha Sadasivam, ... K. Karthik","Proceedings of International Conference on Artificial Intelligence, Smart Grid and Smart City Applications",2020,"<a href=""Springer (2020) : Android Malware Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-24051-6_73]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-24051-6_73]</a>",Smartphones and mobile tablets are rapidly becoming essential in daily life. Android has been the most popular mobile operating system since 2012....,,Springer
Anomaly-Based Detection of System-Level Threats and Statistical Analysis,"Himanshu Mishra, Ram Kumar Karsh, K. Pavani",Smart Computing Paradigms: New Progresses and Challenges,2020,"<a href=""Springer (2020) : Anomaly-Based Detection of System-Level Threats and Statistical Analysis"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-9680-9_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-9680-9_23]</a>",This paper presents various parameters for the analysis of threats to any network or system. These parameters are based on the anomalous behavior of...,,Springer
Application of complex systems in neural networks against Backdoor attacks,"Sara Kaviani, Insoo Sohn, Huaping Liu",ICTC,2020,"<a href=""DBLP (2020) : Application of complex systems in neural networks against Backdoor attacks"" target=""_blank"">[https://doi.org/10.1109/ICTC49870.2020.9289220]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICTC49870.2020.9289220]</a>",,,DBLP
Arguments Against Using the 1998 DARPA Dataset for Cloud IDS Design and Evaluation and Some Alternative,"Onyekachi Nwamuo, Paulo Magella de Faria Quinan, ... Abdulaziz Aldribi",Machine Learning for Networking,2020,"<a href=""Springer (2020) : Arguments Against Using the 1998 DARPA Dataset for Cloud IDS Design and Evaluation and Some Alternative"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-45778-5_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-45778-5_21]</a>","Due to the lack of adequate public datasets, the proponents of many existing cloud intrusion detection systems (IDS) have relied on the DARPA dataset...",,Springer
Associated Hazard Assessment of IoT Vulnerability Based on Risk Matrix,"Jinglan Yang, Yatian Xue, ... Bichen Che",Artificial Intelligence and Security,2020,"<a href=""Springer (2020) : Associated Hazard Assessment of IoT Vulnerability Based on Risk Matrix"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-8086-4_50]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-8086-4_50]</a>","With the rapid development of the technology, a large number of intelligent terminals access the IoT system. Attacks against the IoT system are...",,Springer
Audio CAPTCHA with a Few Cocktails: It’s so Noisy I Can’t Hear You,"Benjamin Maximilian Reinheimer, Fairooz Islam, Ilia Shumailov",Security Protocols XXVII,2020,"<a href=""Springer (2020) : Audio CAPTCHA with a Few Cocktails: It’s so Noisy I Can’t Hear You"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-57043-9_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-57043-9_3]</a>","With crime migrating to the web, the detection of abusive robotic behaviour is becoming more important. In this paper, we propose a new audio CAPTCHA...",,Springer
Authentication of Internet Connected White Goods Using Gestures or Key Sequences,"Thomas Schlechter, Johannes Fischer",Computer Aided Systems Theory – EUROCAST 2019,2020,"<a href=""Springer (2020) : Authentication of Internet Connected White Goods Using Gestures or Key Sequences"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-45096-0_62]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-45096-0_62]</a>","In the context of IoT, more and more white goods (e.g., water boiler, micro wave, washing machine) are connected to the internet for various purposes...",,Springer
Automobile Automation and Lifecycle: How Digitalisation and Security Issues Affect the Car as a Product and Service?,"Antti Hakkala, Olli I. Heimo",Intelligent Systems and Applications,2020,"<a href=""Springer (2020) : Automobile Automation and Lifecycle: How Digitalisation and Security Issues Affect the Car as a Product and Service?"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-29513-4_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-29513-4_9]</a>","In this paper, we argue that to accommodate the change brought by autonomous and semi-autonomous cars, the current lifecycle model of a car should be...",,Springer
Backdoor Attacks and Defenses for Deep Neural Networks in Outsourced Cloud Environments,"Yanjiao Chen, Xueluan Gong, Qian Wang, Xing Di, Huayang Huang",IEEE Netw.,2020,"<a href=""DBLP (2020) : Backdoor Attacks and Defenses for Deep Neural Networks in Outsourced Cloud Environments"" target=""_blank"">[https://doi.org/10.1109/MNET.011.1900577]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/MNET.011.1900577]</a>",,,DBLP
Backdoor Suppression in Neural Networks using Input Fuzzing and Majority Voting,"Esha Sarkar, Yousif Alkindi, Michail Maniatakos",IEEE Des. Test,2020,"<a href=""DBLP (2020) : Backdoor Suppression in Neural Networks using Input Fuzzing and Majority Voting"" target=""_blank"">[https://doi.org/10.1109/MDAT.2020.2968275]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/MDAT.2020.2968275]</a>",,,DBLP
Backdooring Deep Learning Architectures: Threats and (some) Opportunities,Mauro Barni,ICISSP,2020,"<a href=""DBLP (2020) : Backdooring Deep Learning Architectures: Threats and (some) Opportunities"" target=""_blank"">[]</a>","<a href=""DBLP"" target=""_blank"">[]</a>",,,DBLP
Backdoors into Two Occurrences,Jan Johannsen,J. Satisf. Boolean Model. Comput.,2020,"<a href=""DBLP (2020) : Backdoors into Two Occurrences"" target=""_blank"">[https://doi.org/10.3233/sat-200125]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.3233/sat-200125]</a>",,,DBLP
Backstabber’s Knife Collection: A Review of Open Source Software Supply Chain Attacks,"Marc Ohm, Henrik Plate, ... Michael Meier","Detection of Intrusions and Malware, and Vulnerability Assessment",2020,"<a href=""Springer (2020) : Backstabber’s Knife Collection: A Review of Open Source Software Supply Chain Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-52683-2_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-52683-2_2]</a>",A software supply chain attack is characterized by the injection of malicious code into a software package in order to compromise dependent systems...,,Springer
"Beyond the Hype: Internet of Things Concepts, Security and Privacy Concerns","Amit Kumar Tyagi, G. Rekha, N. Sreenath","Advances in Decision Sciences, Image Processing, Security and Computer Vision",2020,"<a href=""Springer (2020) : Beyond the Hype: Internet of Things Concepts, Security and Privacy Concerns"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-24322-7_50]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-24322-7_50]</a>","Today’s Internet of Things (IoTs) has occupied maximum fields/areas where they are working a tremendous works, i.e., providing better life experience...",,Springer
Building a Low-Cost and State-of-the-Art IoT Security Hands-On Laboratory,"Bryan Pearson, Lan Luo, ... Xinwen Fu",Internet of Things. A Confluence of Many Disciplines,2020,"<a href=""Springer (2020) : Building a Low-Cost and State-of-the-Art IoT Security Hands-On Laboratory"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-43605-6_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-43605-6_17]</a>",The popularity of IoT has raised grave security and privacy concerns. The huge IoT botnets Mirai and Reaper were built on compromised IoT devices. In...,,Springer
Building an Ensemble Learning Based Algorithm for Improving Intrusion Detection System,"M. S. Abirami, Umaretiya Yash, Sonal Singh",Artificial Intelligence and Evolutionary Computations in Engineering Systems,2020,"<a href=""Springer (2020) : Building an Ensemble Learning Based Algorithm for Improving Intrusion Detection System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0199-9_55]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0199-9_55]</a>",Intrusion detection system (IDS) alerts the network administrators against intrusive attempts. The anomalies are detected using machine learning...,,Springer
Capacity Building for a Cybersecurity Workforce Through Hands-on Labs for Internet-of-Things Security,"A. Ravishankar Rao, Daniel Clarke",National Cyber Summit (NCS) Research Track,2020,"<a href=""Springer (2020) : Capacity Building for a Cybersecurity Workforce Through Hands-on Labs for Internet-of-Things Security"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-31239-8_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-31239-8_2]</a>","There is growing concern about a cybersecurity skills gap nationwide, as an insufficient number of students are entering this field. Hence,...",,Springer
Characterizing Internet-Scale ICS Automated Attacks Through Long-Term Honeypot Data,"Jianzhou You, Shichao Lv, ... Limin Sun",Information and Communications Security,2020,"<a href=""Springer (2020) : Characterizing Internet-Scale ICS Automated Attacks Through Long-Term Honeypot Data"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-41579-2_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-41579-2_5]</a>",Industrial control system (ICS) devices with IP addresses are accessible on the Internet and become an essential part of critical infrastructures....,,Springer
Collaborative Learning Based Effective Malware Detection System,"Narendra Singh, Harsh Kasyap, Somanath Tripathy",ECML PKDD 2020 Workshops,2020,"<a href=""Springer (2020) : Collaborative Learning Based Effective Malware Detection System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-65965-3_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-65965-3_13]</a>","Malware is overgrowing, causing severe loss to different institutions. The existing techniques, like static and dynamic analysis, fail to mitigate...",,Springer
Complex Evaluation of Information Security of an Object with the Application of a Mathematical Model for Calculation of Risk Indicators,"A. L. Marukhlenko, A. V. Plugatarev, D. O. Bobyntsev",Advances in Automation,2020,"<a href=""Springer (2020) : Complex Evaluation of Information Security of an Object with the Application of a Mathematical Model for Calculation of Risk Indicators"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-39225-3_84]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-39225-3_84]</a>","In modern information systems designing, great attention is paid to information security issues, and therefore minimizing the risk of unauthorized...",,Springer
Computer Network Database Security Management Technology Optimization Path,Jianrong Xi,Cyber Security Intelligence and Analytics,2020,"<a href=""Springer (2020) : Computer Network Database Security Management Technology Optimization Path"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-43306-2_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-43306-2_21]</a>","Along with the progress of network technology, Internet age, the rapid expansion of electronic devices, such as computers, computer bring a lot of...",,Springer
Controlling Uncertainty with Proactive Cyber Defense: A Clausewitzian Perspective,Sampsa Rauti,Security in Computing and Communications,2020,"<a href=""Springer (2020) : Controlling Uncertainty with Proactive Cyber Defense: A Clausewitzian Perspective"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-4825-3_27]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-4825-3_27]</a>",This study argues that the fundamental tenets Carl von Clausewitz presented about warfare in his influential book On War can be applied to defensive...,,Springer
Cracking the DES Cipher with Cost-Optimized FPGA Devices,Jarosław Sugier,Engineering in Dependability of Computer Systems and Networks,2020,"<a href=""Springer (2020) : Cracking the DES Cipher with Cost-Optimized FPGA Devices"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-19501-4_47]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-19501-4_47]</a>","This paper examines efficiency of hardware realizations of DES cracking engines implemented in contemporary low-cost Spartan-7 devices from Xilinx,...",,Springer
Cryptographic Primitives that Resist Backdooring and Subversion,Sogol Mazaheri,,2020,"<a href=""DBLP (2020) : Cryptographic Primitives that Resist Backdooring and Subversion"" target=""_blank"">[http://tuprints.ulb.tu-darmstadt.de/14550/]</a>","<a href=""DBLP"" target=""_blank"">[http://tuprints.ulb.tu-darmstadt.de/14550/]</a>",,,DBLP
Cybersecurity Readiness of E-tail Organisations: A Technical Perspective,"Mahmood Hussain Shah, Raza Muhammad, Nisreen Ameen","Responsible Design, Implementation and Use of Information and Communication Technology",2020,"<a href=""Springer (2020) : Cybersecurity Readiness of E-tail Organisations: A Technical Perspective"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-44999-5_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-44999-5_13]</a>",Cybersecurity readiness is a challenging issue for online retail businesses which are losing billions of dollars due to cyber-crimes and a lack of...,,Springer
DANTE: A Framework for Mining and Monitoring Darknet Traffic,"Dvir Cohen, Yisroel Mirsky, ... Asaf Shabtai",Computer Security – ESORICS 2020,2020,"<a href=""Springer (2020) : DANTE: A Framework for Mining and Monitoring Darknet Traffic"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-58951-6_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-58951-6_5]</a>",Trillions of network packets are sent over the Internet to destinations which do not exist. This ‘darknet’ traffic captures the activity of botnets...,,Springer
Data Flow Verification in SoC Using Formal Techniques,"K. S. Asha, Oswald Sunil Mendonca, ... R. Srinivas","Advances in Communication, Signal Processing, VLSI, and Embedded Systems",2020,"<a href=""Springer (2020) : Data Flow Verification in SoC Using Formal Techniques"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0626-0_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0626-0_2]</a>",Hardware security is becoming increasingly critical in SoC designs. A SoC integrates several modules to form a larger system. To reduce the...,,Springer
Data Poisoning Attacks Against Federated Learning Systems,"Vale Tolpegin, Stacey Truex, ... Ling Liu",Computer Security – ESORICS 2020,2020,"<a href=""Springer (2020) : Data Poisoning Attacks Against Federated Learning Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-58951-6_24]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-58951-6_24]</a>",Federated learning (FL) is an emerging paradigm for distributed training of large-scale deep neural networks in which participants’ data remains on...,,Springer
Decepticon: A Hidden Markov Model Approach to Counter Advanced Persistent Threats,"Rudra Prasad Baksi, Shambhu J. Upadhyaya",Secure Knowledge Management In Artificial Intelligence Era,2020,"<a href=""Springer (2020) : Decepticon: A Hidden Markov Model Approach to Counter Advanced Persistent Threats"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-3817-9_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-3817-9_3]</a>","Deception has been proposed in the literature as an effective defense mechanism to address Advanced Persistent Threats (APT). However, administering...",,Springer
Deep k-NN Defense Against Clean-Label Data Poisoning Attacks,"Neehar Peri, Neal Gupta, ... John P. Dickerson",Computer Vision – ECCV 2020 Workshops,2020,"<a href=""Springer (2020) : Deep k-NN Defense Against Clean-Label Data Poisoning Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-66415-2_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-66415-2_4]</a>",Targeted clean-label data poisoning is a type of adversarial attack on machine learning systems in which an adversary injects a few...,,Springer
Defending Deep Learning Based Anomaly Detection Systems Against White-Box Adversarial Examples and Backdoor Attacks,"Khaled Alrawashdeh, Stephen Goldsmith",ISTAS,2020,"<a href=""DBLP (2020) : Defending Deep Learning Based Anomaly Detection Systems Against White-Box Adversarial Examples and Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1109/ISTAS50296.2020.9462227]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ISTAS50296.2020.9462227]</a>",,,DBLP
Defending Poisoning Attacks in Federated Learning via Adversarial Training Method,"Jiale Zhang, Di Wu, ... Bing Chen",Frontiers in Cyber Security,2020,"<a href=""Springer (2020) : Defending Poisoning Attacks in Federated Learning via Adversarial Training Method"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-9739-8_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-9739-8_7]</a>","Recently, federated learning has shown its significant advantages in protecting training data privacy by maintaining a joint model across multiple...",,Springer
Design Challenges of Trustworthy Artificial Intelligence Learning Systems,"Matthias R. Brust, Pascal Bouvry, ... El-Ghazil Talbi",Intelligent Information and Database Systems,2020,"<a href=""Springer (2020) : Design Challenges of Trustworthy Artificial Intelligence Learning Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-3380-8_50]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-3380-8_50]</a>","In the near future, more than two thirds of the world’s population is expected to be living in cities. In this interconnected world, data collection...",,Springer
Detecting Advanced Persistent Threat in Edge Computing via Federated Learning,"Zitong Li, Jiale Chen, ... Bing Chen",Security and Privacy in Digital Economy,2020,"<a href=""Springer (2020) : Detecting Advanced Persistent Threat in Edge Computing via Federated Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-9129-7_36]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-9129-7_36]</a>",Advanced Persistent Threat (APT) is one of the most menacing and stealthy multiple-steps attacks in the context of information systems and...,,Springer
Detecting Backdoor Attacks via Class Difference in Deep Neural Networks,Hyun Kwon,IEEE Access,2020,"<a href=""DBLP (2020) : Detecting Backdoor Attacks via Class Difference in Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2020.3032411]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2020.3032411]</a>",,,DBLP
Discussion on Computer Network Security Under the Background of Big Data,Haiyan Jiang,Cyber Security Intelligence and Analytics,2020,"<a href=""Springer (2020) : Discussion on Computer Network Security Under the Background of Big Data"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-43306-2_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-43306-2_8]</a>","The rapid development of science and technology is gradually changing people’s lifestyles, not only bringing about changes in behavior habits, but...",,Springer
Efficient Hardware Implementations for Elliptic Curve Cryptography over Curve448,"Mojtaba Bisheh Niasar, Reza Azarderakhsh, Mehran Mozaffari Kermani",Progress in Cryptology – INDOCRYPT 2020,2020,"<a href=""Springer (2020) : Efficient Hardware Implementations for Elliptic Curve Cryptography over Curve448"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-65277-7_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-65277-7_10]</a>","In this paper, we present different implementations of point multiplication over Curve448. Curve448 has recently been recommended by NIST to provide...",,Springer
Efficient Password-Authenticated Key Exchange from RLWE Based on Asymmetric Key Consensus,"Yingshan Yang, Xiaozhuo Gu, ... Taizhong Xu",Information Security and Cryptology,2020,"<a href=""Springer (2020) : Efficient Password-Authenticated Key Exchange from RLWE Based on Asymmetric Key Consensus"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-42921-8_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-42921-8_2]</a>",A password-authenticated key exchange (PAKE) protocol allows two entities sharing a password to perform mutual authentication and establish a session...,,Springer
Evading API Call Sequence Based Malware Classifiers,"Fenil Fadadu, Anand Handa, ... Sandeep Kumar Shukla",Information and Communications Security,2020,"<a href=""Springer (2020) : Evading API Call Sequence Based Malware Classifiers"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-41579-2_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-41579-2_2]</a>","In this paper, we present a mimicry attack to transform malware binary, which can evade detection by API call sequence based malware classifiers....",,Springer
Forensic Analysis on IoT Devices,"A. R. Jayakrishnan, V. Vasanthi",Intelligent Data Communication Technologies and Internet of Things,2020,"<a href=""Springer (2020) : Forensic Analysis on IoT Devices"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-34080-3_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-34080-3_7]</a>","When the number of connected devices increases, the chances of getting hacked also increases. IoT is the interconnection of smart devices to usher...",,Springer
Hacker Forum Exploit and Classification for Proactive Cyber Threat Intelligence,"Apurv Singh Gautam, Yamini Gahlot, Pooja Kamat",Inventive Computation Technologies,2020,"<a href=""Springer (2020) : Hacker Forum Exploit and Classification for Proactive Cyber Threat Intelligence"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-33846-6_32]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-33846-6_32]</a>",The exponential growth in data and technology have brought in prospects for progressively destructive cyber-attacks. Traditional security controls...,,Springer
Hardware Security and Trust: A New Battlefield of Information,Gang Qu,Decision and Game Theory for Security,2020,"<a href=""Springer (2020) : Hardware Security and Trust: A New Battlefield of Information"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-64793-3_27]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-64793-3_27]</a>",Hardware security and trust has received a lot of attention in the past 25 years. The purpose of this paper is to introduce the fundamental problems...,,Springer
ICITPM: Integrity Validation of Software in Iterative Continuous Integration Through the Use of Trusted Platform Module (TPM),"Antonio Muñoz, Aristeidis Farao, ... Christos Xenakis",Computer Security,2020,"<a href=""Springer (2020) : ICITPM: Integrity Validation of Software in Iterative Continuous Integration Through the Use of Trusted Platform Module (TPM)"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-66504-3_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-66504-3_9]</a>","Software development has passed from being rigid and not very flexible, to be automated with constant changes. This happens due to the creation of...",,Springer
Implementing Lightweight IoT-IDS on Raspberry Pi Using Correlation-Based Feature Selection and Its Performance Evaluation,"Yan Naung Soe, Yaokai Feng, ... Kouichi Sakurai",Advanced Information Networking and Applications,2020,"<a href=""Springer (2020) : Implementing Lightweight IoT-IDS on Raspberry Pi Using Correlation-Based Feature Selection and Its Performance Evaluation"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-15032-7_39]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-15032-7_39]</a>","The application of many IoT devices is making our world more convenient and efficient. However, it also makes a large number of cyber-attacks...",,Springer
Information Security Implications of Machine-Learning-Based Automation in ITO Service Delivery – An Agency Theory Perspective,"Baber Majid Bhatti, Sameera Mubarak, Sev Nagalingam",Neural Information Processing,2020,"<a href=""Springer (2020) : Information Security Implications of Machine-Learning-Based Automation in ITO Service Delivery – An Agency Theory Perspective"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-63833-7_41]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-63833-7_41]</a>",The trend of information technology outsourcing (ITO) to service providers (SPs) is growing. SPs bring improvements through transformation projects...,,Springer
Integrated Forensic Tool for Network Attacks,"Chia-Mei Chen, Gu-Hsin Lai, Zheng-Xun Tsai",Security with Intelligent Computing and Big-data Services,2020,"<a href=""Springer (2020) : Integrated Forensic Tool for Network Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-16946-6_35]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-16946-6_35]</a>","With the proliferation of cyber-attacks, Digital Forensic, also known as Computer Forensic, becomes more important to collect and analyze the...",,Springer
Interpretability Derived Backdoor Attacks Detection in Deep Neural Networks: Work-in-Progress,"Xiangyu Wen, Wei Jiang, Jinyu Zhan, Xupeng Wang, Zhiyuan He",EMSOFT,2020,"<a href=""DBLP (2020) : Interpretability Derived Backdoor Attacks Detection in Deep Neural Networks: Work-in-Progress"" target=""_blank"">[https://doi.org/10.1109/EMSOFT51651.2020.9244019]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/EMSOFT51651.2020.9244019]</a>",,,DBLP
Interpreting Machine Learning Malware Detectors Which Leverage N-gram Analysis,"William Briguglio, Sherif Saad",Foundations and Practice of Security,2020,"<a href=""Springer (2020) : Interpreting Machine Learning Malware Detectors Which Leverage N-gram Analysis"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-45371-8_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-45371-8_6]</a>","In cyberattack detection and prevention systems, cybersecurity analysts always prefer solutions that are as interpretable and understandable as...",,Springer
Intrusion Detection Systems (IDS)—An Overview with a Generalized Framework,"Ranjit Panigrahi, Samarjeet Borah, ... Pradeep Kumar Mallick",Cognitive Informatics and Soft Computing,2020,"<a href=""Springer (2020) : Intrusion Detection Systems (IDS)—An Overview with a Generalized Framework"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-1451-7_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-1451-7_11]</a>",The greatest challenge in the present era of Internet and communication technologies is the identification of spiteful activities in a system or...,,Springer
Invisible Poisoning: Highly Stealthy Targeted Poisoning Attack,"Jinyin Chen, Haibin Zheng, ... Shouling Ji",Information Security and Cryptology,2020,"<a href=""Springer (2020) : Invisible Poisoning: Highly Stealthy Targeted Poisoning Attack"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-42921-8_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-42921-8_10]</a>","Deep learning is widely applied to various areas for its great performance. However, it is vulnerable to adversarial attacks and poisoning attacks,...",,Springer
Machine Learning Based Hardware Trojan Detection Using Electromagnetic Emanation,"Junko Takahashi, Keiichi Okabe, ... Patrick Lejoly",Information and Communications Security,2020,"<a href=""Springer (2020) : Machine Learning Based Hardware Trojan Detection Using Electromagnetic Emanation"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-61078-4_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-61078-4_1]</a>",The complexity and outsourcing trend of modern System-on-Chips (SoC) has made Hardware Trojan (HT) a real threat for the SoC security. In the...,,Springer
"Machine Learning – The Results Are Not the only Thing that Matters! What About Security, Explainability and Fairness?","Michał Choraś, Marek Pawlicki, ... Rafał Kozik",Computational Science – ICCS 2020,2020,"<a href=""Springer (2020) : Machine Learning – The Results Are Not the only Thing that Matters! What About Security, Explainability and Fairness?"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-50423-6_46]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-50423-6_46]</a>",Recent advances in machine learning (ML) and the surge in computational power have opened the way to the proliferation of ML and Artificial...,,Springer
Malicious Adware Detection on Android Platform using Dynamic Random Forest,"Kyungmin Lee, Hyunhee Park",Innovative Mobile and Internet Services in Ubiquitous Computing,2020,"<a href=""Springer (2020) : Malicious Adware Detection on Android Platform using Dynamic Random Forest"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-22263-5_57]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-22263-5_57]</a>",As smartphones have evolved into more sophisticated devices that can be used anytime and anywhere and most of the smartphone users save and use all...,,Springer
Malware Detection Using Artificial Neural Networks,"Ivan Dychka, Denys Chernyshev, ... Volodymyr Pogorelov",Advances in Computer Science for Engineering and Education II,2020,"<a href=""Springer (2020) : Malware Detection Using Artificial Neural Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-16621-2_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-16621-2_1]</a>",This paper deals with improvement of malware protection efficiency. The analysis of applied scientific researches devoted to creation of malware...,,Springer
Malware Detection Using Multilevel Ensemble Supervised Learning,"Vidhi Garg, Rajesh Kumar Yadav",Communication and Intelligent Systems,2020,"<a href=""Springer (2020) : Malware Detection Using Multilevel Ensemble Supervised Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-3325-9_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-3325-9_17]</a>","In today’s digitalized world, malware intrudes in the system without the owner’s cognizance with a purpose to damage or steal data is the prime...",,Springer
Man-in-the-browser Attack: A Case Study on Malicious Browser Extensions,Sampsa Rauti,Security in Computing and Communications,2020,"<a href=""Springer (2020) : Man-in-the-browser Attack: A Case Study on Malicious Browser Extensions"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-4825-3_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-4825-3_5]</a>","Man-in-the-browser (MitB) attacks, often implemented as malicious browser extensions, have the ability to alter the structure and contents of web...",,Springer
Model Poisoning Defense on Federated Learning: A Validation Based Approach,"Yuao Wang, Tianqing Zhu, ... Wei Ren",Network and System Security,2020,"<a href=""Springer (2020) : Model Poisoning Defense on Federated Learning: A Validation Based Approach"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-65745-1_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-65745-1_12]</a>",Federated learning is an improved distributed machine learning approach for privacy preservation. All clients collaboratively train the model using...,,Springer
Modelling Medical Devices with Honeypots,"Jouni Ihanus, Tero Kokkonen","Internet of Things, Smart Spaces, and Next Generation Networks and Systems",2020,"<a href=""Springer (2020) : Modelling Medical Devices with Honeypots"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-65726-0_26]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-65726-0_26]</a>",Cyber security is one of the key priorities in the modern digitalised and complex network totality. One of the major domains of interest is the...,,Springer
Multi-Targeted Backdoor: Indentifying Backdoor Attack for Multiple Deep Neural Networks,"Hyun Kwon, Hyunsoo Yoon, Ki-Woong Park",IEICE Trans. Inf. Syst.,2020,"<a href=""DBLP (2020) : Multi-Targeted Backdoor: Indentifying Backdoor Attack for Multiple Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1587/transinf.2019EDL8170]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1587/transinf.2019EDL8170]</a>",,,DBLP
New Encryption Method with Adaptable Computational and Memory Complexity Using Selected Hash Function,"Grzegorz Górski, Mateusz Wojsa",Information Systems Architecture and Technology: Proceedings of 40th Anniversary International Conference on Information Systems Architecture and Technology – ISAT 2019,2020,"<a href=""Springer (2020) : New Encryption Method with Adaptable Computational and Memory Complexity Using Selected Hash Function"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-30440-9_20]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-30440-9_20]</a>","In this paper, we describe the new method of data encryption/decryption with selectable block and key length and hash function. The block size...",,Springer
Novel Scheme for Image Encryption and Decryption Based on a Hermite-Gaussian Matrix,Mohammed Alsaedi,Advances in Computer Vision,2020,"<a href=""Springer (2020) : Novel Scheme for Image Encryption and Decryption Based on a Hermite-Gaussian Matrix"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-17795-9_16]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-17795-9_16]</a>","Media security is an issue of great concern over the internet and during wireless transmissions. In this paper, a novel scheme for image encryption...",,Springer
On the Network Security Prevention Under the Development of the Information Technology,Heping Xi,Data Processing Techniques and Applications for Cyber-Physical Systems (DPTA 2019),2020,"<a href=""Springer (2020) : On the Network Security Prevention Under the Development of the Information Technology"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-1468-5_174]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-1468-5_174]</a>","With the development of the computer technology and the popularization of the network, computer has become an important product in people’s work and...",,Springer
On the Security Relevance of Initial Weights in Deep Neural Networks,"Kathrin Grosse, Thomas A. Trost, ... Dietrich Klakow",Artificial Neural Networks and Machine Learning – ICANN 2020,2020,"<a href=""Springer (2020) : On the Security Relevance of Initial Weights in Deep Neural Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-61609-0_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-61609-0_1]</a>","Recently, a weight-based attack on stochastic gradient descent inducing overfitting has been proposed. We show that the threat is broader: A...",,Springer
Optimizing Deep Learning Based Intrusion Detection Systems Defense Against White-Box and Backdoor Adversarial Attacks Through a Genetic Algorithm,"Khaled Alrawashdeh, Stephen Goldsmith",AIPR,2020,"<a href=""DBLP (2020) : Optimizing Deep Learning Based Intrusion Detection Systems Defense Against White-Box and Backdoor Adversarial Attacks Through a Genetic Algorithm"" target=""_blank"">[https://doi.org/10.1109/AIPR50011.2020.9425293]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/AIPR50011.2020.9425293]</a>",,,DBLP
Overview on Cyber Security Threats Involved in the Implementation of Smart Grid in Countries like India,"Venkatesh Kumar, Prince Inbaraj","Proceeding of the International Conference on Computer Networks, Big Data and IoT (ICCBI - 2018)",2020,"<a href=""Springer (2020) : Overview on Cyber Security Threats Involved in the Implementation of Smart Grid in Countries like India"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-24643-3_81]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-24643-3_81]</a>",Smart grid is an interconnected network of various substations which involves the latest technologies and communication protocols. When a...,,Springer
Overview on Fingerprinting Authentication Technology,"N. Sulaiman, Q. A. Tajul Ariffin",InECCE2019,2020,"<a href=""Springer (2020) : Overview on Fingerprinting Authentication Technology"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-2317-5_38]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-2317-5_38]</a>","This paper addresses the characteristics, technology, and possible future of fingerprints authentication method. Fingerprint physiology makes it an...",,Springer
Perturbing Smart Contract Execution Through the Underlying Runtime,"Pinchen Cui, David Umphress",Security and Privacy in Communication Networks,2020,"<a href=""Springer (2020) : Perturbing Smart Contract Execution Through the Underlying Runtime"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-63095-9_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-63095-9_22]</a>","Because the smart contract is the core element that enables blockchain systems to perform diverse and intelligent operations, the security of smart...",,Springer
Post-exploitation and Persistence Techniques Against Programmable Logic Controller,"Andrei Bytes, Jianying Zhou",Applied Cryptography and Network Security Workshops,2020,"<a href=""Springer (2020) : Post-exploitation and Persistence Techniques Against Programmable Logic Controller"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-61638-0_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-61638-0_15]</a>",The rising appearance of system security threats against real-world Critical Infrastructure (CI) sites over the past years brought significant...,,Springer
Practical Detection of Trojan Neural Networks: Data-Limited and Data-Free Cases,"Ren Wang, Gaoyuan Zhang, ... Meng Wang",Computer Vision – ECCV 2020,2020,"<a href=""Springer (2020) : Practical Detection of Trojan Neural Networks: Data-Limited and Data-Free Cases"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-58592-1_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-58592-1_14]</a>","When the training data are maliciously tampered, the predictions of the acquired deep neural network (DNN) can be manipulated by an adversary known...",,Springer
Practical Poisoning Attacks on Neural Networks,"Junfeng Guo, Cong Liu",Computer Vision – ECCV 2020,2020,"<a href=""Springer (2020) : Practical Poisoning Attacks on Neural Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-58583-9_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-58583-9_9]</a>","Data poisoning attacks on machine learning models have attracted much recent attention, wherein poisoning samples are injected at the training phase...",,Springer
Proposed Solution for HID Fileless Ransomware Using Machine Learning,"Mohamed Amine Kerrich, Adnane Addaim, Loubna Damej",Advanced Communication Systems and Information Security,2020,"<a href=""Springer (2020) : Proposed Solution for HID Fileless Ransomware Using Machine Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-61143-9_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-61143-9_15]</a>",Ransomware is a malware category that asks for payment usually in crypto-currency like Bitcoin after encrypting the files of infected computers. In...,,Springer
Putting Attacks in Context: A Building Automation Testbed for Impact Assessment from the Victim’s Perspective,"Herson Esquivel-Vargas, Marco Caselli, ... Andreas Peter","Detection of Intrusions and Malware, and Vulnerability Assessment",2020,"<a href=""Springer (2020) : Putting Attacks in Context: A Building Automation Testbed for Impact Assessment from the Victim’s Perspective"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-52683-2_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-52683-2_3]</a>",Cybersecurity research relies on the reproducibility and deep understanding of attacks to devise appropriate solutions. Different kinds of testbeds...,,Springer
RF-AdaCost: WebShell Detection Method that Combines Statistical Features and Opcode,"Wenzhuang Kang, Shangping Zhong, ... Guangquan Xu",Frontiers in Cyber Security,2020,"<a href=""Springer (2020) : RF-AdaCost: WebShell Detection Method that Combines Statistical Features and Opcode"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-9739-8_49]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-9739-8_49]</a>","WebShell is called a webpage backdoor. After hackers invade a website, they usually mix backdoor files with normal webpage files in the WEB directory...",,Springer
Re-markable: Stealing Watermarked Neural Networks Through Synthesis,"Nandish Chattopadhyay, Chua Sheng Yang Viroy, Anupam Chattopadhyay","Security, Privacy, and Applied Cryptography Engineering",2020,"<a href=""Springer (2020) : Re-markable: Stealing Watermarked Neural Networks Through Synthesis"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-66626-2_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-66626-2_3]</a>","With the advent of the deep learning paradigm, the state-of-the-art neural network models have achieved unprecedented success in a variety of tasks,...",,Springer
Real Time Analysis of Android Applications by Calculating Risk Factor to Identify Botnet Attack,Sonali Kothari,ICCCE 2019,2020,"<a href=""Springer (2020) : Real Time Analysis of Android Applications by Calculating Risk Factor to Identify Botnet Attack"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-8715-9_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-8715-9_7]</a>","With the rapidly-growing popularity of smartphones, there has been a drastic increase in downloading and sharing of third-party applications and...",,Springer
Research on the Security Analysis and Management of the Network Information System Based on the Big Data Decision Making,Weigang Liu,Cyber Security Intelligence and Analytics,2020,"<a href=""Springer (2020) : Research on the Security Analysis and Management of the Network Information System Based on the Big Data Decision Making"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-15235-2_35]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-15235-2_35]</a>","In the era of the big data, no matter whether it is the government industry or the financial industry or even all walks of life, they are facing the...",,Springer
Reset Attack: An Attack Against Homomorphic Encryption-Based Privacy-Preserving Deep Learning System,"Xiaofeng Wu, Yuanzhi Yao, ... Nenghai Yu",Artificial Intelligence and Security,2020,"<a href=""Springer (2020) : Reset Attack: An Attack Against Homomorphic Encryption-Based Privacy-Preserving Deep Learning System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-8083-3_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-8083-3_3]</a>","In some existing privacy-preserving deep learning systems, additively homomorphic encryption enables ciphertext computation across the gradients....",,Springer
"Review of Digital Data Protection Using the Traditional Methods, Steganography and Cryptography","Chinmaya M. Dharmadhikari, Rejo Mathew","Proceeding of the International Conference on Computer Networks, Big Data and IoT (ICCBI - 2019)",2020,"<a href=""Springer (2020) : Review of Digital Data Protection Using the Traditional Methods, Steganography and Cryptography"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-43192-1_54]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-43192-1_54]</a>","The following paper reviews software protection methods by using the methods of steganography, cryptography to safeguard the running applications on...",,Springer
Risks of the Blockchain Technology,Jinlan Guo,Big Data Analytics for Cyber-Physical System in Smart City,2020,"<a href=""Springer (2020) : Risks of the Blockchain Technology"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-2568-1_266]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-2568-1_266]</a>","This paper comprehensively analyses various risks of the blockchain technology in circulation, including wallet security risks, code vulnerability,...",,Springer
Securing Air-Gapped Systems,"Susmit Sarkar, Aishika Chakraborty, ... Avijit Bose",Proceedings of International Ethical Hacking Conference 2019,2020,"<a href=""Springer (2020) : Securing Air-Gapped Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0361-0_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0361-0_18]</a>","A security measure which involves isolation of a computer or a network in order to prevent it from establishing an external connection, such a...",,Springer
Security Issues Due to Vulnerabilities in the Virtual Machine of Cloud Computing,"Swapnil P. Bhagat, Vikram S. Patil, Bandu B. Meshram",Intelligent Computing and Communication,2020,"<a href=""Springer (2020) : Security Issues Due to Vulnerabilities in the Virtual Machine of Cloud Computing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-1084-7_60]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-1084-7_60]</a>",The virtual machine is a medium for provisioning cloud resources to customers. Cloud customers are accountable for configuration and security of the...,,Springer
Security Problems and Countermeasures of Network Accounting Information System,Yanling Hu,Machine Learning for Cyber Security,2020,"<a href=""Springer (2020) : Security Problems and Countermeasures of Network Accounting Information System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-62223-7_40]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-62223-7_40]</a>","With the rapid development of network technology, the network environment has brought unprecedented opportunities to the development of network...",,Springer
Segmentation Based Backdoor Attack Detection,"Natasha Kees, Yaxuan Wang, Yiling Jiang, Fang Lue, Patrick P. K. Chan",ICMLC,2020,"<a href=""DBLP (2020) : Segmentation Based Backdoor Attack Detection"" target=""_blank"">[https://doi.org/10.1109/ICMLC51923.2020.9469037]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICMLC51923.2020.9469037]</a>",,,DBLP
Selective Forwarding Attack on IoT Home Security Kits,"Ali Hariri, Nicolas Giannelos, Budi Arief",Computer Security,2020,"<a href=""Springer (2020) : Selective Forwarding Attack on IoT Home Security Kits"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-42048-2_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-42048-2_23]</a>","Efforts have been made to improve the security of the Internet of Things (IoT) devices, but there remain some vulnerabilities and misimplementations....",,Springer
Static and Dynamic Malware Analysis Using Machine Learning,"Chandni Raghuraman, Sandhya Suresh, ... Radhika Chapaneri",First International Conference on Sustainable Technologies for Computational Intelligence,2020,"<a href=""Springer (2020) : Static and Dynamic Malware Analysis Using Machine Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0029-9_62]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0029-9_62]</a>",Malware is a section of code written with the intention of harming a device. Attacks on the Android operating system have been on the rise of late as...,,Springer
Statistical Analysis of the UNSW-NB15 Dataset for Intrusion Detection,"Vikash Kumar, Ayan Kumar Das, Ditipriya Sinha",Computational Intelligence in Pattern Recognition,2020,"<a href=""Springer (2020) : Statistical Analysis of the UNSW-NB15 Dataset for Intrusion Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-9042-5_24]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-9042-5_24]</a>",Intrusion Detection System (IDS) has been developed to protect the resources in the network from different types of threats. Existing IDS methods can...,,Springer
Stronger Targeted Poisoning Attacks Against Malware Detection,"Shintaro Narisada, Shoichiro Sasaki, ... Shinsaku Kiyomoto",Cryptology and Network Security,2020,"<a href=""Springer (2020) : Stronger Targeted Poisoning Attacks Against Malware Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-65411-5_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-65411-5_4]</a>",Attacks on machine learning systems such as malware detectors and recommendation systems are becoming a major threat. Data poisoning attacks are the...,,Springer
Survey on Detection and Prediction Techniques of Drive-by Download Attack in OSN,"Madhura Vyawahare, Madhumita Chatterjee",Advanced Computing Technologies and Applications,2020,"<a href=""Springer (2020) : Survey on Detection and Prediction Techniques of Drive-by Download Attack in OSN"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-3242-9_42]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-3242-9_42]</a>",The increasing crime rate in the technology oriented and networked world needs attention to prevent the damage caused by these crimes. Most dangerous...,,Springer
Towards Inspecting and Eliminating Trojan Backdoors in Deep Neural Networks,"Wenbo Guo, Lun Wang, Yan Xu, Xinyu Xing, Min Du, Dawn Song",ICDM,2020,"<a href=""DBLP (2020) : Towards Inspecting and Eliminating Trojan Backdoors in Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1109/ICDM50108.2020.00025]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICDM50108.2020.00025]</a>",,,DBLP
Towards Protection Against a USB Device Whose Firmware Has Been Compromised or Turned as ‘BadUSB’,"Usman Shafique, Shorahbeel Bin Zahur",Advances in Information and Communication,2020,"<a href=""Springer (2020) : Towards Protection Against a USB Device Whose Firmware Has Been Compromised or Turned as ‘BadUSB’"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-12385-7_66]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-12385-7_66]</a>",A BadUSB is a Universal Serial Bus (USB) device (usually a mass storage device) whose firmware has been modified so as to spoof itself as another...,,Springer
TrojDRL: Evaluation of Backdoor Attacks on Deep Reinforcement Learning,"Panagiota Kiourti, Kacper Wardega, Susmit Jha, Wenchao Li",DAC,2020,"<a href=""DBLP (2020) : TrojDRL: Evaluation of Backdoor Attacks on Deep Reinforcement Learning"" target=""_blank"">[https://doi.org/10.1109/DAC18072.2020.9218663]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/DAC18072.2020.9218663]</a>",,,DBLP
Trust Based Intrusion Detection System to Detect Insider Attacks in IoT Systems,"K. N. Ambili, Jimmy Jose",Information Science and Applications,2020,"<a href=""Springer (2020) : Trust Based Intrusion Detection System to Detect Insider Attacks in IoT Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-1465-4_62]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-1465-4_62]</a>",IoT systems are vulnerable to various cyber attacks as they form a subset of the Internet. Insider attacks find more significance since many devices...,,Springer
UltraComm: High-Speed and Inaudible Acoustic Communication,"Guoming Zhang, Xiaoyu Ji, ... Wenyuan Xu","Quality, Reliability, Security and Robustness in Heterogeneous Systems",2020,"<a href=""Springer (2020) : UltraComm: High-Speed and Inaudible Acoustic Communication"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-38819-5_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-38819-5_12]</a>",Acoustic communication has become a research focus without requiring extra hardware on the receiver side and facilitates numerous near-field...,,Springer
Understanding the Security Risks of Docker Hub,"Peiyu Liu, Shouling Ji, ... Raheem Beyah",Computer Security – ESORICS 2020,2020,"<a href=""Springer (2020) : Understanding the Security Risks of Docker Hub"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-58951-6_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-58951-6_13]</a>",Docker has become increasingly popular because it provides efficient containers that are directly run by the host kernel. Docker Hub is one of the...,,Springer
Detection of Malicious Network Traffic using Convolutional Neural Networks,R. Chapaneri S. Shah,"2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",2019-12-30,"<a href=""IEEE (2019-12-30) : Detection of Malicious Network Traffic using Convolutional Neural Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8944814]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICCCNT45670.2019.8944814]</a>","Networks are typically exposed to various attacks such as Denial of Service, Shellcode, and Fuzzers with increasing connectivity of users and organizations. It is crucial to detect such malicious network traffic to alleviate their impact on organizations and provide security administrators with automatic alerts. While conventional machine learning classifiers can solve the binary classification problem of network traffic being normal or anomalous, they do not perform well when detecting multiple attack categories. This work addresses the issue of malicious network traffic detection using deep convolutional neural network architectures on the modern complex and challenging UNSW-NB15 dataset. The proposed work shows a significant improvement relative to existing techniques even for the difficult attack categories of Analysis, Backdoor, Shellcode, and Worms.",,IEEE
Hidden Trigger Backdoor Attacks,"Aniruddha Saha, Akshayvarun Subramanya, Hamed Pirsiavash","arXiv
AAAI
arXiv","2019-12-21
2020
2019-10","<a href=""arXiv (2019-12-21) : Hidden Trigger Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/1910.00033v2]</a>
<a href=""DBLP (2020) : Hidden Trigger Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1609/aaai.v34i07.6871]</a>
<a href=""DBLP (2019-10) : Hidden Trigger Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/1910.00033]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v34i07.6871]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1910.00033]</a>","With the success of deep learning algorithms in various domains, studying adversarial attacks to secure deep models in real world applications has become an important research topic. Backdoor attacks are a form of adversarial attacks on deep networks where the attacker provides poisoned data to the victim to train the model with, and then activates the attack by showing a specific small trigger pattern at the test time. Most state-of-the-art backdoor attacks either provide mislabeled poisoning data that is possible to identify by visual inspection, reveal the trigger in the poisoned data, or use noise to hide the trigger. We propose a novel form of backdoor attack where poisoned data look natural with correct labels and also more importantly, the attacker hides the trigger in the poisoned data and keeps the trigger secret until the test time. We perform an extensive study on various image classification settings and show that our attack can fool the model by pasting the trigger at random locations on unseen images although the model performs well on clean data. We also show that our proposed attack cannot be easily defended using a state-of-the-art defense algorithm for backdoor attacks.

","

","arXiv
DBLP
DBLP"
Trojan Attacks on Wireless Signal Classification with Adversarial Machine Learning,K. Davaslioglu Y. E. Sagduyu,2019 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN),2019-12-19,"<a href=""IEEE (2019-12-19) : Trojan Attacks on Wireless Signal Classification with Adversarial Machine Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8935782]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/DySPAN.2019.8935782]</a>","We present a Trojan (backdoor or trapdoor) attack that targets deep learning applications in wireless communications. A deep learning classifier is considered to classify wireless signals using raw (I/Q) samples as features and modulation types as labels. An adversary slightly manipulates training data by inserting Trojans (i.e., triggers) to only few training data samples by modifying their phases and changing the labels of these samples to a target label. This poisoned training data is used to train the deep learning classifier. In test (inference) time, an adversary transmits signals with the same phase shift that was added as a trigger during training. While the receiver can accurately classify clean (unpoisoned) signals without triggers, it cannot reliably classify signals poisoned with triggers. This stealth attack remains hidden until activated by poisoned inputs (Trojans) to bypass a signal classifier (e.g., for authentication). We show that this attack is successful over different channel conditions and cannot be mitigated by simply preprocessing the training and test data with random phase variations. To detect this attack, activation based outlier detection is considered with statistical as well as clustering techniques. We show that the latter one can detect Trojan attacks even if few samples are poisoned.",,IEEE
Malware visualization methods based on deep convolution neural networks,"Zhuojun Ren, Guang Chen, Wenke Lu",Multimedia Tools and Applications,2019-12-16,"<a href=""Springer (2019-12-16) : Malware visualization methods based on deep convolution neural networks"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-019-08310-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-019-08310-9]</a>","In this paper, we propose two visualization methods for malware analysis based on n-gram features of byte sequences. The space filling curve mapping...",,Springer
A new approach for intrusion detection system based on training multilayer perceptron by using enhanced Bat algorithm,"Waheed A. H. M. Ghanem, Aman Jantan",Neural Computing and Applications,2019-12-10,"<a href=""Springer (2019-12-10) : A new approach for intrusion detection system based on training multilayer perceptron by using enhanced Bat algorithm"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-019-04655-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-019-04655-2]</a>",The most pressing issue in network security is the establishment of an approach that is capable of detecting violations in computer systems and...,,Springer
StriP: A defence against trojan attacks on deep neural networks,Gao Y.,ACM International Conference Proceeding Series,2019-12-09,"<a href=""ScienceDirect (2019-12-09) : StriP: A defence against trojan attacks on deep neural networks"" target=""_blank"">[https://doi.org/10.1145/3359789.3359790]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3359789.3359790]</a>",,,ScienceDirect
Walling up backdoors in intrusion detection systems,Bachl M.,"Big-DAMA 2019 - Proceedings of the 3rd ACM CoNEXT Workshop on Big DAta, Machine Learning and Artificial Intelligence for Data Communication Networks, Part of CoNEXT 2019",2019-12-09,"<a href=""ScienceDirect (2019-12-09) : Walling up backdoors in intrusion detection systems"" target=""_blank"">[https://doi.org/10.1145/3359992.3366638]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3359992.3366638]</a>",,,ScienceDirect
Label-Consistent Backdoor Attacks,"Alexander Turner, Dimitris Tsipras, Aleksander Madry","arXiv
arXiv","2019-12-06
2019-12","<a href=""arXiv (2019-12-06) : Label-Consistent Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/1912.02771v2]</a>
<a href=""DBLP (2019-12) : Label-Consistent Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/1912.02771]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1912.02771]</a>","Deep neural networks have been demonstrated to be vulnerable to backdoor attacks. Specifically, by injecting a small number of maliciously constructed inputs into the training set, an adversary is able to plant a backdoor into the trained model. This backdoor can then be activated during inference by a backdoor trigger to fully control the model's behavior. While such attacks are very effective, they crucially rely on the adversary injecting arbitrary inputs that are---often blatantly---mislabeled. Such samples would raise suspicion upon human inspection, potentially revealing the attack. Thus, for backdoor attacks to remain undetected, it is crucial that they maintain label-consistency---the condition that injected inputs are consistent with their labels. In this work, we leverage adversarial perturbations and generative models to execute efficient, yet label-consistent, backdoor attacks. Our approach is based on injecting inputs that appear plausible, yet are hard to classify, hence causing the model to rely on the (easier-to-learn) backdoor trigger.
","
","arXiv
DBLP"
A Detection Method Based on K-Cores Algorithm for Abnormal Processes in the Server,J. Du F. Ao Y. Zhao Y. Liu,2019 IEEE Fourth International Conference on Data Science in Cyberspace (DSC),2019-12-05,"<a href=""IEEE (2019-12-05) : A Detection Method Based on K-Cores Algorithm for Abnormal Processes in the Server"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8923797]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/DSC.2019.00088]</a>","The server is an important node that provides data processing and service in the network, and its safe operation is critical. In order to prevent malicious attacks, it is necessary to discover malicious processes in the server as early as possible. This paper proposes an innovative method of the process whitelist based on the K-Cores graph algorithm, and takes this as a baseline to detect abnormal processes in the server. This method transforms the relationship between the server and the process into a topology map, and converts the problem of legal process analysis into the problem of the node's importance analysis in the network graph, and then uses the K-Cores algorithm to group the nodes according to the importance. In the network diagram the process corresponding to the node full of importance is written to the whitelist as a legal process. This paper uses an unsupervised method to detect abnormal processes running on the server, remove security risks such as backdoor processes, which effectively improves the security performance of the server.",,IEEE
V-PSC: A Perturbation-Based Causative Attack Against DL Classifiers' Supply Chain in VANET,Y. Zeng M. Qiu J. Niu Y. Long J. Xiong M. Liu,2019 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC),2019-12-05,"<a href=""IEEE (2019-12-05) : V-PSC: A Perturbation-Based Causative Attack Against DL Classifiers' Supply Chain in VANET"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919585]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/CSE/EUC.2019.00026]</a>","DL Based classifiers can attain a higher accuracy with less storage requirement, which suits perfectly with the VANET. However, it has been proved that DL models suffer from crafted perturbation data, a small amount of such can misguide the classifier, thus backdoors can be created for malicious reasons. This paper studies such a causative attack in the VANET. We present a perturbation-based causative attack which targets at the supply chain of DL classifiers in the VANET. We first train a classifier using VANET simulated data which meets the standard accuracy for identifying malicious traffic in the VANET. Then, we elaborate on the effectiveness of our presented attack scheme on this pre-trained classifier. We also explore some feasible approaches to ease the outcome brought by our attack. Experimental results show that the scheme can cause the target DL model a 10.52% drop in accuracy.",,IEEE
A context-aware robust intrusion detection system: a reinforcement learning-based approach,"Kamalakanta Sethi, E. Sai Rupesh, ... Y. Venu Madhav",International Journal of Information Security,2019-12-03,"<a href=""Springer (2019-12-03) : A context-aware robust intrusion detection system: a reinforcement learning-based approach"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-019-00482-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-019-00482-7]</a>","Detection and prevention of intrusions in enterprise networks and systems is an important, but challenging problem due to extensive growth and usage...",,Springer
Can You Really Backdoor Federated Learning?,"Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, H. Brendan McMahan","arXiv
arXiv","2019-12-02
2019-11","<a href=""arXiv (2019-12-02) : Can You Really Backdoor Federated Learning?"" target=""_blank"">[http://arxiv.org/abs/1911.07963v2]</a>
<a href=""DBLP (2019-11) : Can You Really Backdoor Federated Learning?"" target=""_blank"">[http://arxiv.org/abs/1911.07963]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1911.07963]</a>","The decentralized nature of federated learning makes detecting and defending against adversarial attacks a challenging task. This paper focuses on backdoor attacks in the federated learning setting, where the goal of the adversary is to reduce the performance of the model on targeted tasks while maintaining good performance on the main task. Unlike existing works, we allow non-malicious clients to have correctly labeled samples from the targeted tasks. We conduct a comprehensive study of backdoor attacks and defenses for the EMNIST dataset, a real-life, user-partitioned, and non-iid dataset. We observe that in the absence of defenses, the performance of the attack largely depends on the fraction of adversaries present and the ""complexity'' of the targeted task. Moreover, we show that norm clipping and ""weak'' differential privacy mitigate the attacks without hurting the overall performance. We have implemented the attacks and defenses in TensorFlow Federated (TFF), a TensorFlow framework for federated learning. In open-sourcing our code, our goal is to encourage researchers to contribute new attacks and defenses and evaluate them on standard federated datasets.
","
","arXiv
DBLP"
Walling up Backdoors in Intrusion Detection Systems,"Maximilian Bachl, Alexander Hartl, Joachim Fabini, Tanja Zseby","Big-DAMA '19: Proceedings of the 3rd ACM CoNEXT Workshop on Big DAta, Machine Learning and Artificial Intelligence for Data Communication Networks
arXiv
Big-DAMA@CoNEXT
arXiv","2019-12
2020-04-05
2019
2019-09","<a href=""ACM (2019-12) : Walling up Backdoors in Intrusion Detection Systems"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3359992.3366638]</a>
<a href=""arXiv (2020-04-05) : Walling up Backdoors in Intrusion Detection Systems"" target=""_blank"">[http://arxiv.org/abs/1909.07866v3]</a>
<a href=""DBLP (2019) : Walling up Backdoors in Intrusion Detection Systems"" target=""_blank"">[https://doi.org/10.1145/3359992.3366638]</a>
<a href=""DBLP (2019-09) : Walling up Backdoors in Intrusion Detection Systems"" target=""_blank"">[http://arxiv.org/abs/1909.07866]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3359992.3366638]</a>
<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1145/3359992.3366638]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3359992.3366638]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1909.07866]</a>","Interest in poisoning attacks and backdoors recently resurfaced for Deep Learning (DL) applications. Several successful defense mechanisms have been recently proposed for Convolutional Neural Networks (CNNs), for example in the context of autonomous ...
Interest in poisoning attacks and backdoors recently resurfaced for Deep Learning (DL) applications. Several successful defense mechanisms have been recently proposed for Convolutional Neural Networks (CNNs), for example in the context of autonomous driving. We show that visualization approaches can aid in identifying a backdoor independent of the used classifier. Surprisingly, we find that common defense mechanisms fail utterly to remove backdoors in DL for Intrusion Detection Systems (IDSs). Finally, we devise pruning-based approaches to remove backdoors for Decision Trees (DTs) and Random Forests (RFs) and demonstrate their effectiveness for two different network security datasets.

","


","ACM
arXiv
DBLP
DBLP"
STRIP: a defence against trojan attacks on deep neural networks,"Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C. Ranasinghe, Surya Nepal",ACSAC '19: Proceedings of the 35th Annual Computer Security Applications Conference,2019-12,"<a href=""ACM (2019-12) : STRIP: a defence against trojan attacks on deep neural networks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3359789.3359790]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3359789.3359790]</a>",A recent trojan attack on deep neural network (DNN) models is one insidious variant of data poisoning attacks. Trojan attacks exploit an effective backdoor created in a DNN model by leveraging the difficulty in interpretability of the learned model to ...,,ACM
On the Robustness of the Backdoor-based Watermarking in Deep Neural Networks,"Masoumeh Shafieinejad, Jiaqi Wang, Nils Lukas, Xinda Li, Florian Kerschbaum","arXiv
arXiv","2019-11-26
2019-06","<a href=""arXiv (2019-11-26) : On the Robustness of the Backdoor-based Watermarking in Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/1906.07745v2]</a>
<a href=""DBLP (2019-06) : On the Robustness of the Backdoor-based Watermarking in Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/1906.07745]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1906.07745]</a>","Obtaining the state of the art performance of deep learning models imposes a high cost to model generators, due to the tedious data preparation and the substantial processing requirements. To protect the model from unauthorized re-distribution, watermarking approaches have been introduced in the past couple of years. We investigate the robustness and reliability of state-of-the-art deep neural network watermarking schemes. We focus on backdoor-based watermarking and propose two -- a black-box and a white-box -- attacks that remove the watermark. Our black-box attack steals the model and removes the watermark with minimum requirements, it just relies on public unlabeled data and a black-box access to the classification label. It does not need classification confidences or access to the model's sensitive information such as the training data set, the trigger set or the model parameters. The white-box attack, proposes an efficient watermark removal when the parameters of the marked model are available, our white-box attack does not require access to the labeled data or the trigger set and improves the runtime of the black-box attack up to seventeen times. We as well prove the security inadequacy of the backdoor-based watermarking in keeping the watermark undetectable by proposing an attack that detects whether a model contains a watermark. Our attacks show that a recipient of a marked model can remove a backdoor-based watermark with significantly less effort than training a new model and some other techniques are needed to protect against re-distribution by a motivated attacker.
","
","arXiv
DBLP"
Chip-to-Chip Authentication Method Based on SRAM PUF and Public Key Cryptography,"Ioannis Karageorgos, Mehmet M. Isgenc, ... Larry Pileggi",Journal of Hardware and Systems Security,2019-11-26,"<a href=""Springer (2019-11-26) : Chip-to-Chip Authentication Method Based on SRAM PUF and Public Key Cryptography"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-019-00080-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-019-00080-y]</a>","In today’s globalized integrated circuit (IC) ecosystem, untrusted foundries are often procured to build critical systems since they offer...",,Springer
A Decentralized Multi-ruling Arbiter for Cyberspace Mimicry Defense,C. Shen S. -X. Chen C. -M. Wu,"2019 International Symposium on Networks, Computers and Communications (ISNCC)",2019-11-21,"<a href=""IEEE (2019-11-21) : A Decentralized Multi-ruling Arbiter for Cyberspace Mimicry Defense"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8909197]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ISNCC.2019.8909197]</a>","Cyberspace Mimicry Defense (CMD) has been widely used to achieve intrusion prevention against unknown system vulnerabilities or backdoors. The multi-ruling arbiter is a key part in CMD. This paper focuses on the problem of multi-ruling arbiter under data injection attack from the perspective of attacker and defender. We build a decentralized multi-ruling arbiter model for arbitration and introduced a standard iteration process to achieve consensus without attackers. We describe two data injection attack models for decentralized multi-ruling arbiter, namely random data injection attack and stealthy data injection attack. Further, we characterize the negative effect of the data injection attack on the performance of multi-ruling correctness. In order to mitigate the negative effect of random data injection attack, we propose a reliable multi-ruling arbitration approach based on adaptive threshold. By cutting future communication with the malicious neighbor, the decentralized multi-ruling arbiter is robust against random data injection attacks. Simulation results show that the proposed arbitration approach can effectively defend against random data injection attacks.",,IEEE
Performance evaluation of Botnet DDoS attack detection using machine learning,"Tong Anh Tuan, Hoang Viet Long, ... Nguyen Thi Kim Son",Evolutionary Intelligence,2019-11-20,"<a href=""Springer (2019-11-20) : Performance evaluation of Botnet DDoS attack detection using machine learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s12065-019-00310-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12065-019-00310-w]</a>",Botnet is regarded as one of the most sophisticated vulnerability threats nowadays. A large portion of network traffic is dominated by Botnets....,,Springer
Poison as a Cure: Detecting & Neutralizing Variable-Sized Backdoor Attacks in Deep Neural Networks,"Alvin Chan, Yew-Soon Ong","arXiv
arXiv","2019-11-19
2019-11","<a href=""arXiv (2019-11-19) : Poison as a Cure: Detecting & Neutralizing Variable-Sized Backdoor Attacks in Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/1911.08040v1]</a>
<a href=""DBLP (2019-11) : Poison as a Cure: Detecting &amp, Neutralizing Variable-Sized Backdoor Attacks in Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/1911.08040]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1911.08040]</a>","Deep learning models have recently shown to be vulnerable to backdoor poisoning, an insidious attack where the victim model predicts clean images correctly but classifies the same images as the target class when a trigger poison pattern is added. This poison pattern can be embedded in the training dataset by the adversary. Existing defenses are effective under certain conditions such as a small size of the poison pattern, knowledge about the ratio of poisoned training samples or when a validated clean dataset is available. Since a defender may not have such prior knowledge or resources, we propose a defense against backdoor poisoning that is effective even when those prerequisites are not met. It is made up of several parts: one to extract a backdoor poison signal, detect poison target and base classes, and filter out poisoned from clean samples with proven guarantees. The final part of our defense involves retraining the poisoned model on a dataset augmented with the extracted poison signal and corrective relabeling of poisoned samples to neutralize the backdoor. Our approach has shown to be effective in defending against backdoor attacks that use both small and large-sized poison patterns on nine different target-base class pairs from the CIFAR10 dataset.
","
","arXiv
DBLP"
NeuronInspect: Detecting Backdoors in Neural Networks via Output Explanations,"Xijie Huang, Moustafa Alzantot, Mani Srivastava","arXiv
arXiv","2019-11-18
2019-11","<a href=""arXiv (2019-11-18) : NeuronInspect: Detecting Backdoors in Neural Networks via Output Explanations"" target=""_blank"">[http://arxiv.org/abs/1911.07399v1]</a>
<a href=""DBLP (2019-11) : NeuronInspect: Detecting Backdoors in Neural Networks via Output Explanations"" target=""_blank"">[http://arxiv.org/abs/1911.07399]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1911.07399]</a>","Deep neural networks have achieved state-of-the-art performance on various tasks. However, lack of interpretability and transparency makes it easier for malicious attackers to inject trojan backdoor into the neural networks, which will make the model behave abnormally when a backdoor sample with a specific trigger is input. In this paper, we propose NeuronInspect, a framework to detect trojan backdoors in deep neural networks via output explanation techniques. NeuronInspect first identifies the existence of backdoor attack targets by generating the explanation heatmap of the output layer. We observe that generated heatmaps from clean and backdoored models have different characteristics. Therefore we extract features that measure the attributes of explanations from an attacked model namely: sparse, smooth and persistent. We combine these features and use outlier detection to figure out the outliers, which is the set of attack targets. We demonstrate the effectiveness and efficiency of NeuronInspect on MNIST digit recognition dataset and GTSRB traffic sign recognition dataset. We extensively evaluate NeuronInspect on different attack scenarios and prove better robustness and effectiveness over state-of-the-art trojan backdoor detection techniques Neural Cleanse by a great margin.
","
","arXiv
DBLP"
Robust Anomaly Detection and Backdoor Attack Detection Via Differential Privacy,"Min Du, Ruoxi Jia, Dawn Song","arXiv
arXiv","2019-11-16
2019-11","<a href=""arXiv (2019-11-16) : Robust Anomaly Detection and Backdoor Attack Detection Via Differential Privacy"" target=""_blank"">[http://arxiv.org/abs/1911.07116v1]</a>
<a href=""DBLP (2019-11) : Robust Anomaly Detection and Backdoor Attack Detection Via Differential Privacy"" target=""_blank"">[http://arxiv.org/abs/1911.07116]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1911.07116]</a>","Outlier detection and novelty detection are two important topics for anomaly detection. Suppose the majority of a dataset are drawn from a certain distribution, outlier detection and novelty detection both aim to detect data samples that do not fit the distribution. Outliers refer to data samples within this dataset, while novelties refer to new samples. In the meantime, backdoor poisoning attacks for machine learning models are achieved through injecting poisoning samples into the training dataset, which could be regarded as ""outliers"" that are intentionally added by attackers. Differential privacy has been proposed to avoid leaking any individual's information, when aggregated analysis is performed on a given dataset. It is typically achieved by adding random noise, either directly to the input dataset, or to intermediate results of the aggregation mechanism. In this paper, we demonstrate that applying differential privacy can improve the utility of outlier detection and novelty detection, with an extension to detect poisoning samples in backdoor attacks. We first present a theoretical analysis on how differential privacy helps with the detection, and then conduct extensive experiments to validate the effectiveness of differential privacy in improving outlier detection, novelty detection, and backdoor attack detection.
","
","arXiv
DBLP"
Recursive Orthogonal Diffusion for Deeper Encryption of Color Images,N. Chidambaram A. S. Reddy K. Jagadeesh Sai Dheeraj K. V. Varma K. Sai Harshavardhan Reddy R. Amirtharajan,2019 International Conference on Vision Towards Emerging Trends in Communication and Networking (ViTECoN),2019-11-14,"<a href=""IEEE (2019-11-14) : Recursive Orthogonal Diffusion for Deeper Encryption of Color Images"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899423]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ViTECoN.2019.8899423]</a>","Since the advent of the internet, we have got to witness various new inventions and advancements in the field of cyber security and privacy. Earlier, images which were difficult to be sent to the receiver now can be sent at the tap of a button. Hence, in this paper, an encryption algorithm for a colour image using DNA encoding and the chaotic sequence is proposed. As for images, the correlation is the major constraint which gives backdoor for the attackers. To weaken the correlation an adaptive diffusion algorithm using a tent map random number generator is proposed in the paper. These random numbers are used for selecting a reference row and column to carry out the diffusion process, chronologically. The process is carried out recursively until the required correlation is achieved. The suitability of the algorithm against statistical attack is validated through some metric analysis.",,IEEE
Trusting artificial intelligence in cybersecurity is a double-edged sword,"Mariarosaria Taddeo, Tom McCutcheon, Luciano Floridi",Nature Machine Intelligence,2019-11-11,"<a href=""Springer (2019-11-11) : Trusting artificial intelligence in cybersecurity is a double-edged sword"" target=""_blank"">[https://link.springer.com/article/10.1038/s42256-019-0109-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1038/s42256-019-0109-1]</a>",,,Springer
Defending Neural Backdoors via Generative Distribution Modeling,"Ximing Qiao, Yukun Yang, Hai Li","arXiv
NeurIPS
arXiv","2019-11-06
2019
2019-10","<a href=""arXiv (2019-11-06) : Defending Neural Backdoors via Generative Distribution Modeling"" target=""_blank"">[http://arxiv.org/abs/1910.04749v2]</a>
<a href=""DBLP (2019) : Defending Neural Backdoors via Generative Distribution Modeling"" target=""_blank"">[https://proceedings.neurips.cc/paper/2019/hash/78211247db84d96acf4e00092a7fba80-Abstract.html]</a>
<a href=""DBLP (2019-10) : Defending Neural Backdoors via Generative Distribution Modeling"" target=""_blank"">[http://arxiv.org/abs/1910.04749]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://proceedings.neurips.cc/paper/2019/hash/78211247db84d96acf4e00092a7fba80-Abstract.html]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1910.04749]</a>","Neural backdoor attack is emerging as a severe security threat to deep learning, while the capability of existing defense methods is limited, especially for complex backdoor triggers. In the work, we explore the space formed by the pixel values of all possible backdoor triggers. An original trigger used by an attacker to build the backdoored model represents only a point in the space. It then will be generalized into a distribution of valid triggers, all of which can influence the backdoored model. Thus, previous methods that model only one point of the trigger distribution is not sufficient. Getting the entire trigger distribution, e.g., via generative modeling, is a key to effective defense. However, existing generative modeling techniques for image generation are not applicable to the backdoor scenario as the trigger distribution is completely unknown. In this work, we propose max-entropy staircase approximator (MESA), an algorithm for high-dimensional sampling-free generative modeling and use it to recover the trigger distribution. We also develop a defense technique to remove the triggers from the backdoored model. Our experiments on Cifar10/100 dataset demonstrate the effectiveness of MESA in modeling the trigger distribution and the robustness of the proposed defense method.

","

","arXiv
DBLP
DBLP"
Latent backdoor attacks on deep neural networks,Yao Y.,Proceedings of the ACM Conference on Computer and Communications Security,2019-11-06,"<a href=""ScienceDirect (2019-11-06) : Latent backdoor attacks on deep neural networks"" target=""_blank"">[https://doi.org/10.1145/3319535.3354209]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3319535.3354209]</a>",,,ScienceDirect
"Blockchain in IoT: Current Trends, Challenges, and Future Roadmap","Pinchen Cui, Ujjwal Guin, ... David Umphress",Journal of Hardware and Systems Security,2019-11-04,"<a href=""Springer (2019-11-04) : Blockchain in IoT: Current Trends, Challenges, and Future Roadmap"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-019-00079-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-019-00079-5]</a>",The Internet of Things (IoT) is one of the most promising technologies in the era of information technology. IoT enables ubiquitous data collections...,,Springer
AI and Automotive Safety,"Clemens Wasner, Johannes Traxler",ATZelectronics worldwide,2019-11-01,"<a href=""Springer (2019-11-01) : AI and Automotive Safety"" target=""_blank"">[https://link.springer.com/article/10.1007/s38314-019-0128-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s38314-019-0128-z]</a>",,,Springer
Cybersecurity profiles based on human-centric IoT devices,"Ana Nieto, Ruben Rios",Human-centric Computing and Information Sciences,2019-11-01,"<a href=""Springer (2019-11-01) : Cybersecurity profiles based on human-centric IoT devices"" target=""_blank"">[https://link.springer.com/article/10.1186/s13673-019-0200-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13673-019-0200-y]</a>",This paper proposes a methodology based on the concept of Human Factors to obtain Cybersecurity profiles. The profiles are determined by a set of...,,Springer
Detection of malware attacks in smart phones using machine learning,Niveditha V.R.,International Journal of Innovative Technology and Exploring Engineering,2019-11-01,"<a href=""ScienceDirect (2019-11-01) : Detection of malware attacks in smart phones using machine learning"" target=""_blank"">[https://doi.org/10.35940/ijitee.A5082.119119]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.35940/ijitee.A5082.119119]</a>",,,ScienceDirect
Latent Backdoor Attacks on Deep Neural Networks,"Yuanshun Yao, Huiying Li, Haitao Zheng, Ben Y. Zhao","CCS '19: Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security
CCS","2019-11
2019","<a href=""ACM (2019-11) : Latent Backdoor Attacks on Deep Neural Networks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3319535.3354209]</a>
<a href=""DBLP (2019) : Latent Backdoor Attacks on Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1145/3319535.3354209]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3319535.3354209]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3319535.3354209]</a>","Recent work proposed the concept of backdoor attacks on deep neural networks (DNNs), where misclassification rules are hidden inside normal models, only to be triggered by very specific inputs. However, these ""traditional"" backdoors assume a context ...
","
","ACM
DBLP"
Achieving Consistency of Software Updates against Strong Attackers,"Lamya Abdullah, Sebastian Hahn, Felix Freiling",CECC 2019: Proceedings of the Third Central European Cybersecurity Conference,2019-11,"<a href=""ACM (2019-11) : Achieving Consistency of Software Updates against Strong Attackers"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3360664.3360670]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3360664.3360670]</a>",Update systems regularly distribute updates for installed software to end users. Problems arise when the update system is misused and malicious updates are sent to a small set of users only. Such situations can occur if the software supplier has been ...,,ACM
The Tale of Evil Twins: Adversarial Inputs versus Backdoored Models,"Ren Pang, Xinyang Zhang, Shouling Ji, Yevgeniy Vorobeychik, Xiapu Luo, Ting Wang",arXiv,2019-11,"<a href=""DBLP (2019-11) : The Tale of Evil Twins: Adversarial Inputs versus Backdoored Models"" target=""_blank"">[http://arxiv.org/abs/1911.01559]</a>","<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1911.01559]</a>",,,DBLP
An integrated rule based intrusion detection system: analysis on UNSW-NB15 data set and the real time online dataset,"Vikash Kumar, Ditipriya Sinha, ... Radha Tamal Goswami",Cluster Computing,2019-10-29,"<a href=""Springer (2019-10-29) : An integrated rule based intrusion detection system: analysis on UNSW-NB15 data set and the real time online dataset"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-019-03008-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-019-03008-x]</a>",Intrusion detection system (IDS) has been developed to protect the resources in the network from different types of threats. Existing IDS methods can...,,Springer
Threats on the horizon: understanding security threats in the era of cyber-physical systems,"Steven Walker-Roberts, Mohammad Hammoudeh, ... Ali Dehghantanha",The Journal of Supercomputing,2019-10-24,"<a href=""Springer (2019-10-24) : Threats on the horizon: understanding security threats in the era of cyber-physical systems"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-019-03028-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-019-03028-9]</a>","Disruptive innovations of the last few decades, such as smart cities and Industry 4.0, were made possible by higher integration of physical and...",,Springer
"Multi-layer intrusion detection system with ExtraTrees feature selection, extreme learning machine ensemble, and softmax aggregation","Jivitesh Sharma, Charul Giri, ... Morten Goodwin",EURASIP Journal on Information Security,2019-10-22,"<a href=""Springer (2019-10-22) : Multi-layer intrusion detection system with ExtraTrees feature selection, extreme learning machine ensemble, and softmax aggregation"" target=""_blank"">[https://link.springer.com/article/10.1186/s13635-019-0098-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13635-019-0098-y]</a>","Recent advances in intrusion detection systems based on machine learning have indeed outperformed other techniques, but struggle with detecting...",,Springer
Chips under the microscope,,,2019-10-15,"<a href=""Springer (2019-10-15) : Chips under the microscope"" target=""_blank"">[https://link.springer.com/article/10.1038/s41928-019-0325-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1038/s41928-019-0325-z]</a>",in Nature Electronics,,Springer
Securing cryptographic chips against scan-based attacks in wireless sensor network applications,Wang W.,Sensors (Switzerland),2019-10-02,"<a href=""ScienceDirect (2019-10-02) : Securing cryptographic chips against scan-based attacks in wireless sensor network applications"" target=""_blank"">[https://doi.org/10.3390/s19204598]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/s19204598]</a>",,,ScienceDirect
A Benchmark Study of Backdoor Data Poisoning Defenses for Deep Neural Network Classifiers and A Novel Defense,Xiang Z.,"IEEE International Workshop on Machine Learning for Signal Processing, MLSP",2019-10-01,"<a href=""ScienceDirect (2019-10-01) : A Benchmark Study of Backdoor Data Poisoning Defenses for Deep Neural Network Classifiers and A Novel Defense"" target=""_blank"">[https://doi.org/10.1109/MLSP.2019.8918908]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/MLSP.2019.8918908]</a>",,,ScienceDirect
UIDS: a unified intrusion detection system for IoT environment,"Vikash Kumar, Ayan Kumar Das, Ditipriya Sinha",Evolutionary Intelligence,2019-09-20,"<a href=""Springer (2019-09-20) : UIDS: a unified intrusion detection system for IoT environment"" target=""_blank"">[https://link.springer.com/article/10.1007/s12065-019-00291-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12065-019-00291-w]</a>",Intrusion detection system (IDS) using machine learning approach is getting popularity as it has an advantage of getting updated by itself to defend...,,Springer
Countering Botnet of Things using Blockchain-Based Authenticity Framework,P. Cui U. Guin,2019 IEEE Computer Society Annual Symposium on VLSI (ISVLSI),2019-09-19,"<a href=""IEEE (2019-09-19) : Countering Botnet of Things using Blockchain-Based Authenticity Framework"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8839425]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ISVLSI.2019.00112]</a>","The success and widespread use of Internet of Things (IoT) bring remarkable contributions and economic benefits in various fields. However, the increasing number of devices also raises security concerns. The prevalence of Botnet of Things (BoT) has been observed and it has been recently reported that the launched attacks affect multiple domains and have caused unacceptable losses. As majority of IoT devices are manufactured off-shore, ensuring their identity becomes one of the major challenges. Cloned devices, with backdoors for malicious purposes, can provide an undue advantage of the adversary to compromise a system even though proper security measures are in place. In this paper, we propose a novel blockchain-based framework to provide traceability of hardware. A unique identity for every IoT device is ensured using a physically unclonable function (PUF). The blockchain provides the verification of these devices by comparing these unique IDs. HyperLedger is selected to implement the blockchain-based framework, and its performance is being evaluated and analyzed.",,IEEE
Securing Cloud Data Under Key Exposure,G. O. Karame C. Soriente K. Lichota S. Capkun,IEEE Transactions on Cloud Computing,2019-09-04,"<a href=""IEEE (2019-09-04) : Securing Cloud Data Under Key Exposure"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7857724]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TCC.2017.2670559]</a>","Recent news reveal a powerful attacker which breaks data confidentiality by acquiring cryptographic keys, by means of coercion or backdoors in cryptographic software. Once the encryption key is exposed, the only viable measure to preserve data confidentiality is to limit the attacker's access to the ciphertext. This may be achieved, for example, by spreading ciphertext blocks across servers in multiple administrative domains-thus assuming that the adversary cannot compromise all of them. Nevertheless, if data is encrypted with existing schemes, an adversary equipped with the encryption key, can still compromise a single server and decrypt the ciphertext blocks stored therein. In this paper, we study data confidentiality against an adversary which knows the encryption key and has access to a large fraction of the ciphertext blocks. To this end, we propose Bastion, a novel and efficient scheme that guarantees data confidentiality even if the encryption key is leaked and the adversary has access to almost all ciphertext blocks. We analyze the security of Bastion, and we evaluate its performance by means of a prototype implementation. We also discuss practical insights with respect to the integration of Bastion in commercial dispersed storage systems. Our evaluation results suggest that Bastion is well-suited for integration in existing systems since it incurs less than 5 percent overhead compared to existing semantically secure encryption modes.",,IEEE
Cryptography with disposable backdoors,Chung K.M.,Cryptography,2019-09-01,"<a href=""ScienceDirect (2019-09-01) : Cryptography with disposable backdoors"" target=""_blank"">[https://doi.org/10.3390/cryptography3030022]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3390/cryptography3030022]</a>",,,ScienceDirect
Luminance-based video backdoor attack against anti-spoofing rebroadcast detection,Bhalerao A.,"IEEE 21st International Workshop on Multimedia Signal Processing, MMSP 2019",2019-09-01,"<a href=""ScienceDirect (2019-09-01) : Luminance-based video backdoor attack against anti-spoofing rebroadcast detection"" target=""_blank"">[https://doi.org/10.1109/MMSP.2019.8901711]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/MMSP.2019.8901711]</a>",,,ScienceDirect
Invisible Backdoor Attacks Against Deep Neural Networks,"Shaofeng Li, Benjamin Zi Hao Zhao, Jiahao Yu, Minhui Xue, Dali Kaafar, Haojin Zhu",arXiv,2019-09,"<a href=""DBLP (2019-09) : Invisible Backdoor Attacks Against Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/1909.02742]</a>","<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1909.02742]</a>",,,DBLP
Improved Conditional Privacy Protection in Vehicle Ad-hoc Networks,K. Mankar C. T. Wasnik,2019 3rd International Conference on Computing Methodologies and Communication (ICCMC),2019-08-29,"<a href=""IEEE (2019-08-29) : Improved Conditional Privacy Protection in Vehicle Ad-hoc Networks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8819827]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICCMC.2019.8819827]</a>","Vehicle ad-hoc networks (VANETs) are susceptible to numerous attacks since all nodes of the network contain information like driver details, vehicle type, license plate, speed, location, and moving route. Since human lives are at stake, privacy is one of the major challenges. The abovementioned information is required by legal authorities for traffic management and prevention of accidents/crimes. Hence, the requirement of a conditional privacy scheme to achieve both obscurity as well as traceability has become a growing field of research. This paper presents an improved conditional privacy protection scheme in VANETs where an identity-based cryptosystem uses pseudonyms to assure user obscurity at the same time, a backdoor access is provided for legal authorities to track defiant and untrustworthy users.",,IEEE
Reaching Data Confidentiality and Model Accountability on the CalTrain,Z. Gu H. Jamjoom D. Su H. Huang J. Zhang T. Ma D. Pendarakis I. Molloy,2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN),2019-08-22,"<a href=""IEEE (2019-08-22) : Reaching Data Confidentiality and Model Accountability on the CalTrain"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809502]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/DSN.2019.00044]</a>","Distributed collaborative learning (DCL) paradigms enable building joint machine learning models from distrusted multi-party participants. Data confidentiality is guaranteed by retaining private training data on each participant's local infrastructure. However, this approach makes today's DCL design fundamentally vulnerable to data poisoning and backdoor attacks. It limits DCL's model accountability, which is key to backtracking problematic training data instances and their responsible contributors. In this paper, we introduce CALTRAIN, a centralized collaborative learning system that simultaneously achieves data confidentiality and model accountability. CALTRAIN enforces isolated computation via secure enclaves on centrally aggregated training data to guarantee data confidentiality. To support building accountable learning models, we securely maintain the links between training instances and their contributors. Our evaluation shows that the models generated by CALTRAIN can achieve the same prediction accuracy when compared to the models trained in non-protected environments. We also demonstrate that when malicious training participants tend to implant backdoors during model training, CALTRAIN can accurately and precisely discover the poisoned or mislabeled training data that lead to the runtime mispredictions.",,IEEE
Security Implications of Intentional Capacitive Crosstalk,C. Kison O. M. Awad M. Fyrbiak C. Paar,IEEE Transactions on Information Forensics and Security,2019-08-22,"<a href=""IEEE (2019-08-22) : Security Implications of Intentional Capacitive Crosstalk"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673644]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TIFS.2019.2900914]</a>","With advances in shrinking process technology sizes, the parasitic effects of closely routed adjacent wires, crosstalk, still present problems in practice since they directly influence performance and functionality. Even though there is a solid understanding of parasitic effects in hardware designs, the security implications of such undesired effects have been scarcely investigated. In this paper, we leverage the physical routing effects of capacitive crosstalk to demonstrate a new parametric hardware Trojan design methodology. We show that such Trojans can be implemented by only rerouting already existing resources. Thus, our approach possesses a zero-gate area overhead which is both stealthy and challenging to detect with standard visual inspection techniques. In two case studies, we demonstrate its devastating consequences: (1) we realize an implementation attack on a third-party cryptographic AES IP core and (2) we realize a privilege escalation on a general-purpose processor capable of running any modern operating system. In these case studies, we take special care to ensure that the Trojans do not violate design rule checks, which further highlights that the capacitive crosstalk Trojans can be building blocks for malicious circuitry design. We then investigate how state-of-the-art visual inspection techniques can be enhanced to cope with parametric hardware Trojans. In particular, we develop an automated layout-level mitigation approach which exploits the characteristic wire length of capacitive crosstalk Trojans. Finally, we highlight further implementation strategies for capacitive crosstalk Trojans and pinpoint future research directions.",,IEEE
A Practical Split Manufacturing Framework for Trojan Prevention via Simultaneous Wire Lifting and Cell Insertion,M. Li B. Yu Y. Lin X. Xu W. Li D. Z. Pan,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2019-08-20,"<a href=""IEEE (2019-08-20) : A Practical Split Manufacturing Framework for Trojan Prevention via Simultaneous Wire Lifting and Cell Insertion"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8419279]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TCAD.2018.2859402]</a>","Trojans and backdoors inserted by untrusted foundries have become serious threats to hardware security. Split manufacturing is proposed to hide important circuit structures and prevent Trojan insertion by fabricating partial interconnections in trusted foundries. Existing split manufacturing frameworks, however, usually lack security guarantee and suffer from poor scalability. It is observed that inserting dummy cells and wires can have high potential on overcoming the security and scalability problems of existing methods, but it is not compatible with current security definition. In this paper, we focus on answering the questions on how to define the notion of security and how to realize the required security level effectively and efficiently when the insertion of dummy cells and wires is considered. We first generalize existing security criterion by modeling the split manufacturing process as a graph problem. Then, a sufficient condition is derived for the proposed security criterion to avoid the computationally intensive operations in traditional methods. To further enhance the scalability of the framework, we propose a secure-by-construction split manufacturing flow. For the first time, a novel mixed-integer linear programming (MILP) formulation is proposed to simultaneously consider cell and wire insertion together with wire lifting. A Lagrangian relaxation algorithm with a minimum-cost flow transformation technique is employed to solve the MILP formulation efficiently. With extensive experiments, our framework demonstrates significantly better efficiency, overhead reduction and security guarantee compared with the previous state-of-the-art.",,IEEE
Improvement of Malware Classification Using Hybrid Feature Engineering,"Emmanuel Masabo, Kyanda Swaib Kaawaase, ... Damien Hanyurwimfura",SN Computer Science,2019-08-16,"<a href=""Springer (2019-08-16) : Improvement of Malware Classification Using Hybrid Feature Engineering"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-019-0017-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42979-019-0017-9]</a>",Polymorphic malware has evolved as a major threat in Computer Systems. Their creation technology is constantly evolving using sophisticated tactics...,,Springer
External Monitoring Changes in Vehicle Hardware Profiles: Enhancing Automotive Cyber-Security,"Constantinos Patsakis, Kleanthis Dellios, ... Agusti Solanas",Journal of Hardware and Systems Security,2019-08-15,"<a href=""Springer (2019-08-15) : External Monitoring Changes in Vehicle Hardware Profiles: Enhancing Automotive Cyber-Security"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-019-00076-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-019-00076-8]</a>","As the vehicles are gradually transformed into the connected-vehicles, standard features of the past (i.e., immobilizer, keyless entry,...",,Springer
True2F: Backdoor-resistant authentication tokens,"Emma Dauterman, Henry Corrigan-Gibbs, David Mazières, Dan Boneh, Dominic Rizzo","arXiv
arXiv","2019-08-11
2018-10","<a href=""arXiv (2019-08-11) : True2F: Backdoor-resistant authentication tokens"" target=""_blank"">[http://arxiv.org/abs/1810.04660v4]</a>
<a href=""DBLP (2018-10) : True2F: Backdoor-resistant authentication tokens"" target=""_blank"">[http://arxiv.org/abs/1810.04660]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1810.04660]</a>","We present True2F, a system for second-factor authentication that provides the benefits of conventional authentication tokens in the face of phishing and software compromise, while also providing strong protection against token faults and backdoors. To do so, we develop new lightweight two-party protocols for generating cryptographic keys and ECDSA signatures, and we implement new privacy defenses to prevent cross-origin token-fingerprinting attacks. To facilitate real-world deployment, our system is backwards-compatible with today's U2F-enabled web services and runs on commodity hardware tokens after a firmware modification. A True2F-protected authentication takes just 57ms to complete on the token, compared with 23ms for unprotected U2F.
","
","arXiv
DBLP"
TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems,"Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, Dawn Song","arXiv
arXiv","2019-08-08
2019-08","<a href=""arXiv (2019-08-08) : TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems"" target=""_blank"">[http://arxiv.org/abs/1908.01763v2]</a>
<a href=""DBLP (2019-08) : TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems"" target=""_blank"">[http://arxiv.org/abs/1908.01763]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1908.01763]</a>","A trojan backdoor is a hidden pattern typically implanted in a deep neural network. It could be activated and thus forces that infected model behaving abnormally only when an input data sample with a particular trigger present is fed to that model. As such, given a deep neural network model and clean input samples, it is very challenging to inspect and determine the existence of a trojan backdoor. Recently, researchers design and develop several pioneering solutions to address this acute problem. They demonstrate the proposed techniques have a great potential in trojan detection. However, we show that none of these existing techniques completely address the problem. On the one hand, they mostly work under an unrealistic assumption (e.g. assuming availability of the contaminated training database). On the other hand, the proposed techniques cannot accurately detect the existence of trojan backdoors, nor restore high-fidelity trojan backdoor images, especially when the triggers pertaining to the trojan vary in size, shape and position. In this work, we propose TABOR, a new trojan detection technique. Conceptually, it formalizes a trojan detection task as a non-convex optimization problem, and the detection of a trojan backdoor as the task of resolving the optimization through an objective function. Different from the existing technique also modeling trojan detection as an optimization problem, TABOR designs a new objective function--under the guidance of explainable AI techniques as well as heuristics--that could guide optimization to identify a trojan backdoor in a more effective fashion. In addition, TABOR defines a new metric to measure the quality of a trojan backdoor identified. Using an anomaly detection method, we show the new metric could better facilitate TABOR to identify intentionally injected triggers in an infected model and filter out false alarms......
","
","arXiv
DBLP"
How To Backdoor Federated Learning,"Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, Vitaly Shmatikov","arXiv
AISTATS
arXiv","2019-08-06
2020
2018-07","<a href=""arXiv (2019-08-06) : How To Backdoor Federated Learning"" target=""_blank"">[http://arxiv.org/abs/1807.00459v3]</a>
<a href=""DBLP (2020) : How To Backdoor Federated Learning"" target=""_blank"">[http://proceedings.mlr.press/v108/bagdasaryan20a.html]</a>
<a href=""DBLP (2018-07) : How To Backdoor Federated Learning"" target=""_blank"">[http://arxiv.org/abs/1807.00459]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://proceedings.mlr.press/v108/bagdasaryan20a.html]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1807.00459]</a>","Federated learning enables thousands of participants to construct a deep learning model without sharing their private training data with each other. For example, multiple smartphones can jointly train a next-word predictor for keyboards without revealing what individual users type. We demonstrate that any participant in federated learning can introduce hidden backdoor functionality into the joint global model, e.g., to ensure that an image classifier assigns an attacker-chosen label to images with certain features, or that a word predictor completes certain sentences with an attacker-chosen word. We design and evaluate a new model-poisoning methodology based on model replacement. An attacker selected in a single round of federated learning can cause the global model to immediately reach 100% accuracy on the backdoor task. We evaluate the attack under different assumptions for the standard federated-learning tasks and show that it greatly outperforms data poisoning. Our generic constrain-and-scale technique also evades anomaly detection-based defenses by incorporating the evasion into the attacker's loss function during training.

","

","arXiv
DBLP
DBLP"
Security Risk Management in Online System,A. K. AlSalamah,"2017 5th Intl Conf on Applied Computing and Information Technology/4th Intl Conf on Computational Science/Intelligence and Applied Informatics/2nd Intl Conf on Big Data, Cloud Computing, Data Science (ACIT-CSII-BCD)",2019-08-05,"<a href=""IEEE (2019-08-05) : Security Risk Management in Online System"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8787038]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ACIT-CSII-BCD.2017.59]</a>","The contribution of this paper is to study online systems weaknesses and their relationships to relative cyber-attacks such as data breach against SWIFT network in online banking to ensure the effectiveness of the risk management process. The relationships developed in this study are identified by highlighting the latest incidents globally in online banking and discovering new approaches that the attacker can use against common vulnerabilities such as mobile malware and code injection in banking channels causing massive loss of bank's assets values. Additionally, this research analyzes Online Banking security practices that can be a backdoor for serious incidents and highlights risk management process. The goal is to provide a review of major risks including security risks, compliance risk and operational risk that have been developed in online banking. Also, the study reviews threat methodologies that attacker has been followed in recent years. As a result, the research shows that most dangerous attacks came particularly from one vulnerability that was disregard and handled without appropriate strategies and provides guidance on risk monitoring and assessment in Online systems.",,IEEE
Machine Learning-Based Network Vulnerability Analysis of Industrial Internet of Things,M. Zolanvari M. A. Teixeira L. Gupta K. M. Khan R. Jain,IEEE Internet of Things Journal,2019-08-01,"<a href=""IEEE (2019-08-01) : Machine Learning-Based Network Vulnerability Analysis of Industrial Internet of Things"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693904]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/JIOT.2019.2912022]</a>","It is critical to secure the Industrial Internet of Things (IIoT) devices because of potentially devastating consequences in case of an attack. Machine learning (ML) and big data analytics are the two powerful leverages for analyzing and securing the Internet of Things (IoT) technology. By extension, these techniques can help improve the security of the IIoT systems as well. In this paper, we first present common IIoT protocols and their associated vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the utilization of ML in countering these susceptibilities. Following that, a literature review of the available intrusion detection solutions using ML models is presented. Finally, we discuss our case study, which includes details of a real-world testbed that we have built to conduct cyber-attacks and to design an intrusion detection system (IDS). We deploy backdoor, command injection, and Structured Query Language (SQL) injection attacks against the system and demonstrate how a ML-based anomaly detection system can perform well in detecting these attacks. We have evaluated the performance through representative metrics to have a fair point of view on the effectiveness of the methods.",,IEEE
The Research on Application of Software Diversity in Cyberspace Security,J. Zhang J. Pang Z. Zhang Z. Liu,2018 IEEE 4th International Conference on Computer and Communications (ICCC),2019-08-01,"<a href=""IEEE (2019-08-01) : The Research on Application of Software Diversity in Cyberspace Security"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8780735]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/CompComm.2018.8780735]</a>","Nowadays, the cyberspace security situation is unoptimistic. Widespread vulnerabilities and backdoors are the main factors of insecurity in cyberspace. Furthermore, the software monoculture makes the attack have an extensive impact. This article points out that software vulnerabilities can be used to cope with the security threats from vulnerabilities and backdoors. The paper, firstly, puts forward the shortcomings of the software monoculture, analyzes the validity and application of the software diversity, and points out that the software diversity will play a better role when combined with the redundancy and voting techniques. Then the paper introduces the source of software diversity, and analyzes several security systems which utilize software diversity. Finally, the prospects and challenges of using the software diversity to achieve security systems are summarized.",,IEEE
V-PSC: A perturbation-based causative attack against DL classifiers' supply chain in VANET,Zeng Y.,"Proceedings - 22nd IEEE International Conference on Computational Science and Engineering and 17th IEEE International Conference on Embedded and Ubiquitous Computing, CSE/EUC 2019",2019-08-01,"<a href=""ScienceDirect (2019-08-01) : V-PSC: A perturbation-based causative attack against DL classifiers' supply chain in VANET"" target=""_blank"">[https://doi.org/10.1109/CSE/EUC.2019.00026]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CSE/EUC.2019.00026]</a>",,,ScienceDirect
Evasion Attacks Against Watermarking Techniques found in MLaaS Systems,D. Hitaj B. Hitaj L. V. Mancini,2019 Sixth International Conference on Software Defined Systems (SDS),2019-07-25,"<a href=""IEEE (2019-07-25) : Evasion Attacks Against Watermarking Techniques found in MLaaS Systems"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8768572]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SDS.2019.8768572]</a>","Deep neural networks have had enormous impact on various domains of computer science applications, considerably outperforming previous state-of-the-art machine learning techniques. To achieve this performance, neural networks need large quantities of data and huge computational resources, which heavily increase their costs. The increased cost of building a good deep neural network model gives rise to a need for protecting this investment from potential copyright infringements. Legitimate owners of a machine learning model want to be able to reliably track and detect a malicious adversary that tries to steal the intellectual property related to the model. This threat is very relevant to Machine Learning as a Service (MLaaS) systems, where a provider supplies APIs to clients, allowing them to interact with their trained proprietary deep learning models. Recently, this problem was tackled by introducing in deep neural networks the concept of watermarking, which allows a legitimate owner to embed some secret information (watermark) in a given model. Through the use of this watermark, the legitimate owners, remotely interacting with a model through input queries, are able to detect a copyright infringement, and prove the ownership of their models that were stolen/copied illegally. In this paper, we focus on assessing the robustness and reliability of state-of-the-art deep neural network watermarking schemes. In particular we show that, a malicious adversary, even in scenarios where the watermark is difficult to remove, can still evade the verification of copyright infringements from the legitimate owners, thus avoiding the detection of the model theft.",,IEEE
Distributed denial of service attack defence simulation based on honeynet technology,"Xiaoying Wang, Na Guo, ... Jilin Feng",Journal of Ambient Intelligence and Humanized Computing,2019-07-13,"<a href=""Springer (2019-07-13) : Distributed denial of service attack defence simulation based on honeynet technology"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-019-01396-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-019-01396-x]</a>","Distributed denial of service (DDoS) is one of the main threats of Internet security, and the detection and prevention of DDoS has always been a hot...",,Springer
"Day Labor Agencies, “Backdoor” Hires, and the Spread of Unfree Labor",Purser G.,Anthropology of Work Review,2019-07-01,"<a href=""ScienceDirect (2019-07-01) : Day Labor Agencies, “Backdoor” Hires, and the Spread of Unfree Labor"" target=""_blank"">[https://doi.org/10.1111/awr.12158]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1111/awr.12158]</a>",,,ScienceDirect
Designing in-VM-assisted lightweight agent-based malware detection framework for securing virtual machines in cloud computing,"Rajendra Patil, Harsha Dudeja, Chirag Modi",International Journal of Information Security,2019-06-26,"<a href=""Springer (2019-06-26) : Designing in-VM-assisted lightweight agent-based malware detection framework for securing virtual machines in cloud computing"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-019-00447-w]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-019-00447-w]</a>",The security of cloud services and underlying resources is a major concern due to vulnerabilities existing in current implementation of the...,,Springer
A backdoor attack against LSTM-based text classification systems,"Jiazhu Dai, Chuanshuai Chen","arXiv
arXiv","2019-06-04
2019-05","<a href=""arXiv (2019-06-04) : A backdoor attack against LSTM-based text classification systems"" target=""_blank"">[http://arxiv.org/abs/1905.12457v2]</a>
<a href=""DBLP (2019-05) : A backdoor attack against LSTM-based text classification systems"" target=""_blank"">[http://arxiv.org/abs/1905.12457]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1905.12457]</a>","With the widespread use of deep learning system in many applications, the adversary has strong incentive to explore vulnerabilities of deep neural networks and manipulate them. Backdoor attacks against deep neural networks have been reported to be a new type of threat. In this attack, the adversary will inject backdoors into the model and then cause the misbehavior of the model through inputs including backdoor triggers. Existed research mainly focuses on backdoor attacks in image classification based on CNN, little attention has been paid to the backdoor attacks in RNN. In this paper, we implement a backdoor attack in text classification based on LSTM by data poisoning. When the backdoor is injected, the model will misclassify any text samples that contains a specific trigger sentence into the target category determined by the adversary. The existence of the backdoor trigger is stealthy and the backdoor injected has little impact on the performance of the model. We consider the backdoor attack in black-box setting where the adversary has no knowledge of model structures or training algorithms except for small amount of training data. We verify the attack through sentiment analysis on the dataset of IMDB movie reviews. The experimental results indicate that our attack can achieve around 95% success rate with 1% poisoning rate.
","
","arXiv
DBLP"
Classification of web backdoor malware based on function call execution of static analysis,Kurniawan A.,ICIC Express Letters,2019-06-01,"<a href=""ScienceDirect (2019-06-01) : Classification of web backdoor malware based on function call execution of static analysis"" target=""_blank"">[https://doi.org/10.24507/icicel.13.06.445]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.24507/icicel.13.06.445]</a>",,,ScienceDirect
Regula Sub-rosa: Latent Backdoor Attacks on Deep Neural Networks,"Yuanshun Yao, Huiying Li, Haitao Zheng, Ben Y. Zhao","arXiv
arXiv","2019-05-24
2019-05","<a href=""arXiv (2019-05-24) : Regula Sub-rosa: Latent Backdoor Attacks on Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/1905.10447v1]</a>
<a href=""DBLP (2019-05) : Regula Sub-rosa: Latent Backdoor Attacks on Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/1905.10447]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1905.10447]</a>","Recent work has proposed the concept of backdoor attacks on deep neural networks (DNNs), where misbehaviors are hidden inside ""normal"" models, only to be triggered by very specific inputs. In practice, however, these attacks are difficult to perform and highly constrained by sharing of models through transfer learning. Adversaries have a small window during which they must compromise the student model before it is deployed. In this paper, we describe a significantly more powerful variant of the backdoor attack, latent backdoors, where hidden rules can be embedded in a single ""Teacher"" model, and automatically inherited by all ""Student"" models through the transfer learning process. We show that latent backdoors can be quite effective in a variety of application contexts, and validate its practicality through real-world attacks against traffic sign recognition, iris identification of lab volunteers, and facial recognition of public figures (politicians). Finally, we evaluate 4 potential defenses, and find that only one is effective in disrupting latent backdoors, but might incur a cost in classification accuracy as tradeoff.
","
","arXiv
DBLP"
Neural cleanse: Identifying and mitigating backdoor attacks in neural networks,Wang B.,Proceedings - IEEE Symposium on Security and Privacy,2019-05-01,"<a href=""ScienceDirect (2019-05-01) : Neural cleanse: Identifying and mitigating backdoor attacks in neural networks"" target=""_blank"">[https://doi.org/10.1109/SP.2019.00031]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/SP.2019.00031]</a>",,,ScienceDirect
Adversarial Audio: A New Information Hiding Method and Backdoor for DNN-based Speech Recognition Models,"Yehao Kong, Jiliang Zhang","arXiv
arXiv","2019-04-08
2019-04","<a href=""arXiv (2019-04-08) : Adversarial Audio: A New Information Hiding Method and Backdoor for DNN-based Speech Recognition Models"" target=""_blank"">[http://arxiv.org/abs/1904.03829v1]</a>
<a href=""DBLP (2019-04) : Adversarial Audio: A New Information Hiding Method and Backdoor for DNN-based Speech Recognition Models"" target=""_blank"">[http://arxiv.org/abs/1904.03829]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1904.03829]</a>","Audio is an important medium in people's daily life, hidden information can be embedded into audio for covert communication. Current audio information hiding techniques can be roughly classed into time domain-based and transform domain-based techniques. Time domain-based techniques have large hiding capacity but low imperceptibility. Transform domain-based techniques have better imperceptibility, but the hiding capacity is poor. This paper proposes a new audio information hiding technique which shows high hiding capacity and good imperceptibility. The proposed audio information hiding method takes the original audio signal as input and obtains the audio signal embedded with hidden information (called stego audio) through the training of our private automatic speech recognition (ASR) model. Without knowing the internal parameters and structure of the private model, the hidden information can be extracted by the private model but cannot be extracted by public models. We use four other ASR models to extract the hidden information on the stego audios to evaluate the security of the private model. The experimental results show that the proposed audio information hiding technique has a high hiding capacity of 48 cps with good imperceptibility and high security. In addition, our proposed adversarial audio can be used to activate an intrinsic backdoor of DNN-based ASR models, which brings a serious threat to intelligent speakers.
","
","arXiv
DBLP"
On User Selective Eavesdropping Attacks in MU-MIMO: CSI Forgery and Countermeasure,Wang S.,Proceedings - IEEE INFOCOM,2019-04-01,"<a href=""ScienceDirect (2019-04-01) : On User Selective Eavesdropping Attacks in MU-MIMO: CSI Forgery and Countermeasure"" target=""_blank"">[https://doi.org/10.1109/INFOCOM.2019.8737412]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/INFOCOM.2019.8737412]</a>",,,ScienceDirect
The “backdoor pathway” of androgen synthesis in human male sexual development,Miller W.L.,PLoS Biology,2019-04-01,"<a href=""ScienceDirect (2019-04-01) : The “backdoor pathway” of androgen synthesis in human male sexual development"" target=""_blank"">[https://doi.org/10.1371/journal.pbio.3000198]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1371/journal.pbio.3000198]</a>",,,ScienceDirect
BSEA-1 - A Stream Cipher Backdooring Technique,Eric Filiol,"arXiv
arXiv","2019-03-26
2019-03","<a href=""arXiv (2019-03-26) : BSEA-1 - A Stream Cipher Backdooring Technique"" target=""_blank"">[http://arxiv.org/abs/1903.11063v1]</a>
<a href=""DBLP (2019-03) : BSEA-1 - A Stream Cipher Backdooring Technique"" target=""_blank"">[http://arxiv.org/abs/1903.11063]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1903.11063]</a>","Recent years have shown that more than ever governments and intelligence agencies try to control and bypass the cryptographic means used for the protection of data. Backdooring encryption algorithms is considered as the best way to enforce cryptographic control. Implementation backdoors (at the protocol/implementation/ma\-nagement level) are generally considered. In this paper we propose to address the most critical issue of backdoors: mathematical backdoors or by-design backdoors, which are put directly at the mathematical design of the encryption algorithm. Considering a particular family (among all the possible ones) of backdoors, we present BSEA-1, a stream cipher algorithm which contains a design backdoor enabling an effective cryptanalysis. The BSEA-1 algorithm uses a 120-bit key. The exploitation of the backdoor enables to break the cipher with around 2 Kbits of knowplaintext in a few seconds.
","
","arXiv
DBLP"
A big data framework for network security of small and medium enterprises for future computing,"Ha-Kyun Kim, Won-Hyun So, Seung-Mo Je",The Journal of Supercomputing,2019-03-26,"<a href=""Springer (2019-03-26) : A big data framework for network security of small and medium enterprises for future computing"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-019-02815-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-019-02815-8]</a>","Recently, there have been some concerns for the network control systems which have been built for the major closed national communication...",,Springer
"“Backdoor Induction” of Chirality: Trans-1,2-cyclohexanediamine as Key Building Block for Asymmetric Hydrogenation Catalysts",Opačak S.,European Journal of Organic Chemistry,2019-03-21,"<a href=""ScienceDirect (2019-03-21) : “Backdoor Induction” of Chirality: Trans-1,2-cyclohexanediamine as Key Building Block for Asymmetric Hydrogenation Catalysts"" target=""_blank"">[https://doi.org/10.1002/ejoc.201801647]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1002/ejoc.201801647]</a>",,,ScienceDirect
Threat Models and Security of Phase-Change Memory,G. Wang,2019 IEEE International Conference on Consumer Electronics (ICCE),2019-03-07,"<a href=""IEEE (2019-03-07) : Threat Models and Security of Phase-Change Memory"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8662100]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICCE.2019.8662100]</a>","Emerging non-volatile memories (NVMs) have been considered promising alternatives to DRAM for future main memory design. Among the NVMs, Phase-Change Memory (PCM) can serve as a good substitute due to its low standby power, high density, and good scalability. However, PCM material also induces security design challenges mainly due to its interior non-volatility. Designing the memory system necessitates considering the challenges which may open the backdoor for attackers. A threat model can help to identify security vulnerabilities in design processes. It is all about finding the security problems, and therefore it should be done early in the design and adoption of manufacture. To our knowledge, this paper is the first attempt to thoroughly discuss the potential threat models for the PCM memory, which can provide a good reference for designing the new generation of PCM. Meanwhile, this paper gives security advice and potential security solutions to design a secure PCM to protect against these potential threats.",,IEEE
Performance Monitor Counters: Interplay Between Safety and Security in Complex Cyber-Physical Systems,A. Carelli A. Vallero S. Di Carlo,IEEE Transactions on Device and Materials Reliability,2019-03-06,"<a href=""IEEE (2019-03-06) : Performance Monitor Counters: Interplay Between Safety and Security in Complex Cyber-Physical Systems"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8640260]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TDMR.2019.2898882]</a>","Recent years have witnessed the growth of the adoption of cyber-physical systems (CPSs) in many sectors, such as automotive, aerospace, civil infrastructures, and healthcare. Several CPS applications include critical scenarios, where a failure of the system can lead to catastrophic consequences. Therefore, anomalies due to failures or malicious attacks must be detected timely. This paper focuses on two relevant aspects of the design of a CPS: 1) safety and 2) security. It analyzes in a specific scenario how the performance monitor counters (PMCs) available in several commercial microprocessors can be from the one hand a valuable tool to enhance the safety of a system and, on the other hand, a security backdoor. Starting from the example of a PMC-based safety mechanism, this paper shows the implementation of a possible attack and eventually proposes a strategy to mitigate the effectiveness of the attack while preserving the safety of the system.",,IEEE
A Power Analysis Method Against Backdoor Instruction in Chips,Ma X.L.,Tien Tzu Hsueh Pao/Acta Electronica Sinica,2019-03-01,"<a href=""ScienceDirect (2019-03-01) : A Power Analysis Method Against Backdoor Instruction in Chips"" target=""_blank"">[https://doi.org/10.3969/j.issn.0372-2112.2019.03.024]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.3969/j.issn.0372-2112.2019.03.024]</a>",,,ScienceDirect
"Virtualization vulnerabilities, security issues, and solutions: a critical study and comparison","Darshan Tank, Akshai Aggarwal, Nirbhay Chaubey",International Journal of Information Technology,2019-02-27,"<a href=""Springer (2019-02-27) : Virtualization vulnerabilities, security issues, and solutions: a critical study and comparison"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-019-00294-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41870-019-00294-x]</a>",Virtualization is technological revolution that separates functions from underlying hardware and allows us to create useful environment from abstract...,,Springer
Design of intentional backdoors in sequential models,"Zhaoyuan Yang, Naresh Iyer, Johan Reimann, Nurali Virani","arXiv
arXiv","2019-02-26
2019-02","<a href=""arXiv (2019-02-26) : Design of intentional backdoors in sequential models"" target=""_blank"">[http://arxiv.org/abs/1902.09972v1]</a>
<a href=""DBLP (2019-02) : Design of intentional backdoors in sequential models"" target=""_blank"">[http://arxiv.org/abs/1902.09972]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1902.09972]</a>","Recent work has demonstrated robust mechanisms by which attacks can be orchestrated on machine learning models. In contrast to adversarial examples, backdoor or trojan attacks embed surgically modified samples with targeted labels in the model training process to cause the targeted model to learn to misclassify chosen samples in the presence of specific triggers, while keeping the model performance stable across other nominal samples. However, current published research on trojan attacks mainly focuses on classification problems, which ignores sequential dependency between inputs. In this paper, we propose methods to discreetly introduce and exploit novel backdoor attacks within a sequential decision-making agent, such as a reinforcement learning agent, by training multiple benign and malicious policies within a single long short-term memory (LSTM) network. We demonstrate the effectiveness as well as the damaging impact of such attacks through initial outcomes generated from our approach, employed on grid-world environments. We also provide evidence as well as intuition on how the trojan trigger and malicious policy is activated. Challenges with network size and unintentional triggers are identified and analogies with adversarial examples are also discussed. In the end, we propose potential approaches to defend against or serve as early detection for such attacks. Results of our work can also be extended to many applications of LSTM and recurrent networks.
","
","arXiv
DBLP"
Attacking convolutional neural network using differential evolution,"Jiawei Su, Danilo Vasconcellos Vargas, Kouichi Sakurai",IPSJ Transactions on Computer Vision and Applications,2019-02-22,"<a href=""Springer (2019-02-22) : Attacking convolutional neural network using differential evolution"" target=""_blank"">[https://link.springer.com/article/10.1186/s41074-019-0053-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s41074-019-0053-3]</a>",The output of convolutional neural networks (CNNs) has been shown to be discontinuous which can make the CNN image classifier vulnerable to small...,,Springer
BACKDOOR ATTACKS ON NEURAL NETWORK OPERATIONS,J. Clements Y. Lao,2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP),2019-02-21,"<a href=""IEEE (2019-02-21) : BACKDOOR ATTACKS ON NEURAL NETWORK OPERATIONS"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8646335]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/GlobalSIP.2018.8646335]</a>","Machine learning is a rapidly growing field that has been expanding into various aspects of technology and science in recent years. Unfortunately, it has been shown recently that machine learning models are highly vulnerable to well-crafted adversarial attacks. This paper develops a novel method for maliciously inserting a backdoor into a well-trained neural network causing misclassification that is only active under rare input keys. As opposed to the existing backdoor attacks on neural networks that alter the weights of the network, the proposed approach targets the computing operations for malicious behavior injection. Our experiments show that the proposed methodology achieves above 99% success rate on average for altering the neural network into the desired predictions given the selected input keys, while remaining undetectable under normal testing data.",,IEEE
Backdoors for Linear Temporal Logic,Meier A.,"Algorithmica
Leibniz International Proceedings in Informatics, LIPIcs","2019-02-15
2017-02-01","<a href=""ScienceDirect (2019-02-15) : Backdoors for Linear Temporal Logic"" target=""_blank"">[https://doi.org/10.1007/s00453-018-0515-5]</a>
<a href=""ScienceDirect (2017-02-01) : Backdoors for linear temporal logic"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.IPEC.2016.23]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/s00453-018-0515-5]</a>
<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.IPEC.2016.23]</a>","
","
","ScienceDirect
ScienceDirect"
CVSkSA: cross-architecture vulnerability search in firmware based on kNN-SVM and attributed control flow graph,"Dongdong Zhao, Hong Lin, ... Jianwen Xiang",Software Quality Journal,2019-02-14,"<a href=""Springer (2019-02-14) : CVSkSA: cross-architecture vulnerability search in firmware based on kNN-SVM and attributed control flow graph"" target=""_blank"">[https://link.springer.com/article/10.1007/s11219-018-9435-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11219-018-9435-5]</a>","To prevent the same known vulnerabilities from affecting different firmware, searching known vulnerabilities in binary firmware across different...",,Springer
A new Backdoor Attack in CNNs by training set corruption without label poisoning,"Mauro Barni, Kassem Kallas, Benedetta Tondi","arXiv
arXiv","2019-02-12
2019-02","<a href=""arXiv (2019-02-12) : A new Backdoor Attack in CNNs by training set corruption without label poisoning"" target=""_blank"">[http://arxiv.org/abs/1902.11237v1]</a>
<a href=""DBLP (2019-02) : A new Backdoor Attack in CNNs by training set corruption without label poisoning"" target=""_blank"">[http://arxiv.org/abs/1902.11237]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1902.11237]</a>","Backdoor attacks against CNNs represent a new threat against deep learning systems, due to the possibility of corrupting the training set so to induce an incorrect behaviour at test time. To avoid that the trainer recognises the presence of the corrupted samples, the corruption of the training set must be as stealthy as possible. Previous works have focused on the stealthiness of the perturbation injected into the training samples, however they all assume that the labels of the corrupted samples are also poisoned. This greatly reduces the stealthiness of the attack, since samples whose content does not agree with the label can be identified by visual inspection of the training set or by running a pre-classification step. In this paper we present a new backdoor attack without label poisoning Since the attack works by corrupting only samples of the target class, it has the additional advantage that it does not need to identify beforehand the class of the samples to be attacked at test time. Results obtained on the MNIST digits recognition task and the traffic signs classification task show that backdoor attacks without label poisoning are indeed possible, thus raising a new alarm regarding the use of deep learning in security-critical applications.
","
","arXiv
DBLP"
Detecting Abuse of Domain Administrator Privilege Using Windows Event Log,M. Fujimoto W. Matsuda T. Mitsunaga,"2018 IEEE Conference on Application, Information and Network Security (AINS)",2019-02-03,"<a href=""IEEE (2019-02-03) : Detecting Abuse of Domain Administrator Privilege Using Windows Event Log"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8631459]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/AINS.2018.8631459]</a>","In an Advanced persistent threat(APT) attack, attackers that infiltrate into an organization network tend to stay inside the network until they are able to accomplish their final goal that may include exploiting sensitive information. When Active Directory is in place, attackers try to obtain a Domain Administrator account which has a privilege to control all users and files in the AD environment. There are several methods for attackers to abuse a legitimate Domain Administrator account. One is to exploit vulnerabilities on Active Directory such as CVE-2014-0317. The other is to steal credentials with password dump tools such as mimikatz. Moreover, attackers are likely to create a backdoor that disguises itself as a legitimate Domain Administrator account called a “Golden Ticket”, in order to obtain long-term administrative privilege. If an attacker abuses a legitimate Domain Administrator account, it is not easy to differentiate a legitimate access and an malicious access. In order to overcome this difficulty, several methods have already been proposed for detecting attacks against AD by analyzing Windows event logs. Each detection method is useful under specific conditions, however none of them cover the entire scope of multiple attacking methods. In this research, we clarify and evaluate the effectiveness of existing methods using a dataset, and propose a new detection algorithm with improved detection rate.",,IEEE
Alternative (Backdoor) androgen production and masculinization in the human fetus,O’Shaughnessy P.J.,PLoS Biology,2019-02-01,"<a href=""ScienceDirect (2019-02-01) : Alternative (Backdoor) androgen production and masculinization in the human fetus"" target=""_blank"">[https://doi.org/10.1371/journal.pbio.3000002]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1371/journal.pbio.3000002]</a>",,,ScienceDirect
A new evolutionary neural networks based on intrusion detection systems using locust swarm optimization,"Ilyas Benmessahel, Kun Xie, ... Thabo Semong",Evolutionary Intelligence,2019-01-24,"<a href=""Springer (2019-01-24) : A new evolutionary neural networks based on intrusion detection systems using locust swarm optimization"" target=""_blank"">[https://link.springer.com/article/10.1007/s12065-019-00199-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12065-019-00199-5]</a>","The need to avoid computer system breaches is increasing. Many researchers have adopted different approaches, such as intrusion detection systems...",,Springer
Routing Aware and Runtime Detection for Infected Network-on-Chip Routers,L. Daoud N. Rafla,2018 IEEE 61st International Midwest Symposium on Circuits and Systems (MWSCAS),2019-01-24,"<a href=""IEEE (2019-01-24) : Routing Aware and Runtime Detection for Infected Network-on-Chip Routers"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8623972]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/MWSCAS.2018.8623972]</a>","Network-on-Chip (NoC) architecture is the communication heart of the processing cores in Multiprocessors System-on-Chip (MPSoC), where messages are routed from a source to a destination through intermediate nodes. Therefore, NoC has become a target to security attacks. By experiencing outsourcing design, NoC can be infected with a malicious Hardware Trojans (HTs) which potentially degrade the system performance or leave a backdoor for secret key leaking. In this paper, we propose a HT model that applies a denial of service attack by misrouting the packets, which causes deadlock and consequently degrading the NoC performance. We present a secure routing algorithm that provides a runtime HT detection and avoiding scheme. Results show that our proposed model has negligible overhead in area and power, 0.4% and 0.6%, respectively.",,IEEE
Remote Desktop Backdoor Implementation with Reverse TCP Payload using Open Source Tools for Instructional Use,Y. Kolli T. K. Mohd A. Y. Javaid,"2018 IEEE 9th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)",2019-01-17,"<a href=""IEEE (2019-01-17) : Remote Desktop Backdoor Implementation with Reverse TCP Payload using Open Source Tools for Instructional Use"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8614801]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IEMCON.2018.8614801]</a>","In this paper, we present an implementation of remotely hacking into an older version of a popular operating system with reverse TCP payload using an open source tool. Reverse TCP opens a backdoor on the victim system which is remotely operated by the attacker without the victim's knowledge. The firewall, in this particular OS version, only scans the incoming traffic and doesn't examine the outgoing traffic which is the flaw that leads to the back door connection. The victim must initiate the connection in the reverse TCP payload. Armitage is an open source tool that provides a Graphical User Interface (GUI) to the Metasploit. The Metasploit framework is another open source framework which provides information about the vulnerabilities and aids in performing penetration testing. Metasploit contains an extensive database of exploits, payloads, and vulnerabilities that vary for different kind of systems. In the implemented attack, the attacker uploads the payload into a server, and the link to the payload is sent through an email which looks legitimate with the help of Social Engineering Toolkit. As soon as the victim executes the payload, the attacker can access the files, take a screenshot, monitor screen, sniff packets, and take pictures using the webcam. We demonstrate this attack which can be easily incorporated in a foundational cybersecurity course for enhanced learning.",,IEEE
A survey of blockchain from security perspective,"Dipankar Dasgupta, John M. Shrein, Kishor Datta Gupta",Journal of Banking and Financial Technology,2019-01-03,"<a href=""Springer (2019-01-03) : A survey of blockchain from security perspective"" target=""_blank"">[https://link.springer.com/article/10.1007/s42786-018-00002-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s42786-018-00002-6]</a>",The report starts with an overview of the blockchain security system and then highlights the specific security threats and summarizes them. We review...,,Springer
Asymmetric subversion attacks on signature and identification schemes,"Yi Wang, Rongmao Chen, ... Yongjun Wang",Personal and Ubiquitous Computing,2019-01-02,"<a href=""Springer (2019-01-02) : Asymmetric subversion attacks on signature and identification schemes"" target=""_blank"">[https://link.springer.com/article/10.1007/s00779-018-01193-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00779-018-01193-x]</a>","Studies of subversion attack against cryptosystem could be dated to several decades ago, while the Snowden revelation in 2013 has set off a new wave...",,Springer
Existence versus exploitation: The opacity of backdoors and backbones under a weak assumption,Hemaspaandra L.A.,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
Progress in Artificial Intelligence","2019-01-01
2021-09-01","<a href=""ScienceDirect (2019-01-01) : Existence versus exploitation: The opacity of backdoors and backbones under a weak assumption"" target=""_blank"">[https://doi.org/10.1007/978-3-030-10801-4_20]</a>
<a href=""ScienceDirect (2021-09-01) : Existence versus exploitation: the opacity of backdoors and backbones"" target=""_blank"">[https://doi.org/10.1007/s13748-021-00234-6]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-030-10801-4_20]</a>
<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/s13748-021-00234-6]</a>","
","
","ScienceDirect
ScienceDirect"
A legal perspective on the relevance of biometric presentation attack detection (PAD) for payment services under PSDII and the GDPR,Kindt E.J.,Advances in Computer Vision and Pattern Recognition,2019-01-01,"<a href=""ScienceDirect (2019-01-01) : A legal perspective on the relevance of biometric presentation attack detection (PAD) for payment services under PSDII and the GDPR"" target=""_blank"">[https://doi.org/10.1007/978-3-319-92627-8_21]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-319-92627-8_21]</a>",,,ScienceDirect
Backdoor Attacks in Neural Networks – A Systematic Evaluation on Multiple Traffic Sign Datasets,Rehman H.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,"<a href=""ScienceDirect (2019-01-01) : Backdoor Attacks in Neural Networks – A Systematic Evaluation on Multiple Traffic Sign Datasets"" target=""_blank"">[https://doi.org/10.1007/978-3-030-29726-8_18]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-030-29726-8_18]</a>",,,ScienceDirect
Behavioural Analysis of Recent Ransomwares and Prediction of Future Attacks by Polymorphic and Metamorphic Ransomware,Popli N.K.,Advances in Intelligent Systems and Computing,2019-01-01,"<a href=""ScienceDirect (2019-01-01) : Behavioural Analysis of Recent Ransomwares and Prediction of Future Attacks by Polymorphic and Metamorphic Ransomware"" target=""_blank"">[https://doi.org/10.1007/978-981-13-1135-2_6]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-981-13-1135-2_6]</a>",,,ScienceDirect
Defending neural backdoors via generative distribution modeling,Qiao X.,Advances in Neural Information Processing Systems,2019-01-01,"<a href=""ScienceDirect (2019-01-01) : Defending neural backdoors via generative distribution modeling"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Detecting backdoor attacks on deep neural networks by activation clustering,Chen B.,CEUR Workshop Proceedings,2019-01-01,"<a href=""ScienceDirect (2019-01-01) : Detecting backdoor attacks on deep neural networks by activation clustering"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Detection of Trojaning Attack on Neural Networks via Cost of Sample Classification,Gao H.,Security and Communication Networks,2019-01-01,"<a href=""ScienceDirect (2019-01-01) : Detection of Trojaning Attack on Neural Networks via Cost of Sample Classification"" target=""_blank"">[https://doi.org/10.1155/2019/1953839]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1155/2019/1953839]</a>",,,ScienceDirect
I Want to Break Square-free: The 4p−1 factorization method and its RSA backdoor viability,Sedlacek V.,ICETE 2019 - Proceedings of the 16th International Joint Conference on e-Business and Telecommunications,2019-01-01,"<a href=""ScienceDirect (2019-01-01) : I Want to Break Square-free: The 4p−1 factorization method and its RSA backdoor viability"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
IC protection against JTAG-based attacks,Ren X.,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2019-01-01,"<a href=""ScienceDirect (2019-01-01) : IC protection against JTAG-based attacks"" target=""_blank"">[https://doi.org/10.1109/TCAD.2018.2802866]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/TCAD.2018.2802866]</a>",,,ScienceDirect
Java Backdoor Detection Based on Function Code Gadgets,Liu Q.,Journal of Cyber Security,2019-01-01,"<a href=""ScienceDirect (2019-01-01) : Java Backdoor Detection Based on Function Code Gadgets"" target=""_blank"">[https://doi.org/10.19363/J.cnki.cn10-1380/tn.2019.09.04]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.19363/J.cnki.cn10-1380/tn.2019.09.04]</a>",,,ScienceDirect
Lateral backdoor approach versus conventional anterior approach in recurrent Thyroid surgery,El-kordy M.,Journal of Advanced Pharmacy Education and Research,2019-01-01,"<a href=""ScienceDirect (2019-01-01) : Lateral backdoor approach versus conventional anterior approach in recurrent Thyroid surgery"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Threshold kleptographic attacks on discrete logarithm based signatures,Teşeleanu G.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,"<a href=""ScienceDirect (2019-01-01) : Threshold kleptographic attacks on discrete logarithm based signatures"" target=""_blank"">[https://doi.org/10.1007/978-3-030-25283-0_21]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-030-25283-0_21]</a>",,,ScienceDirect
Towards Leveraging Backdoors in Qualitative Constraint Networks,Sioutis M.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,"<a href=""ScienceDirect (2019-01-01) : Towards Leveraging Backdoors in Qualitative Constraint Networks"" target=""_blank"">[https://doi.org/10.1007/978-3-030-30179-8_27]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-030-30179-8_27]</a>",,,ScienceDirect
Backdoor Attacks in Neural Networks – A Systematic Evaluation on Multiple Traffic Sign Datasets,"Huma Rehman, Andreas Ekelhart, Rudolf Mayer","Machine Learning and Knowledge Extraction
CD-MAKE","2019
2019","<a href=""Springer (2019) : Backdoor Attacks in Neural Networks – A Systematic Evaluation on Multiple Traffic Sign Datasets"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-29726-8_18]</a>
<a href=""DBLP (2019) : Backdoor Attacks in Neural Networks - A Systematic Evaluation on Multiple Traffic Sign Datasets"" target=""_blank"">[https://doi.org/10.1007/978-3-030-29726-8_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-29726-8_18]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-030-29726-8_18]</a>","Machine learning, and deep learning in particular, has seen tremendous advances and surpassed human-level performance on a number of tasks....
","
","Springer
DBLP"
A Backdoor Attack Against LSTM-Based Text Classification Systems,"Jiazhu Dai, Chuanshuai Chen, Yufeng Li",IEEE Access,2019,"<a href=""DBLP (2019) : A Backdoor Attack Against LSTM-Based Text Classification Systems"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2019.2941376]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2019.2941376]</a>",,,DBLP
A Benchmark Study Of Backdoor Data Poisoning Defenses For Deep Neural Network Classifiers And A Novel Defense,"Zhen Xiang, David J. Miller, George Kesidis",MLSP,2019,"<a href=""DBLP (2019) : A Benchmark Study Of Backdoor Data Poisoning Defenses For Deep Neural Network Classifiers And A Novel Defense"" target=""_blank"">[https://doi.org/10.1109/MLSP.2019.8918908]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/MLSP.2019.8918908]</a>",,,DBLP
A Comparative Analysis of Different Intrusion Detection Techniques in Cloud Computing,"Aditya Bakshi, Sunanda",Advanced Informatics for Computing Research,2019,"<a href=""Springer (2019) : A Comparative Analysis of Different Intrusion Detection Techniques in Cloud Computing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-3143-5_30]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-3143-5_30]</a>","Nowadays, the foremost optimal choice of every IT organization is cloud computing. Cloud computing technology is very flexible and scalable in...",,Springer
A Comprehensive Survey of Machine Learning-Based Network Intrusion Detection,"Radhika Chapaneri, Seema Shah",Smart Intelligent Computing and Applications,2019,"<a href=""Springer (2019) : A Comprehensive Survey of Machine Learning-Based Network Intrusion Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-1921-1_35]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-1921-1_35]</a>","In this paper, we survey the published work on machine learning-based network intrusion detection systems covering recent state-of-the-art...",,Springer
A Deep Malware Detection Method Based on General-Purpose Register Features,"Fang Li, Chao Yan, ... Dan Meng",Computational Science – ICCS 2019,2019,"<a href=""Springer (2019) : A Deep Malware Detection Method Based on General-Purpose Register Features"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-22744-9_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-22744-9_17]</a>","Based on low-level features at micro-architecture level, the existing detection methods usually need a long sample length to detect malicious...",,Springer
A Digital Forensic Investigation and Verification Model for Industrial Espionage,"Jieun Dokko, Michael Shin",Digital Forensics and Cyber Crime,2019,"<a href=""Springer (2019) : A Digital Forensic Investigation and Verification Model for Industrial Espionage"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-05487-8_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-05487-8_7]</a>",This paper describes a digital forensic investigation and verification model for industrial espionage (DEIV-IE) focusing on insider data thefts at...,,Springer
A New Backdoor Attack in CNNS by Training Set Corruption Without Label Poisoning,"Mauro Barni, Kassem Kallas, Benedetta Tondi",ICIP,2019,"<a href=""DBLP (2019) : A New Backdoor Attack in CNNS by Training Set Corruption Without Label Poisoning"" target=""_blank"">[https://doi.org/10.1109/ICIP.2019.8802997]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICIP.2019.8802997]</a>",,,DBLP
A Survey of Machine Learning Techniques Used to Combat Against the Advanced Persistent Threat,"E. Rajalakshmi, N. Asik Ibrahim, V. Subramaniyaswamy",Applications and Techniques in Information Security,2019,"<a href=""Springer (2019) : A Survey of Machine Learning Techniques Used to Combat Against the Advanced Persistent Threat"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0871-4_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0871-4_12]</a>","The increased dependence of people on online services has led to a rapid increase in Cybercrime. Nowadays, each device is connected to the internet....",,Springer
Acquisition and Analysis of Forensic Artifacts from Raspberry Pi an Internet of Things Prototype Platform,"Nitesh K. Bharadwaj, Upasna Singh",Recent Findings in Intelligent Computing Techniques,2019,"<a href=""Springer (2019) : Acquisition and Analysis of Forensic Artifacts from Raspberry Pi an Internet of Things Prototype Platform"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-8639-7_32]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-8639-7_32]</a>",The emergence of novel devices with admissible computational and communication capabilities has resulted in fabrication of portable and customizable...,,Springer
Active Defense System of Industrial Control System Based on Dynamic Behavior Analysis,"Wenjin Yu, Yixiang Jiang, Yizhen Lin",Artificial Intelligence and Security,2019,"<a href=""Springer (2019) : Active Defense System of Industrial Control System Based on Dynamic Behavior Analysis"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-24268-8_57]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-24268-8_57]</a>",The Internet and the traditional network continue to converge. With the continuous occurrence of security incidents for industrial control systems...,,Springer
Advance Persistent Threat Detection Using Long Short Term Memory (LSTM) Neural Networks,"P. V. Sai Charan, T. Gireesh Kumar, P. Mohan Anand",Emerging Technologies in Computer Engineering: Microservices in Big Data Analytics,2019,"<a href=""Springer (2019) : Advance Persistent Threat Detection Using Long Short Term Memory (LSTM) Neural Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-8300-7_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-8300-7_5]</a>","Advance Persistent Threat (APT) is a malware attack on sensitive corporate, banking networks and stays there for a long time undetected. In real time...",,Springer
Analysis of In-vehicle Security System of Smart Vehicles,"Nazeeruddin Mohammad, Shahabuddin Muhammad, Eman Shaikh",Future Network Systems and Security,2019,"<a href=""Springer (2019) : Analysis of In-vehicle Security System of Smart Vehicles"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-34353-8_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-34353-8_15]</a>",Modern automobiles utilize numerous computerized systems to control and observe the state of the vehicle. These systems use in-vehicle networks to...,,Springer
Are You Tampering with My Data?,"Michele Alberti, Vinaychandran Pondenkandath, ... Marcus Liwicki",Computer Vision – ECCV 2018 Workshops,2019,"<a href=""Springer (2019) : Are You Tampering with My Data?"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-11012-3_25]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-11012-3_25]</a>","We propose a novel approach towards adversarial attacks on neural networks (NN), focusing on tampering the data used for training instead of...",,Springer
Artificial Neural Network Optimized by Genetic Algorithm for Intrusion Detection System,"Mehdi Moukhafi, Khalid El Yassini, ... Kenza Oufaska",Advanced Intelligent Systems for Sustainable Development (AI2SD’2018),2019,"<a href=""Springer (2019) : Artificial Neural Network Optimized by Genetic Algorithm for Intrusion Detection System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-11928-7_35]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-11928-7_35]</a>","Due to the convergence of new communication technologies to compatible platforms, the number of intrusions into computer systems is growing the...",,Springer
Backdoors to planning,"Martin Kronegger, Sebastian Ordyniak, Andreas Pfandler",Artif. Intell.,2019,"<a href=""DBLP (2019) : Backdoors to planning"" target=""_blank"">[https://doi.org/10.1016/j.artint.2018.10.002]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.artint.2018.10.002]</a>",,,DBLP
BadNets: Evaluating Backdooring Attacks on Deep Neural Networks,"Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg",IEEE Access,2019,"<a href=""DBLP (2019) : BadNets: Evaluating Backdooring Attacks on Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2019.2909068]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2019.2909068]</a>",,,DBLP
Black-Box Optimization in an Extended Search Space for SAT Solving,"Oleg Zaikin, Stepan Kochemazov",Mathematical Optimization Theory and Operations Research,2019,"<a href=""Springer (2019) : Black-Box Optimization in an Extended Search Space for SAT Solving"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-22629-9_28]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-22629-9_28]</a>","The Divide-and-Conquer approach is often used to solve hard instances of the Boolean satisfiability problem (SAT). In particular, it implies...",,Springer
Botnets the Cat-Mouse Hunting,"Teresa Guarda, Samuel Bustos, ... Freddy Villao",Digital Science,2019,"<a href=""Springer (2019) : Botnets the Cat-Mouse Hunting"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-02351-5_46]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-02351-5_46]</a>","The evolution of information and communication technology and the exponential growth of the Internet are been accompanied by the spread of malware,...",,Springer
Breaking All the Things—A Systematic Survey of Firmware Extraction Techniques for IoT Devices,"Sebastian Vasile, David Oswald, Tom Chothia",Smart Card Research and Advanced Applications,2019,"<a href=""Springer (2019) : Breaking All the Things—A Systematic Survey of Firmware Extraction Techniques for IoT Devices"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-15462-2_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-15462-2_12]</a>","In this paper, we systematically review and categorize different hardware-based firmware extraction techniques, using 24 examples of real,...",,Springer
Bringing Kleptography to Real-World TLS,"Adam Janovsky, Jan Krhovjak, Vashek Matyas",Information Security Theory and Practice,2019,"<a href=""Springer (2019) : Bringing Kleptography to Real-World TLS"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-20074-9_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-20074-9_3]</a>",Kleptography is a study of stealing information securely and subliminally from black-box cryptographic devices. The stolen information is exfiltrated...,,Springer
BrowserGuard2: A Solution for Drive-by-Download Attacks,"Gireesh Joshi, R. Padmavathy, ... Mani Bhushan Kumar","Proceeding of the Second International Conference on Microelectronics, Computing & Communication Systems (MCCS 2017)",2019,"<a href=""Springer (2019) : BrowserGuard2: A Solution for Drive-by-Download Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-8234-4_59]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-8234-4_59]</a>",Drive-by-download attacks create major threats on web infrastructure. These attacks happen when a user visits a malicious website which downloads and...,,Springer
Cloud and Local Servers for a Federation of Molecular Science Learning Object Repositories,"Sergio Tasso, Simonetta Pallottelli, ... Antonio Laganà",Computational Science and Its Applications – ICCSA 2019,2019,"<a href=""Springer (2019) : Cloud and Local Servers for a Federation of Molecular Science Learning Object Repositories"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-24311-1_26]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-24311-1_26]</a>","The G-Lorep project of the European Chemistry Thematic Network (ECTN), based on a federation of distributed repositories of Molecular Science...",,Springer
Commit Signatures for Centralized Version Control Systems,"Sangat Vaidya, Santiago Torres-Arias, ... Justin Cappos",ICT Systems Security and Privacy Protection,2019,"<a href=""Springer (2019) : Commit Signatures for Centralized Version Control Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-22312-0_25]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-22312-0_25]</a>","Version Control Systems (VCS-es) play a major role in the software development life cycle, yet historically their security has been relatively...",,Springer
Comparative Evaluation of Techniques for Detection of Phishing URLs,"Oluwafemi Osho, Ayanfeoluwa Oluyomi, ... Rytis Maskeliunas",Applied Informatics,2019,"<a href=""Springer (2019) : Comparative Evaluation of Techniques for Detection of Phishing URLs"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-32475-9_28]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-32475-9_28]</a>",One of the popular cyberattacks today is phishing. It combines social engineering and online identity theft to delude Internet users into submitting...,,Springer
Continuous Presentation Attack Detection in Face Biometrics Based on Heart Rate,"Javier Hernandez-Ortega, Julian Fierrez, ... Aythami Morales",Video Analytics. Face and Facial Expression Recognition,2019,"<a href=""Springer (2019) : Continuous Presentation Attack Detection in Face Biometrics Based on Heart Rate"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-12177-8_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-12177-8_7]</a>",In this paper we study face Presentation Attack Detection (PAD) against realistic 3D mask and high quality photo attacks in dynamic scenarios. We...,,Springer
Coordinated Web Scan Detection Based on Hierarchical Correlation,"Jing Yang, Liming Wang, ... Tian Tian",Security and Privacy in New Computing Environments,2019,"<a href=""Springer (2019) : Coordinated Web Scan Detection Based on Hierarchical Correlation"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-21373-2_30]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-21373-2_30]</a>","Web scan is one of the most common network attacks on the Internet, in which an adversary probes one or more websites to discover exploitable...",,Springer
Correlating High- and Low-Level Features:,"Sergii Banin, Geir Olav Dyrkolbotn",Advances in Information and Computer Security,2019,"<a href=""Springer (2019) : Correlating High- and Low-Level Features:"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-26834-3_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-26834-3_9]</a>","Malware brings constant threats to the services and facilities used by modern society. In order to perform and improve anti-malware defense, there is...",,Springer
Cryptographic Reverse Firewalls for Identity-Based Encryption,"Yuyang Zhou, Yuanfeng Guan, ... Fagen Li",Frontiers in Cyber Security,2019,"<a href=""Springer (2019) : Cryptographic Reverse Firewalls for Identity-Based Encryption"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0818-9_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0818-9_3]</a>","The Snowden revelations show that powerful attackers can compromise user’s machines to steal users’ private information. At the same time, many of...",,Springer
Cryptography with Disposable Backdoors,"Kai-Min Chung, Marios Georgiou, Ching-Yi Lai, Vassilis Zikas",Cryptogr.,2019,"<a href=""DBLP (2019) : Cryptography with Disposable Backdoors"" target=""_blank"">[https://doi.org/10.3390/cryptography3030022]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.3390/cryptography3030022]</a>",,,DBLP
Cut-The-Rope: A Game of Stealthy Intrusion,"Stefan Rass, Sandra König, Emmanouil Panaousis",Decision and Game Theory for Security,2019,"<a href=""Springer (2019) : Cut-The-Rope: A Game of Stealthy Intrusion"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-32430-8_24]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-32430-8_24]</a>","A major characteristic of Advanced Persistent Threats (APTs) is their stealthiness over a possibly long period, during which the victim system is...",,Springer
Cyber Battle Damage Assessment Framework and Detection of Unauthorized Wireless Access Point Using Machine Learning,"Duhoe Kim, Doyeon Kim, ... Yong-Hyun Kim",Frontier Computing,2019,"<a href=""Springer (2019) : Cyber Battle Damage Assessment Framework and Detection of Unauthorized Wireless Access Point Using Machine Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-3648-5_59]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-3648-5_59]</a>","Recently, cyber attacks against national institutional information and communication networks have become increasingly intelligent and advanced....",,Springer
DOORchain: Deep Ontology-Based Operation Research to Detect Malicious Smart Contracts,"Mohamed A. El-Dosuky, Gamal H. Eladl",New Knowledge in Information Systems and Technologies,2019,"<a href=""Springer (2019) : DOORchain: Deep Ontology-Based Operation Research to Detect Malicious Smart Contracts"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-16181-1_51]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-16181-1_51]</a>",Blockchains have become of great vogue in different fields after the introduction of Bitcoin. There are some inherent problems that need to be...,,Springer
Designing an Effective Course to Improve Cybersecurity Awareness for Engineering Faculties,"Ghaidaa Shaabany, Reiner Anderl",Advances in Human Factors in Cybersecurity,2019,"<a href=""Springer (2019) : Designing an Effective Course to Improve Cybersecurity Awareness for Engineering Faculties"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-94782-2_20]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-94782-2_20]</a>","In the light of the expanding digitization, connectivity and interconnection of machines, products, and services in Industrie 4.0, the increasing...",,Springer
Detecting Anomalies in Programmable Logic Controllers Using Unsupervised Machine Learning,"Chun-Fai Chan, Kam-Pui Chow, ... Raymond Chan",Advances in Digital Forensics XV,2019,"<a href=""Springer (2019) : Detecting Anomalies in Programmable Logic Controllers Using Unsupervised Machine Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-28752-8_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-28752-8_7]</a>",Supervisory control and data acquisition systems have been employed for decades to communicate with and coordinate industrial processes. These...,,Springer
Detecting Malicious Windows Commands Using Natural Language Processing Techniques,"Muhammd Mudassar Yamin, Basel Katt",Innovative Security Solutions for Information Technology and Communications,2019,"<a href=""Springer (2019) : Detecting Malicious Windows Commands Using Natural Language Processing Techniques"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-12942-2_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-12942-2_13]</a>",Windows command line arguments are used in administration of operating system through a CLI (command line interface). This command line interface...,,Springer
Development of the Unified Security Requirements of PUFs During the Standardization Process,"Nicolas Bruneau, Jean-Luc Danger, ... Alexander Schaub",Innovative Security Solutions for Information Technology and Communications,2019,"<a href=""Springer (2019) : Development of the Unified Security Requirements of PUFs During the Standardization Process"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-12942-2_24]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-12942-2_24]</a>","This paper accounts for some scientific aspects related to the international standardization process about physically unclonable functions (PUFs),...",,Springer
Digital Forensic Readiness Framework for Ransomware Investigation,"Avinash Singh, Adeyemi R. Ikuesan, Hein S. Venter",Digital Forensics and Cyber Crime,2019,"<a href=""Springer (2019) : Digital Forensic Readiness Framework for Ransomware Investigation"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-05487-8_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-05487-8_5]</a>","Over the years there has been a significant increase in the exploitation of the security vulnerabilities of Windows operating systems, the most...",,Springer
Effectiveness and Impact Measurements of a Diversification Based Moving Target Defense,"Manel Smine, Nora Cuppens, Frédéric Cuppens",Risks and Security of Internet and Systems,2019,"<a href=""Springer (2019) : Effectiveness and Impact Measurements of a Diversification Based Moving Target Defense"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-12143-3_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-12143-3_14]</a>","Moving Target Defense techniques have been proposed to increase uncertainty and apparent complexity for attackers. In this paper, we first study the...",,Springer
Effectiveness of Hard Clustering Algorithms for Securing Cyber Space,"Sakib Mahtab Khandaker, Afzal Hussain, Mohiuddin Ahmed",Smart Grid and Internet of Things,2019,"<a href=""Springer (2019) : Effectiveness of Hard Clustering Algorithms for Securing Cyber Space"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-05928-6_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-05928-6_11]</a>","In the era of big data, it is more challenging than before to accurately identify cyber attacks. The characteristics of big data create constraints...",,Springer
Evolution of Advanced Persistent Threat (APT) Attacks and Actors,"Chia-Mei Chen, Gu-Hsin Lai, Dan-Wei (Marian) Wen",New Trends in Computer Technologies and Applications,2019,"<a href=""Springer (2019) : Evolution of Advanced Persistent Threat (APT) Attacks and Actors"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-9190-3_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-9190-3_7]</a>",Advanced Persistent Threat (APT) has become one of the most complicated and intractable cyber attack over the last decade. As APT attacks are...,,Springer
Evolutionary Computation Algorithms for Detecting Known and Unknown Attacks,"Hasanen Alyasiri, John A. Clark, Daniel Kudenko",Innovative Security Solutions for Information Technology and Communications,2019,"<a href=""Springer (2019) : Evolutionary Computation Algorithms for Detecting Known and Unknown Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-12942-2_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-12942-2_14]</a>","Threats against the internet and computer networks are becoming more sophisticated, with attackers using new attacks or modifying existing ones....",,Springer
Evolutionary Computation Techniques for Constructing SAT-Based Attacks in Algebraic Cryptanalysis,"Artem Pavlenko, Alexander Semenov, Vladimir Ulyantsev",Applications of Evolutionary Computation,2019,"<a href=""Springer (2019) : Evolutionary Computation Techniques for Constructing SAT-Based Attacks in Algebraic Cryptanalysis"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-16692-2_16]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-16692-2_16]</a>","In this paper we present the results on applying evolutionary computation techniques to construction of several cryptographic attacks. In particular,...",,Springer
"Explainable Artificial Intelligence Applications in NLP, Biomedical, and Malware Classification: A Literature Review",Sherin Mary Mathews,Intelligent Computing,2019,"<a href=""Springer (2019) : Explainable Artificial Intelligence Applications in NLP, Biomedical, and Malware Classification: A Literature Review"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-22868-2_90]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-22868-2_90]</a>","Deep learning algorithms have achieved high performance accuracy in complex domains such as image classification, face recognition sentiment...",,Springer
Financial Risks of the Blockchain Industry: A Survey of Cyberattacks,"Aleksandr Lazarenko, Sergey Avdoshin",Proceedings of the Future Technologies Conference (FTC) 2018,2019,"<a href=""Springer (2019) : Financial Risks of the Blockchain Industry: A Survey of Cyberattacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-02683-7_26]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-02683-7_26]</a>",This paper reviews successful cyberattacks on the Blockchain industry. The first successful attack was conducted in 2011. Since then the attention...,,Springer
Formalizing and Proving Privacy Properties of Voting Protocols Using Alpha-Beta Privacy,"Sébastien Gondron, Sebastian Mödersheim",Computer Security – ESORICS 2019,2019,"<a href=""Springer (2019) : Formalizing and Proving Privacy Properties of Voting Protocols Using Alpha-Beta Privacy"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-29959-0_26]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-29959-0_26]</a>",Most common formulations of privacy-type properties for security protocols are specified as bisimilarity of processes in applied-...,,Springer
Graphene: A Secure Cloud Communication Architecture,"Abu Faisal, Mohammad Zulkernine",Applied Cryptography and Network Security Workshops,2019,"<a href=""Springer (2019) : Graphene: A Secure Cloud Communication Architecture"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-29729-9_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-29729-9_3]</a>","Due to ubiquitous-elastic computing mechanism, platform independence and sustainable architecture, cloud computing emerged as the most dominant...",,Springer
Hybrid Triodetection Approach: A Framework for Intrusion Detection,"M. Mahithaa Sree, M. Saranya, S. Prayla Shyry",International Conference on Intelligent Data Communication Technologies and Internet of Things (ICICI) 2018,2019,"<a href=""Springer (2019) : Hybrid Triodetection Approach: A Framework for Intrusion Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-03146-6_119]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-03146-6_119]</a>","With the emerging technologies in the internet, internal network security became a challenging issue in cyberspace. Though there are numerous devices...",,Springer
I Want to Break Square-free: The 4p - 1 Factorization Method and Its RSA Backdoor Viability,"Vladimir Sedlacek, Dusan Klinec, Marek Sýs, Petr Svenda, Vashek Matyas",ICETE,2019,"<a href=""DBLP (2019) : I Want to Break Square-free: The 4p - 1 Factorization Method and Its RSA Backdoor Viability"" target=""_blank"">[https://doi.org/10.5220/0007786600250036]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.5220/0007786600250036]</a>",,,DBLP
Implicit Related-Key Factorization Problem on the RSA Cryptosystem,"Mengce Zheng, Honggang Hu",Cryptology and Network Security,2019,"<a href=""Springer (2019) : Implicit Related-Key Factorization Problem on the RSA Cryptosystem"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-31578-8_29]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-31578-8_29]</a>","In this paper, we address the implicit related-key factorization problem on the RSA cryptosystem. Informally, we investigate under what condition it...",,Springer
Instruction Cognitive One-Shot Malware Outbreak Detection,"Sean Park, Iqbal Gondal, ... Jon Oliver",Neural Information Processing,2019,"<a href=""Springer (2019) : Instruction Cognitive One-Shot Malware Outbreak Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-36808-1_84]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-36808-1_84]</a>","New malware outbreaks cannot provide thousands of training samples which are required to counter malware campaigns. In some cases, there could be...",,Springer
Integrated Software Platform for Mobile Malware Analysis – A Potential Vision,"George Suciu, Laurentiu Bezdedeanu, ... Eduard-Cristian Popovici",Future Access Enablers for Ubiquitous and Intelligent Infrastructures,2019,"<a href=""Springer (2019) : Integrated Software Platform for Mobile Malware Analysis – A Potential Vision"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-23976-3_38]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-23976-3_38]</a>","With the evolution of technology, we are witnessing the development of mobile terminals that are getting closer to a personal computer in terms of...",,Springer
"Interbank Networks and Backdoor Bailouts: Benefiting from Other Banks&apos, Government Guarantees","Tim Eisert, Christian Eufinger",Manag. Sci.,2019,"<a href=""DBLP (2019) : Interbank Networks and Backdoor Bailouts: Benefiting from Other Banks&apos, Government Guarantees"" target=""_blank"">[https://doi.org/10.1287/mnsc.2017.2968]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1287/mnsc.2017.2968]</a>",,,DBLP
International Cyber Attackers Eyeing Eastern India: Odisha - A Case Study,"Bhaswati Sahoo, Rabindra Narayan Behera, Sanghamitra Mohanty",Intelligent Computing,2019,"<a href=""Springer (2019) : International Cyber Attackers Eyeing Eastern India: Odisha - A Case Study"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-01177-2_97]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-01177-2_97]</a>","India is a developing country, with various challenges like illiteracy, poverty, tribal and economical backward population and above all present day...",,Springer
IoT Botnets,"Pamela Beltrán-García, Eleazar Aguirre-Anaya, ... Raúl Acosta-Bermejo",Telematics and Computing,2019,"<a href=""Springer (2019) : IoT Botnets"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-33229-7_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-33229-7_21]</a>","This paper presents a comprehensive state-of-art review that discusses the IoT botnet behaviour, including topology and communication between...",,Springer
Is Backside the New Backdoor in Modern SoCs?: Invited Paper,"Nidish Vashistha, M. Tanjidur Rahman, Olivia P. Paradis, Navid Asadizanjani",ITC,2019,"<a href=""DBLP (2019) : Is Backside the New Backdoor in Modern SoCs?: Invited Paper"" target=""_blank"">[https://doi.org/10.1109/ITC44170.2019.9000127]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ITC44170.2019.9000127]</a>",,,DBLP
Let a Non-barking Watchdog Bite: Cliptographic Signatures with an Offline Watchdog,"Sherman S. M. Chow, Alexander Russell, ... Hong-Sheng Zhou",Public-Key Cryptography – PKC 2019,2019,"<a href=""Springer (2019) : Let a Non-barking Watchdog Bite: Cliptographic Signatures with an Offline Watchdog"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-17253-4_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-17253-4_8]</a>",We study how to construct secure digital signature schemes in the presence of kleptographic attacks. Our work utilizes an offline watchdog to clip...,,Springer
M4D: A Malware Detection Method Using Multimodal Features,"Yusheng Dai, Hui Li, ... Min Zheng",Frontiers in Cyber Security,2019,"<a href=""Springer (2019) : M4D: A Malware Detection Method Using Multimodal Features"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0818-9_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-0818-9_15]</a>","With the increasing variants of malware, and it is of great significance to effectively detect malware and secure system. It is easy for malware to...",,Springer
Malware Classification Using Image Representation,"Ajay Singh, Anand Handa, ... Sandeep Kumar Shukla",Cyber Security Cryptography and Machine Learning,2019,"<a href=""Springer (2019) : Malware Classification Using Image Representation"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-20951-3_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-20951-3_6]</a>","In the recent years, there has been a rapid rise in the number of files submitted to anti-virus companies for analysis. It has become very difficult...",,Springer
Malware Squid: A Novel IoT Malware Traffic Analysis Framework Using Convolutional Neural Network and Binary Visualisation,"Robert Shire, Stavros Shiaeles, ... Nicholas Kolokotronis","Internet of Things, Smart Spaces, and Next Generation Networks and Systems",2019,"<a href=""Springer (2019) : Malware Squid: A Novel IoT Malware Traffic Analysis Framework Using Convolutional Neural Network and Binary Visualisation"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-30859-9_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-30859-9_6]</a>",Internet of Things devices have seen a rapid growth and popularity in recent years with many more ordinary devices gaining network capability and...,,Springer
Managing Your Kleptographic Subscription Plan,George Teşeleanu,"Codes, Cryptology and Information Security",2019,"<a href=""Springer (2019) : Managing Your Kleptographic Subscription Plan"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-16458-4_26]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-16458-4_26]</a>","In the classical kleptographic business models, the manufacturer of a device D is paid either in advance or in installments by a malicious entity to...",,Springer
Minimizing Trust in Hardware Wallets with Two Factor Signatures,"Antonio Marcedone, Rafael Pass, Abhi Shelat",Financial Cryptography and Data Security,2019,"<a href=""Springer (2019) : Minimizing Trust in Hardware Wallets with Two Factor Signatures"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-32101-7_25]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-32101-7_25]</a>","We introduce the notion of two-factor signatures (2FS), a generalization of a two-out-of-two threshold signature scheme in which one of the parties...",,Springer
Mining and Utilizing Network Protocol’s Stealth Attack Behaviors,"YanJing Hu, Xu An Wang, ... Shuaishuai Zhu","Advances on P2P, Parallel, Grid, Cloud and Internet Computing",2019,"<a href=""Springer (2019) : Mining and Utilizing Network Protocol’s Stealth Attack Behaviors"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-02607-3_20]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-02607-3_20]</a>","The survivability, concealment and aggression of network protocol’s stealth attack behaviors are very strong, and they are not easy to be detected by...",,Springer
Network Aware Defenses for Intrusion Recognition and Response (NADIR),"Nont Assawakomenkool, Yash Patel, Jonathan Voris",Proceedings of the Future Technologies Conference (FTC) 2018,2019,"<a href=""Springer (2019) : Network Aware Defenses for Intrusion Recognition and Response (NADIR)"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-02683-7_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-02683-7_17]</a>",It has become increasingly difficult to monitor computer networks as they have grown in scale and complexity. This lack of awareness makes responding...,,Springer
Network Malicious Behavior Detection Using Bidirectional LSTM,"Wenwu Chen, Su Yang, ... Jindan Zhang","Complex, Intelligent, and Software Intensive Systems",2019,"<a href=""Springer (2019) : Network Malicious Behavior Detection Using Bidirectional LSTM"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-93659-8_57]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-93659-8_57]</a>","With the rapid development of the Internet, the methods of cyber attack have become more complex and the damage to the world has become increasingly...",,Springer
Network Security Problems and Solutions of Video Surveillance System,"Ming Yang, Zhizong Wu, Jinbo Wu",International Conference on Applications and Techniques in Cyber Security and Intelligence ATCI 2018,2019,"<a href=""Springer (2019) : Network Security Problems and Solutions of Video Surveillance System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-98776-7_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-98776-7_15]</a>","In this paper, the application status and security problems of video surveillance system were analyzed. Solutions and design conceptions targeting...",,Springer
Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks,"Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, Ben Y. Zhao",IEEE Symposium on Security and Privacy,2019,"<a href=""DBLP (2019) : Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks"" target=""_blank"">[https://doi.org/10.1109/SP.2019.00031]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SP.2019.00031]</a>",,,DBLP
On Embedding Backdoor in Malware Detectors Using Machine Learning,"Shoichiro Sasaki, Seira Hidano, Toshihiro Uchibayashi, Takuo Suganuma, Masahiro Hiji, Shinsaku Kiyomoto",PST,2019,"<a href=""DBLP (2019) : On Embedding Backdoor in Malware Detectors Using Machine Learning"" target=""_blank"">[https://doi.org/10.1109/PST47121.2019.8949034]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/PST47121.2019.8949034]</a>",,,DBLP
Open Algorithms for Identity Federation,"Thomas Hardjono, Alex Pentland",Advances in Information and Communication Networks,2019,"<a href=""Springer (2019) : Open Algorithms for Identity Federation"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-03405-4_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-03405-4_3]</a>",The identity problem today is a data-sharing problem. Today the fixed attributes approach adopted by the consumer identity management industry...,,Springer
Password Security in Organizations: User Attitudes and Behaviors Regarding Password Strength,"Tahani Almehmadi, Fahad Alsolami",16th International Conference on Information Technology-New Generations (ITNG 2019),2019,"<a href=""Springer (2019) : Password Security in Organizations: User Attitudes and Behaviors Regarding Password Strength"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-14070-0_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-14070-0_2]</a>","Organizations and, to larger extent governments, have in recent years become more reliant than ever on the consistent operation of their information...",,Springer
Password-Authenticated Public-Key Encryption,"Tatiana Bradley, Jan Camenisch, ... Jiayu Xu",Applied Cryptography and Network Security,2019,"<a href=""Springer (2019) : Password-Authenticated Public-Key Encryption"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-21568-2_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-21568-2_22]</a>","We introduce password-authenticated public-key encryption (PAPKE), a new cryptographic primitive. PAPKE enables secure end-to-end encryption between...",,Springer
Public Immunization Against Complete Subversion Without Random Oracles,"Giuseppe Ateniese, Danilo Francati, ... Daniele Venturi",Applied Cryptography and Network Security,2019,"<a href=""Springer (2019) : Public Immunization Against Complete Subversion Without Random Oracles"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-21568-2_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-21568-2_23]</a>","We seek constructions of general-purpose immunizers that take arbitrary cryptographic primitives, and transform them into ones that withstand a...",,Springer
Quantifying and Analyzing Information Security Risk from Incident Data,Gaute Wangen,Graphical Models for Security,2019,"<a href=""Springer (2019) : Quantifying and Analyzing Information Security Risk from Incident Data"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-36537-0_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-36537-0_7]</a>","Multiple cybersecurity risk assessment and root cause analysis methods propose incident data as a source of information. However, it is not a...",,Springer
Research on Information Security Test Evaluation Method Based on Intelligent Connected Vehicle,"Yanan Zhang, Shengqiang Han, ... Xuebin Shao",Security and Privacy in New Computing Environments,2019,"<a href=""Springer (2019) : Research on Information Security Test Evaluation Method Based on Intelligent Connected Vehicle"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-21373-2_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-21373-2_15]</a>","In order to effectively evaluate the information security level for an intelligent and connected vehicle, a novel Intelligent Connected Vehicle...",,Springer
Risk Management,J. T. Parker M. Gregg,CASP+ CompTIA Advanced Security Practitioner Study Guide: Exam CAS-003,2019,"<a href=""IEEE (2019) : Risk Management"" target=""_blank"">[https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=9822587.pdf&bkn=9820830&pdfType=chapter]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1002/9781119575948.ch6]</a>","This chapter discusses the benefits and importance of risk management. Vulnerability is a weakness in a system design, in the implementation of an operational procedure, or in how software or code was developed (for example, bugs, backdoors, vulnerabilities in code, and so forth). Vulnerabilities may be eliminated or reduced by the correct implementation of safeguards and security countermeasures. When dealing with risk, software is one area with which the security professional must be very concerned. Risk isn't just associated with software it also is related to new products, new technologies, and user behavior. Organizations face many types of risks. Operational risk is risk that is associated with operations. Operational risk is defined by a company's internal and external practices. Such risk can be brought about by the location in which a company does business, or it can be defined by internal and external sources, such as government agencies and regulatory requirements. The business model has a large influence on operational risk. It includes issues such as physical location, production capabilities, purpose, new or changing business strategies, infrastructure, organizational structures, trading practices, and operational processes and policies.",,IEEE
Safety in Numbers: On the Need for Robust Diffie-Hellman Parameter Validation,"Steven Galbraith, Jake Massimo, Kenneth G. Paterson",Public-Key Cryptography – PKC 2019,2019,"<a href=""Springer (2019) : Safety in Numbers: On the Need for Robust Diffie-Hellman Parameter Validation"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-17259-6_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-17259-6_13]</a>",We consider the problem of constructing Diffie-Hellman (DH) parameters which pass standard approaches to parameter validation but for which the...,,Springer
Secure Portable Storage Drive: Secure Information Storage,"Ashish Dhiman, Vishal Gupta, Damanbir Singh","Communication, Networks and Computing",2019,"<a href=""Springer (2019) : Secure Portable Storage Drive: Secure Information Storage"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-2372-0_27]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-2372-0_27]</a>","The aim of this paper is to propose a design for a prototype device, which can further be developed and used to replace existing commercial USB...",,Springer
Security Against Subversion in a Multi-surveillant Setting,"Geng Li, Jianwei Liu, Zongyang Zhang",Information Security and Privacy,2019,"<a href=""Springer (2019) : Security Against Subversion in a Multi-surveillant Setting"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-21548-4_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-21548-4_23]</a>",Mass surveillance attracts much of attentions nowadays. Evidences showed that some intelligence agencies try to monitor public’s communication by...,,Springer
Security Issues and Solutions in Cloud Robotics: A Survey,"Saurabh Jain, Rajesh Doriya",Next Generation Computing Technologies on Computational Intelligence,2019,"<a href=""Springer (2019) : Security Issues and Solutions in Cloud Robotics: A Survey"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-1718-1_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-15-1718-1_6]</a>",Reliability comes with security and security comes with latest technology and any technology is best suited for any application area until it possess...,,Springer
Security Risk Management in Cooperative Intelligent Transportation Systems: A Systematic Literature Review,"Abasi-Amefon O. Affia, Raimundas Matulevičius, Alexander Nolte",On the Move to Meaningful Internet Systems: OTM 2019 Conferences,2019,"<a href=""Springer (2019) : Security Risk Management in Cooperative Intelligent Transportation Systems: A Systematic Literature Review"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-33246-4_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-33246-4_18]</a>",The automotive industry is maximizing cooperative interactions between vehicular sensors and infrastructure components to make intelligent decisions...,,Springer
Security Solution Based on Raspberry PI and IoT,"Bahast Ali, Xiaochun Cheng",Cyberspace Safety and Security,2019,"<a href=""Springer (2019) : Security Solution Based on Raspberry PI and IoT"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-37352-8_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-37352-8_14]</a>",This project focuses on building and developing a secure smart home IoT system that can give near real-time information about the status of the...,,Springer
Security Vulnerability Analysis of Wi-Fi Connection Hijacking on the Linux-Based Robot Operating System for Drone Systems,"Jinyeong Kang, Inwhee Joe","Parallel and Distributed Computing, Applications and Technologies",2019,"<a href=""Springer (2019) : Security Vulnerability Analysis of Wi-Fi Connection Hijacking on the Linux-Based Robot Operating System for Drone Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-5907-1_49]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-5907-1_49]</a>",In this paper we describe the security vulnerabilities of the Erle-Copter quadcopters. Due to the fact that it is promoted as a toy with low...,,Springer
Security and Privacy in Smart Cities: Issues and Current Solutions,"Talal Ashraf Butt, Muhammad Afzaal",Smart Technologies and Innovation for a Sustainable Future,2019,"<a href=""Springer (2019) : Security and Privacy in Smart Cities: Issues and Current Solutions"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-01659-3_37]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-01659-3_37]</a>",Internet of Things (IoT) paradigm is making it possible for everyday objects to integrate with the Internet. This has laid the foundations for the...,,Springer
Siamese Network Based Feature Learning for Improved Intrusion Detection,"Houda Jmila, Mohamed Ibn Khedher, ... Mounim A. El Yacoubi",Neural Information Processing,2019,"<a href=""Springer (2019) : Siamese Network Based Feature Learning for Improved Intrusion Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-36708-4_31]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-36708-4_31]</a>",Intrusion detection is a critical Cyber Security subject. Different Machine Learning (ML) approaches have been proposed for Intrusion Detection...,,Springer
SoK: ATT&CK Techniques and Trends in Windows Malware,"Kris Oosthoek, Christian Doerr",Security and Privacy in Communication Networks,2019,"<a href=""Springer (2019) : SoK: ATT&CK Techniques and Trends in Windows Malware"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-37228-6_20]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-37228-6_20]</a>","In an ever-changing landscape of adversary tactics, techniques and procedures (TTPs), malware remains the tool of choice for attackers to gain a...",,Springer
Stochastic Dynamic Information Flow Tracking Game with Reinforcement Learning,"Dinuka Sahabandu, Shana Moothedath, ... Radha Poovendran",Decision and Game Theory for Security,2019,"<a href=""Springer (2019) : Stochastic Dynamic Information Flow Tracking Game with Reinforcement Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-32430-8_25]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-32430-8_25]</a>","Advanced Persistent Threats (APTs) are stealthy, sophisticated, and long-term attacks that impose significant economic costs and violate the security...",,Springer
Strong Leakage Resilient Encryption by Hiding Partial Ciphertext,"Jia Xu, Jianying Zhou",Applied Cryptography and Network Security Workshops,2019,"<a href=""Springer (2019) : Strong Leakage Resilient Encryption by Hiding Partial Ciphertext"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-29729-9_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-29729-9_10]</a>","Leakage-resilient encryption is a powerful tool to protect data confidentiality against side channel attacks. In this work, we introduce a new and...",,Springer
Subliminal Hash Channels,George Teşeleanu,"Algebra, Codes and Cryptology",2019,"<a href=""Springer (2019) : Subliminal Hash Channels"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-36237-9_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-36237-9_9]</a>","Due to their nature, subliminal channels are mostly regarded as being malicious, but due to recent legislation efforts users’ perception might...",,Springer
Subverting Decryption in AEAD,"Marcel Armour, Bertram Poettering",Cryptography and Coding,2019,"<a href=""Springer (2019) : Subverting Decryption in AEAD"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-35199-1_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-35199-1_2]</a>","This work introduces a new class of Algorithm Substitution Attack (ASA) on Symmetric Encryption Schemes. ASAs were introduced by Bellare, Paterson...",,Springer
Tagging Malware Intentions by Using Attention-Based Sequence-to-Sequence Neural Network,"Yi-Ting Huang, Yu-Yuan Chen, ... Meng Chang Chen",Information Security and Privacy,2019,"<a href=""Springer (2019) : Tagging Malware Intentions by Using Attention-Based Sequence-to-Sequence Neural Network"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-21548-4_38]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-21548-4_38]</a>","Malware detection has noticeably increased in computer security community. However, little is known about a malware’s intentions. In this study, we...",,Springer
Testing the Human Backdoor: Organizational Response to a Phishing Campaign,"Anze Mihelic, Matej Jevscek, Simon Vrhovec, Igor Bernik",J. Univers. Comput. Sci.,2019,"<a href=""DBLP (2019) : Testing the Human Backdoor: Organizational Response to a Phishing Campaign"" target=""_blank"">[http://www.jucs.org/jucs_25_11/testing_the_human_backdoor]</a>","<a href=""DBLP"" target=""_blank"">[http://www.jucs.org/jucs_25_11/testing_the_human_backdoor]</a>",,,DBLP
The Art of Phishing,"Teresa Guarda, Maria Fernanda Augusto, Isabel Lopes",Information Technology and Systems,2019,"<a href=""Springer (2019) : The Art of Phishing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-11890-7_64]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-11890-7_64]</a>","Nowadays there are many threats that a company needs to protect itself. Everyone knows someone who has fallen for a coup by using an email, message...",,Springer
Threshold Kleptographic Attacks on Discrete Logarithm Based Signatures,George Teşeleanu,Progress in Cryptology – LATINCRYPT 2017,2019,"<a href=""Springer (2019) : Threshold Kleptographic Attacks on Discrete Logarithm Based Signatures"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-25283-0_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-25283-0_21]</a>","In an \(\ell \) out of n threshold scheme,...",,Springer
Trojan Attack on Deep Generative Models in Autonomous Driving,"Shaohua Ding, Yulong Tian, ... Sheng Zhong",Security and Privacy in Communication Networks,2019,"<a href=""Springer (2019) : Trojan Attack on Deep Generative Models in Autonomous Driving"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-37228-6_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-37228-6_15]</a>","Deep generative models (DGMs) have empowered unprecedented innovations in many application domains. However, their security has not been thoroughly...",,Springer
True2F: Backdoor-Resistant Authentication Tokens,"Emma Dauterman, Henry Corrigan-Gibbs, David Mazières, Dan Boneh, Dominic Rizzo",IEEE Symposium on Security and Privacy,2019,"<a href=""DBLP (2019) : True2F: Backdoor-Resistant Authentication Tokens"" target=""_blank"">[https://doi.org/10.1109/SP.2019.00048]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SP.2019.00048]</a>",,,DBLP
Verifiable Delay Functions from Supersingular Isogenies and Pairings,"Luca De Feo, Simon Masson, ... Antonio Sanso",Advances in Cryptology – ASIACRYPT 2019,2019,"<a href=""Springer (2019) : Verifiable Delay Functions from Supersingular Isogenies and Pairings"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-34578-5_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-34578-5_10]</a>",We present two new Verifiable Delay Functions (VDF) based on assumptions from elliptic curve cryptography. We discuss both the advantages and...,,Springer
Watermarking PRFs from Lattices: Stronger Security via Extractable PRFs,"Sam Kim, David J. Wu",Advances in Cryptology – CRYPTO 2019,2019,"<a href=""Springer (2019) : Watermarking PRFs from Lattices: Stronger Security via Extractable PRFs"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-26954-8_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-26954-8_11]</a>","A software watermarking scheme enables one to embed a “mark” (i.e., a message) within a program while preserving the program’s functionality....",,Springer
Webshell Detection Model Based on Deep Learning,"Fangjian Tao, Chunjie Cao, Zhihui Liu",Artificial Intelligence and Security,2019,"<a href=""Springer (2019) : Webshell Detection Model Based on Deep Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-24268-8_38]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-24268-8_38]</a>","Aiming at the problem that the existing Webshell detection method relies on manual extraction of features, low automation and easy to bypass, a...",,Springer
Who Activated My Voice Assistant? A Stealthy Attack on Android Phones Without Users’ Awareness,"Rongjunchen Zhang, Xiao Chen, ... James Zheng",Machine Learning for Cyber Security,2019,"<a href=""Springer (2019) : Who Activated My Voice Assistant? A Stealthy Attack on Android Phones Without Users’ Awareness"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-30619-9_27]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-30619-9_27]</a>","Voice Assistant (VAs) are increasingly popular for human-computer interaction (HCI) smartphones. To help users automatically conduct various tasks,...",,Springer
Wild Extensions: Discovering and Analyzing Unlisted Chrome Extensions,"Aidan Beggs, Alexandros Kapravelos","Detection of Intrusions and Malware, and Vulnerability Assessment",2019,"<a href=""Springer (2019) : Wild Extensions: Discovering and Analyzing Unlisted Chrome Extensions"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-22038-9_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-22038-9_1]</a>","With browsers being a ubiquitous, if not required, method to access the web, they represent a unique and universal threat vector. Browsers can run...",,Springer
IC Protection Against JTAG-Based Attacks,X. Ren F. P. Torres R. D. Blanton V. G. Tavares,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2018-12-19,"<a href=""IEEE (2018-12-19) : IC Protection Against JTAG-Based Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8281506]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TCAD.2018.2802866]</a>","Security is now becoming a well-established challenge for integrated circuits (ICs). Various types of IC attacks have been reported, including reverse engineering IPs, dumping on-chip data, and controlling/modifying IC operation. IEEE 1149.1, commonly known as Joint Test Action Group (JTAG), is a standard for providing test access to an IC. JTAG is primarily used for IC manufacturing test, but also for in-field debugging and failure analysis since it gives access to internal subsystems of the IC. Because the JTAG needs to be left intact and operational after fabrication, it inevitably provides a “backdoor” that can be exploited outside its intended use. This paper proposes machine learning-based approaches to detect illegitimate use of the JTAG. Specifically, JTAG operation is characterized using various features that are then classified as either legitimate or attack. Experiments using the OpenSPARC T2 platform demonstrate that the proposed approaches can classify legitimate JTAG operation and known attacks with significantly high accuracy. Experiments also demonstrate that unknown and disguised attacks can be detected with high accuracy as well (99% and 94%, respectively).",,IEEE
Webshell Traffic Detection With Character-Level Features Based on Deep Learning,H. Zhang H. Guan H. Yan W. Li Y. Yu H. Zhou X. Zeng,IEEE Access,2018-12-18,"<a href=""IEEE (2018-12-18) : Webshell Traffic Detection With Character-Level Features Based on Deep Learning"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8540857]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2018.2882517]</a>","Webshell is a kind of backdoor programs based on Web services. Network-based detection could monitor the request and response traffic to find abnormal behaviors and detect the existence of Webshell. Some machine learning and deep learning methods have been used in this field, but the current methods need to be further explored in discovering new attacks and performance. In order to detect large-scale unknown Webshell events, we propose a Webshell traffic detection model combining the characteristics of convolutional neural network and long short-term memory network. At the same time, we propose a character-level traffic content feature transformation method. We apply the method in our proposed model and evaluate our approach on a Webshell detection testbed. The experiment result indicates that the model has a high precision rate and recall rate, and the generalization ability can be guaranteed.",,IEEE
Backdooring Convolutional Neural Networks via Targeted Weight Perturbations,"Jacob Dumford, Walter Scheirer","arXiv
IJCB
arXiv","2018-12-07
2020
2018-12","<a href=""arXiv (2018-12-07) : Backdooring Convolutional Neural Networks via Targeted Weight Perturbations"" target=""_blank"">[http://arxiv.org/abs/1812.03128v1]</a>
<a href=""DBLP (2020) : Backdooring Convolutional Neural Networks via Targeted Weight Perturbations"" target=""_blank"">[https://doi.org/10.1109/IJCB48548.2020.9304875]</a>
<a href=""DBLP (2018-12) : Backdooring Convolutional Neural Networks via Targeted Weight Perturbations"" target=""_blank"">[http://arxiv.org/abs/1812.03128]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/IJCB48548.2020.9304875]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1812.03128]</a>","We present a new type of backdoor attack that exploits a vulnerability of convolutional neural networks (CNNs) that has been previously unstudied. In particular, we examine the application of facial recognition. Deep learning techniques are at the top of the game for facial recognition, which means they have now been implemented in many production-level systems. Alarmingly, unlike other commercial technologies such as operating systems and network devices, deep learning-based facial recognition algorithms are not presently designed with security requirements or audited for security vulnerabilities before deployment. Given how young the technology is and how abstract many of the internal workings of these algorithms are, neural network-based facial recognition systems are prime targets for security breaches. As more and more of our personal information begins to be guarded by facial recognition (e.g., the iPhone X), exploring the security vulnerabilities of these systems from a penetration testing standpoint is crucial. Along these lines, we describe a general methodology for backdooring CNNs via targeted weight perturbations. Using a five-layer CNN and ResNet-50 as case studies, we show that an attacker is able to significantly increase the chance that inputs they supply will be falsely accepted by a CNN while simultaneously preserving the error rates for legitimate enrolled classes.

","

","arXiv
DBLP
DBLP"
A Dynamic-Key Secure Scan Structure Against Scan-Based Side Channel and Memory Cold Boot Attacks,Wu C.,Proceedings of the Asian Test Symposium,2018-12-06,"<a href=""ScienceDirect (2018-12-06) : A Dynamic-Key Secure Scan Structure Against Scan-Based Side Channel and Memory Cold Boot Attacks"" target=""_blank"">[https://doi.org/10.1109/ATS.2018.00020]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ATS.2018.00020]</a>",,,ScienceDirect
Unsupervised Approach for Detecting Low Rate Attacks on Network Traffic with Autoencoder,Pratomo B.A.,"2018 International Conference on Cyber Security and Protection of Digital Services, Cyber Security 2018",2018-12-04,"<a href=""ScienceDirect (2018-12-04) : Unsupervised Approach for Detecting Low Rate Attacks on Network Traffic with Autoencoder"" target=""_blank"">[https://doi.org/10.1109/CyberSecPODS.2018.8560678]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CyberSecPODS.2018.8560678]</a>",,,ScienceDirect
Adversarial fingerprinting of cyber attacks based on stateful honeypots,Cantelli-Forti A.,"Proceedings - 2018 International Conference on Computational Science and Computational Intelligence, CSCI 2018",2018-12-01,"<a href=""ScienceDirect (2018-12-01) : Adversarial fingerprinting of cyber attacks based on stateful honeypots"" target=""_blank"">[https://doi.org/10.1109/CSCI46756.2018.00012]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CSCI46756.2018.00012]</a>",,,ScienceDirect
Security for cyberspace: challenges and opportunities,"Jiang-xing Wu, Jian-hua Li, Xin-sheng Ji",Frontiers of Information Technology & Electronic Engineering,2018-12-01,"<a href=""Springer (2018-12-01) : Security for cyberspace: challenges and opportunities"" target=""_blank"">[https://link.springer.com/article/10.1631/FITEE.1840000]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1631/FITEE.1840000]</a>",,,Springer
How a simple bug in ML compiler could be exploited for backdoors?,Baptiste David,"arXiv
arXiv","2018-11-27
2018-11","<a href=""arXiv (2018-11-27) : How a simple bug in ML compiler could be exploited for backdoors?"" target=""_blank"">[http://arxiv.org/abs/1811.10851v1]</a>
<a href=""DBLP (2018-11) : How a simple bug in ML compiler could be exploited for backdoors?"" target=""_blank"">[http://arxiv.org/abs/1811.10851]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1811.10851]</a>","Whenever a bug occurs in a program, software developers assume that the code is flawed, not the compiler. In fact, if compilers should be correct, they are just normal software with their own bugs. Hard to find, errors in them have significant impact, since it could result to vulnerabilities, especially when they silently miscompile a critical application. Using assembly language to write such software is quite common, especially when time constraint is involved in such program. This paper exposes a bug found in Microsoft Macro Assembler (ml for short) compiler, developed by Microsoft since 1981. This assembly has the characteristics to get high level-like constructs and high level-like records which help the developer to write assembly code. It is in the management of one of this level-like construct the bug has been found. This study aims to show how a compiler-bug can be audited and possibly corrected. For application developers, it shows that even old and mature compilers can present bugs. For security researcher, it shows possibilities to hide some unexpected behavior in software with a clear and officially non-bogus code. It highlights opportunities for including stealth backdoors even in open-source software.
","
","arXiv
DBLP"
"Horst Feistel: the inventor of LUCIFER, the cryptographic algorithm that changed cryptology",Alan G. Konheim,Journal of Cryptographic Engineering,2018-11-21,"<a href=""Springer (2018-11-21) : Horst Feistel: the inventor of LUCIFER, the cryptographic algorithm that changed cryptology"" target=""_blank"">[https://link.springer.com/article/10.1007/s13389-018-0198-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13389-018-0198-5]</a>","This paper documents the early life of Horst Feistel, in particular, the events shaping his career. His creativity led to the development of today’s...",,Springer
UFO - Hidden Backdoor Discovery and Security Verification in IoT Device Firmware,Tien C.W.,"Proceedings - 29th IEEE International Symposium on Software Reliability Engineering Workshops, ISSREW 2018",2018-11-16,"<a href=""ScienceDirect (2018-11-16) : UFO - Hidden Backdoor Discovery and Security Verification in IoT Device Firmware"" target=""_blank"">[https://doi.org/10.1109/ISSREW.2018.00-37]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ISSREW.2018.00-37]</a>",,,ScienceDirect
Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering,"Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, Biplav Srivastava","arXiv
SafeAI@AAAI
arXiv","2018-11-09
2019
2018-11","<a href=""arXiv (2018-11-09) : Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering"" target=""_blank"">[http://arxiv.org/abs/1811.03728v1]</a>
<a href=""DBLP (2019) : Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering"" target=""_blank"">[https://ceur-ws.org/Vol-2301/paper_18.pdf]</a>
<a href=""DBLP (2018-11) : Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering"" target=""_blank"">[http://arxiv.org/abs/1811.03728]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://ceur-ws.org/Vol-2301/paper_18.pdf]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1811.03728]</a>","While machine learning (ML) models are being increasingly trusted to make decisions in different and varying areas, the safety of systems using such models has become an increasing concern. In particular, ML models are often trained on data from potentially untrustworthy sources, providing adversaries with the opportunity to manipulate them by inserting carefully crafted samples into the training set. Recent work has shown that this type of attack, called a poisoning attack, allows adversaries to insert backdoors or trojans into the model, enabling malicious behavior with simple external backdoor triggers at inference time and only a blackbox perspective of the model itself. Detecting this type of attack is challenging because the unexpected behavior occurs only when a backdoor trigger, which is known only to the adversary, is present. Model users, either direct users of training data or users of pre-trained model from a catalog, may not guarantee the safe operation of their ML-based system. In this paper, we propose a novel approach to backdoor detection and removal for neural networks. Through extensive experimental results, we demonstrate its effectiveness for neural networks classifying text and images. To the best of our knowledge, this is the first methodology capable of detecting poisonous data crafted to insert backdoors and repairing the model that does not require a verified and trusted dataset.

","

","arXiv
DBLP
DBLP"
Existence versus Exploitation: The Opacity of Backbones and Backdoors Under a Weak Assumption,"Lane A. Hemaspaandra, David E. Narváez","arXiv
SOFSEM","2018-11-01
2019","<a href=""arXiv (2018-11-01) : Existence versus Exploitation: The Opacity of Backbones and Backdoors Under a Weak Assumption"" target=""_blank"">[http://arxiv.org/abs/1706.04582v8]</a>
<a href=""DBLP (2019) : Existence Versus Exploitation: The Opacity of Backdoors and Backbones Under a Weak Assumption"" target=""_blank"">[https://doi.org/10.1007/978-3-030-10801-4_20]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-030-10801-4_20]</a>","Backdoors and backbones of Boolean formulas are hidden structural properties. A natural goal, already in part realized, is that solver algorithms seek to obtain substantially better performance by exploiting these structures. However, the present paper is not intended to improve the performance of SAT solvers, but rather is a cautionary paper. In particular, the theme of this paper is that there is a potential chasm between the existence of such structures in the Boolean formula and being able to effectively exploit them. This does not mean that these structures are not useful to solvers. It does mean that one must be very careful not to assume that it is computationally easy to go from the existence of a structure to being able to get one's hands on it and/or being able to exploit the structure. For example, in this paper we show that, under the assumption that P $\neq$ NP, there are easily recognizable families of Boolean formulas with strong backdoors that are easy to find, yet for which it is hard (in fact, NP-complete) to determine whether the formulas are satisfiable. We also show that, also under the assumption P $\neq$ NP, there are easily recognizable sets of Boolean formulas for which it is hard (in fact, NP-complete) to determine whether they have a large backbone.
","
","arXiv
DBLP"
Spectral Signatures in Backdoor Attacks,"Brandon Tran, Jerry Li, Aleksander Madry","arXiv
NeurIPS
arXiv","2018-11-01
2018
2018-11","<a href=""arXiv (2018-11-01) : Spectral Signatures in Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/1811.00636v1]</a>
<a href=""DBLP (2018) : Spectral Signatures in Backdoor Attacks"" target=""_blank"">[https://proceedings.neurips.cc/paper/2018/hash/280cf18baf4311c92aa5a042336587d3-Abstract.html]</a>
<a href=""DBLP (2018-11) : Spectral Signatures in Backdoor Attacks"" target=""_blank"">[http://arxiv.org/abs/1811.00636]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://proceedings.neurips.cc/paper/2018/hash/280cf18baf4311c92aa5a042336587d3-Abstract.html]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1811.00636]</a>","A recent line of work has uncovered a new form of data poisoning: so-called \emph{backdoor} attacks. These attacks are particularly dangerous because they do not affect a network's behavior on typical, benign data. Rather, the network only deviates from its expected output when triggered by a perturbation planted by an adversary. In this paper, we identify a new property of all known backdoor attacks, which we call \emph{spectral signatures}. This property allows us to utilize tools from robust statistics to thwart the attacks. We demonstrate the efficacy of these signatures in detecting and removing poisoned examples on real image sets and state of the art neural network architectures. We believe that understanding spectral signatures is a crucial first step towards designing ML systems secure against such backdoor attacks

","

","arXiv
DBLP
DBLP"
Analysis of Mimic Defense and Defense Capabilities based on Four-Executor,L. OuYang K. Song X. Lu X. Li,2018 International Conference on Advanced Mechatronic Systems (ICAMechS),2018-10-25,"<a href=""IEEE (2018-10-25) : Analysis of Mimic Defense and Defense Capabilities based on Four-Executor"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8506861]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICAMechS.2018.8506861]</a>","In recent years, with rapid development of information technology, the network attacks against known or unknown bugs and backdoors are endless and difficult to be solved effectively, so the academician Wu Jiangxing has developed an innovative defense technology that can “change the rules of the game” - mimic defense technology and validated in the fields of web server, etc. But in practice, for the mimic defense technology, the mode of triple redundancy is usually adopted for mimic structural design, in this paper, two types of attack probability model and scheduling arbitration algorithms and triple redundancy structure and quadruple redundancy structure are analyzed, and it is theoretically proven that the triple redundancy structure has limited defense capability in responding to evasive triggered attacks, the quadruple redundancy structure has better defense effect, and the structure have a good reference value for the mimic architectural design.",,IEEE
Online Smart Disguise: real-time diversification evading coresidency-based cloud attacks,"Mona S. Kashkoush, Mohamed Azab, ... Amr S. Abed",Cluster Computing,2018-10-22,"<a href=""Springer (2018-10-22) : Online Smart Disguise: real-time diversification evading coresidency-based cloud attacks"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-018-2851-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-018-2851-2]</a>","Security is a major challenge in Cloud Computing. In this paper, we propose an Online Smart Disguise Framework (OSDF). OSDF employs dynamic,...",,Springer
Multi-layer security scheme for implantable medical devices,"Heena Rathore, Chenglong Fu, ... Zhengtao Yu",Neural Computing and Applications,2018-10-13,"<a href=""Springer (2018-10-13) : Multi-layer security scheme for implantable medical devices"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-018-3819-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-018-3819-0]</a>","Internet of Medical Things (IoMTs) is fast emerging, thereby fostering rapid advances in the areas of sensing, actuation and connectivity to...",,Springer
Shielding Performance Monitor Counters: a double edged weapon for safety and security,A. Carelli A. Vallero S. D. Carlo,2018 IEEE 24th International Symposium on On-Line Testing And Robust System Design (IOLTS),2018-09-30,"<a href=""IEEE (2018-09-30) : Shielding Performance Monitor Counters: a double edged weapon for safety and security"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8474191]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/IOLTS.2018.8474191]</a>","Recent years have witnessed the growth of the adoption of Cyber-Physical Systems (CPSs) in many sectors such as automotive, aerospace, civil infrastructures and healthcare. Several CPS applications include critical scenarios, where a failure of the system can lead to catastrophic consequences. Therefore, anomalies due to failure or malicious attacks must be timely detected. This paper focuses on two relevant aspects of the design of a CPS: safety and security. In particular, it studies how performance monitor counters (PMCs) available in modern microprocessors can be from the one hand a valuable tool to enhance the safety of a system and, on the other hand, a security backdoor. Starting from the example of a PMC based safety mechanism, the paper shows the implementation of a possible attack and eventually proposes a strategy to mitigate the effectiveness of the attack while preserving the safeness of the system.",,IEEE
Detecting Poisoning Attacks on Machine Learning in IoT Environments,N. Baracaldo B. Chen H. Ludwig A. Safavi R. Zhang,2018 IEEE International Congress on Internet of Things (ICIOT),2018-09-27,"<a href=""IEEE (2018-09-27) : Detecting Poisoning Attacks on Machine Learning in IoT Environments"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8473440]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICIOT.2018.00015]</a>","Machine Learning (ML) plays an increasing role in Internet of Things (IoT), both in the Cloud and at the Edge, using trained models for applications from factory automation to environmental sensing. However, using ML in IoT environments presents unique security challenges. In particular, adversaries can manipulate the training data by tampering with sensors' measurements. This type of attack, known as a poisoning attack has been shown to significantly decrease overall performance, cause targeted misclassification or bad behavior, and insert ""backdoors"" and ""neural trojans"". Taking advantage of recently developed tamper-free provenance frameworks, we present a methodology that uses contextual information about the origin and transformation of data points in the training set to identify poisonous data. Our approach works with or without a trusted test data set. Using the proposed approach poisoning attacks can be effectively detected and mitigated in IoT environments with reliable provenance information.",,IEEE
Detecting poisoning attacks on machine learning in IoT environments,Baracaldo N.,"Proceedings - 2018 IEEE International Congress on Internet of Things, ICIOT 2018 - Part of the 2018 IEEE World Congress on Services",2018-09-26,"<a href=""ScienceDirect (2018-09-26) : Detecting poisoning attacks on machine learning in IoT environments"" target=""_blank"">[https://doi.org/10.1109/ICIOT.2018.00015]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICIOT.2018.00015]</a>",,,ScienceDirect
A Study of Cryptographic Backdoors in Cryptographic Primitives,Easttom C.,"26th Iranian Conference on Electrical Engineering, ICEE 2018",2018-09-25,"<a href=""ScienceDirect (2018-09-25) : A Study of Cryptographic Backdoors in Cryptographic Primitives"" target=""_blank"">[https://doi.org/10.1109/ICEE.2018.8472465]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICEE.2018.8472465]</a>",,,ScienceDirect
Windows malware detection system based on LSVC recommended hybrid features,"S. L. Shiva Darshan, C. D. Jaidhar",Journal of Computer Virology and Hacking Techniques,2018-09-11,"<a href=""Springer (2018-09-11) : Windows malware detection system based on LSVC recommended hybrid features"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-018-0327-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-018-0327-9]</a>","To combat exponentially evolved modern malware, an effective Malware Detection System and precise malware classification is highly essential. In this...",,Springer
Sensitive system calls based packed malware variants detection using principal component initialized MultiLayers neural networks,"Jixin Zhang, Kehuan Zhang, ... Qixin Wu",Cybersecurity,2018-09-10,"<a href=""Springer (2018-09-10) : Sensitive system calls based packed malware variants detection using principal component initialized MultiLayers neural networks"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-018-0010-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-018-0010-y]</a>",Malware detection has become mission sensitive as its threats spread from computer systems to Internet of things systems. Modern malware variants are...,,Springer
A Lightweight Host-Based Intrusion Detection Based on Process Generation Patterns,Y. Tsuda J. Nakazato Y. Takagi D. Inoue K. Nakao K. Terada,2018 13th Asia Joint Conference on Information Security (AsiaJCIS),2018-09-02,"<a href=""IEEE (2018-09-02) : A Lightweight Host-Based Intrusion Detection Based on Process Generation Patterns"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453769]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/AsiaJCIS.2018.00025]</a>","Advanced persistent threat (APT) has been considered globally as a serious social problem since the 2010s. Adversaries of this threat, at first, try to penetrate into targeting organizations by using a backdoor which is opened with drive-by-download attacks, malicious e-mail attachments, etc. After adversaries' intruding, they usually execute benign applications (e.g, OS built-in commands, management tools published by OS vendors, etc.) for investigating networks of targeting organizations. Therefore, if they penetrate into networks once, it is difficult to rapidly detect these malicious activities only by using anti-virus software or network-based intrusion systems. Meanwhile, enterprise networks are managed well in general. That means network administrators have a good grasp of installed applications and routinely used applications for employees' daily works. Thereby, in order to find anomaly behaviors on well-managed networks, it is effective to observe changes executing their applications. In this paper, we propose a lightweight host-based intrusion detection system by using process generation patterns. Our system periodically collects lists of active processes from each host, then the system constructs process trees from the lists. In addition, the system detects anomaly processes from the process trees considering parent-child relationships, execution sequences and lifetime of processes. Moreover, we evaluated the system in our organization. The system collected 2, 403, 230 process paths in total from 498 hosts for two months, then the system could extract 38 anomaly processes. Among them, one PowerShell process was also detected by using an anti-virus software running on our organization. Furthermore, our system could filter out the other 18 PowerShell processes, which were used for maintenance of our network.",,IEEE
Multi-tenant intrusion detection system for public cloud (MTIDS),"Mohamed Hawedi, Chamseddine Talhi, Hanifa Boucheneb",The Journal of Supercomputing,2018-09-01,"<a href=""Springer (2018-09-01) : Multi-tenant intrusion detection system for public cloud (MTIDS)"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-018-2572-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-018-2572-6]</a>","Cloud computing is an innovative paradigm technology that is known for its versatility. It provides many creative services as requested, and it is...",,Springer
Analysis of ResNet and GoogleNet models for malware detection,"Riaz Ullah Khan, Xiaosong Zhang, Rajesh Kumar",Journal of Computer Virology and Hacking Techniques,2018-08-28,"<a href=""Springer (2018-08-28) : Analysis of ResNet and GoogleNet models for malware detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-018-0324-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-018-0324-z]</a>",We have utilized two distinct models to identify the obscure or new sort of malware in this paper. GoogleNet and ResNet models are researched and...,,Springer
Hidden Markov models with random restarts versus boosting for malware detection,"Aditya Raghavan, Fabio Di Troia, Mark Stamp",Journal of Computer Virology and Hacking Techniques,2018-08-28,"<a href=""Springer (2018-08-28) : Hidden Markov models with random restarts versus boosting for malware detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-018-0322-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-018-0322-1]</a>","Effective and efficient malware detection is at the forefront of research into building secure digital systems. As with many other fields, malware...",,Springer
Let’s shock our IoT’s heart: ARMv7-M under (fault) attacks,Bukasa S.K.,ACM International Conference Proceeding Series,2018-08-27,"<a href=""ScienceDirect (2018-08-27) : Let’s shock our IoT’s heart: ARMv7-M under (fault) attacks"" target=""_blank"">[https://doi.org/10.1145/3230833.3230842]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3230833.3230842]</a>",,,ScienceDirect
Thoughts on the development of novel network technology,Jiangxing Wu,Science China Information Sciences,2018-08-16,"<a href=""Springer (2018-08-16) : Thoughts on the development of novel network technology"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-018-9456-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11432-018-9456-x]</a>","In this paper, we explore potential innovations that could lead to breakthrough developments in Internet technologies. The deep integration of the...",,Springer
Real-time Detection of Passive Backdoor Behaviors on Android System,Y. Yao L. Zhu H. Wang,"2018 IEEE Conference on Communications and Network Security (CNS)
CNS","2018-08-13
2018","<a href=""IEEE (2018-08-13) : Real-time Detection of Passive Backdoor Behaviors on Android System"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8433190]</a>
<a href=""DBLP (2018) : Real-time Detection of Passive Backdoor Behaviors on Android System"" target=""_blank"">[https://doi.org/10.1109/CNS.2018.8433190]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/CNS.2018.8433190]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CNS.2018.8433190]</a>","Backdoors in legitimate applications allow attackers to obtain the users' private information and achieve remote attacks without downloading any malicious files on the target device. However, some passive backdoor behaviors are very similar to the normal behaviors and are difficult to detect by users and most malicious behavior detection systems on Android. In this paper, we design and implement BDfinder, an easy-to-deploy and dynamical passive backdoor detection system to detect and visualize backdoor behaviors in real time. We evaluate the performance of BDfinder on more than 1800 apps from the Google Play and Xiaomi Market. Our experimental results show that BDfinder not only detects the common backdoor behaviors but also finds undiscovered vulnerabilities. In addition, we propose a simple and safe defense solution based on public key cryptosystem to reduce the malicious use of passive backdoors by attackers. It ensures that even if the attacker has the maximum reverse capability for Android applications, he cannot maliciously make use of the passive backdoors.
","
","IEEE
DBLP"
Real-time detection of passive backdoor behaviors on android system,Yao Y.,"2018 IEEE Conference on Communications and Network Security, CNS 2018",2018-08-10,"<a href=""ScienceDirect (2018-08-10) : Real-time detection of passive backdoor behaviors on android system"" target=""_blank"">[https://doi.org/10.1109/CNS.2018.8433190]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/CNS.2018.8433190]</a>",,,ScienceDirect
Utilizing a lightweight PKI mechanism to guarantee a secure service in a cloud environment,"Sangho Park, Hyunjin Kim, Jaecheol Ryou",The Journal of Supercomputing,2018-08-06,"<a href=""Springer (2018-08-06) : Utilizing a lightweight PKI mechanism to guarantee a secure service in a cloud environment"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-018-2506-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-018-2506-3]</a>","Recently, cloud computing has become popular for smart societies because it made dynamical network without building a physical network. Despite...",,Springer
Let's shock our IoT's heart: ARMv7-M under (fault) attacks,"Sebanjila K. Bukasa, Ronan Lashermes, Jean-Louis Lanet, Axel Leqay","ARES '18: Proceedings of the 13th International Conference on Availability, Reliability and Security",2018-08,"<a href=""ACM (2018-08) : Let's shock our IoT's heart: ARMv7-M under (fault) attacks"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3230833.3230842]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3230833.3230842]</a>","A fault attack is a well-known technique where the behaviour of a chip is voluntarily disturbed by hardware means in order to undermine the security of the information handled by the target. In this paper, we explore how Electromagnetic fault injection (...",,ACM
Load balancing of renewable energy: a cyber security analysis,"Alexandre Vernotte, Margus Välja, ... Robert Lagerström",Energy Informatics,2018-07-26,"<a href=""Springer (2018-07-26) : Load balancing of renewable energy: a cyber security analysis"" target=""_blank"">[https://link.springer.com/article/10.1186/s42162-018-0010-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42162-018-0010-x]</a>","Background In the coming years, the increase of automation in electricity distribution grids, controlled by ICT, will bring major consequences to the...",,Springer
Internet of Things: information security challenges and solutions,"Natalia Miloslavskaya, Alexander Tolstoy",Cluster Computing,2018-07-23,"<a href=""Springer (2018-07-23) : Internet of Things: information security challenges and solutions"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-018-2823-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-018-2823-6]</a>",Keeping up with the burgeoning Internet of Things (IoT) requires staying up to date on the latest network attack trends in dynamic and complicated...,,Springer
Backdoor attacks on neural network operations,Clements J.,"2018 IEEE Global Conference on Signal and Information Processing, GlobalSIP 2018 - Proceedings",2018-07-02,"<a href=""ScienceDirect (2018-07-02) : Backdoor attacks on neural network operations"" target=""_blank"">[https://doi.org/10.1109/GlobalSIP.2018.8646335]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/GlobalSIP.2018.8646335]</a>",,,ScienceDirect
Leakage Perception Method for Backdoor Privacy in Industry Internet of Things Environment,Sha L.T.,Ruan Jian Xue Bao/Journal of Software,2018-07-01,"<a href=""ScienceDirect (2018-07-01) : Leakage Perception Method for Backdoor Privacy in Industry Internet of Things Environment"" target=""_blank"">[https://doi.org/10.13328/j.cnki.jos.005356]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.13328/j.cnki.jos.005356]</a>",,,ScienceDirect
The lateral “backdoor” approach to open thyroid surgery: A comparative study,Singaporewalla R.M.,Asian Journal of Surgery,2018-07-01,"<a href=""ScienceDirect (2018-07-01) : The lateral “backdoor” approach to open thyroid surgery: A comparative study"" target=""_blank"">[https://doi.org/10.1016/j.asjsur.2017.05.003]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.asjsur.2017.05.003]</a>",,,ScienceDirect
Dynamic malware detection and phylogeny analysis using process mining,"Mario Luca Bernardi, Marta Cimitile, ... Francesco Mercaldo",International Journal of Information Security,2018-06-29,"<a href=""Springer (2018-06-29) : Dynamic malware detection and phylogeny analysis using process mining"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-018-0415-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-018-0415-3]</a>","In the last years, mobile phones have become essential communication and productivity tools used daily to access business services and exchange...",,Springer
Going dark: anonymising technology in cyberspace,Ross W. Bellaby,Ethics and Information Technology,2018-06-22,"<a href=""Springer (2018-06-22) : Going dark: anonymising technology in cyberspace"" target=""_blank"">[https://link.springer.com/article/10.1007/s10676-018-9458-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10676-018-9458-4]</a>","Anonymising technologies are cyber-tools that protect people from online surveillance, hiding who they are, what information they have stored and...",,Springer
Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring,"Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, Joseph Keshet","arXiv
USENIX Security Symposium
arXiv","2018-06-11
2018
2018-02","<a href=""arXiv (2018-06-11) : Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring"" target=""_blank"">[http://arxiv.org/abs/1802.04633v3]</a>
<a href=""DBLP (2018) : Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity18/presentation/adi]</a>
<a href=""DBLP (2018-02) : Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring"" target=""_blank"">[http://arxiv.org/abs/1802.04633]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://www.usenix.org/conference/usenixsecurity18/presentation/adi]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1802.04633]</a>","Deep Neural Networks have recently gained lots of success after enabling several breakthroughs in notoriously challenging problems. Training these networks is computationally expensive and requires vast amounts of training data. Selling such pre-trained models can, therefore, be a lucrative business model. Unfortunately, once the models are sold they can be easily copied and redistributed. To avoid this, a tracking mechanism to identify models as the intellectual property of a particular vendor is necessary. In this work, we present an approach for watermarking Deep Neural Networks in a black-box way. Our scheme works for general classification tasks and can easily be combined with current learning algorithms. We show experimentally that such a watermark has no noticeable impact on the primary task that the model is designed for and evaluate the robustness of our proposal against a multitude of practical attacks. Moreover, we provide a theoretical analysis, relating our approach to previous work on backdooring.

","

","arXiv
DBLP
DBLP"
Security-first architecture: deploying physically isolated active security processors for safeguarding the future of computing,"Dan Meng, Rui Hou, ... Peng Liu",Cybersecurity,2018-06-05,"<a href=""Springer (2018-06-05) : Security-first architecture: deploying physically isolated active security processors for safeguarding the future of computing"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-018-0001-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s42400-018-0001-z]</a>","It is fundamentally challenging to build a secure system atop the current computer architecture. The complexity in software, hardware and ASIC...",,Springer
Malvertising: A case study based on analysis of possible solutions,S. Kumar S. S. Rautaray M. Pandey,2017 International Conference on Inventive Computing and Informatics (ICICI),2018-05-28,"<a href=""IEEE (2018-05-28) : Malvertising: A case study based on analysis of possible solutions"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8365356]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICICI.2017.8365356]</a>","Malware can be defined as a malicious program which has been created for a definite motive of harming the Computer System operations or data. Different malicious program created for this intent include viruses, Trojans, spywares, backdoors, adware, worms, rootkits, bots etc. Quite often Malware are laces with online advertisement which is also called as Malvertising. In simple words serving malware though or with the support of advertisements is known as Malvertising In the modern era where network security is directly proportional to revenue in various sectors such as advertisement, it is highly important to perceive and promote ways and means to detect and remove malicious programs such as malware and Malvertising which hinder this advertisement market to a colossal extent. Hence, this paper attempts to perform a case study on Malvertising and analyze its possible solutions. The attempt start with trying to find a suitable and robust definition to a few computer security terms related to Malvertising. Next, the background of the paper presents two different notions which depict the present situation in terms of online security for malicious advertisement. Finally, the solutions segment cites two different approach which has the potential to stop this malaise and a logical analysis is performed.",,IEEE
Malware Analysis and Detection in Enterprise Systems,T. Mokoena T. Zuva,2017 IEEE International Symposium on Parallel and Distributed Processing with Applications and 2017 IEEE International Conference on Ubiquitous Computing and Communications (ISPA/IUCC),2018-05-28,"<a href=""IEEE (2018-05-28) : Malware Analysis and Detection in Enterprise Systems"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367429]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ISPA/IUCC.2017.00199]</a>","Malware is today one of the biggest security threat to the Internet. Malware is any malicious software with the intent to perform malevolent activities on a targeted system. Viruses, worms, trojans, backdoors and adware are but a few examples that fall under the umbrella of malware. The purpose of this research is to investigate techniques that are used in order to effectively perform Malware analysis and detection on enterprise systems to reduce the damage of malware attacks on the operation of organizations. Malware analysis experiments were carried out using the two techniques of malware analysis which are Dynamic and Static analysis on two different malware samples. A Portable executable and Microsoft word document files were the two samples that were analyzed in an isolated sandbox lab environment. The results from the experiments disclosed the behavior, encryption techniques, and other techniques employed by the malware samples. The results showed that Dynamic analysis is more effective than Static analysis. The study proposes the use of both techniques for a comprehensive malware analysis and detection.",,IEEE
Multi-factor user authentication scheme for IoT-based healthcare services,"Parwinder Kaur Dhillon, Sheetal Kalra",Journal of Reliable Intelligent Environments,2018-05-22,"<a href=""Springer (2018-05-22) : Multi-factor user authentication scheme for IoT-based healthcare services"" target=""_blank"">[https://link.springer.com/article/10.1007/s40860-018-0062-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s40860-018-0062-5]</a>","Due to the tremendous rise of the cloud computing and the Internet of Things (IoT) paradigms, the possibility of remote monitoring of the patients in...",,Springer
Low-cost detection of backdoor malware,Loi H.,"2017 12th International Conference for Internet Technology and Secured Transactions, ICITST 2017",2018-05-08,"<a href=""ScienceDirect (2018-05-08) : Low-cost detection of backdoor malware"" target=""_blank"">[https://doi.org/10.23919/ICITST.2017.8356377]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.23919/ICITST.2017.8356377]</a>",,,ScienceDirect
Surveying the Hardware Trojan Threat Landscape for the Internet-of-Things,"Vivek Venugopalan, Cameron D. Patterson",Journal of Hardware and Systems Security,2018-04-30,"<a href=""Springer (2018-04-30) : Surveying the Hardware Trojan Threat Landscape for the Internet-of-Things"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-018-0037-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-018-0037-2]</a>","The Internet-of-Things (IoT) has emerged as one of the most innovative multidisciplinary paradigms combining heterogeneous sensors, software...",,Springer
Security implications of running windows software on a Linux system using Wine: a malware analysis study,"Rory Duncan, Z. Cliffe Schreuders",Journal of Computer Virology and Hacking Techniques,2018-04-26,"<a href=""Springer (2018-04-26) : Security implications of running windows software on a Linux system using Wine: a malware analysis study"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-018-0319-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-018-0319-9]</a>","Linux is considered to be less prone to malware compared to other operating systems, and as a result Linux users rarely run anti-malware. However,...",,Springer
Digital healthcare security,A. E. Rhalibi,"2017 6th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",2018-04-23,"<a href=""IEEE (2018-04-23) : Digital healthcare security"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8342394]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICRITO.2017.8342394]</a>","Healthcare is an essential part of the national critical infrastructure network. Of the four critical infrastructures (safety, mission, business and security), hospital infrastructures are a mission-critical infrastructure. Damage to network communications and the loss of patient data would have a detrimental impact on the health provision. Additionally, many lifesaving medical devices, used by health care infrastructures, are vulnerable to attacks from the digital domain. Pacemakers for example, are calibrated wirelessly and have none or very little security in place. Medical devices are often limited in their computational and communication capabilities. Such devices are not built to accommodate computationally exhaustive operations. Similarly, most medical devices have low on-device memory, leading to the challenge that they are not able to execute complicated security protocols. Wireless links and open connections, present on medical devices, can also be compromised by attackers. Adversaries can manipulate the data transmitted and received by the device, alter dosages, and even turn devices off, putting patients' lives at risk. However, a notable concern is that a successful attack on such device, presents an opportunity for an attacker to gain backdoor access into a healthcare infrastructure. This essentially allows attackers to bypass the network authentication infrastructure required to access systems containing sensitive personal data. In this talk Prof. Abdennour El Rhalibi will present challenges and solutions in digital healthcare security techniques relying on machine learning, visualisation techniques and IDS.",,IEEE
Secure simultaneous bit extraction from Koblitz curves,"Xinxin Fan, Guang Gong, ... Andrey Sidorenko","Designs, Codes and Cryptography",2018-04-12,"<a href=""Springer (2018-04-12) : Secure simultaneous bit extraction from Koblitz curves"" target=""_blank"">[https://link.springer.com/article/10.1007/s10623-018-0484-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10623-018-0484-3]</a>","Secure pseudo-random number generators (PRNGs) have a lot of important applications in cryptography. In this paper, we analyze a new PRNG related to...",,Springer
The in vitro metabolism of 11β-hydroxyprogesterone and 11-ketoprogesterone to 11-ketodihydrotestosterone in the backdoor pathway,van Rooyen D.,Journal of Steroid Biochemistry and Molecular Biology,2018-04-01,"<a href=""ScienceDirect (2018-04-01) : The in vitro metabolism of 11β-hydroxyprogesterone and 11-ketoprogesterone to 11-ketodihydrotestosterone in the backdoor pathway"" target=""_blank"">[https://doi.org/10.1016/j.jsbmb.2017.12.014]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.jsbmb.2017.12.014]</a>",,,ScienceDirect
A Secure Low-Cost Edge Device Authentication Scheme for the Internet of Things,U. Guin A. Singh M. Alam J. Cañedo A. Skjellum,2018 31st International Conference on VLSI Design and 2018 17th International Conference on Embedded Systems (VLSID),2018-03-29,"<a href=""IEEE (2018-03-29) : A Secure Low-Cost Edge Device Authentication Scheme for the Internet of Things"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8326905]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/VLSID.2018.42]</a>","Because of the enhanced capability of adversaries, edge devices of Internet of Things (IoT) infrastructure are now increasingly vulnerable to counterfeiting and piracy. Ensuring the authenticity of such devices is of great concern since an adversary can create a backdoor either to bypass the security, and/or to leak secret information over an unsecured communication channel. The reliability of such devices could also be called into question because they might be counterfeit, defective and/or of inferior quality. It is of prime importance to design and develop solutions for authenticating such edge devices. In this paper, we present a novel low-cost solution for authenticating edge devices. We use SRAM based PUF to generate unique ""digital fingerprints"" for every device, which can be used to generate a unique device ID. We propose a novel ID matching scheme to verify the identity of an edge device even though the PUF is extremely unreliable. We show that the probability of impersonating an ID by an adversary is extremely low. In addition, our proposed solution is resistant to various known attacks.",,IEEE
The Conflicted Usage of RLUTs for Security-Critical Applications on FPGA,"Debapriya Basu Roy, Shivam Bhasin, ... Xuan Thuy Ngo",Journal of Hardware and Systems Security,2018-03-26,"<a href=""Springer (2018-03-26) : The Conflicted Usage of RLUTs for Security-Critical Applications on FPGA"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-018-0035-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-018-0035-4]</a>",Modern field programmable gate arrays (FPGAs) have evolved significantly in recent years and have found applications in various fields like...,,Springer
On Cryptographic Attacks Using Backdoors for SAT,"Alexander Semenov, Oleg Zaikin, Ilya Otpuschennikov, Stepan Kochemazov, Alexey Ignatiev","arXiv
AAAI
arXiv","2018-03-13
2018
2018-03","<a href=""arXiv (2018-03-13) : On Cryptographic Attacks Using Backdoors for SAT"" target=""_blank"">[http://arxiv.org/abs/1803.04646v1]</a>
<a href=""DBLP (2018) : On Cryptographic Attacks Using Backdoors for SAT"" target=""_blank"">[https://doi.org/10.1609/aaai.v32i1.12205]</a>
<a href=""DBLP (2018-03) : On Cryptographic Attacks Using Backdoors for SAT"" target=""_blank"">[http://arxiv.org/abs/1803.04646]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v32i1.12205]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1803.04646]</a>","Propositional satisfiability (SAT) is at the nucleus of state-of-the-art approaches to a variety of computationally hard problems, one of which is cryptanalysis. Moreover, a number of practical applications of SAT can only be tackled efficiently by identifying and exploiting a subset of formula's variables called backdoor set (or simply backdoors). This paper proposes a new class of backdoor sets for SAT used in the context of cryptographic attacks, namely guess-and-determine attacks. The idea is to identify the best set of backdoor variables subject to a statistically estimated hardness of the guess-and-determine attack using a SAT solver. Experimental results on weakened variants of the renowned encryption algorithms exhibit advantage of the proposed approach compared to the state of the art in terms of the estimated hardness of the resulting guess-and-determine attacks.

","

","arXiv
DBLP
DBLP"
Smart environment effectiveness analysis of a pursuit and evasion scenario,"Kuei Min Wang, Lin Hui",Journal of Ambient Intelligence and Humanized Computing,2018-02-26,"<a href=""Springer (2018-02-26) : Smart environment effectiveness analysis of a pursuit and evasion scenario"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-018-0711-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-018-0711-9]</a>",The internet of things (IoT) has become a trend in interactive environments for providing information to decision-makers. Anti-submarine warfare...,,Springer
Analysis of alarms to prevent the organizations network in real-time using process mining approach,"Ved Prakash Mishra, Balvinder Shukla, Abhay Bansal",Cluster Computing,2018-02-24,"<a href=""Springer (2018-02-24) : Analysis of alarms to prevent the organizations network in real-time using process mining approach"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-018-2064-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-018-2064-8]</a>",The analysis of alarms in the current intrusion detection system depends upon the manual system by network administrators. Due to the manual...,,Springer
A practical split manufacturing framework for Trojan prevention via simultaneous wire lifting and cell insertion,M. Li B. Yu Y. Lin X. Xu W. Li D. Z. Pan,2018 23rd Asia and South Pacific Design Automation Conference (ASP-DAC),2018-02-22,"<a href=""IEEE (2018-02-22) : A practical split manufacturing framework for Trojan prevention via simultaneous wire lifting and cell insertion"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8297316]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ASPDAC.2018.8297316]</a>","Trojans and backdoors inserted by untrusted foundries have become serious threats to hardware security. Split manufacturing is proposed to prevent Trojan insertion proactively. Existing methods depend on wire lifting to hide partial circuit interconnections, which usually suffer from large overhead and lack of security guarantee. In this paper, we propose a novel split manufacturing framework that not only guarantees to achieve the required security level but also allows for a drastic reduction of the introduced overhead. In our framework, insertion of dummy circuit cells and wires is considered simultaneously with wire lifting. To support cell and wire insertion, we propose a new security criterion, and further derive its sufficient condition to avoid computation intensive operations in traditional methods. Then, for the first time, a novel mixed integer linear programming formulation is proposed to simultaneously consider cell and wire insertion together with wire lifting, which significantly enlarges the design space to guarantee the realization of the sufficient condition under the security requirements and overhead constraints. With extensive experimental results, our framework demonstrates much better efficiency, overhead reduction, and security guarantee compared with existing methods.",,IEEE
Fast attack detection system using log analysis and attack tree generation,"Duhoe Kim, Yong-Hyun Kim, ... Dongkyoo Shin",Cluster Computing,2018-02-22,"<a href=""Springer (2018-02-22) : Fast attack detection system using log analysis and attack tree generation"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-018-2269-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-018-2269-x]</a>","Many government branches report that internal networks are managed safely by separating them from the outside, but there is often a vulnerability...",,Springer
PMU-Trojan: On exploiting power management side channel for information leakage,M. N. Islam S. Kundu,2018 23rd Asia and South Pacific Design Automation Conference (ASP-DAC),2018-02-22,"<a href=""IEEE (2018-02-22) : PMU-Trojan: On exploiting power management side channel for information leakage"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8297405]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ASPDAC.2018.8297405]</a>","Hardware Trojans are malicious, undesired, intentional modifications introduced in an Integrated Circuit (IC) which can be leveraged by a knowledgeable adversary to compromise the security of the IC. Trojans might be designed to modify the functionality of an IC, access sensitive information or even disable or destroy a system. In this paper, we propose PMU-Trojan, a hardware Trojan for leaking confidential information, such as, cryptographic secret key covertly to an adversary. For information leakage by hardware Trojan, we exploit a backdoor created by Power Management Unit (PMU) in Multi Processor System on Chip (MPSoC). PMU is a system block responsible for initiating voltage and the frequency changes to facilitate flexible power management and energy efficiency. It transmits voltage level change request to power supply. In this paper we leverage this facility as an information side-channel to leak information to power-supply co-tenants. While the proposed approach is general and can be applied for any kind of secret information leakage, for the purpose of illustration, in this study, we focus on leaking Advanced Encryption Standard (AES) key. We demonstrate the working principle of this system in Linux environment where a co-tenant thread monitors the voltage level and receives side-channel information from a thread affected by the Trojan. This scheme also defeats Differential Power Analysis (DPA) based Trojan detection due to low information bit rate spread over long duration by a Trojan unit dissipating power at mere pico-Watts level.",,IEEE
The brain as artificial intelligence: prospecting the frontiers of neuroscience,Steve Fuller,AI & SOCIETY,2018-02-10,"<a href=""Springer (2018-02-10) : The brain as artificial intelligence: prospecting the frontiers of neuroscience"" target=""_blank"">[https://link.springer.com/article/10.1007/s00146-018-0820-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00146-018-0820-1]</a>","This article explores the proposition that the brain, normally seen as an organ of the human body, should be understood as a biologically based form...",,Springer
How to build time-lock encryption,"Jia Liu, Tibor Jager, ... Bogdan Warinschi","Designs, Codes and Cryptography",2018-02-03,"<a href=""Springer (2018-02-03) : How to build time-lock encryption"" target=""_blank"">[https://link.springer.com/article/10.1007/s10623-018-0461-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10623-018-0461-x]</a>",Time-lock encryption is a method to encrypt a message such that it can only be decrypted after a certain deadline has passed. We propose a novel...,,Springer
A Forensic Investigation of the Robot Operating System,I. Abeykoon X. Feng,"2017 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)",2018-02-01,"<a href=""IEEE (2018-02-01) : A Forensic Investigation of the Robot Operating System"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8276850]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/iThings-GreenCom-CPSCom-SmartData.2017.131]</a>","The Robot Operating System (ROS) is a framework that is mostly used in industrial applications such as automotive, healthcare and manufacturing and it is not immune from potential future hacking. By carrying out various types of cyber-attacks, hackers can disrupt the normal operation of a robot. It is easy to get control of communication between a robot and a human due to the open communication network after carrying out malicious attacks to jam the network. As a result, hackers can change commands which are sent by an operator to the robot, making usual activities impossible. For instance, in a case of ROS hacking, man-in-the-middle attacks, Trojans, backdoor attacks, and so on, can change the behaviour of robots to something completely different than expected.Therefore, forensic analysts require a specific method to forensically investigate ROS. This is a new area in the computer forensics field. Therefore, it is proposed to create an analytical framework to facilitate the forensic investigation of the Robot Operating System and methodologies and standards for acquiring related digital evidence using forensic tools. This study addresses a formalized and structured methodology that would assist the forensic investigation approach.This research will help to enhance the gathering, identification and preservation of evidence related to forensics investigations of the Robot Operating System. The forensic analysts could adapt the examination procedure of hacked ROS with a focused, crime-specific, forensics framework.",,IEEE
Commix: automating evaluation and exploitation of command injection vulnerabilities in Web applications,"Anastasios Stasinopoulos, Christoforos Ntantogian, Christos Xenakis",International Journal of Information Security,2018-02-01,"<a href=""Springer (2018-02-01) : Commix: automating evaluation and exploitation of command injection vulnerabilities in Web applications"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-018-0399-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-018-0399-z]</a>","Despite the prevalence and the high impact of command injection attacks, little attention has been given by the research community to this type of...",,Springer
Mitigation of sensor attacks on legacy industrial control systems,L. F. Cómbita A. A. Cárdenas N. Quijano,2017 IEEE 3rd Colombian Conference on Automatic Control (CCAC),2018-02-01,"<a href=""IEEE (2018-02-01) : Mitigation of sensor attacks on legacy industrial control systems"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8276404]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/CCAC.2017.8276404]</a>","Nowadays control systems have a communication infrastructure that enables sensors, actuators, and controllers proper operation. Industrial control systems include diverse technologies composed by novel devices and legacy systems together. Because most of contemporary industrial control systems were designed and put into operation many years ago with little or no consideration of security issues, which emerged from the capabilities of interconnection available these days. Communication infrastructure opens up a backdoor to cyber-attacks on control systems. In this brief, we show how concepts from fault tolerant control can be utilized to mitigate the effect of cyber-attacks on sensors. This paper analyzes how Luenberger Observers (LOs) and Unknown Input Observers (UIOs), two of the traditional tools of Fault Detection and Isolation, can be utilized to detect and isolate attacks on legacy industrial control systems. We show how the simultaneous use of LOs and UIOs can help to compute the malicious injected signal (attack) and the sensor where it happens, in a better way than when only UIOs are used. In addition, the computation of the required modification of the control action (reconfiguration) is exposed. This reconfiguration has as purpose to mitigate the effect of the attack on the control system. A numerical example shows the implementation of the proposed procedure, and the comparison of the effect on the behavior of the control system, with and without reconfiguration, in the presence of attacks is also shown. Finally, some conclusions are exposed, and some open problems are outlined.",,IEEE
Improved elliptical curve cryptography and Abelian group theory to resolve linear system problem in sensor-cloud cluster computing,"N. Thangarasu, A. Arul Lawrence Selvakumar",Cluster Computing,2018-01-27,"<a href=""Springer (2018-01-27) : Improved elliptical curve cryptography and Abelian group theory to resolve linear system problem in sensor-cloud cluster computing"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-017-1573-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-017-1573-1]</a>","At present, the sensor-cloud infrastructure is gaining popularity, since it offers a flexible, open and reconfigurable configuration over monitoring...",,Springer
Entropy-Based Anomaly Detection in a Network,"Ajay Shankar Shukla, Rohit Maurya",Wireless Personal Communications,2018-01-24,"<a href=""Springer (2018-01-24) : Entropy-Based Anomaly Detection in a Network"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-018-5288-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-018-5288-2]</a>",Every computer on the Internet these days is a potential target for a new attack at any moment. In this paper we propose a method to enhance network...,,Springer
Attack Detection Application with Attack Tree for Mobile System using Log Analysis,"Duhoe Kim, Dongil Shin, ... Yong-Hyun Kim",Mobile Networks and Applications,2018-01-20,"<a href=""Springer (2018-01-20) : Attack Detection Application with Attack Tree for Mobile System using Log Analysis"" target=""_blank"">[https://link.springer.com/article/10.1007/s11036-018-1012-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11036-018-1012-4]</a>","Recently, the use of smart phones has greatly increased because of the development of cheap high-performance hardware. The biggest threat to a smart...",,Springer
Threat Hunting Using GRR Rapid Response,H. Rasheed A. Hadi M. Khader,2017 International Conference on New Trends in Computing Sciences (ICTCS),2018-01-11,"<a href=""IEEE (2018-01-11) : Threat Hunting Using GRR Rapid Response"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8250281]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICTCS.2017.22]</a>","Cybercrimes have evolved, and their tactics and techniques are increasingly changing with an alerting pace. This calls for a change in the mindset used to implement security measures, by adopting the approach of continuously and constantly looking for attacks that pass through the deployed security solutions. This approach of searching through the networks for any evidence on threat activity, rather waiting for a breach notification is referred to as cyber threat hunting. This paper discusses the deployment of threat hunting process using GRR Rapid Response. Two experiments were conducted, in which, both remote code execution, client side exploits are tested, and successful exploitation was used to configure a backdoor to the victim's system to achieve persistence. The experiments show that threat hunting can be achieved by the study of the monitored system's normal patterns of behavior, which will help identify the indications and thresholds that can be used in threat hunting.",,IEEE
"21st International Symposium on Research in Attacks, Intrusions and Defenses, RAID 2018",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,"<a href=""ScienceDirect (2018-01-01) : 21st International Symposium on Research in Attacks, Intrusions and Defenses, RAID 2018"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
A systematic classification scheme for cyber-attack taxonomy,Kim S.,"Safety and Reliability - Safe Societies in a Changing World - Proceedings of the 28th International European Safety and Reliability Conference, ESREL 2018",2018-01-01,"<a href=""ScienceDirect (2018-01-01) : A systematic classification scheme for cyber-attack taxonomy"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
ALIAS: A modular tool for finding backdoors for SAT,Kochemazov S.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,"<a href=""ScienceDirect (2018-01-01) : ALIAS: A modular tool for finding backdoors for SAT"" target=""_blank"">[https://doi.org/10.1007/978-3-319-94144-8_25]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-319-94144-8_25]</a>",,,ScienceDirect
"Backdoors: Definition, deniability and detection",Thomas S.L.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,"<a href=""ScienceDirect (2018-01-01) : Backdoors: Definition, deniability and detection"" target=""_blank"">[https://doi.org/10.1007/978-3-030-00470-5_5]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-030-00470-5_5]</a>",,,ScienceDirect
Fine-pruning: Defending against backdooring attacks on deep neural networks,Liu K.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,"<a href=""ScienceDirect (2018-01-01) : Fine-pruning: Defending against backdooring attacks on deep neural networks"" target=""_blank"">[https://doi.org/10.1007/978-3-030-00470-5_13]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-030-00470-5_13]</a>",,,ScienceDirect
From backdoor key to backdoor completability: Improving a known measure of hardness for the satisfiable CSP,Escamocher G.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,"<a href=""ScienceDirect (2018-01-01) : From backdoor key to backdoor completability: Improving a known measure of hardness for the satisfiable CSP"" target=""_blank"">[https://doi.org/10.1007/978-3-319-93031-2_14]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-319-93031-2_14]</a>",,,ScienceDirect
Hardware trojan enabled denial of service attack on CAN bus,Bozdal M.,Procedia Manufacturing,2018-01-01,"<a href=""ScienceDirect (2018-01-01) : Hardware trojan enabled denial of service attack on CAN bus"" target=""_blank"">[https://doi.org/10.1016/j.promfg.2018.10.158]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.promfg.2018.10.158]</a>",,,ScienceDirect
Inhibition of dihydrotestosterone synthesis in prostate cancer by combined frontdoor and backdoor pathway blockade,Fiandalo M.V.,Oncotarget,2018-01-01,"<a href=""ScienceDirect (2018-01-01) : Inhibition of dihydrotestosterone synthesis in prostate cancer by combined frontdoor and backdoor pathway blockade"" target=""_blank"">[https://doi.org/10.18632/oncotarget.24107]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.18632/oncotarget.24107]</a>",,,ScienceDirect
Learning-sensitive backdoors with restarts,Zulkoski E.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,"<a href=""ScienceDirect (2018-01-01) : Learning-sensitive backdoors with restarts"" target=""_blank"">[https://doi.org/10.1007/978-3-319-98334-9_30]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-319-98334-9_30]</a>",,,ScienceDirect
On cryptographic attacks using backdoors for SAT,Semenov A.,"32nd AAAI Conference on Artificial Intelligence, AAAI 2018",2018-01-01,"<a href=""ScienceDirect (2018-01-01) : On cryptographic attacks using backdoors for SAT"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Spectral signatures in backdoor attacks,Tran B.,Advances in Neural Information Processing Systems,2018-01-01,"<a href=""ScienceDirect (2018-01-01) : Spectral signatures in backdoor attacks"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
The fight over encryption: Reasons why congress must block the government from compelling technology companies to create backdoors into their devices,Lear S.,Cleveland State Law Review,2018-01-01,"<a href=""ScienceDirect (2018-01-01) : The fight over encryption: Reasons why congress must block the government from compelling technology companies to create backdoors into their devices"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Unifying Kleptographic Attacks,Teşeleanu G.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,"<a href=""ScienceDirect (2018-01-01) : Unifying Kleptographic Attacks"" target=""_blank"">[https://doi.org/10.1007/978-3-030-03638-6_5]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-030-03638-6_5]</a>",,,ScienceDirect
How to Subvert Backdoored Encryption: Security Against Adversaries that Decrypt All Ciphertexts,"Thibaut Horel, Sunoo Park, Silas Richelson, Vinod Vaikuntanathan","IACR Cryptol. ePrint Arch.
ITCS
arXiv","2018
2019
2018-02","<a href=""DBLP (2018) : How to Subvert Backdoored Encryption: Security Against Adversaries that Decrypt All Ciphertexts"" target=""_blank"">[http://eprint.iacr.org/2018/212]</a>
<a href=""DBLP (2019) : How to Subvert Backdoored Encryption: Security Against Adversaries that Decrypt All Ciphertexts"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.ITCS.2019.42]</a>
<a href=""DBLP (2018-02) : How to Subvert Backdoored Encryption: Security Against Adversaries that Decrypt All Ciphertexts"" target=""_blank"">[http://arxiv.org/abs/1802.07381]</a>","<a href=""DBLP"" target=""_blank"">[http://eprint.iacr.org/2018/212]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.ITCS.2019.42]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1802.07381]</a>","

","

","DBLP
DBLP
DBLP"
Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks,"Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg","Research in Attacks, Intrusions, and Defenses
arXiv
RAID
arXiv","2018
2018-05-30
2018
2018-05","<a href=""Springer (2018) : Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-00470-5_13]</a>
<a href=""arXiv (2018-05-30) : Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/1805.12185v1]</a>
<a href=""DBLP (2018) : Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1007/978-3-030-00470-5_13]</a>
<a href=""DBLP (2018-05) : Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/1805.12185]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-00470-5_13]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-030-00470-5_13]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1805.12185]</a>","Deep neural networks (DNNs) provide excellent performance across a wide range of classification tasks, but their training requires high computational...
Deep neural networks (DNNs) provide excellent performance across a wide range of classification tasks, but their training requires high computational resources and is often outsourced to third parties. Recent work has shown that outsourced training introduces the risk that a malicious trainer will return a backdoored DNN that behaves normally on most inputs but causes targeted misclassifications or degrades the accuracy of the network when a trigger known only to the attacker is present. In this paper, we provide the first effective defenses against backdoor attacks on DNNs. We implement three backdoor attacks from prior work and use them to investigate two promising defenses, pruning and fine-tuning. We show that neither, by itself, is sufficient to defend against sophisticated attackers. We then evaluate fine-pruning, a combination of pruning and fine-tuning, and show that it successfully weakens or even eliminates the backdoors, i.e., in some cases reducing the attack success rate to 0% with only a 0.4% drop in accuracy for clean (non-triggering) inputs. Our work provides the first step toward defenses against backdoor attacks in deep neural networks.

","


","Springer
arXiv
DBLP
DBLP"
Combiners for Backdoored Random Oracles,"Balthazar Bauer, Pooya Farshim, Sogol Mazaheri","Advances in Cryptology – CRYPTO 2018
CRYPTO
IACR Cryptol. ePrint Arch.","2018
2018
2018","<a href=""Springer (2018) : Combiners for Backdoored Random Oracles"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-96881-0_10]</a>
<a href=""DBLP (2018) : Combiners for Backdoored Random Oracles"" target=""_blank"">[https://doi.org/10.1007/978-3-319-96881-0_10]</a>
<a href=""DBLP (2018) : Combiners for Backdoored Random Oracles"" target=""_blank"">[https://eprint.iacr.org/2018/770]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-96881-0_10]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-319-96881-0_10]</a>
<a href=""DBLP"" target=""_blank"">[https://eprint.iacr.org/2018/770]</a>","We formulate and study the security of cryptographic hash functions in the backdoored random-oracle (BRO) model, whereby a big brother designs a...

","

","Springer
DBLP
DBLP"
Backdoored Hash Functions: Immunizing HMAC and HKDF,"Marc Fischlin, Christian Janson, Sogol Mazaheri","CSF
IACR Cryptol. ePrint Arch.","2018
2018","<a href=""DBLP (2018) : Backdoored Hash Functions: Immunizing HMAC and HKDF"" target=""_blank"">[https://doi.org/10.1109/CSF.2018.00015]</a>
<a href=""DBLP (2018) : Backdoored Hash Functions: Immunizing HMAC and HKDF"" target=""_blank"">[https://eprint.iacr.org/2018/362]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CSF.2018.00015]</a>
<a href=""DBLP"" target=""_blank"">[https://eprint.iacr.org/2018/362]</a>","
","
","DBLP
DBLP"
A Bayesian Multi-armed Bandit Approach for Identifying Human Vulnerabilities,"Erik Miehling, Baicen Xiao, ... Tamer Başar",Decision and Game Theory for Security,2018,"<a href=""Springer (2018) : A Bayesian Multi-armed Bandit Approach for Identifying Human Vulnerabilities"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-01554-1_30]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-01554-1_30]</a>",We consider the problem of identifying the set of users in an organization’s network that are most susceptible to falling victim to social...,,Springer
A Context-Based Model for Validating the Ability of Cyber Systems to Defend Against Attacks,"Yosra Lakhdhar, Slim Rekhis, Noureddine Boudriga",Ubiquitous Networking,2018,"<a href=""Springer (2018) : A Context-Based Model for Validating the Ability of Cyber Systems to Defend Against Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-02849-7_27]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-02849-7_27]</a>","Deploying security solutions to defend against known security attacks could fail not only due to policy, design, or implementation flaws in the...",,Springer
A Game Theoretical Framework for Inter-process Adversarial Intervention Detection,"Muhammed O. Sayin, Hossein Hosseini, ... Tamer Başar",Decision and Game Theory for Security,2018,"<a href=""Springer (2018) : A Game Theoretical Framework for Inter-process Adversarial Intervention Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-01554-1_28]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-01554-1_28]</a>","In this paper, we propose and analyze a two-level game theoretical framework to detect advanced and persistent threats across processes. The...",,Springer
A Hybrid Intrusion Detection System for Contemporary Network Intrusion Dataset,"Jheng-Mo Liao, Jui-Sheng Liu, Sheng-De Wang",Security with Intelligent Computing and Big-data Services,2018,"<a href=""Springer (2018) : A Hybrid Intrusion Detection System for Contemporary Network Intrusion Dataset"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-76451-1_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-76451-1_6]</a>",We propose a hybrid intrusion detection approach to detect network anomalies. The proposed approach uses a feature discrete method and a cluster...,,Springer
A Model of a Malware Infected Automated Guided Vehicle for Experimental Cyber-Physical Security,"Richard French, Viktoriya Degeler, Kevin Jones",Proceedings of SAI Intelligent Systems Conference (IntelliSys) 2016,2018,"<a href=""Springer (2018) : A Model of a Malware Infected Automated Guided Vehicle for Experimental Cyber-Physical Security"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-56991-8_49]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-56991-8_49]</a>","As part of a factory’s manufacturing cycle, materials need to move through a sequence of operations provided by work-cells, eventually culminating in...",,Springer
A Security Concern About Deep Learning Models,"Jiaxi Wu, Xiaotong Lin, ... Yi Tang",Science of Cyber Security,2018,"<a href=""Springer (2018) : A Security Concern About Deep Learning Models"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-03026-1_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-03026-1_15]</a>","This paper mainly studies on the potential safety hazards in the obstacle recognition and processing system (ORPS) of the self-driving cars, which is...",,Springer
A Two-Stage Classifier Approach for Network Intrusion Detection,"Wei Zong, Yang-Wai Chow, Willy Susilo",Information Security Practice and Experience,2018,"<a href=""Springer (2018) : A Two-Stage Classifier Approach for Network Intrusion Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-99807-7_20]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-99807-7_20]</a>",Network Intrusion Detection Systems (NIDS) are essential to combat security threats in network environments. These systems monitor and detect...,,Springer
A Vision for Enhancing Security of Cryptography in Executables,"Otto Brechelmacher, Willibald Krenn, Thorsten Tarrach",Engineering Secure Software and Systems,2018,"<a href=""Springer (2018) : A Vision for Enhancing Security of Cryptography in Executables"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-94496-8_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-94496-8_1]</a>",This paper proposes an idea on how to use existing techniques from late stage software customization to improve the security of software employing...,,Springer
ALIAS: A Modular Tool for Finding Backdoors for SAT,"Stepan Kochemazov, Oleg Zaikin",SAT,2018,"<a href=""DBLP (2018) : ALIAS: A Modular Tool for Finding Backdoors for SAT"" target=""_blank"">[https://doi.org/10.1007/978-3-319-94144-8_25]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-319-94144-8_25]</a>",,,DBLP
ATPG Binning and SAT-Based Approach to Hardware Trojan Detection for Safety-Critical Systems,"Animesh BasakChowdhury, Ansuman Banerjee, Bhargab B. Bhattacharya",Network and System Security,2018,"<a href=""Springer (2018) : ATPG Binning and SAT-Based Approach to Hardware Trojan Detection for Safety-Critical Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-02744-5_29]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-02744-5_29]</a>","Combating threats and attacks imposed by Hardware Trojans that are stealthily inserted in hardware systems, has surfaced as a challenging problem in...",,Springer
An Enhanced Approach to Fuzzy C-means Clustering for Anomaly Detection,"Ruby Sharma, Sandeep Chaurasia","Proceedings of First International Conference on Smart System, Innovations and Computing",2018,"<a href=""Springer (2018) : An Enhanced Approach to Fuzzy C-means Clustering for Anomaly Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-5828-8_60]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-5828-8_60]</a>","In the recent years, the improvement in the security is a challenging task in the Internet environment The Intrusion Detection System (IDS) is one of...",,Springer
Analysis and Computation of Adaptive Defense Strategies Against Advanced Persistent Threats for Cyber-Physical Systems,"Linan Huang, Quanyan Zhu",Decision and Game Theory for Security,2018,"<a href=""Springer (2018) : Analysis and Computation of Adaptive Defense Strategies Against Advanced Persistent Threats for Cyber-Physical Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-01554-1_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-01554-1_12]</a>","Cyber-physical systems are facing new security challenges from Advanced Persistent Threats (APTs) due to the stealthy, dynamic and adaptive nature of...",,Springer
Anomaly Detection System Using Beta Mixture Models and Outlier Detection,"Nour Moustafa, Gideon Creech, Jill Slay","Progress in Computing, Analytics and Networking",2018,"<a href=""Springer (2018) : Anomaly Detection System Using Beta Mixture Models and Outlier Detection"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-7871-2_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-7871-2_13]</a>","An intrusion detection system (IDS) plays a significant role in recognising suspicious activities in hosts or networks, even though this system still...",,Springer
"Architecting Cyber-Secure, Resilient System-of-Systems","Kurt Klingensmith, Azad M. Madni",Disciplinary Convergence in Systems Engineering Research,2018,"<a href=""Springer (2018) : Architecting Cyber-Secure, Resilient System-of-Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-62217-0_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-62217-0_12]</a>",The DoD system-of-systems (SoS) relies heavily on cyberspace operations. The latter tend to be vulnerable to a variety of disruptions. These...,,Springer
Asymmetric Subversion Attacks on Signature Schemes,"Chi Liu, Rongmao Chen, ... Yongjun Wang",Information Security and Privacy,2018,"<a href=""Springer (2018) : Asymmetric Subversion Attacks on Signature Schemes"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-93638-3_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-93638-3_22]</a>","Subversion attacks against cryptosystems have already received wide attentions since several decades ago, while the Snowden revelations in 2013...",,Springer
Automated Vulnerability Detection in Embedded Devices,"Danjun Liu, Yong Tang, ... Bo Yu",Advances in Digital Forensics XIV,2018,"<a href=""Springer (2018) : Automated Vulnerability Detection in Embedded Devices"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-99277-8_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-99277-8_17]</a>",Embedded devices are widely used today and are rapidly being incorporated in the Internet of Things that will permeate every aspect of society....,,Springer
Automatic Detection of Various Malicious Traffic Using Side Channel Features on TCP Packets,"George Stergiopoulos, Alexander Talavari, ... Dimitris Gritzalis",Computer Security,2018,"<a href=""Springer (2018) : Automatic Detection of Various Malicious Traffic Using Side Channel Features on TCP Packets"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-99073-6_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-99073-6_17]</a>","Modern intrusion detection systems struggle to detect advanced, custom attacks against most vectors, from web application injections to malware...",,Springer
Automatic Mitigation of Kernel Rootkits in Cloud Environments,"Jonathan Grimm, Irfan Ahmed, ... ManPyo Hong",Information Security Applications,2018,"<a href=""Springer (2018) : Automatic Mitigation of Kernel Rootkits in Cloud Environments"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-93563-8_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-93563-8_12]</a>","In cloud environments, the typical response to a malware attack is to snapshot and shutdown the virtual machine (VM), and revert it to a prior state....",,Springer
Backdoor Attacks on Neural Network Operations,"Joseph Clements, Yingjie Lao",GlobalSIP,2018,"<a href=""DBLP (2018) : Backdoor Attacks on Neural Network Operations"" target=""_blank"">[https://doi.org/10.1109/GlobalSIP.2018.8646335]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/GlobalSIP.2018.8646335]</a>",,,DBLP
Backdoor detection systems for embedded devices,Sam L. Thomas,,2018,"<a href=""DBLP (2018) : Backdoor detection systems for embedded devices"" target=""_blank"">[https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.753100]</a>","<a href=""DBLP"" target=""_blank"">[https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.753100]</a>",,,DBLP
"Backdoors: Definition, Deniability and Detection","Sam L. Thomas, Aurélien Francillon",RAID,2018,"<a href=""DBLP (2018) : Backdoors: Definition, Deniability and Detection"" target=""_blank"">[https://doi.org/10.1007/978-3-030-00470-5_5]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-030-00470-5_5]</a>",,,DBLP
Beneath the Bonnet: A Breakdown of Diagnostic Security,"Jan Van den Herrewegen, Flavio D. Garcia",Computer Security,2018,"<a href=""Springer (2018) : Beneath the Bonnet: A Breakdown of Diagnostic Security"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-99073-6_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-99073-6_15]</a>",An Electronic Control Unit (ECU) is an automotive computer essential to the operation of a modern car. Diagnostic protocols running on these ECUs are...,,Springer
Convolutional Neural Networks for Multi-class Intrusion Detection System,"Sasanka Potluri, Shamim Ahmed, Christian Diedrich",Mining Intelligence and Knowledge Exploration,2018,"<a href=""Springer (2018) : Convolutional Neural Networks for Multi-class Intrusion Detection System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-05918-7_20]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-05918-7_20]</a>",Advances in communication and networking technology leads to the use of internet-based technology in Industrial Control System (ICS) applications....,,Springer
Correcting Subverted Random Oracles,"Alexander Russell, Qiang Tang, ... Hong-Sheng Zhou",Advances in Cryptology – CRYPTO 2018,2018,"<a href=""Springer (2018) : Correcting Subverted Random Oracles"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-96881-0_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-96881-0_9]</a>","The random oracle methodology has proven to be a powerful tool for designing and reasoning about cryptographic schemes, and can often act as an...",,Springer
Cryptography with Dispensable Backdoors,"Kai-Min Chung, Marios Georgiou, Ching-Yi Lai, Vassilis Zikas",IACR Cryptol. ePrint Arch.,2018,"<a href=""DBLP (2018) : Cryptography with Dispensable Backdoors"" target=""_blank"">[https://eprint.iacr.org/2018/352]</a>","<a href=""DBLP"" target=""_blank"">[https://eprint.iacr.org/2018/352]</a>",,,DBLP
Cybersecurity Management Through Logging Analytics,"Michael Muggler, Rekha Eshwarappa, Ebru Celikel Cankaya",Advances in Human Factors in Cybersecurity,2018,"<a href=""Springer (2018) : Cybersecurity Management Through Logging Analytics"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-60585-2_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-60585-2_1]</a>","To make cybersecurity efforts proactive rather than solely reactive, this work proposes using machine learning to process large network related...",,Springer
Cybersecurity Vulnerabilities Assessment (A Systematic Review Approach),"Hossein Zare, Mohammad Jalal Zare, Mojgan Azadi",Information Technology - New Generations,2018,"<a href=""Springer (2018) : Cybersecurity Vulnerabilities Assessment (A Systematic Review Approach)"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-77028-4_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-77028-4_10]</a>","For analysis information technology and computer system vulnerabilities, this paper benefits from “systematic review analysis: 2000–2015” with...",,Springer
Cybersecurity: A Survey of Vulnerability Analysis and Attack Graphs,"Rachid Ait Maalem Lahcen, Ram Mohapatra, Manish Kumar",Mathematics and Computing,2018,"<a href=""Springer (2018) : Cybersecurity: A Survey of Vulnerability Analysis and Attack Graphs"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-2095-8_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-2095-8_9]</a>","The network infrastructure is the most critical technical asset of any organization. This network architecture must be useful, efficient, and secure....",,Springer
Detecting Bogus Messages in Vehicular Ad-Hoc Networks: An Information Fusion Approach,"Jizhao Liu, Heng Pan, ... Qiusheng Zheng",Wireless Sensor Networks,2018,"<a href=""Springer (2018) : Detecting Bogus Messages in Vehicular Ad-Hoc Networks: An Information Fusion Approach"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-8123-1_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-8123-1_17]</a>","In Vehicular ad hoc networks (VANETs), vehicles are allowed to broadcast messages for informing nearby vehicles about road condition and emergent...",,Springer
Discovering Similarities in Malware Behaviors by Clustering of API Call Sequences,"Fatima Al Shamsi, Wei Lee Woon, Zeyar Aung",Neural Information Processing,2018,"<a href=""Springer (2018) : Discovering Similarities in Malware Behaviors by Clustering of API Call Sequences"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-04212-7_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-04212-7_11]</a>","New genres of malware are evading detection by using polymorphism, obfuscation and encryption techniques. Hence, new strategies are needed to...",,Springer
EC135 Helicopter Fuselage,"N. Ashton, M. Fuchs, ... B. Duda",Go4Hybrid: Grey Area Mitigation for Hybrid RANS-LES Methods,2018,"<a href=""Springer (2018) : EC135 Helicopter Fuselage"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-52995-0_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-52995-0_10]</a>","The EC135 helicopter fuselage represents a realistic and challenging test-case with a range of complex flow physics, such as 3D separation and an...",,Springer
Enhancing the Security and Forensic Capabilities of Programmable Logic Controllers,"Chun-Fai Chan, Kam-Pui Chow, ... Ken Yau",Advances in Digital Forensics XIV,2018,"<a href=""Springer (2018) : Enhancing the Security and Forensic Capabilities of Programmable Logic Controllers"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-99277-8_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-99277-8_19]</a>","Industrial control systems are used to monitor and operate critical infrastructures. For decades, the security of industrial control systems was...",,Springer
Firmware Enhancements for BYOD-Aware Network Security,"Massimiliano Pedone, Kamen Kanev, ... Alessandro Mei",Recent Advances in Technology Research and Education,2018,"<a href=""Springer (2018) : Firmware Enhancements for BYOD-Aware Network Security"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-67459-9_34]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-67459-9_34]</a>","In today’s connected world, users migrate within a complex set of networks, including, but not limited to, 3G and 4G (LTE) services provided by...",,Springer
From Backdoor Key to Backdoor Completability: Improving a Known Measure of Hardness for the Satisfiable CSP,"Guillaume Escamocher, Mohamed Siala, Barry O&apos,Sullivan",CPAIOR,2018,"<a href=""DBLP (2018) : From Backdoor Key to Backdoor Completability: Improving a Known Measure of Hardness for the Satisfiable CSP"" target=""_blank"">[https://doi.org/10.1007/978-3-319-93031-2_14]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-319-93031-2_14]</a>",,,DBLP
Generating Misleading Labels in Machine Learning Models,"Xiaotong Lin, Jiaxi Wu, Yi Tang",Algorithms and Architectures for Parallel Processing,2018,"<a href=""Springer (2018) : Generating Misleading Labels in Machine Learning Models"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-05054-2_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-05054-2_12]</a>","Deep learning recently becomes popular because it brings significant improvements on a wide variety of classification and recognition tasks. However,...",,Springer
Hardware Trojans: An Austere Menace Ahead,"Anupam Tiwari, Chetan Soni",Cyber Security,2018,"<a href=""Springer (2018) : Hardware Trojans: An Austere Menace Ahead"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-8536-9_34]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-8536-9_34]</a>","Hardware Trojans, a relatively unheard threat viz-a-viz the typical software-based malwares and virus attacks that keep betiding across is being...",,Springer
Hidden in Plain Sight: Filesystem View Separation for Data Integrity and Deception,"Teryl Taylor, Frederico Araujo, ... Marc Ph. Stoecklin","Detection of Intrusions and Malware, and Vulnerability Assessment",2018,"<a href=""Springer (2018) : Hidden in Plain Sight: Filesystem View Separation for Data Integrity and Deception"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-93411-2_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-93411-2_12]</a>","Cybercrime has become a big money business with sensitive data being a hot commodity on the dark web. In this paper, we introduce and evaluate a...",,Springer
Hijacking Your Routers via Control-Hijacking URLs in Embedded Devices with Web Interfaces,"Ming Yuan, Ye Li, Zhoujun Li",Information and Communications Security,2018,"<a href=""Springer (2018) : Hijacking Your Routers via Control-Hijacking URLs in Embedded Devices with Web Interfaces"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-89500-0_32]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-89500-0_32]</a>","Embedded devices start to get into the lives of ordinary people, such as SOHO routers and IP camera. However, studies have shown that the safety...",,Springer
Host Based Intrusion Detection and Prevention Model Against DDoS Attack in Cloud Computing,"Aws Naser Jaber, Mohamad Fadli Zolkipli, ... Mohammed R. Jassim","Advances on P2P, Parallel, Grid, Cloud and Internet Computing",2018,"<a href=""Springer (2018) : Host Based Intrusion Detection and Prevention Model Against DDoS Attack in Cloud Computing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-69835-9_23]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-69835-9_23]</a>","Cloud computing has become an innovative technology. Recent advances in hardware and software have put tremendous pressure on administrators, who...",,Springer
Investigation on Unauthorized Human Activity Watching Through Leveraging Wi-Fi Signals,"Md Zakirul Alam Bhuiyan, Md. Monirul Islam, ... Tian Wang","Security, Privacy, and Anonymity in Computation, Communication, and Storage",2018,"<a href=""Springer (2018) : Investigation on Unauthorized Human Activity Watching Through Leveraging Wi-Fi Signals"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-05345-1_44]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-05345-1_44]</a>","Wireless signals, including Wi-Fi signals have become ubiquitous. Recent advancement shows that such signals can be leveraged for many applications...",,Springer
Learning-Sensitive Backdoors with Restarts,"Edward Zulkoski, Ruben Martins, Christoph M. Wintersteiger, Robert Robere, Jia Hui Liang, Krzysztof Czarnecki, Vijay Ganesh",CP,2018,"<a href=""DBLP (2018) : Learning-Sensitive Backdoors with Restarts"" target=""_blank"">[https://doi.org/10.1007/978-3-319-98334-9_30]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-319-98334-9_30]</a>",,,DBLP
Liability Exposure when 3D-Printed Parts Fall from the Sky,"Lynne Graves, Mark Yampolskiy, ... Yuval Elovici",Critical Infrastructure Protection XII,2018,"<a href=""Springer (2018) : Liability Exposure when 3D-Printed Parts Fall from the Sky"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-04537-1_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-04537-1_3]</a>","Additive manufacturing, also referred to as 3D printing, has become viable for manufacturing functional parts. For example, the U.S. Federal Aviation...",,Springer
Malware Analysis and Detection Using Data Mining and Machine Learning Classification,"Mozammel Chowdhury, Azizur Rahman, Rafiqul Islam",International Conference on Applications and Techniques in Cyber Security and Intelligence,2018,"<a href=""Springer (2018) : Malware Analysis and Detection Using Data Mining and Machine Learning Classification"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-67071-3_33]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-67071-3_33]</a>",Exfiltration of sensitive data by malicious software or malware is a serious cyber threat around the world that has catastrophic effect on...,,Springer
Mining Unknown Network Protocol’s Stealth Attack Behavior,Yan-Jing Hu,Advances in Intelligent Networking and Collaborative Systems,2018,"<a href=""Springer (2018) : Mining Unknown Network Protocol’s Stealth Attack Behavior"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-65636-6_49]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-65636-6_49]</a>",Unknown network protocol’s Stealth Attack behavior is becoming a new vehicle to conduct invisible cyber attacks. This kind of attack is latent for a...,,Springer
Moving Target Defense for the Placement of Intrusion Detection Systems in the Cloud,"Sailik Sengupta, Ankur Chowdhary, ... Subbarao Kambhampati",Decision and Game Theory for Security,2018,"<a href=""Springer (2018) : Moving Target Defense for the Placement of Intrusion Detection Systems in the Cloud"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-01554-1_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-01554-1_19]</a>","A lot of software systems are deployed in the cloud. Owing to realistic demands for an early product launch, oftentimes there are vulnerabilities...",,Springer
"NOR: Towards Non-intrusive, Real-Time and OS-agnostic Introspection for Virtual Machines in Cloud Environment","Chonghua Wang, Zhiyu Hao, Xiaochun Yun",Information Security and Cryptology,2018,"<a href=""Springer (2018) : NOR: Towards Non-intrusive, Real-Time and OS-agnostic Introspection for Virtual Machines in Cloud Environment"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-75160-3_29]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-75160-3_29]</a>",Cloud platforms of large enterprises are witnessing increasing adoption of the Virtual Machine Introspection (VMI) technology for building a wide...,,Springer
Network Forensics Investigation for Botnet Attack,"Irwan Sembiring, Yonathan Satrio Nugroho",IT Convergence and Security 2017,2018,"<a href=""Springer (2018) : Network Forensics Investigation for Botnet Attack"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-6454-8_29]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-6454-8_29]</a>",Nowadays the internet users manipulated by several web applications which instruct them to download and install programs in order to interfere the...,,Springer
Next Generation Cryptographic Ransomware,"Ziya Alper Genç, Gabriele Lenzini, Peter Y. A. Ryan",Secure IT Systems,2018,"<a href=""Springer (2018) : Next Generation Cryptographic Ransomware"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-03638-6_24]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-03638-6_24]</a>",We are assisting at an evolution in the ecosystem of cryptoware —the malware that encrypts files and makes them unavailable unless the victim pays...,,Springer
On the Existence of Non-Linear Invariants and Algebraic Polynomial Constructive Approach to Backdoors in Block Ciphers,Nicolas T. Courtois,IACR Cryptol. ePrint Arch.,2018,"<a href=""DBLP (2018) : On the Existence of Non-Linear Invariants and Algebraic Polynomial Constructive Approach to Backdoors in Block Ciphers"" target=""_blank"">[https://eprint.iacr.org/2018/807]</a>","<a href=""DBLP"" target=""_blank"">[https://eprint.iacr.org/2018/807]</a>",,,DBLP
Opening Pandora’s Box: Effective Techniques for Reverse Engineering IoT Devices,"Omer Shwartz, Yael Mathov, ... Yossi Oren",Smart Card Research and Advanced Applications,2018,"<a href=""Springer (2018) : Opening Pandora’s Box: Effective Techniques for Reverse Engineering IoT Devices"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-75208-2_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-75208-2_1]</a>","With the growth of the Internet of Things, many insecure embedded devices are entering into our homes and businesses. Some of these web-connected...",,Springer
Perception Mining of Network Protocol’s Dormant Behavior,Yan-Jing Hu,"Advances in Internetworking, Data & Web Technologies",2018,"<a href=""Springer (2018) : Perception Mining of Network Protocol’s Dormant Behavior"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-59463-7_30]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-59463-7_30]</a>","Unknown network protocol’s dormant behavior is becoming a new type of stealth attack, which greatly harms the cyber space security, and seriously...",,Springer
Proof-of-Blackouts? How Proof-of-Work Cryptocurrencies Could Affect Power Grids,"Johanna Ullrich, Nicholas Stifter, ... Edgar Weippl","Research in Attacks, Intrusions, and Defenses",2018,"<a href=""Springer (2018) : Proof-of-Blackouts? How Proof-of-Work Cryptocurrencies Could Affect Power Grids"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-00470-5_9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-00470-5_9]</a>","With respect to power consumption, cryptocurrencies have been discussed in a twofold way: First, the cost-benefit ratio of mining hardware in order...",,Springer
Protection of Information in Networks Based on Methods of Machine Learning,"Sergey G. Antipov, Vadim N. Vagin, ... Marina V. Fomina",Artificial Intelligence,2018,"<a href=""Springer (2018) : Protection of Information in Networks Based on Methods of Machine Learning"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-00617-4_25]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-00617-4_25]</a>",The paper considers the possibility of using artificial intelligence methods in information security tasks: methods for generating inductive concepts...,,Springer
Real-Time Framework for Malware Detection Using Machine Learning Technique,"Sharma Divya Mukesh, Jigar A. Raval, Hardik Upadhyay",Information and Communication Technology for Intelligent Systems (ICTIS 2017) - Volume 1,2018,"<a href=""Springer (2018) : Real-Time Framework for Malware Detection Using Machine Learning Technique"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-63673-3_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-63673-3_21]</a>","In this epoch, current web world where peoples groups are associated through correspondence channel and the majority of their information is...",,Springer
Remote Desktop Backdoor Implementation with Reverse TCP Payload Using Open Source Tools for Instructional Use,"Yaswanth Kolli, Tauheed Khan Mohd, Ahmad Y. Javaid",EIT,2018,"<a href=""DBLP (2018) : Remote Desktop Backdoor Implementation with Reverse TCP Payload Using Open Source Tools for Instructional Use"" target=""_blank"">[https://doi.org/10.1109/EIT.2018.8500174]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/EIT.2018.8500174]</a>",,,DBLP
Research on Cloud-Based on Web Application Malware Detection Methods,"Ki-Hwan Kim, Dong-IL Lee, Yong-Tae Shin",Advances in Computer Science and Ubiquitous Computing,2018,"<a href=""Springer (2018) : Research on Cloud-Based on Web Application Malware Detection Methods"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-7605-3_130]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-7605-3_130]</a>","Recently, the use of the Internet has increased, and the spread of malicious code through web application vulnerabilities has become a major threat....",,Springer
SCADA: Analysis of Attacks on Communication Protocols,"T. C. Pramod, N. R. Sunitha","Proceedings of International Symposium on Sensor Networks, Systems and Security",2018,"<a href=""Springer (2018) : SCADA: Analysis of Attacks on Communication Protocols"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-75683-7_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-75683-7_17]</a>",SCADA (supervisory control and data acquisition) systems are used to monitor and control the processes of industrial facilities remotely. The use of...,,Springer
"Secure Data Deduplication and Efficient Storage Utilization in Cloud Servers Using Encryption, Compression and Integrity Auditing","Arya S. Nair, B. Radhakrishnan, ... Padma Suresh Lekshmi Kanthan",Soft Computing Systems,2018,"<a href=""Springer (2018) : Secure Data Deduplication and Efficient Storage Utilization in Cloud Servers Using Encryption, Compression and Integrity Auditing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-1936-5_35]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-13-1936-5_35]</a>","A burning issue of recent times is the concern regarding the management of huge volume of data. Since the local devices has its limits in storage,...",,Springer
Security Considerations of Modern Embedded Devices and Networking Equipment,Błażej Adamczyk,Computer Networks,2018,"<a href=""Springer (2018) : Security Considerations of Modern Embedded Devices and Networking Equipment"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-92459-5_30]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-92459-5_30]</a>",The aim of this paper is to present the potential impact and risks related with security breaches in modern networking equipment and embedded devices...,,Springer
Security of Mobile Banking Applications,"Michał Szczepanik, Ireneusz Jóźwiak",Advanced Solutions in Diagnostics and Fault Tolerant Control,2018,"<a href=""Springer (2018) : Security of Mobile Banking Applications"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-64474-5_35]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-64474-5_35]</a>","In this paper authors presents report about current Android applications security. OWASP’s top 10 mobile security risks was used, to verify level of...",,Springer
Side-Channel Based Intrusion Detection for Industrial Control Systems,"Pol Van Aubel, Kostas Papagiannopoulos, ... Christian Doerr",Critical Information Infrastructures Security,2018,"<a href=""Springer (2018) : Side-Channel Based Intrusion Detection for Industrial Control Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-99843-5_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-99843-5_19]</a>","Industrial Control Systems are under increased scrutiny. Their security is historically sub-par, and although measures are being taken by the...",,Springer
Subversion-Zero-Knowledge SNARKs,Georg Fuchsbauer,Public-Key Cryptography – PKC 2018,2018,"<a href=""Springer (2018) : Subversion-Zero-Knowledge SNARKs"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-76578-5_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-76578-5_11]</a>",Subversion zero knowledge for non-interactive proof systems demands that zero knowledge (ZK) be maintained even when the common reference string...,,Springer
TProv: Towards a Trusted Provenance-Aware Service Based on Trusted Computing,"Wu Luo, Anbang Ruan, ... Zhonghai Wu",Web Services – ICWS 2018,2018,"<a href=""Springer (2018) : TProv: Towards a Trusted Provenance-Aware Service Based on Trusted Computing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-94289-6_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-94289-6_5]</a>","With the rapid development of cloud computing, system and data security become concerns due to user losing control of his machines and internal...",,Springer
Technical perspective: Backdoor engineering,Markus G. Kuhn,Commun. ACM,2018,"<a href=""DBLP (2018) : Technical perspective: Backdoor engineering"" target=""_blank"">[https://doi.org/10.1145/3266289]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3266289]</a>",,,DBLP
Too Big to FAIL: What You Need to Know Before Attacking a Machine Learning System,"Tudor Dumitraş, Yiğitcan Kaya, ... Octavian Suciu",Security Protocols XXVI,2018,"<a href=""Springer (2018) : Too Big to FAIL: What You Need to Know Before Attacking a Machine Learning System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-03251-7_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-03251-7_17]</a>",There is an emerging arms race in the field of adversarial machine learning (AML). Recent results suggest that machine learning (ML) systems are...,,Springer
Towards Accuracy in Similarity Analysis of Android Applications,"Sreesh Kishore, Renuka Kumar, Sreeranga Rajan",Information Systems Security,2018,"<a href=""Springer (2018) : Towards Accuracy in Similarity Analysis of Android Applications"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-05171-6_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-05171-6_8]</a>",Android malware is most commonly delivered to a user through the many open app marketplaces. Several recent attacks have shown that the same malware...,,Springer
Towards Developing Network Forensic Mechanism for Botnet Activities in the IoT Based on Machine Learning Techniques,"Nickolaos Koroniotis, Nour Moustafa, ... Jill Slay",Mobile Networks and Management,2018,"<a href=""Springer (2018) : Towards Developing Network Forensic Mechanism for Botnet Activities in the IoT Based on Machine Learning Techniques"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-90775-8_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-90775-8_3]</a>",The IoT is a network of interconnected everyday objects called “things” that have been augmented with a small measure of computing capabilities....,,Springer
USBlock: Blocking USB-Based Keypress Injection Attacks,"Sebastian Neuner, Artemios G. Voyiatzis, ... Edgar R. Weippl",Data and Applications Security and Privacy XXXII,2018,"<a href=""Springer (2018) : USBlock: Blocking USB-Based Keypress Injection Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-95729-6_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-95729-6_18]</a>",The Universal Serial Bus (USB) is becoming a prevalent attack vector. Rubber Ducky and BadUSB are two recent classes of a whole spectrum of attacks...,,Springer
Using Combinatorial Benchmarks to Probe the Reasoning Power of Pseudo-Boolean Solvers,"Jan Elffers, Jesús Giráldez-Cru, ... Marc Vinyals",Theory and Applications of Satisfiability Testing – SAT 2018,2018,"<a href=""Springer (2018) : Using Combinatorial Benchmarks to Probe the Reasoning Power of Pseudo-Boolean Solvers"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-94144-8_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-94144-8_5]</a>","We study cdcl-cuttingplanes, Open-WBO, and Sat4j, three successful solvers from the Pseudo-Boolean Competition 2016, and evaluate them by performing...",,Springer
VCIDS: Collaborative Intrusion Detection of Sensor and Actuator Attacks on Connected Vehicles,"Pinyao Guo, Hunmin Kim, ... Peng Liu",Security and Privacy in Communication Networks,2018,"<a href=""Springer (2018) : VCIDS: Collaborative Intrusion Detection of Sensor and Actuator Attacks on Connected Vehicles"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-78813-5_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-78813-5_19]</a>","Modern urban vehicles adopt sensing, communication and computing modules into almost every functioning aspect to assist humans in driving. However,...",,Springer
VeriCount: Verifiable Resource Accounting Using Hardware and Software Isolation,"Shruti Tople, Soyeon Park, ... Prateek Saxena",Applied Cryptography and Network Security,2018,"<a href=""Springer (2018) : VeriCount: Verifiable Resource Accounting Using Hardware and Software Isolation"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-93387-0_34]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-93387-0_34]</a>","In cloud computing, where clients are billed based on the consumed resources for outsourced tasks, both the cloud providers and the clients have the...",,Springer
VulAware: Towards Massive-Scale Vulnerability Detection in Cyberspace,"Zhiqiang Wang, Pingchuan Ma, ... Tao Yang",Machine Learning and Intelligent Communications,2018,"<a href=""Springer (2018) : VulAware: Towards Massive-Scale Vulnerability Detection in Cyberspace"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-00557-3_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-030-00557-3_15]</a>","Due to the delay of threat warning and vulnerability fixing, the critical servers in cyberspace are under potential threat. With the help of...",,Springer
Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning,"Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, Dawn Song","arXiv
arXiv","2017-12-15
2017-12","<a href=""arXiv (2017-12-15) : Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning"" target=""_blank"">[http://arxiv.org/abs/1712.05526v1]</a>
<a href=""DBLP (2017-12) : Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning"" target=""_blank"">[http://arxiv.org/abs/1712.05526]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1712.05526]</a>","Deep learning models have achieved high performance on many tasks, and thus have been applied to many security-critical scenarios. For example, deep learning-based face recognition systems have been used to authenticate users to access many security-sensitive applications like payment apps. Such usages of deep learning systems provide the adversaries with sufficient incentives to perform attacks against these systems for their adversarial purposes. In this work, we consider a new type of attacks, called backdoor attacks, where the attacker's goal is to create a backdoor into a learning-based authentication system, so that he can easily circumvent the system by leveraging the backdoor. Specifically, the adversary aims at creating backdoor instances, so that the victim learning system will be misled to classify the backdoor instances as a target label specified by the adversary. In particular, we study backdoor poisoning attacks, which achieve backdoor attacks using poisoning strategies. Different from all existing work, our studied poisoning strategies can apply under a very weak threat model: (1) the adversary has no knowledge of the model and the training set used by the victim system, (2) the attacker is allowed to inject only a small amount of poisoning samples, (3) the backdoor key is hard to notice even by human beings to achieve stealthiness. We conduct evaluation to demonstrate that a backdoor adversary can inject only around 50 poisoning samples, while achieving an attack success rate of above 90%. We are also the first work to show that a data poisoning attack can create physically implementable backdoors without touching the training process. Our work demonstrates that backdoor poisoning attacks pose real threats to a learning system, and thus highlights the importance of further investigation and proposing defense strategies against them.
","
","arXiv
DBLP"
"Guest Editorial Securing IoT Hardware: Threat Models and Reliable, Low-Power Design Solutions",A. Sengupta S. Kundu,IEEE Transactions on Very Large Scale Integration (VLSI) Systems,2017-11-22,"<a href=""IEEE (2017-11-22) : Guest Editorial Securing IoT Hardware: Threat Models and Reliable, Low-Power Design Solutions"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8118269]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TVLSI.2017.2762398]</a>","It is well understood that for Internet of Things (IoT), security of underlying hardware is the key to safe and reliable operation. IoT service stack relies on security of network, software, and firmware, all of which, in turn, depend on functionality provided by the underlying hardware. The hardware may be compromised or attacked by multiple threat actors. The designer may create a backdoor that leaks vital information such as encryption key used in secure channel the manufacturer may tamper the design by inserting hardware Trojans or introducing artifacts with known reliability vulnerabilities. Either of these actors may enable writing into protected memory areas that may store secure hash of trusted code base, allowing malware to boot directly on the hardware. Today’s designs integrate IP blocks from multiple vendors manufactured, tested, and repaired by different companies spanning across the globe. Consequently, there are many entry points for the hardware to be compromised. For a trusted hardware design, protection and security of intellectual property cores are of paramount importance. This special section aims to publish novel solutions for security problems related to hardware used in IoT. •Secured IoT Hardware: Induction of any form of third-party intervention in the hardware design methodology may raise grave security concern for IoT hardware. Securing IoT hardware can be in the form of protecting intellectual property cores against false claim of ownership/piracy/counterfeit. The first form of security measure requires anti-piracy methodologies such as digital watermarking, hardware metering, computational forensic engineering, and obfuscation that can nullify the false claim of ownership or detect unauthorized pirated designs. The second form of threat, which is formally called “hardware Trojan,” is an act of deliberate insertion into a design (such as intellectual property core, hardware) by a rogue designer or vendor, and also requires detection/correction strategies as a security measure. Both hardware threats discussed above may occur in any of the design abstraction levels (behavioral, register transfer, layout, etc.). Handling the threats higher in the abstraction level provides more assurance against possible attacks, however, it requires a more sophisticated approach. Further more, the level at which protective measure is applied often dictates the preprocessing or postprocessing style of the approach. These calls for novel technique that embeds hardware security measure a higher abstraction level for protection of IoT devices.•Reliable IoT Hardware: Due to multiple factors affecting reliability of hardware used in IoT devices, these devices are always at a risk of malfunctioning. For example, a manufacturer may deliberately change the width of a metal line for causing premature electromigration defect, possibly triggering a timed Trojan. Multiple trigger mechanisms may be used to attack hardware such as: 1) reducing device dimensions 2) scaling supply voltage and 3) modulating frequency of operation. Methodologies should incorporate techniques that provide resiliency/tolerance against such faults at higher abstraction levels to assure greater reliability from the beginning of design flow.•Low-Cost IoT Hardware: Another design aspect of hardware for IoT devices is performance and power. Consumer demand drives integration of multiple functionalities, often achieved by integrating dedicated IP cores and general purpose processors working in tandem. This creates a unique challenge in maintaining security and integrity of data passing through various IP blocks. Standard solutions involving redundancy, diversity, and check run up against power, performance, and latency constraints.",,IEEE
A new evolutionary neural networks based on intrusion detection systems using multiverse optimization,"Ilyas Benmessahel, Kun Xie, Mouna Chellal",Applied Intelligence,2017-11-07,"<a href=""Springer (2017-11-07) : A new evolutionary neural networks based on intrusion detection systems using multiverse optimization"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-017-1085-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10489-017-1085-y]</a>",Building an intrusion detection system (IDS) has become an increasingly urgent issue for detecting network security breaches in computer and network...,,Springer
Dynamically enabled defense effectiveness evaluation of a home Internet based on vulnerability analysis and attack layer measurement,"Min Lei, Yu Yang, ... Minyao Ma",Personal and Ubiquitous Computing,2017-11-02,"<a href=""Springer (2017-11-02) : Dynamically enabled defense effectiveness evaluation of a home Internet based on vulnerability analysis and attack layer measurement"" target=""_blank"">[https://link.springer.com/article/10.1007/s00779-017-1084-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00779-017-1084-3]</a>","Smart devices in a home Internet, such as routers and cameras, suffer malicious attacks from hackers on a daily basis. Defenders should change system...",,Springer
Adrenal C11-oxy C<inf>21</inf> steroids contribute to the C11-oxy C<inf>19</inf> steroid pool via the backdoor pathway in the biosynthesis and metabolism of 21-deoxycortisol and 21-deoxycortisone,Barnard L.,Journal of Steroid Biochemistry and Molecular Biology,2017-11-01,"<a href=""ScienceDirect (2017-11-01) : Adrenal C11-oxy C<inf>21</inf> steroids contribute to the C11-oxy C<inf>19</inf> steroid pool via the backdoor pathway in the biosynthesis and metabolism of 21-deoxycortisol and 21-deoxycortisone"" target=""_blank"">[https://doi.org/10.1016/j.jsbmb.2017.07.034]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.jsbmb.2017.07.034]</a>",,,ScienceDirect
A temporal correlation and traffic analysis approach for APT attacks detection,"Jiazhong Lu, Kai Chen, ... XiaoSong Zhang",Cluster Computing,2017-10-31,"<a href=""Springer (2017-10-31) : A temporal correlation and traffic analysis approach for APT attacks detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-017-1256-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-017-1256-y]</a>",Advanced persist threat (APT for short) is an emerging attack on the Internet. Such attack patterns leave their footprints spatio-temporally...,,Springer
Computer virus and protection methods using lab analysis,H. A. Khan A. Syed A. Mohammad M. N. Halgamuge,2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA),2017-10-23,"<a href=""IEEE (2017-10-23) : Computer virus and protection methods using lab analysis"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8078765]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICBDA.2017.8078765]</a>","The aim of this paper is to explore the hypothesis of a computer virus threat, and how destructive it can be if executed on a targeted machine. What are the possible counter measures to protect computers from these threats? In this study, we performed an analysis from the data extracted from different test of scenarios and labs conducted in a test environment. Information security risks associated with computer viruses can infect computers and other storage devices by copying themselves into a file and other executable programs. These file get infection and allow attackers to connect to target systems by using backdoors. The results of this study show that, the proper security implementations and the use of up to date operating systems patches and anti-virus programs helps users to prevent the loss of data and any viral attack on the system. Nevertheless, this observation could be used for further research in the network security and related fields this study will also help computer users to use the possible steps and techniques to protect their systems and information from any possible attacks on their network systems.",,IEEE
Computing with obfuscated data in arbitrary logic circuits via noise insertion and cancellation,Y. -W. Lee N. A. Touba,2017 IEEE Conference on Dependable and Secure Computing,2017-10-19,"<a href=""IEEE (2017-10-19) : Computing with obfuscated data in arbitrary logic circuits via noise insertion and cancellation"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8073840]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/DESEC.2017.8073840]</a>","In secure computing, sensitive data must be kept private by protecting it from being obtained by an attacker. Existing techniques for computing with encrypted data are either prohibitively expensive (e.g., fully homomorphic encryption) or only work for special cases. (e.g., only for linear circuits). This paper presents a lightweight methodology for computing with noise-obfuscated data by carefully selecting internal locations for noise cancellation in arbitrary logic circuits. Noise is inserted in the data before computation and then partially cancelled during the computation and fully cancelled at the outputs. While the proposed methodology does not provide the level of strong encryption that fully homomorphic encryption would provide, it has the advantage of being lightweight, easy to implement, and can be deployed with relatively minimal performance impact. A key idea in the proposed approach is to reduce the complexity of the noise cancellation logic by carefully selecting internal locations to do local noise canceling. This is done in a way that prevents more than one input per gate from propagating noise thereby avoiding the complexity that arises from reconvergent noise propagation paths. One important application of the proposed scheme is for protecting data inside a computing unit obtained from a third party IP provider where a hidden backdoor access mechanism or hardware Trojan could be maliciously inserted. Experimental results show that noise can be propagated to outputs with overheads ranging from (13%-56%).",,IEEE
Attacks in Online and OTIP,Vivekanandhan Sivasubramani,2017 International Conference on Technical Advancements in Computers and Communications (ICTACC),2017-10-16,"<a href=""IEEE (2017-10-16) : Attacks in Online and OTIP"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8067578]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICTACC.2017.27]</a>","With the developing digital era, the users are more vulnerable to various types of security threats such as Phishing(RAT), a serious security threat to the internet users in which the intruder sends an email which looks legitimate, where the RATs are usually downloaded invisibly with a user - requested program such as game or in this case an email attachment. RATs provide a backdoor for administrative control over the targeted computer, from which the intruder will be allowed to access all sensitive and confidential data such as banking application, which needs more security. It is important to prevent such phishing attacks. One of the ways to prevent the password theft is to authenticate a user without the use of the text password. In this paper we propose an idea which eliminates the use of the permanent text passwords, by authenticating the user through image based password. After image based authentication, the user will obtain the One Time Password (OTP) using the messaging service available in the internet. The image based authentication method relies on the user's ability to recognize the pre-chosen images from a grid of pictures which appears in a random manner. This paper integrates One time Image password based authentication and HMAC based one time password and to achieve high level of security in authenticating the user and these algorithms are very economical to implement.",,IEEE
Attacks in online and OTIP,Vivekanandhan,"Proceedings - 2017 International Conference on Technical Advancements in Computers and Communication, ICTACC 2017",2017-10-12,"<a href=""ScienceDirect (2017-10-12) : Attacks in online and OTIP"" target=""_blank"">[https://doi.org/10.1109/ICTACC.2017.27]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ICTACC.2017.27]</a>",,,ScienceDirect
TestREx: a framework for repeatable exploits,"Stanislav Dashevskyi, Daniel Ricardo dos Santos, ... Antonino Sabetta",International Journal on Software Tools for Technology Transfer,2017-09-25,"<a href=""Springer (2017-09-25) : TestREx: a framework for repeatable exploits"" target=""_blank"">[https://link.springer.com/article/10.1007/s10009-017-0474-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10009-017-0474-1]</a>","Web applications are the target of many well-known exploits and also a fertile ground for the discovery of security vulnerabilities. Yet, the success...",,Springer
Androgen production in pediatric adrenocortical tumors may occur via both the classic and/or the alternative backdoor pathway,Marti N.,Molecular and Cellular Endocrinology,2017-09-05,"<a href=""ScienceDirect (2017-09-05) : Androgen production in pediatric adrenocortical tumors may occur via both the classic and/or the alternative backdoor pathway"" target=""_blank"">[https://doi.org/10.1016/j.mce.2017.05.014]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.mce.2017.05.014]</a>",,,ScienceDirect
"A survey on smart power grid: frameworks, tools, security issues, and solutions","B. B. Gupta, Tafseer Akhtar",Annals of Telecommunications,2017-09-01,"<a href=""Springer (2017-09-01) : A survey on smart power grid: frameworks, tools, security issues, and solutions"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-017-0605-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12243-017-0605-4]</a>","Smart power grid is referred to as the next revolutionary innovation in electric power generation, transmission, and distribution technology. Smart...",,Springer
A Framework for Recognition and Confronting of Obfuscated Malwares Based on Memory Dumping and Filter Drivers,"Danial Javaheri, Mehdi Hosseinzadeh",Wireless Personal Communications,2017-08-17,"<a href=""Springer (2017-08-17) : A Framework for Recognition and Confronting of Obfuscated Malwares Based on Memory Dumping and Filter Drivers"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-017-4859-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-017-4859-y]</a>","In this paper obfuscation techniques used by novel malwares presented and compared. IAT smashing, string encryption and dynamic programing are...",,Springer
Biometrics Data Security Techniques for Portable Mobile Devices,"S. Roy, S. Matloob, ... W. I. Davis",INAE Letters,2017-08-16,"<a href=""Springer (2017-08-16) : Biometrics Data Security Techniques for Portable Mobile Devices"" target=""_blank"">[https://link.springer.com/article/10.1007/s41403-017-0026-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41403-017-0026-8]</a>","In this work, a specific case-study of the development of a biometric authentication system for mobile clients (iPads) has been reported. The...",,Springer
The effect of bias on the guesswork of hash functions,Y. Yona S. Diggavi,2017 IEEE International Symposium on Information Theory (ISIT),2017-08-14,"<a href=""IEEE (2017-08-14) : The effect of bias on the guesswork of hash functions"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8006929]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ISIT.2017.8006929]</a>","In this work we analyze the average guesswork for the problem of hashed password cracking (i.e., finding a password that has the same hash value as the actual password), when averaging over all hash functions whose effective distribution is i.i.d. Bernoulli(p) for any strategy of guessing passwords one by one (i.e., the fractions of passwords that are hashed to any bin, correspond to a probability mass function, which is i.i.d. Bernoulli(p)). We analyze the average guesswork under both online and offline attacks by deriving upper and lower bounds on the average guesswork as a function of the bins to which passwords are hashed, along with the most likely average guesswork, that is, the average guesswork of the most likely set of bins. Furthermore, we provide a concentration result that shows for this problem, that the probability mass function of guesswork is concentrated around its mean value. These results give quantifiable bounds for the effect of bias as well as the number of users on the average guesswork of a hash function, and show that increasing the number of users has a far worse effect than bias in terms of the average guesswork. However, when there exists a backdoor mechanism that enables “beamforming” certain passwords to the least likely bins, bias can in fact increase the average guesswork.",,IEEE
Linux kernel OS local root exploit,A. P. Saleel M. Nazeer B. D. Beheshti,"2017 IEEE Long Island Systems, Applications and Technology Conference (LISAT)",2017-08-07,"<a href=""IEEE (2017-08-07) : Linux kernel OS local root exploit"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8001953]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/LISAT.2017.8001953]</a>","Dirty Copy on Write (COW) vulnerability, discovered by Phil Oester on October 2016, it is a serious vulnerability which could escalate unprivileged user to gain full control on devices (Computers, Mobile Smart Phones, Gaming devices that run Linux based operating systems). This means that any user who exploits this bug, would escalate his/her privileges and can do anything either locally or remotely (with some modifications) to hijack the device, destroy data, create a backdoor, or to record all key strokes, use computer as an attack (object) to attack other computer in the internet (in the wild), etc., COW is a local root exploit in Linux causing vulnerability issues. This paper will discuss the copy on write issue in Linux, it will also explain the nature of the problem and how it is caused, and the different mechanism to mitigate it.",,IEEE
Deep learning based basketball video analysis for intelligent arena application,"Wu Liu, Chenggang Clarence Yan, ... Huadong Ma",Multimedia Tools and Applications,2017-08-02,"<a href=""Springer (2017-08-02) : Deep learning based basketball video analysis for intelligent arena application"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-017-5002-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11042-017-5002-5]</a>","Given the tremendous growth of sport fans, the “Intelligent Arena”, which can greatly improve the fun of traditional sports, becomes one of the...",,Springer
A certificateless signature scheme and a certificateless public auditing scheme with authority trust level 3+,"Fei Li, Dongqing Xie, ... Roberto Metere",Journal of Ambient Intelligence and Humanized Computing,2017-08-01,"<a href=""Springer (2017-08-01) : A certificateless signature scheme and a certificateless public auditing scheme with authority trust level 3+"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-017-0553-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12652-017-0553-x]</a>",Many certificateless cryptosystems have been proposed for cloud security applications. These applications have to face the inherent issues of dealing...,,Springer
Checking virtual machine kernel control-flow integrity using a page-level dynamic tracing approach,"Dongyang Zhan, Lin Ye, ... Xiaojiang Du",Soft Computing,2017-07-31,"<a href=""Springer (2017-07-31) : Checking virtual machine kernel control-flow integrity using a page-level dynamic tracing approach"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-017-2745-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00500-017-2745-x]</a>",Kernel control-flow integrity (CFI) of virtual machines is very important to cloud security. VMI-based dynamic tracing and analyzing methods are...,,Springer
One-to-cloud one-time pad data encryption: Introducing virtual prototyping with PSpice,P. Tobin L. Tobin R. G. Blanquer M. McKeever J. Blackledge,2017 28th Irish Signals and Systems Conference (ISSC),2017-07-20,"<a href=""IEEE (2017-07-20) : One-to-cloud one-time pad data encryption: Introducing virtual prototyping with PSpice"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7983647]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ISSC.2017.7983647]</a>","In this paper, we examine the design and application of a one-time pad encryption system for protecting data stored in the Cloud. Personalising security using a one-time pad generator at the client-end protects data from break-ins, side-channel attacks and backdoors in public encryption algorithms. The one-time pad binary sequences from modified analogue chaos oscillators initiated by noise encoded client data locally. Specific “one-to-Cloud” storage applications give control to the end user but without the key distribution problems associated with one-time pad encryption. Development of the prototype was aided by “Virtual Prototyping” in the latest version of Cadence OrCAD PSpice®. This addition allows the prototype simulation schematic to be connected to an actual microcontroller in real time using device model interface for bi-directional communication. using device model interface.",,IEEE
"“Global privacy and security, by design: Turning the “privacy vs. security” paradigm on its head”",Ann Cavoukian,Health and Technology,2017-07-12,"<a href=""Springer (2017-07-12) : “Global privacy and security, by design: Turning the “privacy vs. security” paradigm on its head”"" target=""_blank"">[https://link.springer.com/article/10.1007/s12553-017-0207-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12553-017-0207-1]</a>",,,Springer
A key to the backdoor into the castle: The clinical ramifications of immunoediting driven by antigenic competition,Hanna M.,Human Vaccines and Immunotherapeutics,2017-07-03,"<a href=""ScienceDirect (2017-07-03) : A key to the backdoor into the castle: The clinical ramifications of immunoediting driven by antigenic competition"" target=""_blank"">[https://doi.org/10.1080/21645515.2017.1301337]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1080/21645515.2017.1301337]</a>",,,ScienceDirect
Malicious Domain Name Detection Based on Extreme Machine Learning,"Yong Shi, Gong Chen, Juntao Li",Neural Processing Letters,2017-07-03,"<a href=""Springer (2017-07-03) : Malicious Domain Name Detection Based on Extreme Machine Learning"" target=""_blank"">[https://link.springer.com/article/10.1007/s11063-017-9666-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11063-017-9666-7]</a>","Malicious domain detection is one of the most effective approaches applied in detecting Advanced Persistent Threat (APT), the most sophisticated and...",,Springer
Outsmarting Network Security with SDN Teleportation,K. Thimmaraju L. Schiff S. Schmid,2017 IEEE European Symposium on Security and Privacy (EuroS&P),2017-07-03,"<a href=""IEEE (2017-07-03) : Outsmarting Network Security with SDN Teleportation"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962003]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/EuroSP.2017.21]</a>","Software-defined networking is considered a promising new paradigm, enabling more reliable and formally verifiable communication networks. However, this paper shows that the separation of the control plane from the data plane, which lies at the heart of Software-Defined Networks (SDNs), introduces a new vulnerability which we call teleportation. An attacker (e.g., a malicious switch in the data plane or a host connected to the network) can use teleportation to transmit information via the control plane and bypass critical network functions in the data plane (e.g., a firewall), and to violate security policies as well as logical and even physical separations. This paper characterizes the design space for teleportation attacks theoretically, and then identifies four different teleportation techniques. We demonstrate and discuss how these techniques can be exploited for different attacks (e.g., exfiltrating confidential data at high rates), and also initiate the discussion of possible countermeasures. Generally, and given today's trend toward more intent-based networking, we believe that our findings are relevant beyond the use cases considered in this paper.",,IEEE
Vigenère scores for malware detection,"Suchita Deshmukh, Fabio Di Troia, Mark Stamp",Journal of Computer Virology and Hacking Techniques,2017-06-19,"<a href=""Springer (2017-06-19) : Vigenère scores for malware detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-017-0300-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-017-0300-z]</a>","Previous research has applied classic cryptanalytic techniques to the malware detection problem. Specifically, scores that are based on simple...",,Springer
BackDoor: Making microphones hear inaudible sounds,Roy N.,"MobiSys 2017 - Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services",2017-06-16,"<a href=""ScienceDirect (2017-06-16) : BackDoor: Making microphones hear inaudible sounds"" target=""_blank"">[https://doi.org/10.1145/3081333.3081366]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1145/3081333.3081366]</a>",,,ScienceDirect
Can Apple build a privacy minded iPhone security system so secure that Apple cannot access it?,Brian L. Owsley,Health and Technology,2017-06-07,"<a href=""Springer (2017-06-07) : Can Apple build a privacy minded iPhone security system so secure that Apple cannot access it?"" target=""_blank"">[https://link.springer.com/article/10.1007/s12553-017-0192-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12553-017-0192-4]</a>","The world has become less secure and less private with the advent of technology. Law enforcement agencies, such as the FBI, have sophisticated...",,Springer
BackDoor: Making Microphones Hear Inaudible Sounds,"Nirupam Roy, Haitham Hassanieh, Romit Roy Choudhury","MobiSys '17: Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services
MobiSys","2017-06
2017","<a href=""ACM (2017-06) : BackDoor: Making Microphones Hear Inaudible Sounds"" target=""_blank"">[https://dl.acm.org/doi/10.1145/3081333.3081366]</a>
<a href=""DBLP (2017) : BackDoor: Making Microphones Hear Inaudible Sounds"" target=""_blank"">[https://doi.org/10.1145/3081333.3081366]</a>","<a href=""ACM"" target=""_blank"">[https://doi.org/10.1145/3081333.3081366]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3081333.3081366]</a>","Consider sounds, say at 40kHz, that are completely outside the human's audible range (20kHz), as well as a microphone's recordable range (24kHz). We show that these high frequency sounds can be designed to become recordable by unmodified microphones, ...
","
","ACM
DBLP"
The Opacity of Backbones and Backdoors Under a Weak Assumption,"Lane A. Hemaspaandra, David E. Narváez",arXiv,2017-06,"<a href=""DBLP (2017-06) : The Opacity of Backbones and Backdoors Under a Weak Assumption"" target=""_blank"">[http://arxiv.org/abs/1706.04582]</a>","<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1706.04582]</a>",,,DBLP
Preventive Maintenance for Advanced Metering Infrastructure Against Malware Propagation,Y. Guo C. -W. Ten S. Hu W. W. Weaver,IEEE Transactions on Smart Grid,2017-05-20,"<a href=""IEEE (2017-05-20) : Preventive Maintenance for Advanced Metering Infrastructure Against Malware Propagation"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7182779]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TSG.2015.2453342]</a>","Advanced metering infrastructure (AMI) deployment has been widely promoted in recent years to improve the accuracy of billing information as well as to facilitate implementation of demand response. Information integrity and availability of the devices is crucial to the billing information that should reflect accurately on how much the household energy is consumed. The IP-based smart metering devices may exist with unknown vulnerabilities that can introduce backdoors to enable worm propagation across AMI network. The infected devices can be attack agents that would largely disable the metering functionalities or manipulate control variables of each meter. This paper proposes an optimal frequency of on-site investigation and the number of monitoring verification to investigate potential anomalies of malware footprinting by applying the decision process framework of Markovian. The proposed method determines the best inspection strategies based on the observation from the existing anomaly detectors deployed in the network. The considerations include malware propagation characteristics, accuracy of anomaly detectors, and investigation and diagnosis costs. Four scenarios are simulated using the proposed method, demonstrating the effectiveness of investigation on potentially infected electronic meters within an AMI network.",,IEEE
A survey on dynamic mobile malware detection,"Ping Yan, Zheng Yan",Software Quality Journal,2017-05-13,"<a href=""Springer (2017-05-13) : A survey on dynamic mobile malware detection"" target=""_blank"">[https://link.springer.com/article/10.1007/s11219-017-9368-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11219-017-9368-4]</a>","The outstanding advances of mobile devices stimulate their wide usage. Since mobile devices are coupled with third-party applications, lots of...",,Springer
SMA: A System-Level Mutual Authentication for Protecting Electronic Hardware and Firmware,U. Guin S. Bhunia D. Forte M. M. Tehranipoor,IEEE Transactions on Dependable and Secure Computing,2017-05-11,"<a href=""IEEE (2017-05-11) : SMA: A System-Level Mutual Authentication for Protecting Electronic Hardware and Firmware"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7585119]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/TDSC.2016.2615609]</a>","Due to the enhanced capability of adversaries, electronic systems are now increasingly vulnerable to counterfeiting and piracy. The majority of counterfeit systems today are of cloned type, which have been on the rise in the recent years. Ensuring the security of such systems is of great concern as an adversary can create a backdoor or insert a malware to bypass security modules. The reliability of such systems could also be questionable as the components used in these systems may be counterfeit and/or of inferior quality. It is of prime importance to develop solutions that can prevent an adversary from creating these non-authentic systems. In this paper, we present a novel system-level mutual authentication approach for both the hardware and firmware. The hardware authenticates the firmware by verifying the checksum during the power-up. On the other hand, firmware verifies the identity of the hardware and cannot produce correct results unless it receives a unique hardware fingerprint, which we call as system ID. We propose two secure protocols, TIDP and TIDS, to construct the system ID and authenticate the system by using this unique ID. We show that our approach is resistant to various known attacks.",,IEEE
A Security Perspective on Battery Systems of the Internet of Things,"Anthony Bahadir Lopez, Korosh Vatanparvar, ... Mohammad Abdullah Al Faruque",Journal of Hardware and Systems Security,2017-05-10,"<a href=""Springer (2017-05-10) : A Security Perspective on Battery Systems of the Internet of Things"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-017-0007-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41635-017-0007-0]</a>","Battery (sub)systems are used in many systems (systems-of-systems) in the Internet of Things (IoT) ranging from everyday ones (e.g., mobile systems,...",,Springer
Another dimension in integrated circuit trust,"John DeVale, Ryan Rakvic, Kevin Rudd",Journal of Cryptographic Engineering,2017-05-09,"<a href=""Springer (2017-05-09) : Another dimension in integrated circuit trust"" target=""_blank"">[https://link.springer.com/article/10.1007/s13389-017-0164-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13389-017-0164-7]</a>","Although 3D integrated circuit technology has typically been used to solve specific design goals, it has great potential for protecting intellectual...",,Springer
A Novel LWCSO-PKM-Based Feature Optimization and Classification of Attack Types in SCADA Network,"Dhanalakshmi Krishnan Sadhasivan, Kannapiran Balasubramanian",Arabian Journal for Science and Engineering,2017-04-24,"<a href=""Springer (2017-04-24) : A Novel LWCSO-PKM-Based Feature Optimization and Classification of Attack Types in SCADA Network"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-017-2524-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-017-2524-0]</a>","Currently, Supervisory Control and Data Acquisition (SCADA) systems are widely used in the remote monitoring and control of the large-scale...",,Springer
IIoT-SIDefender: Detecting and defense against the sensitive information leakage in industry IoT,"Letian Sha, Fu Xiao, ... Jing Sun",World Wide Web,2017-04-14,"<a href=""Springer (2017-04-14) : IIoT-SIDefender: Detecting and defense against the sensitive information leakage in industry IoT"" target=""_blank"">[https://link.springer.com/article/10.1007/s11280-017-0459-8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11280-017-0459-8]</a>","With Industry 4.0 and Internet of Things (IoT) era coming, remote passwords and control-flow vulnerabilities play a key role to detect attackers in...",,Springer
An enhanced security framework for home appliances in smart home,"Won Min Kang, Seo Yeon Moon, Jong Hyuk Park",Human-centric Computing and Information Sciences,2017-03-05,"<a href=""Springer (2017-03-05) : An enhanced security framework for home appliances in smart home"" target=""_blank"">[https://link.springer.com/article/10.1186/s13673-017-0087-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1186/s13673-017-0087-4]</a>","Since the end of 2000, smartphones have explosively spread and have made people’s lives plentiful. With the start of smartphones, new smart devices,...",,Springer
Combining treewidth and backdoors for CSP,Ganian R.,"Leibniz International Proceedings in Informatics, LIPIcs",2017-03-01,"<a href=""ScienceDirect (2017-03-01) : Combining treewidth and backdoors for CSP"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.STACS.2017.36]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.STACS.2017.36]</a>",,,ScienceDirect
Mathematical Backdoors in Symmetric Encryption Systems - Proposal for a Backdoored AES-like Block Cipher,"Arnaud Bannier, Eric Filiol","arXiv
ICISSP
arXiv","2017-02-21
2017
2017-02","<a href=""arXiv (2017-02-21) : Mathematical Backdoors in Symmetric Encryption Systems - Proposal for a Backdoored AES-like Block Cipher"" target=""_blank"">[http://arxiv.org/abs/1702.06475v1]</a>
<a href=""DBLP (2017) : Mathematical Backdoors in Symmetric Encryption Systems - Proposal for a Backdoored AES-like Block Cipher"" target=""_blank"">[https://doi.org/10.5220/0006244406220631]</a>
<a href=""DBLP (2017-02) : Mathematical Backdoors in Symmetric Encryption Systems - Proposal for a Backdoored AES-like Block Cipher"" target=""_blank"">[http://arxiv.org/abs/1702.06475]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.5220/0006244406220631]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1702.06475]</a>","Recent years have shown that more than ever governments and intelligence agencies try to control and bypass the cryptographic means used for the protection of data. Backdooring encryption algorithms is considered as the best way to enforce cryptographic control. Until now, only implementation backdoors (at the protocol/implementation/management level) are generally considered. In this paper we propose to address the most critical issue of backdoors: mathematical backdoors or by-design backdoors, which are put directly at the mathematical design of the encryption algorithm. While the algorithm may be totally public, proving that there is a backdoor, identifying it and exploiting it, may be an intractable problem. We intend to explain that it is probably possible to design and put such backdoors. Considering a particular family (among all the possible ones), we present BEA-1, a block cipher algorithm which is similar to the AES and which contains a mathematical backdoor enabling an operational and effective cryptanalysis. The BEA-1 algorithm (80-bit block size, 120-bit key, 11 rounds) is designed to resist to linear and differential cryptanalyses. A challenge will be proposed to the cryptography community soon. Its aim is to assess whether our backdoor is easily detectable and exploitable or not.

","

","arXiv
DBLP
DBLP"
Online detection of continuous changes in stochastic processes,"Kohei Miyaguchi, Kenji Yamanishi",International Journal of Data Science and Analytics,2017-02-21,"<a href=""Springer (2017-02-21) : Online detection of continuous changes in stochastic processes"" target=""_blank"">[https://link.springer.com/article/10.1007/s41060-017-0045-2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s41060-017-0045-2]</a>","We are concerned with detecting continuous changes in stochastic processes. In conventional studies on non-stationary stochastic processes, it is...",,Springer
Trojan localization using symbolic algebra,F. Farahmandi Y. Huang P. Mishra,2017 22nd Asia and South Pacific Design Automation Conference (ASP-DAC),2017-02-20,"<a href=""IEEE (2017-02-20) : Trojan localization using symbolic algebra"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858388]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ASPDAC.2017.7858388]</a>","Growing reliance on reusable hardware Intellectual Property (IP) blocks, severely affects the security and trustworthiness of System-on-Chips (SoCs) since untrusted third-party vendors may deliberately insert malicious components to incorporate undesired functionality. Malicious implants may also work as hidden backdoor and leak protected information. In this paper, we propose an automated approach to identify untrustworthy IPs and localize malicious functional modifications (if any). The technique is based on extracting polynomials from gate-level implementation of the untrustworthy IP and comparing them with specification polynomials. The proposed approach is applicable when the specification is available. Our approach is scalable due to manipulation of polynomials instead of BDD-based analysis used in traditional equivalence checking techniques. Experimental results using Trust-HUB benchmarks demonstrate that our approach improves both localization and test generation efficiency by several orders of magnitude compared to the state-of-the-art Trojan detection techniques.",,IEEE
A methodology to measure and monitor level of operational effectiveness of a CSOC,"Ankit Shah, Rajesh Ganesan, ... Hasan Cam",International Journal of Information Security,2017-02-16,"<a href=""Springer (2017-02-16) : A methodology to measure and monitor level of operational effectiveness of a CSOC"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-017-0365-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-017-0365-1]</a>","In a cybersecurity operations center (CSOC), under normal operating conditions in a day, sufficient numbers of analysts are available to analyze the...",,Springer
Machine learning classification model for Network based Intrusion Detection System,S. Kumar A. Viinikainen T. Hamalainen,2016 11th International Conference for Internet Technology and Secured Transactions (ICITST),2017-02-16,"<a href=""IEEE (2017-02-16) : Machine learning classification model for Network based Intrusion Detection System"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7856705]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICITST.2016.7856705]</a>","With an enormous increase in number of mobile users, mobile threats are also growing rapidly. Mobile malwares can lead to several cybersecurity threats i.e. stealing sensitive information, installing backdoors, ransomware attacks and sending premium SMSs etc. Previous studies have shown that due to the sophistication of threats and tailored techniques to avoid detection, not every antivirus system is capable of detecting advance threats. However, an extra layer of security at the network side can protect users from these advanced threats by analyzing the traffic patterns. To detect these threats, this paper proposes and evaluates, a Machine Learning (ML) based model for Network based Intrusion Detection Systems (NIDS). In this research, several supervised ML classifiers were built using data-sets containing labeled instances of network traffic features generated by several malicious and benign applications. The focus of this research is on Android based malwares due to its global share in mobile malware and popularity among users. Based on the evaluation results, the model was able to detect known and unknown threats with the accuracy of up to 99.4%. This ML model can also be integrated with traditional intrusion detection systems in order to detect advanced threats and reduce false positives.",,IEEE
Strengthening the security of authenticated key exchange against bad randomness,"Michèle Feltz, Cas Cremers","Designs, Codes and Cryptography",2017-02-13,"<a href=""Springer (2017-02-13) : Strengthening the security of authenticated key exchange against bad randomness"" target=""_blank"">[https://link.springer.com/article/10.1007/s10623-017-0337-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10623-017-0337-5]</a>",Recent history has revealed that many random number generators (RNGs) used in cryptographic algorithms and protocols were not providing appropriate...,,Springer
Analysis of machine learning solutions to detect malware in android,Q. Jamil M. A. Shah,2016 Sixth International Conference on Innovative Computing Technology (INTECH),2017-02-09,"<a href=""IEEE (2017-02-09) : Analysis of machine learning solutions to detect malware in android"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7845073]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/INTECH.2016.7845073]</a>","The recent use of mobile devices and increase in connectivity technologies(GSM, GPRS, Bluetooth & WiFi enable us to access abundant services. These services and communication channels are exploited by susceptibilities immensely. Hence, for malware writers, mobile devices became ideal target. Applications installed on smartphones request access to the sensitive information which may lead to security vulnerabilities. Different malwares named as Botnet, Backdoor, Rootkits, Virus, Worms, and Trojans can attack android Operating System (OS). Due to these attacks privacy of the users is compromised. This paper surveys the already proposed security solutions by using machine learning approaches especially focused on supervised, semi supervised and unsupervised approaches. We also analyzed the architecture of these approaches and present the taxonomy of Android OS based security solutions. Our aim is to provide the best approach for malware detection in Android OS.",,IEEE
Network Moving Target Defense Technique Based on Self-Adaptive End-Point Hopping,"Cheng Lei, Hong-qi Zhang, ... Ying-jie Yang",Arabian Journal for Science and Engineering,2017-02-09,"<a href=""Springer (2017-02-09) : Network Moving Target Defense Technique Based on Self-Adaptive End-Point Hopping"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-017-2430-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13369-017-2430-5]</a>","Moving target defense is a revolutionary technology changing the antagonistic pattern between attack and defense, with end-point information hopping...",,Springer
Genes and proteins of the alternative steroid backdoor pathway for dihydrotestosterone synthesis are expressed in the human ovary and seem enhanced in the polycystic ovary syndrome,Marti N.,Molecular and Cellular Endocrinology,2017-02-05,"<a href=""ScienceDirect (2017-02-05) : Genes and proteins of the alternative steroid backdoor pathway for dihydrotestosterone synthesis are expressed in the human ovary and seem enhanced in the polycystic ovary syndrome"" target=""_blank"">[https://doi.org/10.1016/j.mce.2016.07.029]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.mce.2016.07.029]</a>",,,ScienceDirect
HoneyCirculator: distributing credential honeytoken for introspection of web-based attack cycle,"Mitsuaki Akiyama, Takeshi Yagi, ... Youki Kadobayashi",International Journal of Information Security,2017-01-30,"<a href=""Springer (2017-01-30) : HoneyCirculator: distributing credential honeytoken for introspection of web-based attack cycle"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-017-0361-5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-017-0361-5]</a>",A web user who falsely accesses a compromised website is usually redirected to an adversary’s website and is forced to download malware after being...,,Springer
A new approach for root-causing attacks on digital microfluidic devices,Roy P.,"Proceedings of the 2016 IEEE Asian Hardware Oriented Security and Trust Symposium, AsianHOST 2016",2017-01-26,"<a href=""ScienceDirect (2017-01-26) : A new approach for root-causing attacks on digital microfluidic devices"" target=""_blank"">[https://doi.org/10.1109/AsianHOST.2016.7835550]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/AsianHOST.2016.7835550]</a>",,,ScienceDirect
A Backdoor to Policy Making: The Use of Philosophers by the Supreme Court,Rao N.,Rawls and Law,2017-01-01,"<a href=""ScienceDirect (2017-01-01) : A Backdoor to Policy Making: The Use of Philosophers by the Supreme Court"" target=""_blank"">[https://doi.org/10.4324/9781315089188-4]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.4324/9781315089188-4]</a>",,,ScienceDirect
Androgen biosynthesis during minipuberty favors the backdoor pathway over the classic pathway: Insights into enzyme activities and steroid fluxes in healthy infants during the first year of life from the urinary steroid metabolome,Dhayat N.A.,Journal of Steroid Biochemistry and Molecular Biology,2017-01-01,"<a href=""ScienceDirect (2017-01-01) : Androgen biosynthesis during minipuberty favors the backdoor pathway over the classic pathway: Insights into enzyme activities and steroid fluxes in healthy infants during the first year of life from the urinary steroid metabolome"" target=""_blank"">[https://doi.org/10.1016/j.jsbmb.2016.07.009]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.jsbmb.2016.07.009]</a>",,,ScienceDirect
Backdoor trees for answer set programming,Fichte J.K.,CEUR Workshop Proceedings,2017-01-01,"<a href=""ScienceDirect (2017-01-01) : Backdoor trees for answer set programming"" target=""_blank"">[]</a>","<a href=""ScienceDirect"" target=""_blank"">[]</a>",,,ScienceDirect
Backdoor treewidth for SAT,Ganian R.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,"<a href=""ScienceDirect (2017-01-01) : Backdoor treewidth for SAT"" target=""_blank"">[https://doi.org/10.1007/978-3-319-66263-3_2]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-319-66263-3_2]</a>",,,ScienceDirect
Controlled randomness – a defense against backdoors in cryptographic devices,Hanzlik L.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,"<a href=""ScienceDirect (2017-01-01) : Controlled randomness – a defense against backdoors in cryptographic devices"" target=""_blank"">[https://doi.org/10.1007/978-3-319-61273-7_11]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-319-61273-7_11]</a>",,,ScienceDirect
Development of the experimental setup for investigation of latching of superconducting single-photon detector caused by blinding attack on the quantum key distribution system,Elezov M.,EPJ Web of Conferences,2017-01-01,"<a href=""ScienceDirect (2017-01-01) : Development of the experimental setup for investigation of latching of superconducting single-photon detector caused by blinding attack on the quantum key distribution system"" target=""_blank"">[https://doi.org/10.1051/epjconf/201713201004]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1051/epjconf/201713201004]</a>",,,ScienceDirect
Mathematical backdoors in symmetric encryption systems proposal for a backdoored AES-like block cipher,Bannier A.,ICISSP 2017 - Proceedings of the 3rd International Conference on Information Systems Security and Privacy,2017-01-01,"<a href=""ScienceDirect (2017-01-01) : Mathematical backdoors in symmetric encryption systems proposal for a backdoored AES-like block cipher"" target=""_blank"">[https://doi.org/10.5220/0006244406220631]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.5220/0006244406220631]</a>",,,ScienceDirect
Stringer: Measuring the importance of static data comparisons to detect backdoors and undocumented functionality,Thomas S.L.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,"<a href=""ScienceDirect (2017-01-01) : Stringer: Measuring the importance of static data comparisons to detect backdoors and undocumented functionality"" target=""_blank"">[https://doi.org/10.1007/978-3-319-66399-9_28]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-319-66399-9_28]</a>",,,ScienceDirect
Getting Security Objectives Wrong: A Cautionary Tale of an Industrial Control System,Simon N. Foley,"Security Protocols XXV
Security Protocols XXV","2017
2017","<a href=""Springer (2017) : Getting Security Objectives Wrong: A Cautionary Tale of an Industrial Control System"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-71075-4_3]</a>
<a href=""Springer (2017) : Getting Security Objectives Wrong: A Cautionary Tale of an Industrial Control System (Transcript of Discussion)"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-71075-4_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-71075-4_3]</a>
<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-71075-4_4]</a>","We relate a story about an Industrial Control System in order to illustrate that simple security objectives can be deceptive: there are many things...
Simon Foley: This is work that evolved by accident. Last year I started a project on industrial control system security, and by way of...","
","Springer
Springer"
Indiscreet Logs: Diffie-Hellman Backdoors in TLS,"Kristen Dorey, Nicholas Chang-Fong, Aleksander Essex","NDSS
IACR Cryptol. ePrint Arch.","2017
2016","<a href=""DBLP (2017) : Indiscreet Logs: Diffie-Hellman Backdoors in TLS"" target=""_blank"">[https://www.ndss-symposium.org/ndss2017/ndss-2017-programme/indiscreet-logs-persistent-diffie-hellman-backdoors-tls/]</a>
<a href=""DBLP (2016) : Indiscreet Logs: Persistent Diffie-Hellman Backdoors in TLS"" target=""_blank"">[http://eprint.iacr.org/2016/999]</a>","<a href=""DBLP"" target=""_blank"">[https://www.ndss-symposium.org/ndss2017/ndss-2017-programme/indiscreet-logs-persistent-diffie-hellman-backdoors-tls/]</a>
<a href=""DBLP"" target=""_blank"">[http://eprint.iacr.org/2016/999]</a>","
","
","DBLP
DBLP"
A Behavior-Based Online Engine for Detecting Distributed Cyber-Attacks,"Yaokai Feng, Yoshiaki Hori, Kouichi Sakurai",Information Security Applications,2017,"<a href=""Springer (2017) : A Behavior-Based Online Engine for Detecting Distributed Cyber-Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-56549-1_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-56549-1_7]</a>","Distributed attacks have reportedly caused the most serious losses in recent years. Here, distributed attacks means those attacks conducted...",,Springer
A Case Study Assessing the Effects of Cyber Attacks on a River Zonal Dispatcher,"Ronald Joseph Wright, Ken Keefe, ... William H. Sanders",Critical Information Infrastructures Security,2017,"<a href=""Springer (2017) : A Case Study Assessing the Effects of Cyber Attacks on a River Zonal Dispatcher"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-71368-7_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-71368-7_21]</a>",A river zonal dispatcher is a system that sends collected environmental data to a national dispatcher and sends warnings in case of danger (such as...,,Springer
A Hybrid Approach for Malware Family Classification,"Naqqash Aman, Yasir Saleem, ... Farrukh Shahzad",Applications and Techniques in Information Security,2017,"<a href=""Springer (2017) : A Hybrid Approach for Malware Family Classification"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-5421-1_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-5421-1_14]</a>",One of the top most cyber security threats – in today’s world – are malware applications. Traditional signature and static analysis based malware...,,Springer
A Kilobit Hidden SNFS Discrete Logarithm Computation,"Joshua Fried, Pierrick Gaudry, ... Emmanuel Thomé",Advances in Cryptology – EUROCRYPT 2017,2017,"<a href=""Springer (2017) : A Kilobit Hidden SNFS Discrete Logarithm Computation"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-56620-7_8]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-56620-7_8]</a>","We perform a special number field sieve discrete logarithm computation in a 1024-bit prime field. To our knowledge, this is the first kilobit-sized...",,Springer
Analysis of Potential Code Vulnerabilities Involving Overlapping Instructions,"Loui Al Sardy, Tong Tang, ... Francesca Saglietti","Computer Safety, Reliability, and Security",2017,"<a href=""Springer (2017) : Analysis of Potential Code Vulnerabilities Involving Overlapping Instructions"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-66284-8_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-66284-8_10]</a>",This article proposes approaches supporting the analysis of code vulnerabilities based on overlapping machine instructions of variable length. For...,,Springer
Analysis on Attack Scenarios and Countermeasures for Self-driving Car and Its Infrastructures,"Dohyun Lim, Kitaek Park, ... Jungtaek Seo","Advances on Broad-Band Wireless Computing, Communication and Applications",2017,"<a href=""Springer (2017) : Analysis on Attack Scenarios and Countermeasures for Self-driving Car and Its Infrastructures"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-49106-6_42]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-49106-6_42]</a>",Autonomous vehicles collect and process information required for driving autonomously and apply the processed result for vehicle driving thereby...,,Springer
Assisting Malware Analysis with Symbolic Execution: A Case Study,"Roberto Baldoni, Emilio Coppa, ... Camil Demetrescu",Cyber Security Cryptography and Machine Learning,2017,"<a href=""Springer (2017) : Assisting Malware Analysis with Symbolic Execution: A Case Study"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-60080-2_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-60080-2_12]</a>","Security analysts spend days or even weeks in trying to understand the inner workings of malicious software, using a plethora of manually...",,Springer
"BackDoor: Sounds that a microphone can record, but that humans can&apos,t hear","Nirupam Roy, Haitham Hassanieh, Romit Roy Choudhury",GetMobile Mob. Comput. Commun.,2017,"<a href=""DBLP (2017) : BackDoor: Sounds that a microphone can record, but that humans can&apos,t hear"" target=""_blank"">[https://doi.org/10.1145/3191789.3191799]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/3191789.3191799]</a>",,,DBLP
Backdoor Sets for CSP,"Serge Gaspers, Sebastian Ordyniak, Stefan Szeider",The Constraint Satisfaction Problem,2017,"<a href=""DBLP (2017) : Backdoor Sets for CSP"" target=""_blank"">[https://doi.org/10.4230/DFU.Vol7.15301.5]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.4230/DFU.Vol7.15301.5]</a>",,,DBLP
Backdoor Trees for Answer Set Programming,"Johannes Klaus Fichte, Stefan Szeider",ASPOCP@LPNMR,2017,"<a href=""DBLP (2017) : Backdoor Trees for Answer Set Programming"" target=""_blank"">[https://ceur-ws.org/Vol-1868/p9.pdf]</a>","<a href=""DBLP"" target=""_blank"">[https://ceur-ws.org/Vol-1868/p9.pdf]</a>",,,DBLP
Backdoor Treewidth for SAT,"Robert Ganian, M. S. Ramanujan, Stefan Szeider",SAT,2017,"<a href=""DBLP (2017) : Backdoor Treewidth for SAT"" target=""_blank"">[https://doi.org/10.1007/978-3-319-66263-3_2]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-319-66263-3_2]</a>",,,DBLP
Backdoor attacks against learning systems,"Yujie Ji, Xinyang Zhang, Ting Wang",CNS,2017,"<a href=""DBLP (2017) : Backdoor attacks against learning systems"" target=""_blank"">[https://doi.org/10.1109/CNS.2017.8228656]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CNS.2017.8228656]</a>",,,DBLP
Backdoors into heterogeneous classes of SAT and CSP,"Serge Gaspers, Neeldhara Misra, Sebastian Ordyniak, Stefan Szeider, Stanislav Zivný",J. Comput. Syst. Sci.,2017,"<a href=""DBLP (2017) : Backdoors into heterogeneous classes of SAT and CSP"" target=""_blank"">[https://doi.org/10.1016/j.jcss.2016.10.007]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1016/j.jcss.2016.10.007]</a>",,,DBLP
Behavioral Analysis of Bot Activity in Infected Systems Using Honeypots,"Matej Zuzcak, Tomas Sochor",Computer Networks,2017,"<a href=""Springer (2017) : Behavioral Analysis of Bot Activity in Infected Systems Using Honeypots"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-59767-6_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-59767-6_10]</a>",New Internet threats emerge on daily basis and honeypots have become widely used for capturing them in order to investigate their activities. The...,,Springer
Breaking Fitness Records Without Moving: Reverse Engineering and Spoofing Fitbit,"Hossein Fereidooni, Jiska Classen, ... Mauro Conti","Research in Attacks, Intrusions, and Defenses",2017,"<a href=""Springer (2017) : Breaking Fitness Records Without Moving: Reverse Engineering and Spoofing Fitbit"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-66332-6_3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-66332-6_3]</a>",Tens of millions of wearable fitness trackers are shipped yearly to consumers who routinely collect information about their exercising patterns....,,Springer
Bridging the Gap from Cyber Security to Resilience,"Paul E. Roege, Zachary A. Collier, ... Branislav Todorovic",Resilience and Risk,2017,"<a href=""Springer (2017) : Bridging the Gap from Cyber Security to Resilience"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-94-024-1123-2_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-94-024-1123-2_14]</a>",This chapter describes an evolution of practices in community and business assurance from protective programs based upon risk management to the...,,Springer
Combating Control Flow Linearization,"Julian Kirsch, Clemens Jonischkeit, ... Claudia Eckert",ICT Systems Security and Privacy Protection,2017,"<a href=""Springer (2017) : Combating Control Flow Linearization"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-58469-0_26]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-58469-0_26]</a>",Piracy is a persistent headache for software companies that try to protect their assets by investing both time and money. Program code obfuscation as...,,Springer
Contribution to Enhancement of Critical Infrastructure Resilience in Serbia,"Branislav Todorovic, Darko Trifunovic, ... Marina Filipovic",Resilience and Risk,2017,"<a href=""Springer (2017) : Contribution to Enhancement of Critical Infrastructure Resilience in Serbia"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-94-024-1123-2_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-94-024-1123-2_22]</a>","This chapter provides an overview of the current situation with critical infrastructure (CI) resilience in the Republic of Serbia, with an emphasis...",,Springer
Controlled Randomness – A Defense Against Backdoors in Cryptographic Devices,"Lucjan Hanzlik, Kamil Kluczniak, Mirosław Kutyłowski",Paradigms in Cryptology – Mycrypt 2016. Malicious and Exploratory Cryptology,2017,"<a href=""Springer (2017) : Controlled Randomness – A Defense Against Backdoors in Cryptographic Devices"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-61273-7_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-61273-7_11]</a>",Security of many cryptographic protocols is conditioned by quality of the random elements generated in the course of the protocol execution. On the...,,Springer
Cyber Security Provision for Industrial Control Systems,"Marek Amanowicz, Jacek Jarmakiewicz","Trends in Advanced Intelligent Control, Optimization and Automation",2017,"<a href=""Springer (2017) : Cyber Security Provision for Industrial Control Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-60699-6_59]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-60699-6_59]</a>",The paper gives an overview of the infrastructure of the electric power control system. It describes the basic threats caused by unauthorized actions...,,Springer
Cyber Targets Water Management,"Pieter Burghouwt, Marinus Maris, ... Marcel Spruit",Critical Information Infrastructures Security,2017,"<a href=""Springer (2017) : Cyber Targets Water Management"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-71368-7_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-71368-7_4]</a>","Water management is a critical infrastructure activity in The Netherlands. Many organizations, ranging from local municipalities to national...",,Springer
Deep Learning Based Intelligent Basketball Arena with Energy Image,"Wu Liu, Jiangyu Liu, ... Huadong Ma",MultiMedia Modeling,2017,"<a href=""Springer (2017) : Deep Learning Based Intelligent Basketball Arena with Energy Image"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-51811-4_49]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-51811-4_49]</a>","With the development of computer vision and artificial intelligence technologies, the “Intelligent Arena” is becoming one of the new-emerging...",,Springer
Defeating HaTCh: Building Malicious IP Cores,"Anshu Bhardwaj, Subir Kumar Roy",VLSI Design and Test,2017,"<a href=""Springer (2017) : Defeating HaTCh: Building Malicious IP Cores"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-7470-7_34]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-7470-7_34]</a>",Possibility of Hardware Trojans (HT) being present in SOCs designed by integrating hundreds of third party IP (3PIP) cores provided by different...,,Springer
Developing Countermeasures against Cloning of Identity Tokens in Legacy Systems,"Pavel Moravec, Michal Krumnikl",Computer Information Systems and Industrial Management,2017,"<a href=""Springer (2017) : Developing Countermeasures against Cloning of Identity Tokens in Legacy Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-59105-6_58]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-59105-6_58]</a>",During the development of a new access system based on modern RFID technologies it was found that companies producing access control systems for...,,Springer
Dynamically-Enabled Defense Effectiveness Evaluation in Home Internet Based on Vulnerability Analysis,"Ting Wang, Min Lei, ... Yu Yang",Cloud Computing and Security,2017,"<a href=""Springer (2017) : Dynamically-Enabled Defense Effectiveness Evaluation in Home Internet Based on Vulnerability Analysis"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-68542-7_71]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-68542-7_71]</a>","Current intelligent devices in Home Internet, such as routers and cameras, have suffered malicious attacks from hackers. Therefore, security for Home...",,Springer
EVALUATION OF ADDITIVE AND SUBTRACTIVE MANUFACTURING FROM THE SECURITY PERSPECTIVE,"Mark Yampolskiy, Wayne King, ... Yuval Elovici",Critical Infrastructure Protection XI,2017,"<a href=""Springer (2017) : EVALUATION OF ADDITIVE AND SUBTRACTIVE MANUFACTURING FROM THE SECURITY PERSPECTIVE"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-70395-4_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-70395-4_2]</a>",Additive manufacturing involves a new class of cyber-physical systems that manufacture 3D objects incrementally by depositing and fusing together...,,Springer
End to End Security is Not Enough,"Dylan Clarke, Syed Taha Ali",Security Protocols XXV,2017,"<a href=""Springer (2017) : End to End Security is Not Enough"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-71075-4_29]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-71075-4_29]</a>","End-to-end (E2E) security is commonly marketed as a panacea to all of a user’s security requirements. We contend that this optimism is misplaced, and...",,Springer
Exploring Energy Consumption of Juice Filming Charging Attack on Smartphones: A Pilot Study,"Lijun Jiang, Weizhi Meng, ... Jin Li",Network and System Security,2017,"<a href=""Springer (2017) : Exploring Energy Consumption of Juice Filming Charging Attack on Smartphones: A Pilot Study"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-64701-2_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-64701-2_15]</a>","With the increasing demand of smartphone charging, more and more public charging stations are under construction (e.g., airports, subways, shops)....",,Springer
Faster Algorithms for Isogeny Problems Using Torsion Point Images,Christophe Petit,Advances in Cryptology – ASIACRYPT 2017,2017,"<a href=""Springer (2017) : Faster Algorithms for Isogeny Problems Using Torsion Point Images"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-70697-9_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-70697-9_12]</a>",There is a recent trend in cryptography to construct protocols based on the hardness of computing isogenies between supersingular elliptic curves....,,Springer
Features for Behavioral Anomaly Detection of Connectionless Network Buffer Overflow Attacks,"Ivan Homoliak, Ladislav Sulak, Petr Hanacek",Information Security Applications,2017,"<a href=""Springer (2017) : Features for Behavioral Anomaly Detection of Connectionless Network Buffer Overflow Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-56549-1_6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-56549-1_6]</a>",Buffer overflow (BO) attacks are one of the most dangerous threats in the area of network security. Methods for detection of BO attacks basically use...,,Springer
Formal Abstractions for Attested Execution Secure Processors,"Rafael Pass, Elaine Shi, Florian Tramèr",Advances in Cryptology – EUROCRYPT 2017,2017,"<a href=""Springer (2017) : Formal Abstractions for Attested Execution Secure Processors"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-56620-7_10]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-56620-7_10]</a>","Realistic secure processors, including those built for academic and commercial purposes, commonly realize an “attested execution” abstraction....",,Springer
Framework of Cyber Attack Attribution Based on Threat Intelligence,"Li Qiang, Yang Zeming, ... Yan Jian","Interoperability, Safety and Security in IoT",2017,"<a href=""Springer (2017) : Framework of Cyber Attack Attribution Based on Threat Intelligence"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-52727-7_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-52727-7_11]</a>","With the rapid growth of information technology, more and more devices are connected to the network. Cyber security environment has become...",,Springer
Hedging Public-Key Encryption in the Real World,"Alexandra Boldyreva, Christopher Patton, Thomas Shrimpton",Advances in Cryptology – CRYPTO 2017,2017,"<a href=""Springer (2017) : Hedging Public-Key Encryption in the Real World"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-63697-9_16]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-63697-9_16]</a>","Hedged PKE schemes are designed to provide useful security when the per-message randomness fails to be uniform, say, due to faulty implementations or...",,Springer
Hiding Kernel Level Rootkits Using Buffer Overflow and Return Oriented Programming,"Amrita Milind Honap, Wonjun Lee",Information Systems Security,2017,"<a href=""Springer (2017) : Hiding Kernel Level Rootkits Using Buffer Overflow and Return Oriented Programming"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-72598-7_7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-72598-7_7]</a>",Kernel Level Rootkits are malwares that can be installed and hidden on a user’s computer without revealing their existence. The goal of all rootkits...,,Springer
How Anywhere Computing Just Killed Your Phone-Based Two-Factor Authentication,"Radhesh Krishnan Konoth, Victor van der Veen, Herbert Bos",Financial Cryptography and Data Security,2017,"<a href=""Springer (2017) : How Anywhere Computing Just Killed Your Phone-Based Two-Factor Authentication"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-54970-4_24]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-54970-4_24]</a>",Exponential growth in smartphone usage combined with recent advances in mobile technology is causing a shift in (mobile) app behavior: application...,,Springer
MTD CBITS: Moving Target Defense for Cloud-Based IT Systems,"Alexandru G. Bardas, Sathya Chandran Sundaramurthy, ... Scott A. DeLoach",Computer Security – ESORICS 2017,2017,"<a href=""Springer (2017) : MTD CBITS: Moving Target Defense for Cloud-Based IT Systems"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-66402-6_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-66402-6_11]</a>","The static nature of current IT systems gives attackers the extremely valuable advantage of time, as adversaries can take their time and plan attacks...",,Springer
Malware Analysis and Detection via Activity Trees in User-Dependent Environment,"Arnur Tokhtabayev, Anton Kopeikin, ... Dina Satybaldina",Computer Network Security,2017,"<a href=""Springer (2017) : Malware Analysis and Detection via Activity Trees in User-Dependent Environment"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-65127-9_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-65127-9_17]</a>",We present a new system that offers detection and analysis of modern complex malware including user-oriented and targeted attacks. These attacks...,,Springer
Open Sesame! Design and Implementation of Backdoor to Secretly Unlock Android Devices,"Junsung Cho, Geumhwan Cho, Sangwon Hyun, Hyoungshick Kim",J. Internet Serv. Inf. Secur.,2017,"<a href=""DBLP (2017) : Open Sesame! Design and Implementation of Backdoor to Secretly Unlock Android Devices"" target=""_blank"">[https://doi.org/10.22667/JISIS.2017.11.30.035]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.22667/JISIS.2017.11.30.035]</a>",,,DBLP
Preventing Advanced Persistent Threats in Complex Control Networks,"Juan E. Rubio, Cristina Alcaraz, Javier Lopez",Computer Security – ESORICS 2017,2017,"<a href=""Springer (2017) : Preventing Advanced Persistent Threats in Complex Control Networks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-66399-9_22]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-66399-9_22]</a>","An Advanced Persistent Threat (APT) is an emerging attack against Industrial Control and Automation Systems, that is executed over a long period of...",,Springer
Security Analysis of Vendor Customized Code in Firmware of Embedded Device,"Muqing Liu, Yuanyuan Zhang, ... Dawu Gu",Security and Privacy in Communication Networks,2017,"<a href=""Springer (2017) : Security Analysis of Vendor Customized Code in Firmware of Embedded Device"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-59608-2_40]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-59608-2_40]</a>","Despite the increased concerning about embedded system security, the security assessment of commodity embedded devices is far from being adequate....",,Springer
Security Issues in Cloud Computing,Parnian Najafi Borazjani,"Green, Pervasive, and Cloud Computing",2017,"<a href=""Springer (2017) : Security Issues in Cloud Computing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-57186-7_58]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-57186-7_58]</a>","Cloud computing is a mixture of resources and services that are offered through the internet. Despite flexibility, efficiency, and lower costs,...",,Springer
Security and Attack Vector Analysis of IoT Devices,"Marc Capellupo, Jimmy Liranzo, ... Guojun Wang","Security, Privacy, and Anonymity in Computation, Communication, and Storage",2017,"<a href=""Springer (2017) : Security and Attack Vector Analysis of IoT Devices"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-72395-2_54]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-72395-2_54]</a>",The goal of this paper is to research and review through experimental testing the security of home automation devices. The methodology includes...,,Springer
"Security in Mobile Computing: Attack Vectors, Solutions, and Challenges","Sara Alwahedi, Mariam Al Ali, ... Zeyar Aung",Mobile Networks and Management,2017,"<a href=""Springer (2017) : Security in Mobile Computing: Attack Vectors, Solutions, and Challenges"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-52712-3_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-52712-3_13]</a>","With the growth of the mobile industry, a smart phone has the ability to store large amounts of valuable data such as personal and bank information,...",,Springer
Semi Supervised NLP Based Classification of Malware Documents,"Mayukh Rath, Shivali Agarwal, R. K. Shyamasundar",Information Systems Security,2017,"<a href=""Springer (2017) : Semi Supervised NLP Based Classification of Malware Documents"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-72598-7_21]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-72598-7_21]</a>","Proper classification of the available data into viable Malware classes is very important to analyze the causes, vulnerabilities & intents behind...",,Springer
Stringer: Measuring the Importance of Static Data Comparisons to Detect Backdoors and Undocumented Functionality,"Sam L. Thomas, Tom Chothia, Flavio D. Garcia",ESORICS,2017,"<a href=""DBLP (2017) : Stringer: Measuring the Importance of Static Data Comparisons to Detect Backdoors and Undocumented Functionality"" target=""_blank"">[https://doi.org/10.1007/978-3-319-66399-9_28]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-319-66399-9_28]</a>",,,DBLP
The Fallout of Key Compromise in a Proxy-Mediated Key Agreement Protocol,"David Nuñez, Isaac Agudo, Javier Lopez",Data and Applications Security and Privacy XXXI,2017,"<a href=""Springer (2017) : The Fallout of Key Compromise in a Proxy-Mediated Key Agreement Protocol"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-61176-1_25]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-61176-1_25]</a>","In this paper, we analyze how key compromise affects the protocol by Nguyen et al. presented at ESORICS 2016, an authenticated key agreement protocol...",,Springer
Threat Modeling for Cloud Data Center Infrastructures,"Nawaf Alhebaishi, Lingyu Wang, ... Anoop Singhal",Foundations and Practice of Security,2017,"<a href=""Springer (2017) : Threat Modeling for Cloud Data Center Infrastructures"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-51966-1_20]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-51966-1_20]</a>",Cloud computing has undergone rapid expansion throughout the last decade. Many companies and organizations have made the transition from traditional...,,Springer
Towards Actionable Mission Impact Assessment in the Context of Cloud Computing,"Xiaoyan Sun, Anoop Singhal, Peng Liu",Data and Applications Security and Privacy XXXI,2017,"<a href=""Springer (2017) : Towards Actionable Mission Impact Assessment in the Context of Cloud Computing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-61176-1_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-61176-1_14]</a>",Today’s cyber-attacks towards enterprise networks often undermine and even fail the mission assurance of victim networks. Mission cyber resilience...,,Springer
Towards a Cybersecurity Game: Operation Digital Chameleon,"Andreas Rieb, Ulrike Lechner",Critical Information Infrastructures Security,2017,"<a href=""Springer (2017) : Towards a Cybersecurity Game: Operation Digital Chameleon"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-71368-7_24]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-71368-7_24]</a>",In the Serious Game “Operation Digital Chameleon” red and blue teams develop attack and defense strategies to explore IT-Security of Critical...,,Springer
tLab: A System Enabling Malware Clustering Based on Suspicious Activity Trees,"Anton Kopeikin, Arnur Tokhtabayev, ... Dina Satybaldina",Computer Network Security,2017,"<a href=""Springer (2017) : tLab: A System Enabling Malware Clustering Based on Suspicious Activity Trees"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-65127-9_16]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-65127-9_16]</a>","We present a new approach for malware clustering in the domain of their behavior. To this end, we use a system called tLab that offers analysis and...",,Springer
A survey of remote patient monitoring systems for the measurement of multiple physiological parameters,"K. Sruthi, E. V. Kripesh, K. A. Unnikrishna Menon",Health and Technology,2016-12-27,"<a href=""Springer (2016-12-27) : A survey of remote patient monitoring systems for the measurement of multiple physiological parameters"" target=""_blank"">[https://link.springer.com/article/10.1007/s12553-016-0171-1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s12553-016-0171-1]</a>","In rural areas, people die at their early ages because of lack of proper facilities and infrastructure for monitoring patient’s health at the right...",,Springer
An Efficient Reverse Engineering Hardware Trojan Detector Using Histogram of Oriented Gradients,"Abdurrahman A. Nasr, Mohamed Z. Abdulmageed",Journal of Electronic Testing,2016-12-22,"<a href=""Springer (2016-12-22) : An Efficient Reverse Engineering Hardware Trojan Detector Using Histogram of Oriented Gradients"" target=""_blank"">[https://link.springer.com/article/10.1007/s10836-016-5631-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10836-016-5631-z]</a>","The problem of hardware Trojan is certainly serious, complex and tricky. Therefore, hardware Trojan (HT) detection is difficult, time and effort...",,Springer
DFA-AD: a distributed framework architecture for the detection of advanced persistent threats,"Pradip Kumar Sharma, Seo Yeon Moon, ... Jong Hyuk Park",Cluster Computing,2016-12-20,"<a href=""Springer (2016-12-20) : DFA-AD: a distributed framework architecture for the detection of advanced persistent threats"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-016-0716-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10586-016-0716-0]</a>",Advanced persistent threats (APTs) are target-oriented and advanced cyber-attacks which often leverage the bot control and customized malware...,,Springer
Backdoors to Tractable Valued CSP,"Robert Ganian, M. S. Ramanujan, Stefan Szeider","arXiv
CP
arXiv","2016-12-17
2016
2016-12","<a href=""arXiv (2016-12-17) : Backdoors to Tractable Valued CSP"" target=""_blank"">[http://arxiv.org/abs/1612.05733v1]</a>
<a href=""DBLP (2016) : Backdoors to Tractable Valued CSP"" target=""_blank"">[https://doi.org/10.1007/978-3-319-44953-1_16]</a>
<a href=""DBLP (2016-12) : Backdoors to Tractable Valued CSP"" target=""_blank"">[http://arxiv.org/abs/1612.05733]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-319-44953-1_16]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1612.05733]</a>","We extend the notion of a strong backdoor from the CSP setting to the Valued CSP setting (VCSP, for short). This provides a means for augmenting a class of tractable VCSP instances to instances that are outside the class but of small distance to the class, where the distance is measured in terms of the size of a smallest backdoor. We establish that VCSP is fixed-parameter tractable when parameterized by the size of a smallest backdoor into every tractable class of VCSP instances characterized by a (possibly infinite) tractable valued constraint language of finite arity and finite domain. We further extend this fixed-parameter tractability result to so-called ""scattered classes"" of VCSP instances where each connected component may belong to a different tractable class.

","

","arXiv
DBLP
DBLP"
Review on the IT security: Attack and defense,S. -d. Krit E. Haimoud,2016 International Conference on Engineering & MIS (ICEMIS),2016-11-17,"<a href=""IEEE (2016-11-17) : Review on the IT security: Attack and defense"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7745386]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICEMIS.2016.7745386]</a>","Information technology security is the collection of technologies, standards, policies and management practices that are applied to information to keep it secure. This paper gives an overview of information security management systems. By the end of this paper you should have developed an understanding of: some tactics used by hackers to hack into systems, computers, web servers, android phone, email, infrastructure systems and more. And conversely some precautions used to raise up the systems security level. This paper is dedicated for institutions, individuals, and every person interested in IT security.",,IEEE
Finding the malicious URLs using search engines,A. R. Nagaonkar U. L. Kulkarni,2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom),2016-10-31,"<a href=""IEEE (2016-10-31) : Finding the malicious URLs using search engines"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724951]</a>","<a href=""IEEE"" target=""_blank"">[]</a>","Malware is malicious software consist of malignant program code which are synchronized by attackers or hackers. Analogously maliciousness is propagated through URLs to cause undesirable consequences. These URLs sites having pernicious code which consist of malicious contents like worms, Trojan horses, backdoors etc. Hence it becomes essential task for detecting and analyzing malicious URLs to thwart malware attacks in web security. To maintain efficiency of web security, these malicious URLs have to be detected, identified as well as their corresponding links should be found out. Hence users get protected from it and effectiveness of network security gets increased. For such identification there must be analyzer which should not only detect such URLs but analyze them also. And methods to detect corresponding links of malicious URLs. This approach will prevent the users from attacks and increase efficiency of web crawling phase.",,IEEE
Backdoors into Heterogeneous Classes of SAT and CSP,"Serge Gaspers, Neeldhara Misra, Sebastian Ordyniak, Stefan Szeider, Stanislav Živný",arXiv,2016-10-25,"<a href=""arXiv (2016-10-25) : Backdoors into Heterogeneous Classes of SAT and CSP"" target=""_blank"">[http://arxiv.org/abs/1509.05725v2]</a>","<a href=""arXiv"" target=""_blank"">[https://doi.org/10.1016/j.jcss.2016.10.007]</a>","In this paper we extend the classical notion of strong and weak backdoor sets for SAT and CSP by allowing that different instantiations of the backdoor variables result in instances that belong to different base classes, the union of the base classes forms a heterogeneous base class. Backdoor sets to heterogeneous base classes can be much smaller than backdoor sets to homogeneous ones, hence they are much more desirable but possibly harder to find. We draw a detailed complexity landscape for the problem of detecting strong and weak backdoor sets into heterogeneous base classes for SAT and CSP.",,arXiv
Combining Treewidth and Backdoors for CSP,"Robert Ganian, M. S. Ramanujan, Stefan Szeider","arXiv
STACS
arXiv","2016-10-11
2017
2016-10","<a href=""arXiv (2016-10-11) : Combining Treewidth and Backdoors for CSP"" target=""_blank"">[http://arxiv.org/abs/1610.03298v1]</a>
<a href=""DBLP (2017) : Combining Treewidth and Backdoors for CSP"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.STACS.2017.36]</a>
<a href=""DBLP (2016-10) : Combining Treewidth and Backdoors for CSP"" target=""_blank"">[http://arxiv.org/abs/1610.03298]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.4230/LIPIcs.STACS.2017.36]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1610.03298]</a>","We show that CSP is fixed-parameter tractable when parameterized by the treewidth of a backdoor into any tractable CSP problem over a finite constraint language. This result combines the two prominent approaches for achieving tractability for CSP: (i) by structural restrictions on the interaction between the variables and the constraints and (ii) by language restrictions on the relations that can be used inside the constraints. Apart from defining the notion of backdoor-treewidth and showing how backdoors of small treewidth can be used to efficiently solve CSP, our main technical contribution is a fixed-parameter algorithm that finds a backdoor of small treewidth.

","

","arXiv
DBLP
DBLP"
NetCo: Reliable Routing With Unreliable Routers,A. Feldmann P. Heyder M. Kreutzer S. Schmid J. -P. Seifert H. Shulman K. Thimmaraju M. Waidner J. Sieberg,2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshop (DSN-W),2016-10-03,"<a href=""IEEE (2016-10-03) : NetCo: Reliable Routing With Unreliable Routers"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575362]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/DSN-W.2016.38]</a>","Software-Defined Networks (SDNs) are typicallydesigned and operated under the assumption that the underlyingrouters (and switches) are trustworthy. Recent incidents, however, suggest that this assumption is questionable. The possibility ofincorrect or even malicious router behavior introduces a widerange of security problems. The problem is exacerbated by thefact that governments and companies do not have the expertisenor budget to build their own trusted high-performance routinghardware. This paper presents NetCo, an approach to build securerouting using insecure routers. NetCo is inspired by the robustcombiner concept known from cryptography, and leveragesredundancy to compile a secure whole from insecure parts. Wepresent the basic design of NetCo, and report on a prototypeimplementation in OpenFlow.",,IEEE
Software backdoor analysis based on sensitive flow tracking and concolic execution,Xu X.,Wuhan University Journal of Natural Sciences,2016-10-01,"<a href=""ScienceDirect (2016-10-01) : Software backdoor analysis based on sensitive flow tracking and concolic execution"" target=""_blank"">[https://doi.org/10.1007/s11859-016-1190-5]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/s11859-016-1190-5]</a>",,,ScienceDirect
Diagnosing bot infections using Bayesian inference,"Ayesha Binte Ashfaq, Zainab Abaid, ... Syed Ali Khayam",Journal of Computer Virology and Hacking Techniques,2016-09-30,"<a href=""Springer (2016-09-30) : Diagnosing bot infections using Bayesian inference"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-016-0286-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-016-0286-y]</a>","Prior research in botnet detection has used the bot lifecycle to build detection systems. These systems, however, use rule-based decision engines...",,Springer
Anonymous ECC-Authentication and Intrusion Detection Based on Execution Tracing for Mobile Agent Security,Hind Idrissi,Wireless Personal Communications,2016-09-23,"<a href=""Springer (2016-09-23) : Anonymous ECC-Authentication and Intrusion Detection Based on Execution Tracing for Mobile Agent Security"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-016-3712-z]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-016-3712-z]</a>",Mobile agents are software entities able to move from one host to another across networks. They are autonomous and independent from the environment...,,Springer
Mathematical analysis of Penetration Testing and vulnerability countermeasures,M. R. Reddy P. Yalla,2016 IEEE International Conference on Engineering and Technology (ICETECH),2016-09-19,"<a href=""IEEE (2016-09-19) : Mathematical analysis of Penetration Testing and vulnerability countermeasures"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7569185]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ICETECH.2016.7569185]</a>","Penetration Testing is a security research and study of exploitation methods with counter measures to protect web applications from attacks and intruders. It is the art of exploiting the weakness that has been identified in the system under test. Identifying the insecure areas is the major task the goal is to protect sensitive and the valuable data. All the safety vulnerabilities which are present in the system should be exposed with penetration testing. Vulnerabilities are caused due to Design and development errors, Human errors, poor system configuration In this paper we concentrated on different types of penetration testing methods such as Social Engineering, Application Security Testing and Physical Penetration Testing. We focused on different tools involved at different situations at different methods, specifications, requirements, planning and scoping for successful penetration testing using automation tools, manual procedures and auto-manual procedural tools. The mathematical and algorithmic procedure is discussed and proved along with the simulation and graphs, finally design and implementation of penetration testing tool is given with practical analysis and result. Cyber Security and Code Security are the major tasks in Testing, where security is the major task in businesses world as attacks on code or cyber can cutoff the profits as well as reputation of the business enterprise. The major role of penetration testing is to detect and fix the vulnerabilities like malicious code or backdoors. Finally we concluded by development of data security strategies and tools which support the Penetration Testing and role of Advanced Penetration Testing and scope of feature work.",,IEEE
Mutual authentications to parties with QR-code applications in mobile systems,"Cheng-Ta Huang, Yu-Hong Zhang, ... Shiuh-Jeng Wang",International Journal of Information Security,2016-09-14,"<a href=""Springer (2016-09-14) : Mutual authentications to parties with QR-code applications in mobile systems"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-016-0349-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s10207-016-0349-6]</a>",User authentication over the Internet has long been an issue for Internet service providers and users. A good authentication protocol must provide...,,Springer
A Learning-Based Approach to Secure JTAG Against Unseen Scan-Based Attacks,X. Ren R. D. Blanton V. G. Tavares,2016 IEEE Computer Society Annual Symposium on VLSI (ISVLSI),2016-09-08,"<a href=""IEEE (2016-09-08) : A Learning-Based Approach to Secure JTAG Against Unseen Scan-Based Attacks"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7560255]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ISVLSI.2016.107]</a>","Security is becoming an essential problem for integrated circuits (ICs). Various attacks, such as reverse engineering and dumping on-chip data, have been reported to undermine IC security. IEEE 1149.1, also known as JTAG, is primarily used for IC manufacturing test but inevitably provides a ""backdoor"" that can be exploited to attack ICs. Encryption has been used extensively as an effective mean to protect ICs through authentication, but a few weaknesses subsist, such as key leakage. Signature-based techniques ensure security using a database that includes known attacks, but fail to detect attacks that are not contained by the database. To overcome these drawbacks, a two-layer learning-based protection scheme is proposed. Specifically, the scheme monitors the execution of JTAG instructions and uses support vector machines (SVM) to identify abnormal instruction sequences. The use of machine learning enables the detection of unseen attacks without the need for key-based authentication. The experiments based on the OpenSPARC T2 platform demonstrate that the proposed scheme improves the accuracy of detecting unseen attacks by 50% on average when compared to previous work.",,IEEE
A comprehensive study on APT attacks and countermeasures for future networks and communications: challenges and solutions,"Saurabh Singh, Pradip Kumar Sharma, ... Jong Hyuk Park",The Journal of Supercomputing,2016-09-07,"<a href=""Springer (2016-09-07) : A comprehensive study on APT attacks and countermeasures for future networks and communications: challenges and solutions"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-016-1850-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-016-1850-4]</a>","Recently in the connected digital world, targeted attack has become one of the most serious threats to conventional computing systems. Advanced...",,Springer
A learning-based approach to secure JTAG against unseen scan-based attacks,Ren X.,"Proceedings of IEEE Computer Society Annual Symposium on VLSI, ISVLSI",2016-09-02,"<a href=""ScienceDirect (2016-09-02) : A learning-based approach to secure JTAG against unseen scan-based attacks"" target=""_blank"">[https://doi.org/10.1109/ISVLSI.2016.107]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/ISVLSI.2016.107]</a>",,,ScienceDirect
TriggerScope: Towards Detecting Logic Bombs in Android Applications,Y. Fratantonio A. Bianchi W. Robertson E. Kirda C. Kruegel G. Vigna,2016 IEEE Symposium on Security and Privacy (SP),2016-08-18,"<a href=""IEEE (2016-08-18) : TriggerScope: Towards Detecting Logic Bombs in Android Applications"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7546513]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/SP.2016.30]</a>","Android is the most popular mobile platform today, and it is also the mobile operating system that is most heavily targeted by malware. Existing static analyses are effective in detecting the presence of most malicious code and unwanted information flows. However, certain types of malice are very difficult to capture explicitly by modeling permission sets, suspicious API calls, or unwanted information flows. One important type of such malice is malicious application logic, where a program (often subtly) modifies its outputs or performs actions that violate the expectations of the user. Malicious application logic is very hard to identify without a specification of the ""normal,"" expected functionality of the application. We refer to malicious application logic that is executed, or triggered, only under certain (often narrow) circumstances as a logic bomb. This is a powerful mechanism that is commonly employed by targeted malware, often used as part of APTs and state-sponsored attacks: in fact, in this scenario, the malware is designed to target specific victims and to only activate under certain circumstances. In this paper, we make a first step towards detecting logic bombs. In particular, we propose trigger analysis, a new static analysis technique that seeks to automatically identify triggers in Android applications. Our analysis combines symbolic execution, path predicate reconstruction and minimization, and inter-procedural control-dependency analysis to enable the precise detection and characterization of triggers, and it overcomes several limitations of existing approaches. We implemented a prototype of our analysis, called TriggerScope, and we evaluated it over a large corpus of 9,582 benign apps from the Google Play Store and a set of trigger-based malware, including the recently-discovered HackingTeam's RCSAndroid advanced malware. Our system is capable of automatically identify several interesting time-, location-, and SMS-related triggers, is affected by a low false positive rate (0.38%), and it achieves 100% detection rate on the malware set. We also show how existing approaches, specifically when tasked to detect logic bombs, are affected by either a very high false positive rate or false negative rate. Finally, we discuss the logic bombs identified by our analysis, including two previously-unknown backdoors in benign apps.",,IEEE
Virtualization layer security challenges and intrusion detection/prevention systems in cloud computing: a comprehensive review,"Chirag N. Modi, Kamatchi Acha",The Journal of Supercomputing,2016-07-14,"<a href=""Springer (2016-07-14) : Virtualization layer security challenges and intrusion detection/prevention systems in cloud computing: a comprehensive review"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-016-1805-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11227-016-1805-9]</a>","Virtualization plays a vital role in the construction of cloud computing. However, various vulnerabilities are existing in current virtualization...",,Springer
Backdoor use of philosophers in judicial decision-making? Antipodean reflections,Schultz K.,Griffith Law Review,2016-07-02,"<a href=""ScienceDirect (2016-07-02) : Backdoor use of philosophers in judicial decision-making? Antipodean reflections"" target=""_blank"">[https://doi.org/10.1080/10383441.2016.1254012]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1080/10383441.2016.1254012]</a>",,,ScienceDirect
Scalable SoC trust verification using integrated theorem proving and model checking,X. Guo R. G. Dutta P. Mishra Y. Jin,2016 IEEE International Symposium on Hardware Oriented Security and Trust (HOST),2016-06-23,"<a href=""IEEE (2016-06-23) : Scalable SoC trust verification using integrated theorem proving and model checking"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7495569]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/HST.2016.7495569]</a>","The wide usage of hardware Intellectual Property (IP) cores and software programs from untrusted vendors have raised security concerns for system designers. Existing solutions for detecting and preventing software attacks do not usually consider the presence of malicious logic in hardware. Similarly, hardware solutions for detecting Trojans and/or design backdoors do not consider the software running on it. Formal methods provide powerful solutions in detecting malicious behaviors in both hardware and software. However, they suffer from scalability issues and cannot be easily used for large-scale computer systems. To alleviate the scalability challenge, we propose a new integrated formal verification framework to evaluate the trust of computer systems constructed from untrusted third-party software and hardware resources. This framework combines an automated model checker with an interactive theorem prover for proving system-level security properties. We evaluate a vulnerable program executed on a bare metal LEON3 SPARC V8 processor and prove system security with considerable reduction in effort. Our method systematically reduces the effort required for verifying the program running on the System-on-Chip (SoC) compared to traditional interactive theorem proving methods.",,IEEE
Cybersecurity of Connected and Automated Vehicles,André Weimerskirch,ATZelektronik worldwide,2016-06-17,"<a href=""Springer (2016-06-17) : Cybersecurity of Connected and Automated Vehicles"" target=""_blank"">[https://link.springer.com/article/10.1007/s38314-016-0031-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s38314-016-0031-9]</a>",,,Springer
"Rogue Access Point Detection: Taxonomy, Challenges, and Future Directions","Bandar Alotaibi, Khaled Elleithy",Wireless Personal Communications,2016-06-11,"<a href=""Springer (2016-06-11) : Rogue Access Point Detection: Taxonomy, Challenges, and Future Directions"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-016-3390-x]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11277-016-3390-x]</a>",Wireless Local Area Networks (WLANs) are increasingly integrated into our daily lives. Access Points (APs) are an integral part of the WLAN...,,Springer
Interdiction in practice—Hardware Trojan against a high-security USB flash drive,"Pawel Swierczynski, Marc Fyrbiak, ... Christof Paar",Journal of Cryptographic Engineering,2016-06-07,"<a href=""Springer (2016-06-07) : Interdiction in practice—Hardware Trojan against a high-security USB flash drive"" target=""_blank"">[https://link.springer.com/article/10.1007/s13389-016-0132-7]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s13389-016-0132-7]</a>","As part of the revelations about the NSA activities, the notion of interdiction has become known to the public: the interception of deliveries to...",,Springer
Impeding behavior-based malware analysis via replacement attacks to malware specifications,"Jiang Ming, Zhi Xin, ... Bing Mao",Journal of Computer Virology and Hacking Techniques,2016-05-31,"<a href=""Springer (2016-05-31) : Impeding behavior-based malware analysis via replacement attacks to malware specifications"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-016-0281-3]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11416-016-0281-3]</a>","As the underground market of malware flourishes, there is an exponential increase in the number and diversity of malware. A crucial question in...",,Springer
A state feedback impulse model for computer worm control,"Meng Zhang, Guohua Song, Lansun Chen",Nonlinear Dynamics,2016-04-25,"<a href=""Springer (2016-04-25) : A state feedback impulse model for computer worm control"" target=""_blank"">[https://link.springer.com/article/10.1007/s11071-016-2779-0]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11071-016-2779-0]</a>","Computer worm is a worldwide threat to the safety of Internet, which caused billions of dollars in damages over the past decade. Software patches...",,Springer
Us and them: identifying cyber hate on Twitter across multiple protected characteristics,"Pete Burnap, Matthew L Williams",EPJ Data Science,2016-03-23,"<a href=""Springer (2016-03-23) : Us and them: identifying cyber hate on Twitter across multiple protected characteristics"" target=""_blank"">[https://link.springer.com/article/10.1140/epjds/s13688-016-0072-6]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1140/epjds/s13688-016-0072-6]</a>",Hateful and antagonistic content published and propagated via the World Wide Web has the potential to cause harm and suffering on an individual...,,Springer
Fighting against phishing attacks: state of the art and future challenges,"B. B. Gupta, Aakanksha Tewari, ... Dharma P. Agrawal",Neural Computing and Applications,2016-03-17,"<a href=""Springer (2016-03-17) : Fighting against phishing attacks: state of the art and future challenges"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-016-2275-y]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s00521-016-2275-y]</a>","In the last few years, phishing scams have rapidly grown posing huge threat to global Internet security. Today, phishing attack is one of the most...",,Springer
Two Countermeasures Against Hardware Trojans Exploiting Non-Zero Aliasing Probability of BIST,"Elena Dubrova, Mats Näslund, ... Ben Smeets",Journal of Signal Processing Systems,2016-03-11,"<a href=""Springer (2016-03-11) : Two Countermeasures Against Hardware Trojans Exploiting Non-Zero Aliasing Probability of BIST"" target=""_blank"">[https://link.springer.com/article/10.1007/s11265-016-1127-4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11265-016-1127-4]</a>","The threat of hardware Trojans has been widely recognized by academia, industry, and government agencies. A Trojan can compromise security of a...",,Springer
Sandboxing and reasoning on malware infection trees,K. Ghosh J. A. Morales W. Casey B. Mishra,2015 10th International Conference on Malicious and Unwanted Software (MALWARE),2016-02-25,"<a href=""IEEE (2016-02-25) : Sandboxing and reasoning on malware infection trees"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7413686]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/MALWARE.2015.7413686]</a>","Malware infection trees are computational structures for analyzing and identifying different processes and files during the execution of malware. In this paper, we describe a sandboxing-based formalization to predict malware behaviors such as the possibility of file and process creation. Model checking is used as a querying mechanism on a labeled transition system representing a malware infection tree. We evaluate computational feasibility of our formalism using a case study on Backdoor.WIN32.Poison malware and behavior specified by malware infection trees.",,IEEE
Covert remote syscall communication at kernel level: A SPOOKY backdoor,Kerber F.,"2015 10th International Conference on Malicious and Unwanted Software, MALWARE 2015",2016-02-18,"<a href=""ScienceDirect (2016-02-18) : Covert remote syscall communication at kernel level: A SPOOKY backdoor"" target=""_blank"">[https://doi.org/10.1109/MALWARE.2015.7413687]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1109/MALWARE.2015.7413687]</a>",,,ScienceDirect
Strong Backdoors for Linear Temporal Logic,"Arne Meier, Sebastian Ordyniak, M. S. Ramanujan, Irena Schindler","arXiv
arXiv","2016-02-16
2016-02","<a href=""arXiv (2016-02-16) : Strong Backdoors for Linear Temporal Logic"" target=""_blank"">[http://arxiv.org/abs/1602.04934v1]</a>
<a href=""DBLP (2016-02) : Strong Backdoors for Linear Temporal Logic"" target=""_blank"">[http://arxiv.org/abs/1602.04934]</a>","<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://arxiv.org/abs/1602.04934]</a>","In the present paper we introduce the notion of strong backdoors into the field of temporal logic for the CNF-fragment of linear temporal logic introduced by Fisher. We study the parameterised complexity of the satisfiability problem parameterised by the size of the backdoor. We distinguish between backdoor detection and evaluation of backdoors into the fragments of horn and krom formulas. Here we classify the operator fragments of globally-operators for past or future, and the combination of both. Detection is shown to be in FPT whereas the complexity of evaluation behaves different. We show that for krom formulas the problem is paraNP-complete. For horn formulas the complexity is shown to be either fixed parameter tractable or paraNP-complete depending on the considered operator fragment.
","
","arXiv
DBLP"
"Epistatic Signaling and Minority Games, the Adversarial Dynamics in Social Technological Systems","William Casey, Rhiannon Weaver, ... Bud Mishra",Mobile Networks and Applications,2016-02-01,"<a href=""Springer (2016-02-01) : Epistatic Signaling and Minority Games, the Adversarial Dynamics in Social Technological Systems"" target=""_blank"">[https://link.springer.com/article/10.1007/s11036-016-0705-9]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/article/10.1007/s11036-016-0705-9]</a>",We present a game theoretic framework that models strategic interactions among humans and things that are assumed to be interconnected by a...,,Springer
Interior Noise Reduction in a Passenger Vehicle through Mode Modulation of Backdoor,Pimpalkhare N.,SAE Technical Papers,2016-02-01,"<a href=""ScienceDirect (2016-02-01) : Interior Noise Reduction in a Passenger Vehicle through Mode Modulation of Backdoor"" target=""_blank"">[https://doi.org/10.4271/2016-28-0058]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.4271/2016-28-0058]</a>",,,ScienceDirect
A study of android malware detection technology evolution,W. -C. Hsieh C. -C. Wu Y. -W. Kao,2015 International Carnahan Conference on Security Technology (ICCST),2016-01-25,"<a href=""IEEE (2016-01-25) : A study of android malware detection technology evolution"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7389671]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/CCST.2015.7389671]</a>","According to the report of International Data Corporation (IDC), Android OS has dominated the worldwide smart phone Operating System (OS) Market with a 78% share at the first quarter of 2015 also, in the report of F-Secure, 99% of new smart phone threats emerged in the first quarter of 2014 are designed for Android. In recent years, many kinds of malware, such as Botnet, Backdoor, Rootkits, and Trojans, start to attack smart phones for conducting crimes such as fraud, service misuse, information stealing, and root access. In general, they have some shared characteristics, such as constantly scanning for Bluetooth to shorten the device's battery life, accessing the GPS to send the position information to Internet, and jamming the communication between device and the base station to paralyze the wireless network. According to these characteristics, there are a lot of detection method proposed, such as behavior checking, permission-based analysis, and Static Analysis, applied in malware detection software and anti-virus software. However, advanced hackers can utilize some techniques, such as emulator detection, packer, and code obfuscation, to prevent their attacks from being detected. This paper focuses on reviewing the malware evolution which makes malware detection more and more difficult, as well as the development of malware detection software which makes smart phones safer. Finally, our survey gives an insight into the malware evolution trend to increase the detecting rate of unknown malware for malware detection software.",,IEEE
Behavior and system based backdoor detection focusing on CMD phase,Y. FarzaneNia A. Nowroozi,2015 12th International Iranian Society of Cryptology Conference on Information Security and Cryptology (ISCISC),2016-01-21,"<a href=""IEEE (2016-01-21) : Behavior and system based backdoor detection focusing on CMD phase"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7387910]</a>","<a href=""IEEE"" target=""_blank"">[https://doi.org/10.1109/ISCISC.2015.7387910]</a>","Backdoor as a mechanism surreptitiously introduced into a computer system is widely used in performing network attacks. In this article, it is considered to detect its presence while helping an attacker to bypass normal authentication methods of a computer to maintain the access gained. In the latest researches have been done on this field so far, it is emphasized on analyzing only the behavior of backdoors. However, in this paper we propose a novel approach, combining systemic and behavioral features focusing on the ""CMD"" phase that the attacker sends commands to the victim. Through the detection method driven in this article, at first we gather the systemic and behavioral alerts produced while the attacker is installing and utilizing the backdoor interactively and then categorize them by specific features selected to give scores to the both aspects seen. Scores are given in two steps. The first step based on the prominent systemic alerts selected which are specified to backdoors and in the second step we give scores to the behavior it has in the command phase by creating and running a Markov Model. Literally, the scores are normalized and aggregated to determine the probability of backdoor residence on the computer monitored. We evaluated the algorithm in six different scenarios and by a group of well-known backdoors to make distinction between the proposed method and prior works.",,IEEE
Backdoors in pseudorandom number generators: Possibility and impossibility results,Degabriele J.P.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2016-01-01,"<a href=""ScienceDirect (2016-01-01) : Backdoors in pseudorandom number generators: Possibility and impossibility results"" target=""_blank"">[https://doi.org/10.1007/978-3-662-53018-4_15]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-662-53018-4_15]</a>",,,ScienceDirect
Backdoors to tractable Valued CSP,Ganian R.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2016-01-01,"<a href=""ScienceDirect (2016-01-01) : Backdoors to tractable Valued CSP"" target=""_blank"">[https://doi.org/10.1007/978-3-319-44953-1_16]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-319-44953-1_16]</a>",,,ScienceDirect
Cliptography: Clipping the power of kleptographic attacks,Russell A.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2016-01-01,"<a href=""ScienceDirect (2016-01-01) : Cliptography: Clipping the power of kleptographic attacks"" target=""_blank"">[https://doi.org/10.1007/978-3-662-53890-6_2]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-662-53890-6_2]</a>",,,ScienceDirect
Judicial Protection against Plans and Programmes Affecting the Environment: A Backdoor Solution to Get an Answer from Luxembourg,Squintani L.,Journal for European Environmental and Planning Law,2016-01-01,"<a href=""ScienceDirect (2016-01-01) : Judicial Protection against Plans and Programmes Affecting the Environment: A Backdoor Solution to Get an Answer from Luxembourg"" target=""_blank"">[https://doi.org/10.1163/18760104-01303005]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1163/18760104-01303005]</a>",,,ScienceDirect
Secure Authentication: Eliminating Possible Backdoors in Client-Server Endorsement,Jyotiyana J.,Procedia Computer Science,2016-01-01,"<a href=""ScienceDirect (2016-01-01) : Secure Authentication: Eliminating Possible Backdoors in Client-Server Endorsement"" target=""_blank"">[https://doi.org/10.1016/j.procs.2016.05.227]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1016/j.procs.2016.05.227]</a>",,,ScienceDirect
Strong backdoors for default logic,Fichte J.K.,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2016-01-01,"<a href=""ScienceDirect (2016-01-01) : Strong backdoors for default logic"" target=""_blank"">[https://doi.org/10.1007/978-3-319-40970-2_4]</a>","<a href=""ScienceDirect"" target=""_blank"">[https://doi.org/10.1007/978-3-319-40970-2_4]</a>",,,ScienceDirect
Backdoors in Pseudorandom Number Generators: Possibility and Impossibility Results,"Jean Paul Degabriele, Kenneth G. Paterson, ... Joanne Woodage","Advances in Cryptology – CRYPTO 2016
CRYPTO
IACR Cryptol. ePrint Arch.","2016
2016
2016","<a href=""Springer (2016) : Backdoors in Pseudorandom Number Generators: Possibility and Impossibility Results"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53018-4_15]</a>
<a href=""DBLP (2016) : Backdoors in Pseudorandom Number Generators: Possibility and Impossibility Results"" target=""_blank"">[https://doi.org/10.1007/978-3-662-53018-4_15]</a>
<a href=""DBLP (2016) : Backdoors in Pseudorandom Number Generators: Possibility and Impossibility Results"" target=""_blank"">[http://eprint.iacr.org/2016/577]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53018-4_15]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-662-53018-4_15]</a>
<a href=""DBLP"" target=""_blank"">[http://eprint.iacr.org/2016/577]</a>","Inspired by the Dual EC DBRG incident, Dodis et al. (Eurocrypt 2015) initiated the formal study of backdoored PRGs, showing that backdoored PRGs are...

","

","Springer
DBLP
DBLP"
A Cryptographic Analysis of UMTS/LTE AKA,"Stephanie Alt, Pierre-Alain Fouque, ... Benjamin Richard",Applied Cryptography and Network Security,2016,"<a href=""Springer (2016) : A Cryptographic Analysis of UMTS/LTE AKA"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-39555-5_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-39555-5_2]</a>",Secure communications between mobile subscribers and their associated operator networks require mutual authentication and key deri-vation protocols....,,Springer
A Design Methodology for Stealthy Parametric Trojans and Its Application to Bug Attacks,"Samaneh Ghandali, Georg T. Becker, ... Christof Paar",Cryptographic Hardware and Embedded Systems – CHES 2016,2016,"<a href=""Springer (2016) : A Design Methodology for Stealthy Parametric Trojans and Its Application to Bug Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53140-2_30]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53140-2_30]</a>","Over the last decade, hardware Trojans have gained increasing attention in academia, industry and by government agencies. In order to design reliable...",,Springer
A Formal Treatment of Backdoored Pseudorandom Generators,"Yevgeniy Dodis, Chaya Ganesh, Alexander Golovnev, Ari Juels, Thomas Ristenpart",IACR Cryptol. ePrint Arch.,2016,"<a href=""DBLP (2016) : A Formal Treatment of Backdoored Pseudorandom Generators"" target=""_blank"">[http://eprint.iacr.org/2016/306]</a>","<a href=""DBLP"" target=""_blank"">[http://eprint.iacr.org/2016/306]</a>",,,DBLP
A Study on Realtime Detecting Smishing on Cloud Computing Environments,"Ayoung Lee, Kyounghun Kim, ... Moonseog Jun",Advanced Multimedia and Ubiquitous Engineering,2016,"<a href=""Springer (2016) : A Study on Realtime Detecting Smishing on Cloud Computing Environments"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-47895-0_60]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-47895-0_60]</a>","Learning By smartphone developed, mobile malicious code will produce many new species of fraud technique that Smishing using a mobile malicious code...",,Springer
A Survey on Security Analysis in Cloud Computing,"Suryambika, Abhishek Bajpai, Shruti Singh",Proceedings of the International Conference on Recent Cognizance in Wireless Communication & Image Processing,2016,"<a href=""Springer (2016) : A Survey on Security Analysis in Cloud Computing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-81-322-2638-3_29]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-81-322-2638-3_29]</a>","In the era of mobile computing, the number of mobile users has increased day by day. The computing capability, storage capability, processing speed...",,Springer
Antikernel: A Decentralized Secure Hardware-Software Operating System Architecture,"Andrew Zonenberg, Bülent Yener",Cryptographic Hardware and Embedded Systems – CHES 2016,2016,"<a href=""Springer (2016) : Antikernel: A Decentralized Secure Hardware-Software Operating System Architecture"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53140-2_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53140-2_12]</a>","The “kernel” model has been part of operating system architecture for decades, but upon closer inspection it clearly violates the principle of least...",,Springer
Applying Recurrent Neural Network to Intrusion Detection with Hessian Free Optimization,"Jihyun Kim, Howon Kim",Information Security Applications,2016,"<a href=""Springer (2016) : Applying Recurrent Neural Network to Intrusion Detection with Hessian Free Optimization"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-31875-2_30]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-31875-2_30]</a>","With developing a network communication technology, cyber attacks which threaten users safety are increasing. Consequently, many studies are being...",,Springer
Assisted Identification of Mode of Operation in Binary Code with Dynamic Data Flow Slicing,"Pierre Lestringant, Frédéric Guihéry, Pierre-Alain Fouque",Applied Cryptography and Network Security,2016,"<a href=""Springer (2016) : Assisted Identification of Mode of Operation in Binary Code with Dynamic Data Flow Slicing"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-39555-5_30]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-39555-5_30]</a>","Verification of software security properties, when conducted at the binary code level, is a difficult and cumbersome task. This paper is focused on...",,Springer
Backdoors to SAT,Serge Gaspers,Encyclopedia of Algorithms,2016,"<a href=""DBLP (2016) : Backdoors to SAT"" target=""_blank"">[https://doi.org/10.1007/978-1-4939-2864-4_781]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-1-4939-2864-4_781]</a>",,,DBLP
Backdoors to q-Horn,"Serge Gaspers, Sebastian Ordyniak, M. S. Ramanujan, Saket Saurabh, Stefan Szeider",Algorithmica,2016,"<a href=""DBLP (2016) : Backdoors to q-Horn"" target=""_blank"">[https://doi.org/10.1007/s00453-014-9958-5]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/s00453-014-9958-5]</a>",,,DBLP
Cliptography: Clipping the Power of Kleptographic Attacks,"Alexander Russell, Qiang Tang, ... Hong-Sheng Zhou",Advances in Cryptology – ASIACRYPT 2016,2016,"<a href=""Springer (2016) : Cliptography: Clipping the Power of Kleptographic Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53890-6_2]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53890-6_2]</a>","Kleptography, introduced 20 years ago by Young and Yung [Crypto ’96], considers the (in)security of malicious implementations (or instantiations) of...",,Springer
Controlled Randomness - A Defense Against Backdoors in Cryptographic Devices,"Lucjan Hanzlik, Kamil Kluczniak, Miroslaw Kutylowski",Mycrypt,2016,"<a href=""DBLP (2016) : Controlled Randomness - A Defense Against Backdoors in Cryptographic Devices"" target=""_blank"">[https://doi.org/10.1007/978-3-319-61273-7_11]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1007/978-3-319-61273-7_11]</a>",,,DBLP
Cyber Attacks Analysis Using Decision Tree Technique for Improving Cyber Situational Awareness,"Sina Pournouri, Babak Akhgar, Petra Saskia Bayerl","Global Security, Safety and Sustainability - The Security Challenges of the Connected World",2016,"<a href=""Springer (2016) : Cyber Attacks Analysis Using Decision Tree Technique for Improving Cyber Situational Awareness"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-51064-4_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-51064-4_14]</a>",Cyber Security experts are trying to find solutions to prevent cyber-attacks and one of the main solutions is improving cyber situational awareness...,,Springer
DEcryption Contract ENforcement Tool (DECENT): A Practical Alternative to Government Decryption Backdoors,Peter Linder,IACR Cryptol. ePrint Arch.,2016,"<a href=""DBLP (2016) : DEcryption Contract ENforcement Tool (DECENT): A Practical Alternative to Government Decryption Backdoors"" target=""_blank"">[http://eprint.iacr.org/2016/245]</a>","<a href=""DBLP"" target=""_blank"">[http://eprint.iacr.org/2016/245]</a>",,,DBLP
Designing Proof of Human-Work Puzzles for Cryptocurrency and Beyond,"Jeremiah Blocki, Hong-Sheng Zhou",Theory of Cryptography,2016,"<a href=""Springer (2016) : Designing Proof of Human-Work Puzzles for Cryptocurrency and Beyond"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53644-5_20]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53644-5_20]</a>",We introduce the novel notion of a Proof of Human-work (PoH) and present the first distributed consensus protocol from hard Artificial Intelligence...,,Springer
Detecting Stealthy Backdoors and Port Knocking Sequences through Flow Analysis,"Felix von Eye, Michael Grabatin, Wolfgang Hommel",Prax. Inf.verarb. Kommun.,2016,"<a href=""DBLP (2016) : Detecting Stealthy Backdoors and Port Knocking Sequences through Flow Analysis"" target=""_blank"">[http://www.degruyter.com/view/j/piko.2015.38.issue-3-4/pik-2015-0011/pik-2015-0011.xml]</a>","<a href=""DBLP"" target=""_blank"">[http://www.degruyter.com/view/j/piko.2015.38.issue-3-4/pik-2015-0011/pik-2015-0011.xml]</a>",,,DBLP
Detection of Privacy Threat by Peculiar Feature Extraction in Malwares to Combat Targeted Cyber Attacks,"Farhan Habib Ahmad, Komal Batool, Azhar Javed",Advanced Computer and Communication Engineering Technology,2016,"<a href=""Springer (2016) : Detection of Privacy Threat by Peculiar Feature Extraction in Malwares to Combat Targeted Cyber Attacks"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-24584-3_106]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-24584-3_106]</a>",Targeted cyber-threats are topmost concern of organizations and technologies of today. Malwares having similar objectives bear common artifacts. Thus...,,Springer
Exploiting Trust in Deterministic Builds,"Christopher Jämthagen, Patrik Lantz, Martin Hell","Computer Safety, Reliability, and Security",2016,"<a href=""Springer (2016) : Exploiting Trust in Deterministic Builds"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-45477-1_19]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-45477-1_19]</a>","Deterministic builds, where the compile and build processes are reproducible, can be used to achieve increased trust in distributed binaries. As the...",,Springer
GADAPT: A Sequential Game-Theoretic Framework for Designing Defense-in-Depth Strategies Against Advanced Persistent Threats,"Stefan Rass, Quanyan Zhu",Decision and Game Theory for Security,2016,"<a href=""Springer (2016) : GADAPT: A Sequential Game-Theoretic Framework for Designing Defense-in-Depth Strategies Against Advanced Persistent Threats"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-47413-7_18]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-47413-7_18]</a>",We present a dynamic game framework to model and design defense strategies for advanced persistent threats (APTs). The model is based on a sequence...,,Springer
High-Interaction Linux Honeypot Architecture in Recent Perspective,"Tomas Sochor, Matej Zuzcak",Computer Networks,2016,"<a href=""Springer (2016) : High-Interaction Linux Honeypot Architecture in Recent Perspective"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-39207-3_11]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-39207-3_11]</a>",High-interaction honeypots providing virtually an unlimited set of OS services to attackers are necessary to capture the most sophisticated...,,Springer
How to Backdoor Diffie-Hellman,David Wong,IACR Cryptol. ePrint Arch.,2016,"<a href=""DBLP (2016) : How to Backdoor Diffie-Hellman"" target=""_blank"">[http://eprint.iacr.org/2016/644]</a>","<a href=""DBLP"" target=""_blank"">[http://eprint.iacr.org/2016/644]</a>",,,DBLP
How to Generate and Use Universal Samplers,"Dennis Hofheinz, Tibor Jager, ... Mark Zhandry",Advances in Cryptology – ASIACRYPT 2016,2016,"<a href=""Springer (2016) : How to Generate and Use Universal Samplers"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53890-6_24]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53890-6_24]</a>",A random oracle is an idealization that allows us to model a hash function as an oracle that will output a uniformly random string given any input....,,Springer
"MalJs: Lexical, Structural and Behavioral Analysis of Malicious JavaScripts Using Ensemble Classifier","Surendran K, Prabaharan Poornachandran, ... Hrudya P",Security in Computing and Communications,2016,"<a href=""Springer (2016) : MalJs: Lexical, Structural and Behavioral Analysis of Malicious JavaScripts Using Ensemble Classifier"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-2738-3_38]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-981-10-2738-3_38]</a>",Over the past few years javascript has grown up and revolutionized the web by allowing user defined scripts to run inside a web browser. The...,,Springer
NIZKs with an Untrusted CRS: Security in the Face of Parameter Subversion,"Mihir Bellare, Georg Fuchsbauer, Alessandra Scafuro",Advances in Cryptology – ASIACRYPT 2016,2016,"<a href=""Springer (2016) : NIZKs with an Untrusted CRS: Security in the Face of Parameter Subversion"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53890-6_26]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53890-6_26]</a>","Motivated by the subversion of “trusted” public parameters in mass-surveillance activities, this paper studies the security of NIZKs in the presence...",,Springer
No backdoor required or expected,,Commun. ACM,2016,"<a href=""DBLP (2016) : No backdoor required or expected"" target=""_blank"">[https://doi.org/10.1145/2931085]</a>","<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1145/2931085]</a>",,,DBLP
Nonce-Based Cryptography: Retaining Security When Randomness Fails,"Mihir Bellare, Björn Tackmann",Advances in Cryptology – EUROCRYPT 2016,2016,"<a href=""Springer (2016) : Nonce-Based Cryptography: Retaining Security When Randomness Fails"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-49890-3_28]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-49890-3_28]</a>","We take nonce-based cryptography beyond symmetric encryption, developing it as a broad and practical way to mitigate damage caused by failures in...",,Springer
QDQ vs. UCT,Wilhelm Winter,Operator Algebras and Applications,2016,"<a href=""Springer (2016) : QDQ vs. UCT"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-39286-8_15]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-39286-8_15]</a>","This is a survey of recent progress in the structure and classification theory of nuclear C∗-algebras. In particular, I outline how the Universal...",,Springer
QcBits: Constant-Time Small-Key Code-Based Cryptography,Tung Chou,Cryptographic Hardware and Embedded Systems – CHES 2016,2016,"<a href=""Springer (2016) : QcBits: Constant-Time Small-Key Code-Based Cryptography"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53140-2_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53140-2_14]</a>",This paper introduces a constant-time implementation for a quasi-cyclic moderate-density-parity-check (QC-MDPC) code based encryption scheme. At a...,,Springer
Ransomware Steals Your Phone. Formal Methods Rescue It,"Francesco Mercaldo, Vittoria Nardone, ... Corrado Aaron Visaggio","Formal Techniques for Distributed Objects, Components, and Systems",2016,"<a href=""Springer (2016) : Ransomware Steals Your Phone. Formal Methods Rescue It"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-39570-8_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-39570-8_14]</a>",Ransomware is a recent type of malware which makes inaccessible the files or the device of the victim. The only way to unlock the infected device or...,,Springer
Security Challenges of Small Cell as a Service in Virtualized Mobile Edge Computing Environments,"Vassilios Vassilakis, Emmanouil Panaousis, Haralambos Mouratidis",Information Security Theory and Practice,2016,"<a href=""Springer (2016) : Security Challenges of Small Cell as a Service in Virtualized Mobile Edge Computing Environments"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-45931-8_5]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-45931-8_5]</a>",Research on next-generation 5G wireless networks is currently attracting a lot of attention in both academia and industry. While 5G development and...,,Springer
Security Testing Beyond Functional Tests,"Mohammad Torabi Dashti , David Basin",Engineering Secure Software and Systems,2016,"<a href=""Springer (2016) : Security Testing Beyond Functional Tests"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-30806-7_1]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-30806-7_1]</a>",We present a theory of security testing based on the basic distinction between system specifications and security requirements. Specifications...,,Springer
Signature Schemes with Efficient Protocols and Dynamic Group Signatures from Lattice Assumptions,"Benoît Libert, San Ling, ... Huaxiong Wang",Advances in Cryptology – ASIACRYPT 2016,2016,"<a href=""Springer (2016) : Signature Schemes with Efficient Protocols and Dynamic Group Signatures from Lattice Assumptions"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53890-6_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53890-6_13]</a>","A recent line of works – initiated by Gordon, Katz and Vaikuntanathan (Asiacrypt 2010) – gave lattice-based constructions allowing users to...",,Springer
Simpira v2: A Family of Efficient Permutations Using the AES Round Function,"Shay Gueron, Nicky Mouha",Advances in Cryptology – ASIACRYPT 2016,2016,"<a href=""Springer (2016) : Simpira v2: A Family of Efficient Permutations Using the AES Round Function"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53887-6_4]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-662-53887-6_4]</a>","This paper introduces Simpira, a family of cryptographic permutations that supports inputs of...",,Springer
Speeding up R-LWE Post-quantum Key Exchange,"Shay Gueron, Fabian Schlieker",Secure IT Systems,2016,"<a href=""Springer (2016) : Speeding up R-LWE Post-quantum Key Exchange"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-47560-8_12]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-47560-8_12]</a>","Post-quantum cryptography has attracted increased attention in the last couple of years, due to the threat of quantum computers breaking current...",,Springer
Stepping Stone Detection Techniques: Classification and State-of-the-Art,"Rahul Kumar, B. B. Gupta",Proceedings of the International Conference on Recent Cognizance in Wireless Communication & Image Processing,2016,"<a href=""Springer (2016) : Stepping Stone Detection Techniques: Classification and State-of-the-Art"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-81-322-2638-3_59]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-81-322-2638-3_59]</a>","Today, the most common way to perform various attacks is to use stepping stone hosts in the attacking path. In stepping stone attacks, attacker...",,Springer
The Cost-Benefit Analysis of Vulnerability of Web Server Through Investigation,"Seema Verma, Tanya Singh","Emerging Research in Computing, Information, Communication and Applications",2016,"<a href=""Springer (2016) : The Cost-Benefit Analysis of Vulnerability of Web Server Through Investigation"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-81-322-2553-9_53]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-81-322-2553-9_53]</a>","With the rise of web applications, there has been a paradigm shift as web servers are no longer a technical requirement for any organization, but it...",,Springer
The Threat of Virtualization: Hypervisor-Based Rootkits on the ARM Architecture,"Robert Buhren, Julian Vetter, Jan Nordholz",Information and Communications Security,2016,"<a href=""Springer (2016) : The Threat of Virtualization: Hypervisor-Based Rootkits on the ARM Architecture"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-50011-9_29]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-50011-9_29]</a>","The virtualization capabilities of today’s systems offer rootkits excellent hideouts, where they are fairly immune to countermeasures. In this paper,...",,Springer
TransPro: Mandatory Sensitive Information Protection Based on Virtualization and Encryption,"Xue-Zhi Xie, Hu-Qiu Liu, Yu-Ping Wang",Cloud Computing and Security,2016,"<a href=""Springer (2016) : TransPro: Mandatory Sensitive Information Protection Based on Virtualization and Encryption"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-48671-0_39]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-48671-0_39]</a>","With the growing population of networked devices, the potential risk of leaking sensitive data has been seriously increased. This paper proposes a...",,Springer
Uncloaking Rootkits on Mobile Devices with a Hypervisor-Based Detector,"Julian Vetter, Matthias Junker-Petschick, ... Janis Danisevskis",Information Security and Cryptology - ICISC 2015,2016,"<a href=""Springer (2016) : Uncloaking Rootkits on Mobile Devices with a Hypervisor-Based Detector"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-30840-1_17]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-30840-1_17]</a>","Cell phones have evolved into general purpose computing devices, which are tightly integrated into many IT infrastructures. As such, they provide a...",,Springer
Vulnerabilities of Government Websites in a Developing Country – the Case of Burkina Faso,"Tegawendé F. Bissyandé, Jonathan Ouoba, ... Oumarou Sié",e-Infrastructure and e-Services,2016,"<a href=""Springer (2016) : Vulnerabilities of Government Websites in a Developing Country – the Case of Burkina Faso"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-43696-8_14]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-43696-8_14]</a>","Slowly, but consistently, the digital gap between developing and developed countries is being closed. Everyday, there are initiatives towards relying...",,Springer
When Pythons Bite,"Alecsandru Pătraşcu, Ştefan Popa",Innovative Security Solutions for Information Technology and Communications,2016,"<a href=""Springer (2016) : When Pythons Bite"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-47238-6_13]</a>","<a href=""Springer"" target=""_blank"">[https://link.springer.com/chapter/10.1007/978-3-319-47238-6_13]</a>","Python is a common used programming language in many environments, such as datacenter software, embedded programming or regular desktop computers,...",,Springer
CorruptEncoder: Data Poisoning Based Backdoor Attacks to Contrastive Learning,"Jinghuai Zhang, Hongbin Liu, Jinyuan Jia, Neil Zhenqiang Gong","OpenReview
arXiv
arXiv","
2024-02-29
2022-11","<a href=""OpenReview () : CorruptEncoder: Data Poisoning Based Backdoor Attacks to Contrastive Learning"" target=""_blank"">[https://openreview.net/pdf/a71769013eb8042087131d5a81891020c7af2964.pdf]</a>
<a href=""arXiv (2024-02-29) : CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning"" target=""_blank"">[http://arxiv.org/abs/2211.08229v5]</a>
<a href=""DBLP (2022-11) : CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.08229]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/a71769013eb8042087131d5a81891020c7af2964.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.08229]</a>","Contrastive learning (CL) pre-trains general-purpose encoders using an unlabeled pre-training dataset, which consists of images (called single-modal CL) or image-text pairs (called multi-modal CL). CL is vulnerable to data poisoning based backdoor attacks (DPBAs), in which an attacker injects poisoned inputs into the pre-training dataset so the pre-trained encoder is backdoored. However, existing DPBAs achieve limited effectiveness. In this work, we propose new DPBAs called CorruptEncoder to CL. Our experiments show that CorruptEncoder substantially outperforms existing DPBAs for both single-modal and multi-modal CL. Moreover, we also propose a defense, called localized cropping, to defend single-modal CL against DPBAs. Our results show that our defense can reduce the effectiveness of DPBAs, but it sacrifices the utility of the encoder, highlighting the needs of new defenses. We will release our code upon paper acceptance.
Contrastive learning (CL) pre-trains general-purpose encoders using an unlabeled pre-training dataset, which consists of images or image-text pairs. CL is vulnerable to data poisoning based backdoor attacks (DPBAs), in which an attacker injects poisoned inputs into the pre-training dataset so the encoder is backdoored. However, existing DPBAs achieve limited effectiveness. In this work, we take the first step to analyze the limitations of existing backdoor attacks and propose new DPBAs called CorruptEncoder to CL. CorruptEncoder introduces a new attack strategy to create poisoned inputs and uses a theory-guided method to maximize attack effectiveness. Our experiments show that CorruptEncoder substantially outperforms existing DPBAs. In particular, CorruptEncoder is the first DPBA that achieves more than 90% attack success rates with only a few (3) reference images and a small poisoning ratio 0.5%. Moreover, we also propose a defense, called localized cropping, to defend against DPBAs. Our results show that our defense can reduce the effectiveness of DPBAs, but it sacrifices the utility of the encoder, highlighting the need for new defenses.
","

","OpenReview
arXiv
DBLP"
COMBAT: Alternated Training for Near-Perfect Clean-Label Backdoor Attacks,"Tran Ngoc Huynh, Dang Minh Nguyen, Tung Pham, Anh Tuan Tran","OpenReview
AAAI","
2024","<a href=""OpenReview () : COMBAT: Alternated Training for Near-Perfect Clean-Label Backdoor Attacks"" target=""_blank"">[https://openreview.net/pdf/c182fdd518fe8ec0aeafeb8d1b2b55bb8e46a463.pdf]</a>
<a href=""DBLP (2024) : COMBAT: Alternated Training for Effective Clean-Label Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i3.28019]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/c182fdd518fe8ec0aeafeb8d1b2b55bb8e46a463.pdf]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1609/aaai.v38i3.28019]</a>","Backdoor attacks pose a critical concern to the practice of using third-party data for AI development. The data can be poisoned to make a trained model misbehaves when a predefined trigger pattern appears, granting the attackers illegal benefits. While most proposed backdoor attacks are dirty-label, clean-label attacks are more desirable by keeping data labels unchanged to dodge human inspection. However, designing a working clean-label attack is a challenging task, and existing clean-label attacks show underwhelming performance. In this paper, we propose a novel mechanism to develop clean-label attacks with near-perfect attack performance. The key component is a trigger pattern generator, which is trained together with a surrogate model in an alternate manner. Our proposed mechanism is flexible and customizable, allowing different backdoor trigger types and behaviors for either single or multiple target labels. Our backdoor attacks can reach near-perfect attack success rates and bypass all state-of-the-art backdoor defenses, as illustrated via comprehensive experiments on three standard benchmark datasets, including CIFAR-10, GTSRB, and CelebA.
","
","OpenReview
DBLP"
Turning a Curse Into a Blessing: Enabling Data-Free Backdoor Unlearning via Stabilized Model Inversion,"Si Chen, Yi Zeng, Won Park, Tianhao Wang, Xun Chen, Lingjuan Lyu, Zhuoqing Mao, Ruoxi Jia","OpenReview
arXiv","
2023-03-24","<a href=""OpenReview () : Turning a Curse Into a Blessing: Enabling Data-Free Backdoor Unlearning via Stabilized Model Inversion"" target=""_blank"">[https://openreview.net/pdf/1324ba3b9a0f28c0414ba3a49258c5b38ca5213b.pdf]</a>
<a href=""arXiv (2023-03-24) : Turning a Curse into a Blessing: Enabling In-Distribution-Data-Free Backdoor Removal via Stabilized Model Inversion"" target=""_blank"">[http://arxiv.org/abs/2206.07018v3]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/1324ba3b9a0f28c0414ba3a49258c5b38ca5213b.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>","Effectiveness of many existing backdoor removal techniques crucially rely on access to clean in-distribution data. However, as model is often trained on sensitive or proprietary datasets, it might not be practical to assume the availability of in-distribution samples. To address this problem, we propose a novel approach to reconstruct samples from a backdoored model and then use the reconstructed samples as a proxy for clean in-distribution data needed by the defenses. We observe an interesting phenomenon that ensuring perceptual similarity between the synthesized samples and the clean training data is \emph{not} adequate to enable effective defenses. We show that the model predictions at such synthesized samples can be unstable to small input perturbations, which misleads downstream backdoor removal techniques to remove these perturbations instead of underlying backdoor triggers. Moreover, unlike clean samples, the predictions at the synthesized samples can also be unstable to small model parameter changes. To tackle these issues, we design an optimization-based data reconstruction technique that ensures visual quality while promoting the stability to perturbations in both data and parameter space. We also observe that while reconstructed from a backdoored model, the synthesized samples do not contain backdoors, and further provide a theoretical analysis that sheds light on this observation. Our evaluation shows that our data synthesis technique can lead to state-of-the-art backdoor removal performance without clean in-distribution data access and the performance is on par with or sometimes even better than using the same amount of clean samples.
Many backdoor removal techniques in machine learning models require clean in-distribution data, which may not always be available due to proprietary datasets. Model inversion techniques, often considered privacy threats, can reconstruct realistic training samples, potentially eliminating the need for in-distribution data. Prior attempts to combine backdoor removal and model inversion yielded limited results. Our work is the first to provide a thorough understanding of leveraging model inversion for effective backdoor removal by addressing key questions about reconstructed samples' properties, perceptual similarity, and the potential presence of backdoor triggers. We establish that relying solely on perceptual similarity is insufficient for robust defenses, and the stability of model predictions in response to input and parameter perturbations is also crucial. To tackle this, we introduce a novel bi-level optimization-based framework for model inversion, promoting stability and visual quality. Interestingly, we discover that reconstructed samples from a pre-trained generator's latent space are backdoor-free, even when utilizing signals from a backdoored model. We provide a theoretical analysis to support this finding. Our evaluation demonstrates that our stabilized model inversion technique achieves state-of-the-art backdoor removal performance without clean in-distribution data, matching or surpassing performance using the same amount of clean samples.","
","OpenReview
arXiv"
Towards Understanding How Self-training Tolerates Data Backdoor Poisoning,"Soumyadeep Pal, Ren Wang, Yuguang Yao, Sijia Liu","
arXiv
SafeAI@AAAI
arXiv","
2023-01-20
2023
2023-01","<a href=""OpenReview () : Towards Understanding How Self-training Tolerates Data Backdoor Poisoning"" target=""_blank"">[https://arxiv.org/pdf/2301.08751.pdf]</a>
<a href=""arXiv (2023-01-20) : Towards Understanding How Self-training Tolerates Data Backdoor Poisoning"" target=""_blank"">[http://arxiv.org/abs/2301.08751v1]</a>
<a href=""DBLP (2023) : Towards Understanding How Self-training Tolerates Data Backdoor Poisoning"" target=""_blank"">[https://ceur-ws.org/Vol-3381/31.pdf]</a>
<a href=""DBLP (2023-01) : Towards Understanding How Self-training Tolerates Data Backdoor Poisoning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.08751]</a>","<a href=""OpenReview"" target=""_blank"">[https://arxiv.org/pdf/2301.08751.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://ceur-ws.org/Vol-3381/31.pdf]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2301.08751]</a>","Recent studies on backdoor attacks in model training have shown that polluting a small portion of training data is sufficient to produce incorrect manipulated predictions on poisoned test-time data while maintaining high clean accuracy in downstream tasks. The stealthiness of backdoor attacks has imposed tremendous defense challenges in today's machine learning paradigm. In this paper, we explore the potential of self-training via additional unlabeled data for mitigating backdoor attacks. We begin by making a pilot study to show that vanilla self-training is not effective in backdoor mitigation. Spurred by that, we propose to defend the backdoor attacks by leveraging strong but proper data augmentations in the self-training pseudo-labeling stage. We find that the new self-training regime help in defending against backdoor attacks to a great extent. Its effectiveness is demonstrated through experiments for different backdoor triggers on CIFAR-10 and a combination of CIFAR-10 with an additional unlabeled 500K TinyImages dataset. Finally, we explore the direction of combining self-supervised representation learning with self-training for further improvement in backdoor defense.
Recent studies on backdoor attacks in model training have shown that polluting a small portion of training data is sufficient to produce incorrect manipulated predictions on poisoned test-time data while maintaining high clean accuracy in downstream tasks. The stealthiness of backdoor attacks has imposed tremendous defense challenges in today's machine learning paradigm. In this paper, we explore the potential of self-training via additional unlabeled data for mitigating backdoor attacks. We begin by making a pilot study to show that vanilla self-training is not effective in backdoor mitigation. Spurred by that, we propose to defend the backdoor attacks by leveraging strong but proper data augmentations in the self-training pseudo-labeling stage. We find that the new self-training regime help in defending against backdoor attacks to a great extent. Its effectiveness is demonstrated through experiments for different backdoor triggers on CIFAR-10 and a combination of CIFAR-10 with an additional unlabeled 500K TinyImages dataset. Finally, we explore the direction of combining self-supervised representation learning with self-training for further improvement in backdoor defense.

","


","OpenReview
arXiv
DBLP
DBLP"
Removing Backdoors in Pre-trained Models by Regularized Continual Pre-training,"Biru Zhu, Ganqu Cui, Yangyi Chen, Yujia Qin, Lifan Yuan, Chong Fu, Yangdong Deng, Zhiyuan Liu, Maosong Sun, Ming Gu","OpenReview
Trans. Assoc. Comput. Linguistics","
2023","<a href=""OpenReview () : Removing Backdoors in Pre-trained Models by Regularized Continual Pre-training"" target=""_blank"">[https://openreview.net/pdf/b90599e4935794e4f111f07737fb0e5a485048f3.pdf]</a>
<a href=""DBLP (2023) : Removing Backdoors in Pre-trained Models by Regularized Continual Pre-training"" target=""_blank"">[https://doi.org/10.1162/tacl_a_00622]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/b90599e4935794e4f111f07737fb0e5a485048f3.pdf]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1162/tacl_a_00622]</a>","Large-scale pre-trained models (PTMs) have become the cornerstones of deep learning. Trained on massive data, general-purpose PTMs allow quick adaptation to a broad range of downstream tasks with superior performance. However, recent researches reveal that PTMs are vulnerable to backdoor attacks even before being fine-tuned on downstream tasks. By associating specific triggers with pre-defined embeddings, the attackers are capable of implanting transferable task-agnostic backdoors in PTMs, and controlling model outputs on any downstream task at inference time. As a result, all downstream applications can be highly risky after the backdoored PTMs are released and deployed. Given such an emergent threat, it is essential to defend PTMs against backdoor attacks and thus build reliable AI systems. Although there are a series of works aiming to erase backdoors on downstream models, as far as we know, no defenses against PTMs have been proposed. Worse still, existing backdoor-repairing defenses require task-specific knowledge (i.e., some clean downstream data), making them unsuitable for backdoored PTMs. To this end, we propose the first task-irrelevant backdoor removal method for PTMs. Motivated by the sparse activation phenomenon, we design a simple and effective backdoor eraser by continually pre-training the backdoored PTMs with a regularization term, guiding the models to ""forget'' backdoors. Our method only needs a few auxiliary task-irrelevant data, e.g., unlabelled plain texts, and thus is practical in typical applications. We conduct extensive experiments across modalities (vision and language) and architectures (CNNs and Transformers) on pre-trained VGG, ViT, BERT and CLIP models. The results show that our method can effectively remove backdoors and preserve benign functionalities in PTMs.
","
","OpenReview
DBLP"
The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning,"Virat Shejwalkar, Lingjuan Lyu, Amir Houmansadr","
arXiv
ICCV
arXiv","
2022-11-01
2023
2022-11","<a href=""OpenReview () : The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning"" target=""_blank"">[https://openaccess.thecvf.com/content/ICCV2023/papers/Shejwalkar_The_Perils_of_Learning_From_Unlabeled_Data_Backdoor_Attacks_on_ICCV_2023_paper.pdf]</a>
<a href=""arXiv (2022-11-01) : The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning"" target=""_blank"">[http://arxiv.org/abs/2211.00453v1]</a>
<a href=""DBLP (2023) : The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00436]</a>
<a href=""DBLP (2022-11) : The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.00453]</a>","<a href=""OpenReview"" target=""_blank"">[https://openaccess.thecvf.com/content/ICCV2023/papers/Shejwalkar_The_Perils_of_Learning_From_Unlabeled_Data_Backdoor_Attacks_on_ICCV_2023_paper.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICCV51070.2023.00436]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2211.00453]</a>","Semi-supervised learning (SSL) is gaining popularity as it reduces cost of machine learning (ML) by training high performance models using unlabeled data. In this paper, we reveal that the key feature of SSL, i.e., learning from (noninspected) unlabeled data, exposes SSL to strong poisoning attacks that can significantly damage its security. Poisoning is a long-standing problem in conventional supervised ML, but we argue that, as SSL relies on non-inspected unlabeled data, poisoning poses a more significant threat to SSL. We demonstrate this by designing a backdoor poisoning attack on SSL that can be conducted by a weak adversary with no knowledge of the target SSL pipeline. This is unlike prior poisoning attacks on supervised ML that assume strong adversaries with impractical capabilities. We show that by poisoning only 0.2% of the unlabeled training data, our (weak) adversary can successfully cause misclassification on more than 80% of test inputs (when they contain the backdoor trigger). Our attack remains effective across different benchmark datasets and SSL algorithms, and even circumvents state-of-the-art defenses against backdoor attacks. Our work raises significant concerns about the security of SSL in real-world security critical applications
Semi-supervised machine learning (SSL) is gaining popularity as it reduces the cost of training ML models. It does so by using very small amounts of (expensive, well-inspected) labeled data and large amounts of (cheap, non-inspected) unlabeled data. SSL has shown comparable or even superior performances compared to conventional fully-supervised ML techniques. In this paper, we show that the key feature of SSL that it can learn from (non-inspected) unlabeled data exposes SSL to strong poisoning attacks. In fact, we argue that, due to its reliance on non-inspected unlabeled data, poisoning is a much more severe problem in SSL than in conventional fully-supervised ML. Specifically, we design a backdoor poisoning attack on SSL that can be conducted by a weak adversary with no knowledge of target SSL pipeline. This is unlike prior poisoning attacks in fully-supervised settings that assume strong adversaries with practically-unrealistic capabilities. We show that by poisoning only 0.2% of the unlabeled training data, our attack can cause misclassification of more than 80% of test inputs (when they contain the adversary's backdoor trigger). Our attacks remain effective across twenty combinations of benchmark datasets and SSL algorithms, and even circumvent the state-of-the-art defenses against backdoor attacks. Our work raises significant concerns about the practical utility of existing SSL algorithms.

","


","OpenReview
arXiv
DBLP
DBLP"
Backdoor Attacks in the Supply Chain of Masked Image Modeling,"Xinyue Shen, Xinlei He, Zheng Li, Yun Shen, Michael Backes, Yang Zhang","OpenReview
arXiv
arXiv","
2022-10-04
2022-10","<a href=""OpenReview () : Backdoor Attacks in the Supply Chain of Masked Image Modeling"" target=""_blank"">[https://openreview.net/pdf/0c5ec0b08ce9e3512fdc3d80cd06802dbb8ef089.pdf]</a>
<a href=""arXiv (2022-10-04) : Backdoor Attacks in the Supply Chain of Masked Image Modeling"" target=""_blank"">[http://arxiv.org/abs/2210.01632v1]</a>
<a href=""DBLP (2022-10) : Backdoor Attacks in the Supply Chain of Masked Image Modeling"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.01632]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/0c5ec0b08ce9e3512fdc3d80cd06802dbb8ef089.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2210.01632]</a>","Masked image modeling (MIM) revolutionizes self-supervised learning (SSL) for image pre-training. In contrast to previous dominating self-supervised methods, i.e., contrastive learning, MIM attains state-of-the-art performance by masking and reconstructing random patches of the input image. However, the associated security and privacy risks of this novel generative method are unexplored. In this paper, we perform the first security risk quantification of MIM through the lens of backdoor attacks. Different from previous work, we are the first to systematically threat modeling on SSL in every phase of model supply chain, i.e., pre-training, release, and downstream phases. Our evaluation shows that models built with MIM are vulnerable to existing backdoor attacks in release and downstream phases and are compromised by our proposed method in pre-training phase. For instance, on CIFAR10 dataset, the attack success rate can reach 99.62%, 96.48%, and 98.89% in the downstream phase, release phase, and pre-training phase, respectively. We also take the first step to investigate the success factors of backdoor attacks in the pre-training phase and find the trigger number and trigger pattern play key roles in the success of backdoor attacks while trigger location has only tiny effects. In the end, our empirical study of the defense mechanisms across three detection-level on model supply chain phases indicates that different defenses are suitable for backdoor attacks in different phases. However, backdoor attacks in the release phase cannot be detected by all three detection-level methods, calling for more effective defenses in future research.
Masked image modeling (MIM) revolutionizes self-supervised learning (SSL) for image pre-training. In contrast to previous dominating self-supervised methods, i.e., contrastive learning, MIM attains state-of-the-art performance by masking and reconstructing random patches of the input image. However, the associated security and privacy risks of this novel generative method are unexplored. In this paper, we perform the first security risk quantification of MIM through the lens of backdoor attacks. Different from previous work, we are the first to systematically threat modeling on SSL in every phase of the model supply chain, i.e., pre-training, release, and downstream phases. Our evaluation shows that models built with MIM are vulnerable to existing backdoor attacks in release and downstream phases and are compromised by our proposed method in pre-training phase. For instance, on CIFAR10, the attack success rate can reach 99.62%, 96.48%, and 98.89% in the downstream phase, release phase, and pre-training phase, respectively. We also take the first step to investigate the success factors of backdoor attacks in the pre-training phase and find the trigger number and trigger pattern play key roles in the success of backdoor attacks while trigger location has only tiny effects. In the end, our empirical study of the defense mechanisms across three detection-level on model supply chain phases indicates that different defenses are suitable for backdoor attacks in different phases. However, backdoor attacks in the release phase cannot be detected by all three detection-level methods, calling for more effective defenses in future research.
","

","OpenReview
arXiv
DBLP"
Architectural Backdoors in Neural Networks,"Mikel Bober-Irizar, Ilia Shumailov, Yiren Zhao, Robert D. Mullins, Nicolas Papernot","OpenReview
arXiv
CVPR
arXiv","
2022-06-15
2023
2022-06","<a href=""OpenReview () : Architectural Backdoors in Neural Networks"" target=""_blank"">[https://openreview.net/pdf/c202e3f7b58579019c2ae7534b94815d06eda13d.pdf]</a>
<a href=""arXiv (2022-06-15) : Architectural Backdoors in Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2206.07840v1]</a>
<a href=""DBLP (2023) : Architectural Backdoors in Neural Networks"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.02356]</a>
<a href=""DBLP (2022-06) : Architectural Backdoors in Neural Networks"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.07840]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/c202e3f7b58579019c2ae7534b94815d06eda13d.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/CVPR52729.2023.02356]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.07840]</a>","Machine learning is vulnerable to adversarial manipulation. Previous literature has demonstrated that at the training stage attackers can manipulate data (Gu et al.) and data sampling procedures (Shumailov et al.) to control model behaviour. A common attack goal is to plant backdoors i.e. force the victim model to learn to recognise a trigger known only by the adversary. In this paper, we introduce a new class of backdoor attacks that hide inside model architectures i.e. in the inductive bias of the functions used to train. These backdoors are simple to implement, for instance by publishing open-source code for a backdoored model architecture that others will reuse unknowingly. We demonstrate that model architectural backdoors represent a real threat and, unlike other approaches, can survive a complete re-training from scratch. We formalise the main construction principles behind architectural backdoors, such as a link between the input and the output, and describe some possible protections against them. We evaluate our attacks on computer vision benchmarks of different scales and demonstrate the underlying vulnerability is pervasive in a variety of common training settings.
Machine learning is vulnerable to adversarial manipulation. Previous literature has demonstrated that at the training stage attackers can manipulate data and data sampling procedures to control model behaviour. A common attack goal is to plant backdoors i.e. force the victim model to learn to recognise a trigger known only by the adversary. In this paper, we introduce a new class of backdoor attacks that hide inside model architectures i.e. in the inductive bias of the functions used to train. These backdoors are simple to implement, for instance by publishing open-source code for a backdoored model architecture that others will reuse unknowingly. We demonstrate that model architectural backdoors represent a real threat and, unlike other approaches, can survive a complete re-training from scratch. We formalise the main construction principles behind architectural backdoors, such as a link between the input and the output, and describe some possible protections against them. We evaluate our attacks on computer vision benchmarks of different scales and demonstrate the underlying vulnerability is pervasive in a variety of training settings.

","


","OpenReview
arXiv
DBLP
DBLP"
Neurotoxin: durable backdoors in federated learning,"Zhengming Zhang, Ashwinee Panda, Linyue Song, Yaoqing Yang, Michael W. Mahoney, Joseph E. Gonzalez, Kannan Ramchandran, Prateek Mittal","
arXiv
arXiv
ICML","
2022-06-12
2022-06
2022","<a href=""OpenReview () : Neurotoxin: durable backdoors in federated learning"" target=""_blank"">[https://proceedings.mlr.press/v162/zhang22w/zhang22w.pdf]</a>
<a href=""arXiv (2022-06-12) : Neurotoxin: Durable Backdoors in Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2206.10341v1]</a>
<a href=""DBLP (2022-06) : Neurotoxin: Durable Backdoors in Federated Learning"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.10341]</a>
<a href=""DBLP (2022) : Neurotoxin: Durable Backdoors in Federated Learning"" target=""_blank"">[https://proceedings.mlr.press/v162/zhang22w.html]</a>","<a href=""OpenReview"" target=""_blank"">[https://proceedings.mlr.press/v162/zhang22w/zhang22w.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.48550/arXiv.2206.10341]</a>
<a href=""DBLP"" target=""_blank"">[https://proceedings.mlr.press/v162/zhang22w.html]</a>","Due to their decentralized nature, federated learning (FL) systems have an inherent vulnerability during their training to adversarial backdoor attacks. In this type of attack, the goal of the attacker is to use poisoned updates to implant so-called backdoors into the learned model such that, at test time, the model’s outputs can be fixed to a given target for certain inputs. (As a simple toy example, if a user types “people from New York” into a mobile keyboard app that uses a backdoored next word prediction model, then the model could autocomplete the sentence to “people from New York are rude”). Prior work has shown that backdoors can be inserted into FL models, but these backdoors are often not durable, i.e., they do not remain in the model after the attacker stops uploading poisoned updates. Thus, since training typically continues progressively in production FL systems, an inserted backdoor may not survive until deployment. Here, we propose Neurotoxin, a simple one-line modification to existing backdoor attacks that acts by attacking parameters that are changed less in magnitude during training. We conduct an exhaustive evaluation across ten natural language processing and computer vision tasks, and we find that we can double the durability of state-of-the-art backdoors.
Due to their decentralized nature, federated learning (FL) systems have an inherent vulnerability during their training to adversarial backdoor attacks. In this type of attack, the goal of the attacker is to use poisoned updates to implant so-called backdoors into the learned model such that, at test time, the model's outputs can be fixed to a given target for certain inputs. (As a simple toy example, if a user types ""people from New York"" into a mobile keyboard app that uses a backdoored next word prediction model, then the model could autocomplete the sentence to ""people from New York are rude""). Prior work has shown that backdoors can be inserted into FL models, but these backdoors are often not durable, i.e., they do not remain in the model after the attacker stops uploading poisoned updates. Thus, since training typically continues progressively in production FL systems, an inserted backdoor may not survive until deployment. Here, we propose Neurotoxin, a simple one-line modification to existing backdoor attacks that acts by attacking parameters that are changed less in magnitude during training. We conduct an exhaustive evaluation across ten natural language processing and computer vision tasks, and we find that we can double the durability of state of the art backdoors.

","


","OpenReview
arXiv
DBLP
DBLP"
A Feature-Based On-Line Detector to Remove Adversarial-Backdoors by Iterative Demarcation,"Hao Fu, Akshaj Kumar Veldanda, Prashanth Krishnamurthy, Siddharth Garg, Farshad Khorrami","
IEEE Access","
2022","<a href=""OpenReview () : A Feature-Based On-Line Detector to Remove Adversarial-Backdoors by Iterative Demarcation"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9673744]</a>
<a href=""DBLP (2022) : A Feature-Based On-Line Detector to Remove Adversarial-Backdoors by Iterative Demarcation"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2022.3141077]</a>","<a href=""OpenReview"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9673744]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ACCESS.2022.3141077]</a>","This paper proposes a novel feature-based on-line detection strategy, Removing Adversarial-Backdoors by Iterative Demarcation (RAID), for backdoor attacks. The proposed method is comprised of two parts: off-line training and on-line retraining. In the off-line training, a novelty detector and a shallow neural network are trained with clean validation data. During the on-line implementation, both models attempt to detect samples from the streaming data that differ from the validation data (i.e., flag likely-poisoned samples and possibly a few clean samples as false positives). An anomaly detector is used to purify the anomalous data by removing the clean samples. A binary support vector machine (SVM) is trained with the purified anomalous data and the clean validation data. RAID uses the SVM to detect poisoned inputs. To increase the accuracy as new anomalous data is being detected, the SVM is updated as well in real-time. It is shown that with updating, RAID can efficiently reduce the attack success rate while maintaining the classification accuracy against various types of backdoor attacks. The efficacy of RAID is compared against several state-of-the-art techniques. Additionally, it is shown that RAID only requires a small clean validation dataset to achieve such performance, and therefore provides a practical and efficient approach.
","
","OpenReview
DBLP"
A General Framework for Defending against Backdoor Attacks via Influence Graph,"Xiaofei Sun, Jiwei Li, Tianwei Zhang, Han Qiu, Fei Wu, Chun Fan","OpenReview
arXiv
arXiv","
2021-11-29
2021-11","<a href=""OpenReview () : A General Framework for Defending against Backdoor Attacks via Influence Graph"" target=""_blank"">[https://openreview.net/pdf/836f6e103d3102b699cb11bf95b64e7f0b387852.pdf]</a>
<a href=""arXiv (2021-11-29) : A General Framework for Defending Against Backdoor Attacks via Influence Graph"" target=""_blank"">[http://arxiv.org/abs/2111.14309v1]</a>
<a href=""DBLP (2021-11) : A General Framework for Defending Against Backdoor Attacks via Influence Graph"" target=""_blank"">[https://arxiv.org/abs/2111.14309]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/836f6e103d3102b699cb11bf95b64e7f0b387852.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2111.14309]</a>","In this work, we propose a new and general framework to defend against backdoor attacks, inspired by the fact that attack triggers usually follow a \textsc{specific} type of attacking pattern, and therefore, they have greater connections with and larger impacts on each other during training. We introduce the notion of the {\it influence graph}, which consists of nodes and edges respectively representative of individual training points and associated pair-wise influences. The influence between a pair of training points represents the degree to which removing one training point would impact the prediction the model makes on the other training point, approximated by influence function. Malicious training points are extracted by finding the maximum average sub-graph subject to a particular size. Extensive experiments on computer vision and natural language processing tasks demonstrate the effectiveness and generality of the proposed framework.
In this work, we propose a new and general framework to defend against backdoor attacks, inspired by the fact that attack triggers usually follow a \textsc{specific} type of attacking pattern, and therefore, poisoned training examples have greater impacts on each other during training. We introduce the notion of the {\it influence graph}, which consists of nodes and edges respectively representative of individual training points and associated pair-wise influences. The influence between a pair of training points represents the impact of removing one training point on the prediction of another, approximated by the influence function \citep{koh2017understanding}. Malicious training points are extracted by finding the maximum average sub-graph subject to a particular size. Extensive experiments on computer vision and natural language processing tasks demonstrate the effectiveness and generality of the proposed framework.
","

","OpenReview
arXiv
DBLP"
"Poisoned classifiers are not only backdoored, they are fundamentally broken","Mingjie Sun, Siddhant Agarwal, J Zico Kolter","
arXiv
arXiv","
2021-10-05
2020-10","<a href=""OpenReview () : Poisoned classifiers are not only backdoored, they are fundamentally broken"" target=""_blank"">[https://openreview.net/pdf/4959459ccc8a6c2d401fe6ca978ce4b82b4f3ff0.pdf]</a>
<a href=""arXiv (2021-10-05) : Poisoned classifiers are not only backdoored, they are fundamentally broken"" target=""_blank"">[http://arxiv.org/abs/2010.09080v2]</a>
<a href=""DBLP (2020-10) : Poisoned classifiers are not only backdoored, they are fundamentally broken"" target=""_blank"">[https://arxiv.org/abs/2010.09080]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/4959459ccc8a6c2d401fe6ca978ce4b82b4f3ff0.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2010.09080]</a>","Under a commonly-studied “backdoor” poisoning attack against classification models, an attacker adds a small “trigger” to a subset of the training data, such that the presence of this trigger at test time causes the classifier to always predict some target class. It is often implicitly assumed that the poisoned classifier is vulnerable exclusively to the adversary who possesses the trigger. In this paper, we show empirically that this view of backdoored classifiers is fundamentally incorrect. We demonstrate that anyone with access to the classifier, even without access to any original training data or trigger, can construct several alternative triggers that are as effective or more so at eliciting the target class at test time. We construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a recent process called Denoised Smoothing, and then extracting colors or cropped portions of adversarial images. We demonstrate the effectiveness of our attack through extensive experiments on ImageNet and TrojAI datasets, including a user study which demonstrates that our method allows users to easily determine the existence of such backdoors in existing poisoned classifiers. Furthermore, we demonstrate that our alternative triggers can in fact look entirely different from the original trigger, highlighting that the backdoor actually learned by the classifier differs substantially from the trigger image itself. Thus, we argue that there is no such thing as a “secret” backdoor in poisoned classifiers: poisoning a classifier invites attacks not just by the party that possesses the trigger, but from anyone with access to the classifier.
Under a commonly-studied backdoor poisoning attack against classification models, an attacker adds a small trigger to a subset of the training data, such that the presence of this trigger at test time causes the classifier to always predict some target class. It is often implicitly assumed that the poisoned classifier is vulnerable exclusively to the adversary who possesses the trigger. In this paper, we show empirically that this view of backdoored classifiers is incorrect. We describe a new threat model for poisoned classifier, where one without knowledge of the original trigger, would want to control the poisoned classifier. Under this threat model, we propose a test-time, human-in-the-loop attack method to generate multiple effective alternative triggers without access to the initial backdoor and the training data. We construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a procedure called Denoised Smoothing, and then extracting colors or cropped portions of smoothed adversarial images with human interaction. We demonstrate the effectiveness of our attack through extensive experiments on high-resolution datasets: ImageNet and TrojAI. We also compare our approach to previous work on modeling trigger distributions and find that our method are more scalable and efficient in generating effective triggers. Last, we include a user study which demonstrates that our method allows users to easily determine the existence of such backdoors in existing poisoned classifiers. Thus, we argue that there is no such thing as a secret backdoor in poisoned classifiers: poisoning a classifier invites attacks not just by the party that possesses the trigger, but from anyone with access to the classifier.
","

","OpenReview
arXiv
DBLP"
BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning,"Jinyuan Jia, Yupei Liu, Neil Zhenqiang Gong","
arXiv
SP
arXiv","
2021-08-01
2022
2021-08","<a href=""OpenReview () : BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning"" target=""_blank"">[https://arxiv.org/pdf/2108.00352.pdf]</a>
<a href=""arXiv (2021-08-01) : BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning"" target=""_blank"">[http://arxiv.org/abs/2108.00352v1]</a>
<a href=""DBLP (2022) : BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning"" target=""_blank"">[https://doi.org/10.1109/SP46214.2022.9833644]</a>
<a href=""DBLP (2021-08) : BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning"" target=""_blank"">[https://arxiv.org/abs/2108.00352]</a>","<a href=""OpenReview"" target=""_blank"">[https://arxiv.org/pdf/2108.00352.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/SP46214.2022.9833644]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2108.00352]</a>","Self-supervised learning in computer vision aims to pre-train an image encoder using a large amount of unlabeled images or (image, text) pairs. The pre-trained image encoder can then be used as a feature extractor to build downstream classifiers for many downstream tasks with a small amount of or no labeled training data. In this work, we propose BadEncoder, the first backdoor attack to self-supervised learning. In particular, our BadEncoder injects backdoors into a pre-trained image encoder such that the downstream classifiers built based on the backdoored image encoder for different downstream tasks simultaneously inherit the backdoor behavior. We formulate our BadEncoder as an optimization problem and we propose a gradient descent based method to solve it, which produces a backdoored image encoder from a clean one. Our extensive empirical evaluation results on multiple datasets show that our BadEncoder achieves high attack success rates while preserving the accuracy of the downstream classifiers. We also show the effectiveness of BadEncoder using two publicly available, real-world image encoders, i.e., Google's image encoder pre-trained on ImageNet and OpenAI's Contrastive Language-Image Pre-training (CLIP) image encoder pre-trained on 400 million (image, text) pairs collected from the Internet. Moreover, we consider defenses including Neural Cleanse and MNTD (empirical defenses) as well as PatchGuard (a provable defense). Our results show that these defenses are insufficient to defend against BadEncoder, highlighting the needs for new defenses against our BadEncoder. Our code is publicly available at: this https URL.
Self-supervised learning in computer vision aims to pre-train an image encoder using a large amount of unlabeled images or (image, text) pairs. The pre-trained image encoder can then be used as a feature extractor to build downstream classifiers for many downstream tasks with a small amount of or no labeled training data. In this work, we propose BadEncoder, the first backdoor attack to self-supervised learning. In particular, our BadEncoder injects backdoors into a pre-trained image encoder such that the downstream classifiers built based on the backdoored image encoder for different downstream tasks simultaneously inherit the backdoor behavior. We formulate our BadEncoder as an optimization problem and we propose a gradient descent based method to solve it, which produces a backdoored image encoder from a clean one. Our extensive empirical evaluation results on multiple datasets show that our BadEncoder achieves high attack success rates while preserving the accuracy of the downstream classifiers. We also show the effectiveness of BadEncoder using two publicly available, real-world image encoders, i.e., Google's image encoder pre-trained on ImageNet and OpenAI's Contrastive Language-Image Pre-training (CLIP) image encoder pre-trained on 400 million (image, text) pairs collected from the Internet. Moreover, we consider defenses including Neural Cleanse and MNTD (empirical defenses) as well as PatchGuard (a provable defense). Our results show that these defenses are insufficient to defend against BadEncoder, highlighting the needs for new defenses against our BadEncoder. Our code is publicly available at: https://github.com/jjy1994/BadEncoder.

","
<a href=""arXiv"" target=""_blank"">[https://github.com/jjy1994/BadEncoder]</a>

","OpenReview
arXiv
DBLP
DBLP"
Just How Toxic is Data Poisoning? A Benchmark for Backdoor and Data Poisoning Attacks,"Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, Tom Goldstein","
arXiv
ICML
arXiv","
2021-06-17
2021
2020-06","<a href=""OpenReview () : Just How Toxic is Data Poisoning? A Benchmark for Backdoor and Data Poisoning Attacks"" target=""_blank"">[https://openreview.net/pdf/51c0801d766028729a95dc23201520e869fdf8ad.pdf]</a>
<a href=""arXiv (2021-06-17) : Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks"" target=""_blank"">[http://arxiv.org/abs/2006.12557v3]</a>
<a href=""DBLP (2021) : Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks"" target=""_blank"">[http://proceedings.mlr.press/v139/schwarzschild21a.html]</a>
<a href=""DBLP (2020-06) : Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks"" target=""_blank"">[https://arxiv.org/abs/2006.12557]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/51c0801d766028729a95dc23201520e869fdf8ad.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[http://proceedings.mlr.press/v139/schwarzschild21a.html]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2006.12557]</a>","Data poisoning and backdoor attacks manipulate training data in order to cause models to fail during inference. A recent survey of industry practitioners found that data poisoning is the number one concern among threats ranging from model stealing to adversarial attacks. However, we find that the impressive performance evaluations from data poisoning attacks are, in large part, artifacts of inconsistent experimental design. Moreover, we find that existing poisoning methods have been tested in contrived scenarios, and many fail in more realistic settings. In order to promote fair comparison in future work, we develop standardized benchmarks for data poisoning and backdoor attacks.
Data poisoning and backdoor attacks manipulate training data in order to cause models to fail during inference. A recent survey of industry practitioners found that data poisoning is the number one concern among threats ranging from model stealing to adversarial attacks. However, it remains unclear exactly how dangerous poisoning methods are and which ones are more effective considering that these methods, even ones with identical objectives, have not been tested in consistent or realistic settings. We observe that data poisoning and backdoor attacks are highly sensitive to variations in the testing setup. Moreover, we find that existing methods may not generalize to realistic settings. While these existing works serve as valuable prototypes for data poisoning, we apply rigorous tests to determine the extent to which we should fear them. In order to promote fair comparison in future work, we develop standardized benchmarks for data poisoning and backdoor attacks.

","


","OpenReview
arXiv
DBLP
DBLP"
Rethinking the Trigger of Backdoor Attack,"Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, Shu-Tao Xia","
arXiv
arXiv","
2021-01-31
2020-04","<a href=""OpenReview () : Rethinking the Trigger of Backdoor Attack"" target=""_blank"">[https://openreview.net/pdf/f41085225b4c2960c0e50e0201c0c0ab536e020f.pdf]</a>
<a href=""arXiv (2021-01-31) : Rethinking the Trigger of Backdoor Attack"" target=""_blank"">[http://arxiv.org/abs/2004.04692v3]</a>
<a href=""DBLP (2020-04) : Rethinking the Trigger of Backdoor Attack"" target=""_blank"">[https://arxiv.org/abs/2004.04692]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/f41085225b4c2960c0e50e0201c0c0ab536e020f.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2004.04692]</a>","Backdoor attack intends to inject hidden backdoor into the deep neural networks (DNNs), such that the prediction of the infected model will be maliciously changed if the hidden backdoor is activated by the attacker-defined trigger, while it performs well on benign samples. Currently, most of existing backdoor attacks adopted the setting of \emph{static} trigger, $i.e.,$ triggers across the training and testing images follow the same appearance and are located in the same area. In this paper, we revisit this attack paradigm by analyzing the characteristics of the static trigger. We demonstrate that such an attack paradigm is vulnerable when the trigger in testing images is not consistent with the one used for training. We further explore how to utilize this property for backdoor defense, and discuss how to alleviate such vulnerability of existing attacks. We hope that this work could inspire more explorations on backdoor properties, to help the design of more advanced backdoor defense and attack methods.
Backdoor attack intends to inject hidden backdoor into the deep neural networks (DNNs), such that the prediction of the infected model will be maliciously changed if the hidden backdoor is activated by the attacker-defined trigger, while it performs well on benign samples. Currently, most of existing backdoor attacks adopted the setting of \emph{static} trigger, $i.e.,$ triggers across the training and testing images follow the same appearance and are located in the same area. In this paper, we revisit this attack paradigm by analyzing the characteristics of the static trigger. We demonstrate that such an attack paradigm is vulnerable when the trigger in testing images is not consistent with the one used for training. We further explore how to utilize this property for backdoor defense, and discuss how to alleviate such vulnerability of existing attacks.
","

","OpenReview
arXiv
DBLP"
Invisible Poison: A Blackbox Clean Label Backdoor Attack to Deep Neural Networks,"RUI NING, Jiang Li, Chunsheng Xin, Hong Wu","
INFOCOM","
2021","<a href=""OpenReview () : Invisible Poison: A Blackbox Clean Label Backdoor Attack to Deep Neural Networks"" target=""_blank"">[]</a>
<a href=""DBLP (2021) : Invisible Poison: A Blackbox Clean Label Backdoor Attack to Deep Neural Networks"" target=""_blank"">[https://doi.org/10.1109/INFOCOM42981.2021.9488902]</a>","<a href=""OpenReview"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/INFOCOM42981.2021.9488902]</a>","This paper reports a new clean-label data poisoning backdoor attack, named Invisible Poison, which stealthily and aggressively plants a backdoor in neural networks. It converts a regular trigger to a noised trigger that can be easily concealed inside images for training NN, with the objective to plant a backdoor that can be later activated by the trigger. Compared with existing data poisoning backdoor attacks, this newfound attack has the following distinct properties. First, it is a blackbox attack, requiring zero-knowledge of the target model. Second, this attack utilizes “invisible poison” to achieve stealthiness where the trigger is disguised as ‘noise’, and thus can easily evade human inspection. On the other hand, this noised trigger remains effective in the feature space to poison training data. Third, the attack is practical and aggressive. A backdoor can be effectively planted with a small amount of poisoned data and is robust to most data augmentation methods during training. The attack is fully tested on multiple benchmark datasets including MNIST, Cifar10, and ImageNet10, as well as application specific data sets such as Yahoo Adblocker and GTSRB. Two countermeasures, namely Supervised and Unsupervised Poison Sample Detection, are introduced to defend the attack.
","
","OpenReview
DBLP"
"LIRA: Learnable, Imperceptible and Robust Backdoor Attacks","Khoa D Doan, Yingjie Lao, Weijie Zhao, Ping Li","
ICCV","
2021","<a href=""OpenReview () : LIRA: Learnable, Imperceptible and Robust Backdoor Attacks"" target=""_blank"">[https://openaccess.thecvf.com/content/ICCV2021/papers/Doan_LIRA_Learnable_Imperceptible_and_Robust_Backdoor_Attacks_ICCV_2021_paper.pdf]</a>
<a href=""DBLP (2021) : LIRA: Learnable, Imperceptible and Robust Backdoor Attacks"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01175]</a>","<a href=""OpenReview"" target=""_blank"">[https://openaccess.thecvf.com/content/ICCV2021/papers/Doan_LIRA_Learnable_Imperceptible_and_Robust_Backdoor_Attacks_ICCV_2021_paper.pdf]</a>
<a href=""DBLP"" target=""_blank"">[https://doi.org/10.1109/ICCV48922.2021.01175]</a>","Recently, machine learning models have been demonstrated to be vulnerable to backdoor attacks, primarily due to the lack of transparency in black-box models such as deep neural networks. A third-party model can be poisoned such that it works adequately in normal conditions but behaves maliciously on samples with specific trigger patterns. However, the trigger injection function is manually defined in most existing backdoor attack methods, e.g., placing a small patch of pixels on an image or slightly deforming the image before poisoning the model. This results in a two-stage approach with a sub-optimal attack success rate and a lack of complete stealthiness under human inspection. In this paper, we propose a novel and stealthy backdoor attack framework, LIRA, which jointly learns the optimal, stealthy trigger injection function and poisons the model. We formulate such an objective as a non-convex, constrained optimization problem. Under this optimization framework, the trigger generator function will learn to manipulate the input with imperceptible noise to preserve the model performance on the clean data and maximize the attack success rate on the poisoned data. Then, we solve this challenging optimization problem with an efficient, two-stage stochastic optimization procedure. Finally, the proposed attack framework achieves 100% success rates in several benchmark datasets, including MNIST, CIFAR10, GTSRB, and T-ImageNet, while simultaneously bypassing existing backdoor defense methods and human inspection.
","
","OpenReview
DBLP"
BAAAN: Backdoor Attacks Against Auto-encoder and GAN-Based Machine Learning Models,"Ahmed Salem, Yannick Sautter, Michael Backes, Mathias Humbert, Yang Zhang","
arXiv
arXiv","
2020-10-08
2020-10","<a href=""OpenReview () : BAAAN: Backdoor Attacks Against Auto-encoder and GAN-Based Machine Learning Models"" target=""_blank"">[https://openreview.net/pdf/18dba5149a566e40050818492fbee983f949b06e.pdf]</a>
<a href=""arXiv (2020-10-08) : BAAAN: Backdoor Attacks Against Autoencoder and GAN-Based Machine Learning Models"" target=""_blank"">[http://arxiv.org/abs/2010.03007v2]</a>
<a href=""DBLP (2020-10) : BAAAN: Backdoor Attacks Against Autoencoder and GAN-Based Machine Learning Models"" target=""_blank"">[https://arxiv.org/abs/2010.03007]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/18dba5149a566e40050818492fbee983f949b06e.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2010.03007]</a>","The tremendous progress of autoencoders and generative adversarial networks (GANs) has led to their application to multiple critical tasks, such as fraud detection and sanitized data generation. This increasing adoption has fostered the study of security and privacy risks stemming from these models. However, previous works have mainly focused on membership inference attacks. In this work, we explore one of the most severe attacks against machine learning models, namely the backdoor attack, against both autoencoders and GANs. The backdoor attack is a training time attack where the adversary implements a hidden backdoor in the target model that can only be activated by a secret trigger. State-of-the-art backdoor attacks focus on classification-based tasks. We extend the applicability of backdoor attacks to autoencoders and GAN-based models. More concretely, we propose the first backdoor attack against autoencoders and GANs where the adversary can control what the decoded or generated images are when the backdoor is activated. Our results show that the adversary can build a backdoored autoencoder that returns a target output for all backdoored inputs, while behaving perfectly normal on clean inputs. Similarly, for the GANs, our experiments show that the adversary can generate data from a different distribution when the backdoor is activated, while maintaining the same utility when the backdoor is not.
The tremendous progress of autoencoders and generative adversarial networks (GANs) has led to their application to multiple critical tasks, such as fraud detection and sanitized data generation. This increasing adoption has fostered the study of security and privacy risks stemming from these models. However, previous works have mainly focused on membership inference attacks. In this work, we explore one of the most severe attacks against machine learning models, namely the backdoor attack, against both autoencoders and GANs. The backdoor attack is a training time attack where the adversary implements a hidden backdoor in the target model that can only be activated by a secret trigger. State-of-the-art backdoor attacks focus on classification-based tasks. We extend the applicability of backdoor attacks to autoencoders and GAN-based models. More concretely, we propose the first backdoor attack against autoencoders and GANs where the adversary can control what the decoded or generated images are when the backdoor is activated. Our results show that the adversary can build a backdoored autoencoder that returns a target output for all backdoored inputs, while behaving perfectly normal on clean inputs. Similarly, for the GANs, our experiments show that the adversary can generate data from a different distribution when the backdoor is activated, while maintaining the same utility when the backdoor is not.
","

","OpenReview
arXiv
DBLP"
Don't Trigger Me! A Triggerless Backdoor Attack Against Deep Neural Networks,"Ahmed Salem, Michael Backes, Yang Zhang","
arXiv","
2020-10-07","<a href=""OpenReview () : Don't Trigger Me! A Triggerless Backdoor Attack Against Deep Neural Networks"" target=""_blank"">[https://openreview.net/pdf/012a9500ac9524824b3c8c1364dbd45440db346f.pdf]</a>
<a href=""arXiv (2020-10-07) : Don't Trigger Me! A Triggerless Backdoor Attack Against Deep Neural Networks"" target=""_blank"">[http://arxiv.org/abs/2010.03282v1]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/012a9500ac9524824b3c8c1364dbd45440db346f.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>","Backdoor attack against deep neural networks is currently being profoundly investigated due to its severe security consequences. Current state-of-the-art backdoor attacks require the adversary to modify the input, usually by adding a trigger to it, for the target model to activate the backdoor. This added trigger not only increases the difficulty of launching the backdoor attack in the physical world, but also can be easily detected by multiple defense mechanisms. In this paper, we present the first triggerless backdoor attack against deep neural networks, where the adversary does not need to modify the input for triggering the backdoor. Our attack is based on the dropout technique. Concretely, we associate a set of target neurons that are dropped out during model training with the target label. In the prediction phase, the model will output the target label when the target neurons are dropped again, i.e., the backdoor attack is launched. This triggerless feature of our attack makes it practical in the physical world. Extensive experiments show that our triggerless backdoor attack achieves a perfect attack success rate with a negligible damage to the model's utility.
Backdoor attack against deep neural networks is currently being profoundly investigated due to its severe security consequences. Current state-of-the-art backdoor attacks require the adversary to modify the input, usually by adding a trigger to it, for the target model to activate the backdoor. This added trigger not only increases the difficulty of launching the backdoor attack in the physical world, but also can be easily detected by multiple defense mechanisms. In this paper, we present the first triggerless backdoor attack against deep neural networks, where the adversary does not need to modify the input for triggering the backdoor. Our attack is based on the dropout technique. Concretely, we associate a set of target neurons that are dropped out during model training with the target label. In the prediction phase, the model will output the target label when the target neurons are dropped again, i.e., the backdoor attack is launched. This triggerless feature of our attack makes it practical in the physical world. Extensive experiments show that our triggerless backdoor attack achieves a perfect attack success rate with a negligible damage to the model's utility.","
","OpenReview
arXiv"
"Attack of the Tails: Yes, You Really Can Backdoor Federated Learning","Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, Dimitris Papailiopoulos","
arXiv
NeurIPS
arXiv","
2020-07-09
2020
2020-07","<a href=""OpenReview () : Attack of the Tails: Yes, You Really Can Backdoor Federated Learning"" target=""_blank"">[https://arxiv.org/pdf/2007.05084.pdf]</a>
<a href=""arXiv (2020-07-09) : Attack of the Tails: Yes, You Really Can Backdoor Federated Learning"" target=""_blank"">[http://arxiv.org/abs/2007.05084v1]</a>
<a href=""DBLP (2020) : Attack of the Tails: Yes, You Really Can Backdoor Federated Learning"" target=""_blank"">[https://proceedings.neurips.cc/paper/2020/hash/b8ffa41d4e492f0fad2f13e29e1762eb-Abstract.html]</a>
<a href=""DBLP (2020-07) : Attack of the Tails: Yes, You Really Can Backdoor Federated Learning"" target=""_blank"">[https://arxiv.org/abs/2007.05084]</a>","<a href=""OpenReview"" target=""_blank"">[https://arxiv.org/pdf/2007.05084.pdf]</a>
<a href=""arXiv"" target=""_blank"">[]</a>
<a href=""DBLP"" target=""_blank"">[https://proceedings.neurips.cc/paper/2020/hash/b8ffa41d4e492f0fad2f13e29e1762eb-Abstract.html]</a>
<a href=""DBLP"" target=""_blank"">[https://arxiv.org/abs/2007.05084]</a>","Due to its decentralized nature, Federated Learning (FL) lends itself to adversarial attacks in the form of backdoors during training. The goal of a backdoor is to corrupt the performance of the trained model on specific sub-tasks (e.g., by classifying green cars as frogs). A range of FL backdoor attacks have been introduced in the literature, but also methods to defend against them, and it is currently an open question whether FL systems can be tailored to be robust against backdoors. In this work, we provide evidence to the contrary. We first establish that, in the general case, robustness to backdoors implies model robustness to adversarial examples, a major open problem in itself. Furthermore, detecting the presence of a backdoor in a FL model is unlikely assuming first order oracles or polynomial time. We couple our theoretical results with a new family of backdoor attacks, which we refer to as edge-case backdoors. An edge-case backdoor forces a model to misclassify on seemingly easy inputs that are however unlikely to be part of the training, or test data, i.e., they live on the tail of the input distribution. We explain how these edge-case backdoors can lead to unsavory failures and may have serious repercussions on fairness, and exhibit that with careful tuning at the side of the adversary, one can insert them across a range of machine learning tasks (e.g., image classification, OCR, text prediction, sentiment analysis).
Due to its decentralized nature, Federated Learning (FL) lends itself to adversarial attacks in the form of backdoors during training. The goal of a backdoor is to corrupt the performance of the trained model on specific sub-tasks (e.g., by classifying green cars as frogs). A range of FL backdoor attacks have been introduced in the literature, but also methods to defend against them, and it is currently an open question whether FL systems can be tailored to be robust against backdoors. In this work, we provide evidence to the contrary. We first establish that, in the general case, robustness to backdoors implies model robustness to adversarial examples, a major open problem in itself. Furthermore, detecting the presence of a backdoor in a FL model is unlikely assuming first order oracles or polynomial time. We couple our theoretical results with a new family of backdoor attacks, which we refer to as edge-case backdoors. An edge-case backdoor forces a model to misclassify on seemingly easy inputs that are however unlikely to be part of the training, or test data, i.e., they live on the tail of the input distribution. We explain how these edge-case backdoors can lead to unsavory failures and may have serious repercussions on fairness, and exhibit that with careful tuning at the side of the adversary, one can insert them across a range of machine learning tasks (e.g., image classification, OCR, text prediction, sentiment analysis).

","


","OpenReview
arXiv
DBLP
DBLP"
DBA: Distributed Backdoor Attacks against Federated Learning,"Chulin Xie, Keli Huang, Pin-Yu Chen, Bo Li","
ICLR","
2020","<a href=""OpenReview () : DBA: Distributed Backdoor Attacks against Federated Learning"" target=""_blank"">[https://openreview.net/pdf/61dc789b9f12be96506a23ddb7670ac132a51d6d.pdf]</a>
<a href=""DBLP (2020) : DBA: Distributed Backdoor Attacks against Federated Learning"" target=""_blank"">[https://openreview.net/forum?id=rkgyS0VFvr]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/61dc789b9f12be96506a23ddb7670ac132a51d6d.pdf]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/forum?id=rkgyS0VFvr]</a>","Backdoor attacks aim to manipulate a subset of training data by injecting adversarial triggers such that machine learning models trained on the tampered dataset will make arbitrarily (targeted) incorrect prediction on the testset with the same trigger embedded. While federated learning (FL) is capable of aggregating information provided by different parties for training a better model, its distributed learning methodology and inherently heterogeneous data distribution across parties may bring new vulnerabilities. In addition to recent centralized backdoor attacks on FL where each party embeds the same global trigger during training, we propose the distributed backdoor attack (DBA) --- a novel threat assessment framework developed by fully exploiting the distributed nature of FL. DBA decomposes a global trigger pattern into separate local patterns and embed them into the training set of different adversarial parties respectively. Compared to standard centralized backdoors, we show that DBA is substantially more persistent and stealthy against FL on diverse datasets such as finance and image data. We conduct extensive experiments to show that the attack success rate of DBA is significantly higher than centralized backdoors under different settings. Moreover, we find that distributed attacks are indeed more insidious, as DBA can evade two state-of-the-art robust FL algorithms against centralized backdoors. We also provide explanations for the effectiveness of DBA via feature visual interpretation and feature importance ranking. To further explore the properties of DBA, we test the attack performance by varying different trigger factors, including local trigger variations (size, gap, and location), scaling factor in FL, data distribution, and poison ratio and interval. Our proposed DBA and thorough evaluation results shed lights on characterizing the robustness of FL.
","
","OpenReview
DBLP"
Robust anomaly detection and backdoor attack detection via differential privacy,"Min Du, Ruoxi Jia, Dawn Song","
ICLR","
2020","<a href=""OpenReview () : Robust anomaly detection and backdoor attack detection via differential privacy"" target=""_blank"">[https://openreview.net/pdf/25a0f08ab66eaa891efe6bde148812b559ab3700.pdf]</a>
<a href=""DBLP (2020) : Robust anomaly detection and backdoor attack detection via differential privacy"" target=""_blank"">[https://openreview.net/forum?id=SJx0q1rtvS]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/25a0f08ab66eaa891efe6bde148812b559ab3700.pdf]</a>
<a href=""DBLP"" target=""_blank"">[https://openreview.net/forum?id=SJx0q1rtvS]</a>","Outlier detection and novelty detection are two important topics for anomaly detection. Suppose the majority of a dataset are drawn from a certain distribution, outlier detection and novelty detection both aim to detect data samples that do not fit the distribution. Outliers refer to data samples within this dataset, while novelties refer to new samples. In the meantime, backdoor poisoning attacks for machine learning models are achieved through injecting poisoning samples into the training dataset, which could be regarded as “outliers” that are intentionally added by attackers. Differential privacy has been proposed to avoid leaking any individual’s information, when aggregated analysis is performed on a given dataset. It is typically achieved by adding random noise, either directly to the input dataset, or to intermediate results of the aggregation mechanism. In this paper, we demonstrate that applying differential privacy could improve the utility of outlier detection and novelty detection, with an extension to detect poisoning samples in backdoor attacks. We first present a theoretical analysis on how differential privacy helps with the detection, and then conduct extensive experiments to validate the effectiveness of differential privacy in improving outlier detection, novelty detection, and backdoor attack detection.
","
","OpenReview
DBLP"
DP-InstaHide: Data Augmentations Provably Enhance Guarantees Against Dataset Manipulations,"Eitan Borgnia, Jonas Geiping, Valeriia Cherepanova, Liam H Fowl, Arjun Gupta, Amin Ghiasi, Furong Huang, Micah Goldblum, Tom Goldstein","OpenReview
OpenReview","
","<a href=""OpenReview () : DP-InstaHide: Data Augmentations Provably Enhance Guarantees Against Dataset Manipulations"" target=""_blank"">[https://openreview.net/pdf/36a8fa7f7e46829be25c30f5a0e0f57e87b903b0.pdf]</a>
<a href=""OpenReview () : DP-InstaHide: Data Augmentations Provably Enhance Guarantees Against Dataset Manipulations"" target=""_blank"">[https://openreview.net/pdf/eab859351425b5d5bc29e562ccfc1e05bc1eed0b.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/36a8fa7f7e46829be25c30f5a0e0f57e87b903b0.pdf]</a>
<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/eab859351425b5d5bc29e562ccfc1e05bc1eed0b.pdf]</a>","Data poisoning and backdoor attacks manipulate training data to induce security breaches in a victim model. These attacks can be provably deflected using differentially private (DP) training methods, although this comes with a sharp decrease in model performance. The InstaHide method has recently been proposed as an alternative to DP training that leverages supposed privacy properties of the mixup augmentation, although without rigorous guarantees. In this paper, we rigorously show that k-way mixup provably yields at least k times stronger DP guarantees than a naive DP mechanism, and we observe that this enhanced privacy guarantee is a strong foundation for building defenses against poisoning.
Data poisoning and backdoor attacks manipulate training data to induce security breaches in a victim model. These attacks can be provably deflected using differentially private (DP) training methods, although this comes with a sharp decrease in model performance. The InstaHide method has recently been proposed as an alternative to DP training that leverages supposed privacy properties of the mixup augmentation, although without rigorous guarantees. In this paper, we rigorously show that $k$-way mixup provably yields at least $k$ times stronger DP guarantees than a naive DP mechanism, and we observe that this enhanced privacy guarantee is a strong foundation for building defenses against poisoning.","
","OpenReview
OpenReview"
Defense Against Textual Backdoor Attacks with Token Substitution,Anonymous,"
","
","<a href=""OpenReview () : Defense Against Textual Backdoor Attacks with Token Substitution"" target=""_blank"">[https://openreview.net/pdf/a3c3823a309bb99c32b90035baaebbef6938bf8b.pdf]</a>
<a href=""OpenReview () : Defense Against Textual Backdoor Attacks with Token Substitution"" target=""_blank"">[https://openreview.net/pdf/ea482696b4261bae0a9d0ae9170140b5da572433.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/a3c3823a309bb99c32b90035baaebbef6938bf8b.pdf]</a>
<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/ea482696b4261bae0a9d0ae9170140b5da572433.pdf]</a>","Backdoor attack is a type of malicious threat to deep neural networks. The attacker embeds a backdoor into the model during the training process by poisoning the data with triggers. The victim model behaves normally on clean data, but predicts inputs with triggers as the trigger-associated class. Backdoor attacks have been investigated in both computer vision and natural language processing (NLP) fields. However, the study of defense methods against textual backdoor attacks in NLP is insufficient. To our best knowledge, there is no method available to defend against syntactic backdoor attacks. In this paper, we propose a novel defense method against textual backdoor attacks, including syntactic backdoor attacks. Experiments show the effectiveness of our method against both insertion-based and syntactic backdoor attacks on three benchmark datasets. We will release the code once the paper is published.
Backdoor attack is a type of malicious threat to deep neural networks. The attacker embeds a backdoor into the model during the training process by poisoning the data with triggers. The victim model behaves normally on clean data, but predicts inputs with triggers as the trigger-associated class. Backdoor attacks have been investigated in both computer vision and natural language processing (NLP) fields. However, the study of defense methods against textual backdoor attacks in NLP is insufficient. To our best knowledge, there is no method available to defend against syntactic backdoor attacks. In this paper, we propose a novel defense method against textual backdoor attacks, including syntactic backdoor attacks. Experiments show the effectiveness of our method against two state-of-the-art textual backdoor attacks on three benchmark datasets. We will release the code once the paper is published.","
","OpenReview
OpenReview"
Exploring the Universal Vulnerability of Prompt-based Learning Paradigm,Anonymous,"
","
","<a href=""OpenReview () : Exploring the Universal Vulnerability of Prompt-based Learning Paradigm"" target=""_blank"">[https://openreview.net/pdf/7a6a313717022caaf178fbc4540f5a031a141e1f.pdf]</a>
<a href=""OpenReview () : Exploring the Universal Vulnerability of Prompt-based Learning Paradigm"" target=""_blank"">[https://openreview.net/pdf/e918831af081f0f241bdfd079dba9e92afb32a2f.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/7a6a313717022caaf178fbc4540f5a031a141e1f.pdf]</a>
<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/e918831af081f0f241bdfd079dba9e92afb32a2f.pdf]</a>","Prompt-based learning paradigm bridges the gap between pre-training and fine-tuning, and works effectively under the few-shot setting. However, we find that this learning paradigm inherits the vulnerability from the pre-training stage, where model predictions can be misled by inserting certain triggers into the text. In this paper, we explore this universal vulnerability by either injecting backdoor triggers or searching for adversarial triggers on pre-trained language models using only plain text. In both scenarios, we demonstrate that our triggers can totally control or severely decrease the performance of prompt-based models fine-tuned on arbitrary downstream tasks, reflecting the universal vulnerability of the prompt-based learning paradigm. Further experiments show that adversarial triggers have good transferability among language models. We also find conventional fine-tuning models are not vulnerable to adversarial triggers constructed from pre-trained language models. We conclude by proposing a potential solution to mitigate our attack methods. Code and data are publicly available.
Prompt-based learning paradigm bridges the gap between pre-training and fine-tuning, and works effectively under the few-shot setting. However, we find that this learning paradigm inherits the vulnerability from the pre-training stage, where model predictions can be misled by inserting certain triggers into the text. In this paper, we explore this universal vulnerability by either injecting \textit{backdoor triggers} or searching for \textit{adversarial triggers} on pre-trained language models using only plain text. In both scenarios, we demonstrate that our triggers can totally control or severely decrease the performance of prompt-based models fine-tuned on arbitrary downstream tasks, reflecting the universal vulnerability of the prompt-based learning paradigm. Further experiments show that adversarial triggers have good transferability among language models. We also find conventional fine-tuning models are not vulnerable to adversarial triggers constructed from pre-trained language models. We conclude by proposing a potential solution to mitigate our attack methods. All the code and data will be made public.","
","OpenReview
OpenReview"
Are vision transformers more robust than CNNs for Backdoor attacks?,"Akshayvarun Subramanya, Aniruddha Saha, Soroush Abbasi Koohpayegani, Ajinkya Tejankar, Hamed Pirsiavash",OpenReview,,"<a href=""OpenReview () : Are vision transformers more robust than CNNs for Backdoor attacks?"" target=""_blank"">[https://openreview.net/pdf/769d621c491a52ad7e98f7efa69949a09d56f6af.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/769d621c491a52ad7e98f7efa69949a09d56f6af.pdf]</a>","Transformer architectures are based on a self-attention mechanism that processes images as a sequence of patches. As their design is quite different compared to CNNs, it is interesting to study if transformers are vulnerable to backdoor attacks and how different transformer architectures affect attack success rates. Backdoor attacks happen when an attacker poisons a small part of the training images with a specific trigger or backdoor which will be activated later. The model performance is good on clean test images, but the attacker can manipulate the decision of the model by showing the trigger on an image at test time. In this paper, we perform a comparative study of state-of-the-art architectures through the lens of backdoor robustness, specifically how attention mechanisms affect robustness. We show that the popular vision transformer architecture (ViT) is the least robust architecture and ResMLP, which belongs to a class called Feed Forward Networks (FFN), is the most robust one to backdoor attacks among state-of-the-art architectures. We also find an intriguing difference between transformers and CNNs – interpretation algorithms effectively highlight the trigger on test images for transformers but not for CNNs. Based on this observation, we find that a test-time image blocking defense reduces the attack success rate by a large margin for transformers. We also show that such blocking mechanisms can be incorporated during the training process to improve robustness even further. We believe our experimental findings will encourage the community to understand the building block components in developing novel architectures robust to backdoor attacks.",,OpenReview
Attack-Resistant Federated Learning with Residual-based Reweighting,"Shuhao Fu, Chulin Xie, Bo Li, Qifeng Chen",,,"<a href=""OpenReview () : Attack-Resistant Federated Learning with Residual-based Reweighting"" target=""_blank"">[https://openreview.net/pdf/1ea807b624ecc563e3b617f0948502afeee0ec8c.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/1ea807b624ecc563e3b617f0948502afeee0ec8c.pdf]</a>","Federated learning has a variety of applications in multiple domains by utilizing private training data stored on different devices. However, the aggregation process in federated learning is highly vulnerable to adversarial attacks so that the global model may behave abnormally under attacks. To tackle this challenge, we present a novel aggregation algorithm with residual-based reweighting to defend federated learning. Our aggregation algorithm combines repeated median regression with the reweighting scheme in iteratively reweighted least squares. Our experiments show that our aggression algorithm outperforms other alternative algorithms in the presence of label-flipping, backdoor, and Gaussian noise attacks. We also provide theoretical guarantees for our aggregation algorithm.",,OpenReview
BAFFLE: TOWARDS RESOLVING FEDERATED LEARNING’S DILEMMA - THWARTING BACKDOOR AND INFERENCE ATTACKS,"Thien Duc Nguyen, Phillip Rieger, Hossein Yalame, Helen Möllering, Hossein Fereidooni, Samuel Marchal, Markus Miettinen, Azalia Mirhoseini, Ahmad-Reza Sadeghi, Thomas Schneider, Shaza Zeitouni",,,"<a href=""OpenReview () : BAFFLE: TOWARDS RESOLVING FEDERATED LEARNING’S DILEMMA - THWARTING BACKDOOR AND INFERENCE ATTACKS"" target=""_blank"">[https://openreview.net/pdf/9a628e0b39be7ca1cceb483ece7a4c21ab42c640.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/9a628e0b39be7ca1cceb483ece7a4c21ab42c640.pdf]</a>","Recently, federated learning (FL) has been subject to both security and privacy attacks posing a dilemmatic challenge on the underlying algorithmic designs: On the one hand, FL is shown to be vulnerable to backdoor attacks that stealthily manipulate the global model output using malicious model updates, and on the other hand, FL is shown vulnerable to inference attacks by a malicious aggregator inferring information about clients’ data from their model updates. Unfortunately, existing defenses against these attacks are insufficient and mitigating both attacks at the same time is highly challenging, because while defeating backdoor attacks requires the analysis of model updates, protection against inference attacks prohibits access to the model updates to avoid information leakage. In this work, we introduce BAFFLE, a novel in-depth defense for FL that tackles this challenge. To mitigate backdoor attacks, it applies a multilayered defense by using a Model Filtering layer to detect and reject malicious model updates and a Poison Elimination layer to eliminate any effect of a remaining undetected weak manipulation. To impede inference attacks, we build private BAFFLE that securely evaluates the BAFFLE algorithm under encryption using sophisticated secure computation techniques. We extensively evaluate BAFFLE against state-of-the-art backdoor attacks on several datasets and applications, including image classification, word prediction, and IoT intrusion. We show that BAFFLE can entirely remove backdoors with a negligible effect on accuracy and that private BAFFLE is practical.",,OpenReview
"Backdoor Attacks and Defenses in Federated Learning: Survey, Challenges and Future Research Directions",Tuan Minh Nguyen,,,"<a href=""OpenReview () : Backdoor Attacks and Defenses in Federated Learning: Survey, Challenges and Future Research Directions"" target=""_blank"">[https://arxiv.org/pdf/2303.02213.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://arxiv.org/pdf/2303.02213.pdf]</a>","Federated learning (FL) is a machine learning (ML) approach that allows the use of distributed data without compromising personal privacy. However, the heterogeneous distribution of data among clients in FL can make it difficult for the orchestration server to validate the integrity of local model updates, making FL vulnerable to various threats, including backdoor attacks. Backdoor attacks involve the insertion of malicious functionality into a targeted model through poisoned updates from malicious clients. These attacks can cause the global model to misbehave on specific inputs while appearing normal in other cases. Backdoor attacks have received significant attention in the literature due to their potential to impact real-world deep learning applications. However, they have not been thoroughly studied in the context of FL. In this survey, we provide a comprehensive survey of current backdoor attack strategies and defenses in FL, including a comprehensive analysis of different approaches. We also discuss the challenges and potential future directions for attacks and defenses in the context of FL.",,OpenReview
Backdoor Attacks on Multilingual Machine Translation,Anonymous,,,"<a href=""OpenReview () : Backdoor Attacks on Multilingual Machine Translation"" target=""_blank"">[https://openreview.net/pdf/22729d5cbee3df7d426762c40bbe68861e26e326.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/22729d5cbee3df7d426762c40bbe68861e26e326.pdf]</a>","While multilingual machine translation (MNMT) systems hold substantial promise, they also have security vulnerabilities. Our research highlights that MNMT systems can be susceptible to a particularly devious style of backdoor attack, whereby an attacker can inject poisoned data into a low-resource language pair in order to malicious translations in a high-resource language. Our experimental results reveal that injecting less than 0.01% poisoned data into a low-resource language pair can achieve an average 20% attack success rate in attacking high-resource language pairs. This type of attack is of particular concern, given the larger attack surface of languages inherent to low-resource settings. Our aim is to bring attention to these vulnerabilities within MNMT systems with the hope of encouraging the community to address the security concerns in machine translation, especially in the context of low-resource languages.",,OpenReview
Backdoor DNFs,Stefan Szeider,,,"<a href=""OpenReview () : Backdoor DNFs"" target=""_blank"">[]</a>","<a href=""OpenReview"" target=""_blank"">[]</a>","We introduce backdoor DNFs, as a tool to measure the theoretical hardness of CNF formulas. Like backdoor sets, backdoor DNFs are defined relative to a tractable class of CNF formulas. Each con- junctive term of a backdoor DNF defines a partial assignment which moves the input CNF formula into the base class. Backdoor DNFs are more expressive and potentially smaller than its prede- cessors backdoor sets and backdoor trees. We es- tablish the fixed-parameter tractability of the back- door DNF detection problem. Our result holds for the fundamental base classes Horn and 2CNF, and their combination. We complement our theoretical findings by an empirical study. Our experiments show that backdoor DNFs provide a significant im- provement over their predecessors.",,OpenReview
Backdoor Mitigation by Correcting Activation Distribution Alteration,"Xi Li, Zhen Xiang, David Miller, George Kesidis",,,"<a href=""OpenReview () : Backdoor Mitigation by Correcting Activation Distribution Alteration"" target=""_blank"">[https://openreview.net/forum?id=Yc9tld-ENbf&noteId=Sw-sjLyA9fx]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/forum?id=Yc9tld-ENbf&noteId=Sw-sjLyA9fx]</a>","Backdoor (Trojan) attacks are an important type of adversarial exploit against deep neural networks (DNNs), wherein a test instance is (mis)classified to the attacker's target class whenever a backdoor trigger is present. In this paper, we reveal and analyze an important property of backdoor attacks: a successful attack causes an alteration in the distribution of internal layer activations for backdoor-trigger instances, compared to that for clean instances. Even more importantly, we find that instances with the backdoor trigger will be correctly classified to their original source classes if this distribution alteration is reversed. Based on our observations, we propose an efficient and effective method that achieves post-training backdoor mitigation by correcting the distribution alteration using reverse-engineered triggers. Notably, our method does not change any trainable parameters of the DNN, but achieves generally better mitigation performance than existing methods that do require intensive DNN parameter tuning. It also efficiently detects test instances with the trigger, which may help to catch adversarial entities.",,OpenReview
Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness,"Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, Xue Lin",,,"<a href=""OpenReview () : Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness"" target=""_blank"">[https://openreview.net/pdf/fb8082dd5515e11c88f59b0f4911266f1891fb61.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/fb8082dd5515e11c88f59b0f4911266f1891fb61.pdf]</a>","Mode connectivity provides novel geometric insights on analyzing loss landscapes and enables building high-accuracy pathways between well-trained neural networks. In this work, we propose to employ mode connectivity in loss landscapes to study the adversarial robustness of deep neural networks, and provide novel methods for improving this robustness. Our experiments cover various types of adversarial attacks applied to different network architectures and datasets. When network models are tampered with backdoor or error-injection attacks, our results demonstrate that the path connection learned using limited amount of bonafide data can effectively mitigate adversarial effects while maintaining the original accuracy on clean data. Therefore, mode connectivity provides users with the power to repair backdoored or error-injected models. We also use mode connectivity to investigate the loss landscapes of regular and robust models against evasion attacks. Experiments show that there exists a barrier in adversarial robustness loss on the path connecting regular and adversarially-trained models. A high correlation is observed between the adversarial robustness loss and the largest eigenvalue of the input Hessian matrix, for which theoretical justifications are provided. Our results suggest that mode connectivity offers a holistic tool and practical means for evaluating and improving adversarial robustness.",,OpenReview
CBAs: Character-level Backdoor Attacks against Chinese Language Models,Anonymous,,,"<a href=""OpenReview () : CBAs: Character-level Backdoor Attacks against Chinese Language Models"" target=""_blank"">[https://openreview.net/pdf/4111314dc03e4f62952815dc19a57e3fbdfd41a1.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/4111314dc03e4f62952815dc19a57e3fbdfd41a1.pdf]</a>","The language models (LMs) aim to assist computers in various domains to provide natural and efficient language interaction and text processing capabilities. However, recent studies have shown that LMs are highly vulnerable to malicious backdoor attacks, where triggers could be injected into the models to guide them to exhibit the expected behavior of the attackers. Unfortunately, existing researches on backdoor attacks have mainly focused on English LMs, but paid less attention to the Chinese LMs. Moreover, these extant backdoor attacks don’t work well against Chinese LMs. In this paper, we disclose the limitations of English backdoor attacks against Chinese LMs, and propose the character-level backdoor attacks (CBAs) against the Chinese LMs. Specifically, we first design three Chinese trigger generation strategies to ensure the backdoor being effectively triggered while improving the effectiveness of the backdoor attacks. Then, based on the attacker's capabilities of accessing the training dataset, we develop trigger injection mechanisms with either the target label similarity or the masked language model, which select the most influential position and insert the trigger to maximize the stealth of backdoor attacks. Extensive experiments on three major NLP tasks on four LMs demonstrate the effectiveness and stealthiness of our method.",,OpenReview
Can adversarial weight perturbations inject neural backdoors,Adarsh Kumar,,,"<a href=""OpenReview () : Can adversarial weight perturbations inject neural backdoors"" target=""_blank"">[]</a>","<a href=""OpenReview"" target=""_blank"">[]</a>","Adversarial machine learning has exposed several security hazards of neural models. Thus far, the concept of an ""adversarial perturbation"" has exclusively been used with reference to the input space referring to a small, imperceptible change which can cause a ML model to err. In this work we extend the idea of ""adversarial perturbations"" to the space of model weights, specifically to inject backdoors in trained DNNs, which exposes a security risk of publicly available trained models. Here, injecting a backdoor refers to obtaining a desired outcome from the model when a trigger pattern is added to the input, while retaining the original predictions on a non-triggered input. From the perspective of an adversary, we characterize these adversarial perturbations to be constrained within an ℓ∞ norm around the original model weights. We introduce adversarial perturbations in model weights using a composite loss on the …",,OpenReview
Certified Watermarks for Neural Networks,"Arpit Amit Bansal, Ping-yeh Chiang, Michael Curry, Hossein Souri, Rama Chellappa, John P Dickerson, Rajiv Jain, Tom Goldstein",,,"<a href=""OpenReview () : Certified Watermarks for Neural Networks"" target=""_blank"">[https://openreview.net/pdf/7ccb1fcac9aa5cf20a3356589c284c72d18fdfe8.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/7ccb1fcac9aa5cf20a3356589c284c72d18fdfe8.pdf]</a>","Watermarking is a commonly used strategy to protect creators' rights to digital images, videos and audio. Recently, watermarking methods have been extended to deep learning models -- in principle, the watermark should be preserved when an adversary tries to copy the model. However, in practice, watermarks can often be removed by an intelligent adversary. Several papers have proposed watermarking methods that claim to be empirically resistant to different types of removal attacks, but these new techniques often fail in the face of new or better-tuned adversaries. In this paper, we propose the first certifiable watermarking method. Using the randomized smoothing technique proposed in Chiang et al., we show that our watermark is guaranteed to be unremovable unless the model parameters are changed by more than a certain $\ell_2$ threshold. In addition to being certifiable, our watermark is also empirically more robust compared to previous watermarking methods.",,OpenReview
Characterizing convolutional neural networks with one-pixel signature,"Shanjiaoyang Huang, Weiqi Peng, Zhuowen Tu",,,"<a href=""OpenReview () : Characterizing convolutional neural networks with one-pixel signature"" target=""_blank"">[https://openreview.net/pdf/02454a76ff420d6bbda4e733bf172561c25c7e44.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/02454a76ff420d6bbda4e733bf172561c25c7e44.pdf]</a>","We propose a new representation, one-pixel signature, that can be used to reveal the characteristics of the convolution neural networks (CNNs). Here, each CNN classifier is associated with a signature that is created by generating, pixel-by-pixel, an adversarial value that is the result of the largest change to the class prediction. The one-pixel signature is agnostic to the design choices of CNN architectures such as type, depth, activation function, and how they were trained. It can be computed efficiently for a black-box classifier without accessing the network parameters. Classic networks such as LetNet, VGG, AlexNet, and ResNet demonstrate different characteristics in their signature images. For application, we focus on the classifier backdoor detection problem where a CNN classifier has been maliciously inserted with an unknown Trojan. We show the effectiveness of the one-pixel signature in detecting backdoored CNN. Our proposed one-pixel signature representation is general and it can be applied in problems where discriminative classifiers, particularly neural network based, are to be characterized.",,OpenReview
Class-wise Visual Explanations for Deep Neural Networks,"Minhao Cheng, Zeyu Qin",OpenReview,,"<a href=""OpenReview () : Class-wise Visual Explanations for Deep Neural Networks"" target=""_blank"">[https://openreview.net/pdf/c14c2c1a813b067760015531ed6b79febf98a7bc.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/c14c2c1a813b067760015531ed6b79febf98a7bc.pdf]</a>","Many explainable AI (XAI) methods have been proposed to interpret neural net- work’s decisions on why they predict what they predict locally through gradient information. Yet, existing works mainly for local explanation lack global knowledge to show class-wise explanations in the whole training procedure. To fill this gap, we proposed to visualize global explanation in the input space for every class learned in the training procedure. Specifically, our solution finds a representation set that could demonstrate the learned knowledge for each class. To achieve this goal, we optimize the representation set by imitating the model training procedure over the full dataset. Experimental results show that our method could generate class-wise explanations with high quality in a series of image classification datasets. Using our global explanation, we further analyze the model knowledge in different training procedures, including adversarial training and noisy label learning. Moreover, we illustrate that the generated explanations could lend insights into diagnosing model failures, such as revealing triggers in a backdoored model.",,OpenReview
Clean-Label Backdoor Attacks,"Alexander Turner, Dimitris Tsipras, Aleksander Madry",,,"<a href=""OpenReview () : Clean-Label Backdoor Attacks"" target=""_blank"">[https://openreview.net/pdf/8d05f33ab5ec0b3fa39117b269adfa15a0b98ddd.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/8d05f33ab5ec0b3fa39117b269adfa15a0b98ddd.pdf]</a>","Deep neural networks have been recently demonstrated to be vulnerable to backdoor attacks. Specifically, by altering a small set of training examples, an adversary is able to install a backdoor that can be used during inference to fully control the model’s behavior. While the attack is very powerful, it crucially relies on the adversary being able to introduce arbitrary, often clearly mislabeled, inputs to the training set and can thus be detected even by fairly rudimentary data filtering. In this paper, we introduce a new approach to executing backdoor attacks, utilizing adversarial examples and GAN-generated data. The key feature is that the resulting poisoned inputs appear to be consistent with their label and thus seem benign even upon human inspection.",,OpenReview
Context-aware Information-theoretic Causal De-biasing for Interactive Sequence Labeling,"Junda Wu, Rui Wang, Tong Yu, Ruiyi Zhang, Handong Zhao, Shuai Li, Ricardo Henao, Ani Nenkova",,,"<a href=""OpenReview () : Context-aware Information-theoretic Causal De-biasing for Interactive Sequence Labeling"" target=""_blank"">[https://aclanthology.org/2022.findings-emnlp.251.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://aclanthology.org/2022.findings-emnlp.251.pdf]</a>","Supervised training of existing deep learning models for sequence labeling relies on large scale labeled datasets. Such datasets are generally created with crowd-source labeling. However, crowd-source labeling for tasks of sequence labeling can be expensive and time-consuming. Further, crowd-source labeling by external annotators may not be appropriate for data that contains user private information. Considering the above limitations of crowd-source labeling, we study interactive sequence labeling that allows training directly with the user feedback, which alleviates the annotation cost and maintains the user privacy. We identify two bias, namely, context bias and feedback bias, by formulating interactive sequence labeling via a Structural Causal Model (SCM). To alleviate the context and feedback bias based on the SCM, we identify the frequent context tokens as confounders in the backdoor adjustment and further propose an entropy-based modulation that is inspired by information theory. entities more sample-efficiently. With extensive experiments, we validate that our approach can effectively alleviate the biases and our models can be efficiently learnt with the user feedback.",,OpenReview
Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning,Feng Yan,,,"<a href=""OpenReview () : Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/17291/17098]</a>","<a href=""OpenReview"" target=""_blank"">[https://ojs.aaai.org/index.php/AAAI/article/view/17291/17098]</a>","Data heterogeneity has been identified as one of the key features in federated learning but often overlooked in the lens of robustness to adversarial attacks. This paper focuses on characterizing and understanding its impact on backdooring attacks in federated learning through comprehensive experiments using synthetic and the LEAF benchmarks. The initial impression driven by our experimental results suggests that data heterogeneity is the dominant factor in the effectiveness of attacks and it may be a redemption for defending against backdooring as it makes the attack less efficient, more challenging to design effective attack strategies, and the attack result also becomes less predictable. However, with further investigations, we found data heterogeneity is more of a curse than a redemption as the attack effectiveness can be significantly boosted by simply adjusting the client-side backdooring timing. More importantly, data heterogeneity may result in overfitting at the local training of benign clients, which can be utilized by attackers to disguise themselves and fool skewed-feature based defenses. In addition, effective attack strategies can be made by adjusting attack data distribution. Finally, we discuss the potential directions of defending the curses brought by data heterogeneity. The results and lessons learned from our extensive experiments and analysis offer new insights for designing robust federated learning methods and systems.",,OpenReview
DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference,Anonymous,,,"<a href=""OpenReview () : DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference"" target=""_blank"">[https://openreview.net/pdf/8fe95791f155f5411fed66b91542b726cb2644da.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/8fe95791f155f5411fed66b91542b726cb2644da.pdf]</a>","Though notable progress has been made, neural-based aspect-based sentiment analysis (ABSA) models are prone to learn spurious correlations from annotation biases, resulting in poor robustness on adversarial data transformations. Among the debiasing solutions, causal inference-based methods have attracted much research attention, which can be mainly categorized into causal intervention methods and counterfactual reasoning methods. However, most of the present debiasing methods focus on single-variable causal inference, which is not suitable for ABSA with two input variables (the target aspect and the review). In this paper, we propose a novel framework based on multi-variable causal inference for debiasing ABSA. In this framework, different types of biases are tackled based on different causal intervention methods. For the review branch, the bias is modeled as indirect confounding from context, where backdoor adjustment intervention is employed for debiasing. For the aspect branch, the bias is described as a direct correlation with labels, where counterfactual reasoning is adopted for debiasing. Extensive experiments demonstrate the effectiveness of the proposed method compared to various baselines on the two widely used real-world aspect robustness test set datasets.",,OpenReview
Delve into the Layer Choice of BP-based Attribution Explanations,"Guanhua Zheng, Jitao Sang, Duo Zhang, Haonan Wang, Changsheng Xu",OpenReview,,"<a href=""OpenReview () : Delve into the Layer Choice of BP-based Attribution Explanations"" target=""_blank"">[https://openreview.net/pdf/ede9e7279b3b7b6aa5fadfc6eb9089af295e9d09.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/ede9e7279b3b7b6aa5fadfc6eb9089af295e9d09.pdf]</a>","Many issues in attribution methods have been recognized to be related to the choice of target layers, such as class insensitivity in earlier layers and low resolution in deeper layers. However, as the ground truth of the decision process is unknown, the effect of layer selection has not been well-studied. In this paper, we first employ backdoor attacks to control the decision-making process of the model and quantify the influence of layer choice on class sensitivity, fine-grained localization, and completeness. We obtain three observations: (1) We find that energy distributions of the bottom layer attribution are class-sensitive, and the class-insensitive visualizations come from the presence of a large number of class-insensitive low-score pixels. (2) The choice of target layers determines the completeness and the granularity of attributions. (3) We find that single-layer attributions cannot perform well both on the LeRF and MoRF reliability evaluations. To address these issues, we propose TIF (Threshold Interception and Fusion), a technique to combine the attribution results of all layers. Qualitative and quantitative experiments show that the proposed solution is visually sharper and more tightly constrained to the object region than other methods, addresses all issues, and outperforms mainstream methods in reliability and localization evaluations.",,OpenReview
Design and evaluation of a multi-domain trojan detection method on deep neural networks,"Yansong Gao, Yeonjae Kim, Bao Gia Doan, Zhi Zhang, Gongxuan Zhang, Surya Nepal, Damith Ranasinghe, Hyoungshick Kim",,,"<a href=""OpenReview () : Design and evaluation of a multi-domain trojan detection method on deep neural networks"" target=""_blank"">[https://ieeexplore.ieee.org/document/9343758]</a>","<a href=""OpenReview"" target=""_blank"">[https://ieeexplore.ieee.org/document/9343758]</a>","Trojan attacks on deep neural networks (DNNs) exploit a backdoor embedded in a DNN model to hijack any input with an attacker’s chosen signature trigger. All emerging defence mechanisms are only validated on vision domain tasks (e.g., image classification) on 2D Convolutional Neural Network (CNN) model architectures, whether a defence mechanism is general across vision, text, and audio domain tasks remains unclear. This work corroborates a run-time Trojan detection method exploiting STRong Intentional Perturbation of inputs, is a multi-domain Trojan detection defence across Vision, Text and Audio domains—thus termed as STRIP-ViTA. Specifically, STRIP-ViTA is the first confirmed Trojan detection method that is demonstratively independent of both the task domain and model architectures. We have extensively evaluated the performance of STRIP-ViTA over: i) CIFAR10 and GTSRB datasets using 2D CNNs, and a public third party Trojaned model for vision tasks, ii) IMDB and consumer complaint datasets using both LSTM and 1D CNNs for text tasks, and speech command dataset using both 1D CNNs and 2D CNNs for audio tasks. Experimental results based on 28 tested Trojaned models demonstrate that STRIP-ViTA performs well across all nine architectures and five datasets. In general, STRIP-ViTA can effectively detect Trojan inputs with small false acceptance rate (FAR) with an acceptable preset false rejection rate (FRR). In particular, for vision tasks, we can always achieve a 0% FRR and FAR. By setting FRR to be 3%, average FAR of 1.1% and 3.55% are achieved for text and audio tasks, respectively. Moreover, we have evaluated and shown the effectiveness of STRIP-ViTA against a number of advanced backdoor attacks whilst other state-of-the-art methods lose effectiveness in front of one or all of these advanced backdoor attacks.",,OpenReview
Detecting Backdoor Attacks via Layer-wise Feature Analysis,"Najeeb Moharram Jebreel, Yiming Li, Josep Domingo-Ferrer, Shu-Tao Xia",OpenReview,,"<a href=""OpenReview () : Detecting Backdoor Attacks via Layer-wise Feature Analysis"" target=""_blank"">[https://openreview.net/pdf/fceec6cedb5cbcb82297f2e5a861cdbf21aab7af.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/fceec6cedb5cbcb82297f2e5a861cdbf21aab7af.pdf]</a>","Training well-performing deep neural networks (DNNs) usually requires massive training data and computational resources, which might not be affordable for some users. For this reason, users may prefer to outsource their training process to a third party or directly exploit publicly available pre-trained models. Unfortunately, doing so opens the possibility of a new dangerous training-time attack (dubbed backdoor attack) against DNNs. Currently, most of the existing backdoor detectors filter poisoned samples based on the latent feature representations generated by convolutional layers. In this paper, we first conduct a layer-wise feature analysis of poisoned and benign samples from the target class. We find out that the feature difference between benign and poisoned samples tends to reach the maximum at a critical layer, which is not always the one typically used in existing defenses, namely the layer before fully-connected layers. In particular, we can locate this critical layer easily based on the behaviors of benign samples. Based on this finding, we propose a simple yet effective method to filter poisoned samples by analyzing the feature differences between suspicious and benign samples at the critical layer. We conduct extensive experiments on two benchmark datasets, which confirm the effectiveness of our backdoor detection.",,OpenReview
Does Adversarial Robustness Really Imply Backdoor Vulnerability?,"Yinghua Gao, Dongxian Wu, Jingfeng Zhang, Shu-Tao Xia, Gang Niu, Masashi Sugiyama",OpenReview,,"<a href=""OpenReview () : Does Adversarial Robustness Really Imply Backdoor Vulnerability?"" target=""_blank"">[https://openreview.net/pdf/149e7de65f8fe1c4739d000b609d9b3a4db3148b.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/149e7de65f8fe1c4739d000b609d9b3a4db3148b.pdf]</a>","Recent research has revealed a trade-off between the robustness against adversarial attacks and backdoor attacks. Specifically, with the increasing adversarial robustness obtained through adversarial training, the model easily memorizes the malicious behaviors embedded in poisoned data and becomes more vulnerable to backdoor attacks. Meanwhile, some studies have demonstrated that adversarial training can somewhat mitigate the effect of poisoned data during training. This paper revisits the trade-off and raises a question \textit{whether adversarial robustness really implies backdoor vulnerability.} Based on thorough experiments, we find that such trade-off ignores the interactions between the perturbation budget of adversarial training and the magnitude of the backdoor trigger. Indeed, an adversarially trained model is capable of achieving backdoor robustness as long as the perturbation budget surpasses the trigger magnitude, while it is vulnerable to backdoor attacks only for adversarial training with a small perturbation budget. To always mitigate the backdoor vulnerability, we propose an adversarial-training based detection strategy and a general pipeline against backdoor attacks, which consistently brings backdoor robustness regardless of the perturbation budget.",,OpenReview
Dynamic Backdoor Attacks Against Deep Neural Networks,"Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, Yang Zhang",,,"<a href=""OpenReview () : Dynamic Backdoor Attacks Against Deep Neural Networks"" target=""_blank"">[https://openreview.net/pdf/f6dbabd06db1c1d23533d53f3104bfa4e62cc6ae.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/f6dbabd06db1c1d23533d53f3104bfa4e62cc6ae.pdf]</a>","Current Deep Neural Network (DNN) backdooring attacks rely on adding static triggers (with fixed patterns and locations) on model inputs that are prone to detection. In this paper, we propose the first class of dynamic backdooring techniques: Random Backdoor, Backdoor Generating Network (BaN), and conditional Backdoor Generating Network (c-BaN). Triggers generated by our techniques have random patterns and locations. In particular, BaN and c-BaN based on a novel generative network are the first two schemes that algorithmically generate triggers. Moreover, c-BaN is the first conditional backdooring technique that given a target label, it can generate a target-specific trigger. Both BaN and c-BaN are essentially a general framework which renders the adversary the flexibility for further customizing backdoor attacks. We extensively evaluate our techniques on three benchmark datasets and show that our techniques achieve almost perfect attack performance on backdoored data with a negligible utility loss. More importantly, our techniques can bypass state-of-the-art defense mechanisms.",,OpenReview
EFFECTIVE FREQUENCY-BASED BACKDOOR ATTACKS WITH LOW POISONING RATIOS,"Danni Yuan, Mingda Zhang, Shaokui Wei, Shicai Yang, Baoyuan Wu",OpenReview,,"<a href=""OpenReview () : EFFECTIVE FREQUENCY-BASED BACKDOOR ATTACKS WITH LOW POISONING RATIOS"" target=""_blank"">[https://openreview.net/pdf/7b68b04ba1f19cef7554d236f5e813b1279f055b.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/7b68b04ba1f19cef7554d236f5e813b1279f055b.pdf]</a>","Backdoor attack has been considered a serious threat to deep learning. Although several seminal backdoor attack methods have been proposed, they often required at least a certain poisoning ratio (\eg, 1\% or more) to achieve high attack success rate (ASR). However, the attack with a large poisoning ratio may be difficult to evade human inspection or backdoor defenses, \ie, low stealthiness. To tackle the dilemma between high ASR and low stealthiness, we aim to enhance ASR under low poisoning ratio, \ie, pursuing high ASR and high stealthiness simultaneously. To achieve this goal, we propose a novel frequency-based backdoor attack, where the trigger is generated based on important frequencies that contribute positively to the model prediction with respect to the target class. Extensive experiments on four benchmark datasets (CIFAR-10, CIFAR-100, GTSRB, Tiny ImageNet) verify the effectiveness and stealthiness of the proposed method under extremely low poisoning ratios. Specifically, with only 0.01\% poisoning ratio, our attack could achieve the ASR of 80.51%, 51.3%, 76.3%, and 87.2% on above four datasets, respectively, while the ASR values of most state-of-the-art (SOTA) attack methods are close to 0. Meanwhile, our method could well evade several SOTA backdoor defense methods, \ie, the ASR values are not significantly affected under defense.",,OpenReview
Economics Assistant for Robustness Checks (EconARC): Identifying Confounders from Causal Knowledge Graphs,"Fiona Anting Tan, See-Kiong Ng",,,"<a href=""OpenReview () : Economics Assistant for Robustness Checks (EconARC): Identifying Confounders from Causal Knowledge Graphs"" target=""_blank"">[https://hozo.jp/ISWC2023_PD-Industry-proc/ISWC2023_paper_414.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://hozo.jp/ISWC2023_PD-Industry-proc/ISWC2023_paper_414.pdf]</a>","In Economics, authors conduct comprehensive robustness checks to prevent drawing misleading conclusions from their causal analyses by accounting for potential confounding factors. To assist in this process, we propose EconARC which offers automated identification of confounders from the literature. Our methodology involves extracting cause-and-effect arguments using a fine-tuned sequence-to-sequence model, clustering semantically similar arguments into topics, and utilizing the backdoor criterion on the causal graph to detect confounders. Our study is the first to employ text mining techniques for generating confounders in Economics, with implications for advancing Artificial Intelligence towards human-level capabilities like engaging in academic discourse.",,OpenReview
Fatty and Skinny: A Joint Training Method of Watermark Encoder and Decoder,"Sanghyun Hong, Mahmoud Mohammadi, Noseong Park",,,"<a href=""OpenReview () : Fatty and Skinny: A Joint Training Method of Watermark Encoder and Decoder"" target=""_blank"">[https://openreview.net/pdf/599b84fb70332752b375b408dd50c91869f727e9.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/599b84fb70332752b375b408dd50c91869f727e9.pdf]</a>","Watermarks have been used for various purposes. Recently, researchers started to look into using them for deep neural networks. Some works try to hide attack triggers on their adversarial samples when attacking neural networks and others want to watermark neural networks to prove their ownership against plagiarism. Implanting a backdoor watermark module into a neural network is getting more attention from the community. In this paper, we present a general purpose encoder-decoder joint training method, inspired by generative adversarial networks (GANs). Unlike GANs, however, our encoder and decoder neural networks cooperate to find the best watermarking scheme given data samples. In other words, we do not design any new watermarking strategy but our proposed two neural networks will find the best suited method on their own. After being trained, the decoder can be implanted into other neural networks to attack or protect them (see Appendix for their use cases and real implementations). To this end, the decoder should be very tiny in order not to incur any overhead when attached to other neural networks but at the same time provide very high decoding success rates, which is very challenging. Our joint training method successfully solves the problem and in our experiments maintain almost 100\% encoding-decoding success rates for multiple datasets with very little modifications on data samples to hide watermarks. We also present several real-world use cases in Appendix.",,OpenReview
Feature Grinding: Efficient Backdoor Sanitation in Deep Neural Networks,"Nils Lukas, Charles Zhang, Florian Kerschbaum",OpenReview,,"<a href=""OpenReview () : Feature Grinding: Efficient Backdoor Sanitation in Deep Neural Networks"" target=""_blank"">[https://openreview.net/pdf/2a957c2a6cdb20875c59543b3de24ba82906d16b.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/2a957c2a6cdb20875c59543b3de24ba82906d16b.pdf]</a>","Training deep neural networks (DNNs) is expensive and for this reason, third parties provide computational resources to train models. This makes DNNs vulnerable to backdoor attacks, in which the third party maliciously injects hidden functionalities in the model at training time. Removing a backdoor is challenging because although the defender has access to a clean, labeled dataset, they only have limited computational resources which are a fraction of the resources required to train a model from scratch. We propose Feature Grinding as an efficient, randomized backdoor sanitation technique against seven contemporary backdoors on CIFAR-10 and ImageNet. Feature Grinding requires at most six percent of the model's training time on CIFAR-10 and at most two percent on ImageNet for sanitizing the surveyed backdoors. We compare Feature Grinding with five other sanitation methods and find that it is often the most effective at decreasing the backdoor's success rate while preserving a high model accuracy. Our experiments include an ablation study over multiple parameters for each backdoor attack and sanitation technique to ensure a fair evaluation of all methods. Models suspected of containing a backdoor can be Feature Grinded using limited resources, which makes it a practical defense against backdoors that can be incorporated into any standard training procedure.",,OpenReview
Feature Partition Aggregation: A Fast Certified Defense Against a Union of Sparse Adversarial Attacks,"Zayd Hammoudeh, Daniel Lowd",,,"<a href=""OpenReview () : Feature Partition Aggregation: A Fast Certified Defense Against a Union of Sparse Adversarial Attacks"" target=""_blank"">[https://arxiv.org/abs/2302.11628]</a>","<a href=""OpenReview"" target=""_blank"">[https://arxiv.org/abs/2302.11628]</a>","Deep networks are susceptible to numerous types of adversarial attacks. Certified defenses provide guarantees on a model's robustness, but most of these defenses are restricted to a single attack type. In contrast, this paper proposes feature partition aggregation (FPA) - a certified defense against a union of attack types, namely evasion, backdoor, and poisoning attacks. We specifically consider an $\ell_0$ or sparse attacker that arbitrarily controls an unknown subset of the training and test features - even across all instances. FPA generates robustness guarantees via an ensemble whose submodels are trained on disjoint feature sets. Following existing certified sparse defenses, we generalize FPA's guarantees to top-$k$ predictions. FPA significantly outperforms state-of-the-art sparse defenses providing larger and stronger robustness guarantees while simultaneously being up to 5,000$\times$ faster.",,OpenReview
Feature Synchronization in Backdoor Attacks,"Zihan Guan, Lichao Sun, Mengnan Du, Ninghao Liu",OpenReview,,"<a href=""OpenReview () : Feature Synchronization in Backdoor Attacks"" target=""_blank"">[https://openreview.net/pdf/846d8a53ff56f5072ec29c23b053e1c97cb5785f.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/846d8a53ff56f5072ec29c23b053e1c97cb5785f.pdf]</a>","Backdoor attacks train models on a mixture of poisoned data and clean data to implant backdoor triggers into the model. An interesting phenomenon has been observed in the training process: the loss of poisoned samples tends to drop significantly faster than that of clean samples, which we call the early-fitting phenomenon. Early-fitting provides a simple but effective method to defend against backdoor attacks, as the poisoned samples can be identified by picking the samples with the lowest loss values in the early training epochs. Therefore, two natural questions arise: (1) What characteristics of poisoned samples cause early-fitting? (2) Is it possible to design stronger attacks to circumvent existing defense methods? To answer the first question, we find that early-fitting could be attributed to a unique property of poisoned samples called synchronization, which depicts the latent similarity between two samples. Meanwhile, the degree of synchronization could be explicitly controlled based on whether it is captured by shallow or deep layers of the model. Then, we give an affirmative answer to the second question by proposing a new backdoor attack method, Deep Backdoor Attack (DBA), which utilizes deep synchronization to reversely generate trigger patterns by activating neurons in the deep layer. Experimental results validate our propositions and the effectiveness of DBA. Our code is available at https://anonymous.4open.science/r/Deep-Backdoor-Attack-8875",,OpenReview
GINN: Fast GPU-TEE Based Integrity for Neural Network Training,"Aref Asvadishirehjini, Murat Kantarcioglu, Bradley A. Malin",,,"<a href=""OpenReview () : GINN: Fast GPU-TEE Based Integrity for Neural Network Training"" target=""_blank"">[https://openreview.net/pdf/ea90589cdf26d8d458a3e43a88f85b8f193e8ccd.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/ea90589cdf26d8d458a3e43a88f85b8f193e8ccd.pdf]</a>","Machine learning models based on Deep Neural Networks (DNNs) are increasingly being deployed in a wide range of applications ranging from self-driving cars to Covid-19 diagnostics. The computational power necessary to learn a DNN is non-trivial. So, as a result, cloud environments with dedicated hardware support emerged as important infrastructure. However, outsourcing computation to the cloud raises security, privacy, and integrity challenges. To address these challenges, previous works tried to leverage homomorphic encryption, secure multi-party computation, and trusted execution environments (TEE). Yet, none of these approaches can scale up to support realistic DNN model training workloads with deep architectures and millions of training examples without sustaining a significant performance hit. In this work, we focus on the setting where the integrity of the outsourced Deep Learning (DL) model training is ensured by TEE. We choose the TEE based approach because it has been shown to be more efficient compared to the pure cryptographic solutions, and the availability of TEEs on cloud environments. To mitigate the loss in performance, we combine random verification of selected computation steps with careful adjustments of DNN used for training. Our experimental results show that the proposed approach may achieve 2X to 20X performance improvement compared to the pure TEE based solution while guaranteeing the integrity of the computation with high probability (e.g., 0.999) against the state-of-the-art DNN backdoor attacks.",,OpenReview
GPTs Don’t Keep Secrets: Searching for Backdoor Watermark Triggers in Autoregressive Language Models,"Evan Lucas, Timothy Havens",,,"<a href=""OpenReview () : GPTs Don’t Keep Secrets: Searching for Backdoor Watermark Triggers in Autoregressive Language Models"" target=""_blank"">[https://aclanthology.org/2023.trustnlp-1.21.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://aclanthology.org/2023.trustnlp-1.21.pdf]</a>","This work analyzes backdoor watermarks in an autoregressive transformer fine-tuned to perform a generative sequence-to-sequence task, specifically summarization. We propose and demonstrate an attack to identify trigger words or phrases by analyzing open ended generations from autoregressive models that have backdoor watermarks inserted. It is shown in our work that triggers based on random common words are easier to identify than those based on single, rare tokens. The attack proposed is easy to implement and only requires access to the model weights. Code used to create the backdoor watermarked models and analyze their outputs is shared at [github link to be inserted for camera ready version].",,OpenReview
Generalizable Multi-Relational Graph Representation Learning: A Message Intervention Approach,"Haoran Xin, Xinjiang Lu, Tong Xu, Dejing Dou, Hui Xiong",OpenReview,,"<a href=""OpenReview () : Generalizable Multi-Relational Graph Representation Learning: A Message Intervention Approach"" target=""_blank"">[https://openreview.net/pdf/255644c9dee71e168941ff9a1afc5ab649533749.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/255644c9dee71e168941ff9a1afc5ab649533749.pdf]</a>","With the edges associated with labels and directions, the so-called multi-relational graph possesses powerful expressiveness, which is beneficial to many applications. However, as the heterogeneity brought by the higher cardinality of edges and relations climbs up, more trivial relations are taken into account for the downstream task since they are often highly correlated to the target. As a result, with being forced to fit the non-causal relational patterns on the training set, the downstream model, like graph neural network (GNN), may suffer from poor generalizability on the testing set since the inference is mainly made according to misleading clues. In this paper, under the paradigm of graph convolution, we probe the multi-relational message passing process from the perspective of causality and then propose a Message Intervention method for learning generalizable muLtirElational gRaph representations, coined MILER. In particular, MILER first encodes the vertices and relations into embeddings with relational and directional awareness, then a message diverter is employed to split the original message flow into two flows of interest, i.e., the causal and trivial message flows. Afterward, the message intervention is carried out with the guidance of the backdoor adjustment rule. Extensive experiments on several knowledge graph benchmarks validate the effectiveness as well as the superior generalization ability of MILER.",,OpenReview
HufuNet: Embedding the Left Piece as Watermark and Keeping the Right Piece for Ownership Verification in Deep Neural Networks.,"Peizhuo Lv, Li Pan, Shengzhi Zhang, Kai Chen, Ruigang Liang, Yue Zhao, Li Yingjiu",,,"<a href=""OpenReview () : HufuNet: Embedding the Left Piece as Watermark and Keeping the Right Piece for Ownership Verification in Deep Neural Networks."" target=""_blank"">[https://arxiv.org/pdf/2103.13628]</a>","<a href=""OpenReview"" target=""_blank"">[https://arxiv.org/pdf/2103.13628]</a>","Due to the wide use of highly-valuable and large-scale deep neural networks (DNNs), it becomes crucial to protect the intellectual property of DNNs so that the ownership of disputed or stolen DNNs can be verified. Most existing solutions embed backdoors in DNN model training such that DNN ownership can be verified by triggering distinguishable model behaviors with a set of secret inputs. However, such solutions are vulnerable to model fine-tuning and pruning. They also suffer from fraudulent ownership claim as attackers can discover adversarial samples and use them as secret inputs to trigger distinguishable behaviors from stolen models. To address these problems, we propose a novel DNN watermarking solution, named HufuNet, for protecting the ownership of DNN models. We evaluate HufuNet rigorously on four benchmark datasets with five popular DNN models, including convolutional neural network (CNN) and recurrent neural network (RNN). The experiments demonstrate HufuNet is highly robust against model fine-tuning/pruning, kernels cutoff/supplement, functionality-equivalent attack, and fraudulent ownership claims, thus highly promising to protect large-scale DNN models in the real-world.",,OpenReview
Interpreting Graph Neural Networks via Unrevealed Causal Learning,"Wanyu Lin, Hao Lan, Hao Wang, Baochun Li",OpenReview,,"<a href=""OpenReview () : Interpreting Graph Neural Networks via Unrevealed Causal Learning"" target=""_blank"">[https://openreview.net/pdf/fb9f0d795f4b618d589d1616a5c449ceee2e4ff7.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/fb9f0d795f4b618d589d1616a5c449ceee2e4ff7.pdf]</a>","This paper proposes a new explanation framework, called OrphicX, for generating causal explanations for any graph neural networks (GNNs) based on learned latent causal factors. Specifically, we construct a distinct generative model and design an objective function that encourages the generative model to produce causal, compact, and faithful explanations. This is achieved by isolating the causal factors in the latent space of graphs by maximizing the information flow measurements. We theoretically analyze the cause-effect relationships in the proposed causal graph, identify node attributes as confounders between graphs and GNN predictions, and circumvent such confounder effect by leveraging the backdoor adjustment formula. Our framework is compatible with any GNNs, and it does not require access to the process by which the target GNN produces its predictions. In addition, it does not rely on the linear-independence assumption of the explained features, nor require prior knowledge on the graph learning tasks. Empirically, we show that OrphicX can effectively identify the causal semantics for generating causal explanations, significantly outperforming its alternatives.",,OpenReview
Interventional Few-Shot Learning,Lukasz Bala,,,"<a href=""OpenReview () : Interventional Few-Shot Learning"" target=""_blank"">[https://openreview.net/pdf/a3e8e20306d3900c32c022334a220001fd1f7868.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/a3e8e20306d3900c32c022334a220001fd1f7868.pdf]</a>","We uncover an ever-overlooked deficiency in the prevailing Few-Shot Learning (FSL) methods: the pre-trained knowledge is indeed a confounder that limits the performance. This finding is rooted from our causal assumption: a Structural Causal Model (SCM) for the causalities among the pre-trained knowledge, sample features, and labels. Thanks to it, we propose a novel FSL paradigm: Interventional FewShot Learning (IFSL). Specifically, we develop three effective IFSL algorithmic implementations based on the backdoor adjustment, which is essentially a causal intervention towards the SCM of many-shot learning: the upper-bound of FSL in a causal view. It is worth noting that the contribution of IFSL is orthogonal to existing fine-tuning and meta-learning based FSL methods, hence IFSL can improve all of them, achieving a new 1-/5-shot state-of-the-art on miniImageNet, tieredImageNet, and cross-domain CUB. Code is released at https://github. com/yue-zhongqi/ifsl.","<a href=""OpenReview"" target=""_blank"">[https://github]</a>",OpenReview
LoneNeuron: a Highly-effective Feature-domain Neural Trojan using Invisible and Polymorphic Watermarks,"Zeyan Liu, Fengjun Li, Zhu Li, Bo Luo",,,"<a href=""OpenReview () : LoneNeuron: a Highly-effective Feature-domain Neural Trojan using Invisible and Polymorphic Watermarks"" target=""_blank"">[]</a>","<a href=""OpenReview"" target=""_blank"">[]</a>","The wide adoption of deep neural networks (DNNs) in real-world applications raises increasing security concerns. Neural Trojans embedded in pre-trained neural networks are a harmful attack against the DNN model supply chain. They generate false outputs when certain stealthy triggers appear in the inputs. While data-poisoning attacks have been well studied in the literature, code-poisoning and model-poisoning backdoors only start to attract attention until recently. We present a novel model-poisoning neural Trojan, namely LoneNeuron, which responds to feature-domain patterns that transform into invisible, sample-specific, and polymorphic pixel-domain watermarks. With high attack specificity, LoneNeuron achieves a 100% attack success rate, while not affecting the main task performance. With LoneNeuron's unique watermark polymorphism property, the same feature-domain trigger is resolved to multiple watermarks in the pixel domain, which further improves watermark randomness, stealthiness, and resistance against Trojan detection. Extensive experiments show that LoneNeuron could escape state-of-the-art Trojan detectors. LoneNeuron~is also the first effective backdoor attack against vision transformers (ViTs).",,OpenReview
MARNET: Backdoor Attacks against Value-Decomposition Multi-Agent Reinforcement Learning,"Yanjiao Chen, Zhicong Zheng, Xueluan Gong",OpenReview,,"<a href=""OpenReview () : MARNET: Backdoor Attacks against Value-Decomposition Multi-Agent Reinforcement Learning"" target=""_blank"">[https://openreview.net/pdf/8ba0a6b607f4f781d65a4894fd982984e823fe4e.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/8ba0a6b607f4f781d65a4894fd982984e823fe4e.pdf]</a>","Recent works have revealed that backdoor attacks against Deep Reinforcement Learning (DRL) could lead to abnormal action selection of the agent, which may result in failure or even catastrophe in crucial decision processes. However, existing attacks only consider single-agent RL systems, in which the only agent can observe the global state and have full control of the decision process. In this paper, we explore a new backdoor attack paradigm in cooperative multi-agent reinforcement learning (CMARL) scenarios, where a group of agents coordinate with each other to achieve a common goal, while each agent can only observe the local state, e.g., StarCraft II (Vinyals et al. (2017)). In the proposed MARNet attack framework, we carefully design a pipeline of trigger design, action poisoning and reward hacking modules to accommodate the cooperative multi-agent momentums. In particular, as only a subset of agents can observe the triggers in their local observations, we maneuver their actions to the worst actions suggested by an expert policy model. Since the global reward in CMARL is aggregated by individual rewards from all agents, we propose to modify the reward in a way that boosts the bad actions of poisoned agents (agents who observe the triggers) but mitigates the influence on non-poisoned agents. We conduct extensive experiments on two classical MARL algorithms VDN (Sunehag et al. (2018)) and QMIX (Rashid et al. (2018)), in two popular CMARL games Predator Prey (Boehmer et al. (2020)) and SMAC (Samvelyan et al. (2019)). The results show that MARNet outperforms baselines extended from single-agent DRL backdoor attacks TrojDRL (Kiourti et al. (2020)) and Multitasking learning (Ashcraft & Karra (2021)) by reducing the utility under attack by as much as 100%. We apply fine-tuning as a defense against MARNet, and demonstrate that fine-tuning cannot entirely eliminate the effect of the attack.",,OpenReview
MEDIC: Model Backdoor Removal by Importance Driven Cloning,"Qiuling Xu, Guanhong Tao, Jean Honorio, Yingqi Liu, Shengwei An, Guangyu Shen, Siyuan Cheng, Xiangyu Zhang",OpenReview,,"<a href=""OpenReview () : MEDIC: Model Backdoor Removal by Importance Driven Cloning"" target=""_blank"">[https://openreview.net/pdf/2086e2dbd11b82dcaa26661291bcccf3c8081dc9.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/2086e2dbd11b82dcaa26661291bcccf3c8081dc9.pdf]</a>","We develop a novel method to remove injected backdoors in Deep Learning models. It works by cloning the benign behaviors of a trojaned model to a new model of the same structure. It trains the clone model from scratch on a very small subset of samples and aims to minimize a cloning loss that denotes the differences between the activations of important neurons across the two models. The set of important neurons varies for each input, depending on their magnitude of activations and their impact on the classification result. Our experiments show that our technique can effectively remove nine different types of backdoors with minor benign accuracy degradation, outperforming the state-of-the-art backdoor removal techniques that are based on fine-tuning, knowledge distillation, and neuron pruning.",,OpenReview
MetaPoison: Learning to craft adversarial poisoning examples via meta-learning,"W. Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, Tom Goldstein",,,"<a href=""OpenReview () : MetaPoison: Learning to craft adversarial poisoning examples via meta-learning"" target=""_blank"">[https://openreview.net/pdf/5f78928102aaa8be2c8a7134096ffecf8733f894.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/5f78928102aaa8be2c8a7134096ffecf8733f894.pdf]</a>","We consider a new class of \emph{data poisoning} attacks on neural networks, in which the attacker takes control of a model by making small perturbations to a subset of its training data. We formulate the task of finding poisons as a bi-level optimization problem, which can be solved using methods borrowed from the meta-learning community. Unlike previous poisoning strategies, the meta-poisoning can poison networks that are trained from scratch using an initialization unknown to the attacker and transfer across hyperparameters. Further we show that our attacks are more versatile: they can cause misclassification of the target image into an arbitrarily chosen class. Our results show above 50% attack success rate when poisoning just 3-10% of the training dataset.",,OpenReview
Mitigating input-causing confounding in multimodal learning via the backdoor adjustment,"Taro Makino, Krzysztof J. Geras, Kyunghyun Cho",CML4Impact,,"<a href=""OpenReview () : Mitigating input-causing confounding in multimodal learning via the backdoor adjustment"" target=""_blank"">[https://openreview.net/pdf/25a1b2b97204d87e63e6d72fc3b177f4b313abe9.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/25a1b2b97204d87e63e6d72fc3b177f4b313abe9.pdf]</a>","We adopt a causal perspective to address why multimodal learning often performs worse than unimodal learning. We put forth a structural causal model (SCM) for which multimodal learning is preferable over unimodal learning. In this SCM, which we call the multimodal SCM, a latent variable causes the inputs, and the inputs cause the target. We refer to this latent variable as an input-causing confounder. By conditioning on all inputs, multimodal learning $d$-separates the input-causing confounder and the target, resulting in a causal model that is more robust than the statistical model learned by unimodal learning. We argue that multimodal learning fails in practice because our finite datasets appear to come from an alternative SCM, which we call the spurious SCM. In the spurious SCM, the input-causing confounder and target are conditionally dependent given the inputs. This means that multimodal learning no longer $d$-separates the input-causing confounder and the target, and fails to estimate a causal model. We use a latent variable model to model the input-causing confounder, and test whether its undesirable dependence with the target is present in the data. We then use the same model to remove this dependence and estimate a causal model, which corresponds to the backdoor adjustment. We use synthetic data experiments to validate our claims.",,OpenReview
On the Trade-off between Adversarial and Backdoor Robustness,"Zheng-Xin Weng, Yan-Ting Lee, Shan-Hung Wu",,,"<a href=""OpenReview () : On the Trade-off between Adversarial and Backdoor Robustness"" target=""_blank"">[]</a>","<a href=""OpenReview"" target=""_blank"">[]</a>","Deep neural networks are shown to be susceptible to both adversarial attacks and backdoor attacks. Although many defenses against an individual type of the above attacks have been proposed, the interactions between the vulnerabilities of a network to both types of attacks have not been carefully investigated yet. In this paper, we conduct experiments to study whether adversarial robustness and backdoor robustness can affect each other and find a trade-off—by increasing the robustness of a network to adversarial examples, the network becomes more vulnerable to backdoor attacks. We then investigate the cause and show how such a trade-off can be exploited for either good or bad purposes. Our findings suggest that future research on defense should take both adversarial and backdoor attacks into account when designing algorithms or robustness measures to avoid pitfalls and a false sense of security.",,OpenReview
Opportunistic Backdoor Attacks: Exploring Human-imperceptible Vulnerabilities on Speech Recognition Systems,Zhiping Cai,,,"<a href=""OpenReview () : Opportunistic Backdoor Attacks: Exploring Human-imperceptible Vulnerabilities on Speech Recognition Systems"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3503161.3548261]</a>","<a href=""OpenReview"" target=""_blank"">[https://dl.acm.org/doi/abs/10.1145/3503161.3548261]</a>","Speech recognition systems, trained and updated based on large-scale audio data, are vulnerable to backdoor attacks that inject dedicated triggers in system training. The used triggers are generally human-inaudible audio, such as ultrasonic waves. However, we note that such a design is not feasible, as it can be easily filtered out via pre-processing. In this work, we propose the first audible backdoor attack paradigm for speech recognition, characterized by passively triggering and opportunistically invoking. Traditional device-synthetic triggers are replaced with ambient noise in daily scenarios. For adapting triggers to the application dynamics of speech interaction, we exploit the observed knowledge inherited from the context to a trained model and accommodate the injection and poisoning with certainty-based trigger selection, performance-oblivious sample binding, and trigger late-augmentation. Experiments on two datasets under various environments evaluate the proposal's effectiveness in maintaining a high benign rate and facilitating outstanding attack success rate (99.27%, ~4% higher than BadNets), robustness (bounded infectious triggers), feasibility in real-world scenarios. It requires less than 1% data to be poisoned and is demonstrated to be able to resist typical speech enhancement techniques and general countermeasures (e.g., dedicated fine-tuning). The code and data will be made available at https://github.com/lqsunshine/DABA.","<a href=""OpenReview"" target=""_blank"">[https://github.com/lqsunshine/DABA]</a>",OpenReview
Poison ink: Robust and invisible backdoor attack,Jie Zhang,,,"<a href=""OpenReview () : Poison ink: Robust and invisible backdoor attack"" target=""_blank"">[https://arxiv.org/pdf/2108.02488.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://arxiv.org/pdf/2108.02488.pdf]</a>","Recent research shows deep neural networks are vulnerable to different types of attacks, such as adversarial attack, data poisoning attack and backdoor attack. Among them, backdoor attack is the most cunning one and can occur in almost every stage of deep learning pipeline. Therefore, backdoor attack has attracted lots of interests from both academia and industry. However, most existing backdoor attack methods are either visible or fragile to some effortless pre-processing such as common data transformations. To address these limitations, we propose a robust and invisible backdoor attack called ""Poison Ink"". Concretely, we first leverage the image structures as target poisoning areas, and fill them with poison ink (information) to generate the trigger pattern. As the image structure can keep its semantic meaning during the data transformation, such trigger pattern is inherently robust to data transformations. Then we leverage a deep injection network to embed such trigger pattern into the cover image to achieve stealthiness. Compared to existing popular backdoor attack methods, Poison Ink outperforms both in stealthiness and robustness. Through extensive experiments, we demonstrate Poison Ink is not only general to different datasets and network architectures, but also flexible for different attack scenarios. Besides, it also has very strong resistance against many state-of-the-art defense techniques.",,OpenReview
RVFR: Robust Vertical Federated Learning via Feature Subspace Recovery,"Jing Liu, Chulin Xie, Krishnaram Kenthapadi, Oluwasanmi O Koyejo, Bo Li",OpenReview,,"<a href=""OpenReview () : RVFR: Robust Vertical Federated Learning via Feature Subspace Recovery"" target=""_blank"">[https://openreview.net/pdf/218cac5cc281bc853458722ad05a5e24ca21d133.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/218cac5cc281bc853458722ad05a5e24ca21d133.pdf]</a>","Vertical Federated Learning (VFL) is a distributed learning paradigm that allows multiple agents to jointly train a global model when each agent holds a different subset of features for the same sample(s). VFL is known to be vulnerable to backdoor attacks, where data from malicious agents are manipulated during training, and vulnerable to test-time attacks, where malicious agents manipulate the test data. However, unlike the standard horizontal federated learning, improving the robustness of robust VFL remains challenging. To this end, we propose RVFR, a novel robust VFL training and inference framework. The key to our approach is to ensure that with a low-rank feature subspace, a small number of attacked samples, and other mild assumptions, RVFR recovers the underlying uncorrupted features with guarantees, thus sanitizes the model against a vast range of backdoor attacks. Further, RVFR also defends against test-time adversarial and missing feature attacks. We conduct extensive experiments on several datasets and show that the robustness of RVFR outperforms different baselines against diverse types of attacks.",,OpenReview
Risk-optimized Outlier Removal for Robust Point Cloud Classification,"Xinke Li, Junchi Lu, Henghui Ding, Changsheng Sun, Joey Tianyi Zhou, Yeow Meng Chee",,,"<a href=""OpenReview () : Risk-optimized Outlier Removal for Robust Point Cloud Classification"" target=""_blank"">[https://arxiv.org/pdf/2307.10875]</a>","<a href=""OpenReview"" target=""_blank"">[https://arxiv.org/pdf/2307.10875]</a>","With the growth of 3D sensing technology, deep learning system for 3D point clouds has become increasingly important, especially in applications like autonomous vehicles where safety is a primary concern. However, there are also growing concerns about the reliability of these systems when they encounter noisy point clouds, whether occurring naturally or introduced with malicious intent. This paper highlights the challenges of point cloud classification posed by various forms of noise, from simple background noise to malicious backdoor attacks that can intentionally skew model predictions. While there's an urgent need for optimized point cloud denoising, current point outlier removal approaches, an essential step for denoising, rely heavily on handcrafted strategies and are not adapted for higher-level tasks, such as classification. To address this issue, we introduce an innovative point outlier cleansing method that harnesses the power of downstream classification models. By employing gradient-based attribution analysis, we define a novel concept: point risk. Drawing inspiration from tail risk minimization in finance, we recast the outlier removal process as an optimization problem, named PointCVaR. Extensive experiments show that our proposed technique not only robustly filters diverse point cloud outliers but also consistently and significantly enhances existing robust methods for point cloud classification.",,OpenReview
SSDA: Secure Source-Free Domain Adaptation,"Sabbir Ahmed, Abdullah Al Arafat, Mamshad Nayeem Rizve, Rahim Hossain, Zhishan Guo, Adnan Siraj Rakin",,,"<a href=""OpenReview () : SSDA: Secure Source-Free Domain Adaptation"" target=""_blank"">[https://openaccess.thecvf.com/content/ICCV2023/html/Ahmed_SSDA_Secure_Source-Free_Domain_Adaptation_ICCV_2023_paper.html]</a>","<a href=""OpenReview"" target=""_blank"">[https://openaccess.thecvf.com/content/ICCV2023/html/Ahmed_SSDA_Secure_Source-Free_Domain_Adaptation_ICCV_2023_paper.html]</a>","Source-free domain adaptation (SFDA) is a popular unsupervised domain adaptation method where a pre-trained model from a source domain is adapted to a target domain without accessing any source data. Despite rich results in this area, existing literature overlooks the security challenges of the unsupervised SFDA setting in presence of a malicious source domain owner. This work investigates the effect of a source adversary which may inject a hidden malicious behavior (Backdoor/Trojan) during source training and potentially transfer it to the target domain even after benign training by the victim (target domain owner). Our investigation of the current SFDA setting reveals that because of the unique challenges present in SFDA (e.g., no source data, target label), defending against backdoor attack using existing defenses become practically ineffective in protecting the target model. To address this, we propose a novel target domain protection scheme called secure source-free domain adaptation (SSDA). SSDA adopts a single-shot model compression of a pre-trained source model and a novel knowledge transfer scheme with a spectral-norm-based loss penalty for target training. The proposed static compression and the dynamic training loss penalty are designed to suppress the malicious channels responsive to the backdoor during the adaptation stage. At the same time, the knowledge transfer from an uncompressed auxiliary model helps to recover the benign test accuracy. Our extensive evaluation on multiple dataset and domain tasks against recent backdoor attacks reveal that the proposed SSDA can successfully defend against strong backdoor attacks with little to no degradation in test accuracy compared to the vulnerable baseline SFDA methods. Our code is available at https://github.com/ML-Security-Research-LAB/SSDA.","<a href=""OpenReview"" target=""_blank"">[https://github.com/ML-Security-Research-LAB/SSDA]</a>",OpenReview
Shape Features Improve General Model Robustness,"Chaowei Xiao, Mingjie Sun, Haonan Qiu, Han Liu, Mingyan Liu, Bo Li",,,"<a href=""OpenReview () : Shape Features Improve General Model Robustness"" target=""_blank"">[https://openreview.net/pdf/47c36d06005b6d0ed9d288f9d4b05eb26216ce52.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/47c36d06005b6d0ed9d288f9d4b05eb26216ce52.pdf]</a>","Recent studies show that convolutional neural networks (CNNs) are vulnerable under various settings, including adversarial examples, backdoor attacks, and distribution shifting. Motivated by the findings that human visual system pays more attention to global structure (e.g., shape) for recognition while CNNs are biased towards local texture features in images, we propose a unified framework EdgeGANRob based on robust edge features to improve the robustness of CNNs in general, which first explicitly extracts shape/structure features from a given image and then reconstructs a new image by refilling the texture information with a trained generative adversarial network (GAN). In addition, to reduce the sensitivity of edge detection algorithm to adversarial perturbation, we propose a robust edge detection approach Robust Canny based on the vanilla Canny algorithm. To gain more insights, we also compare EdgeGANRob with its simplified backbone procedure EdgeNetRob, which performs learning tasks directly on the extracted robust edge features. We find that EdgeNetRob can help boost model robustness significantly but at the cost of the clean model accuracy. EdgeGANRob, on the other hand, is able to improve clean model accuracy compared with EdgeNetRob and without losing the robustness benefits introduced by EdgeNetRob. Extensive experiments show that EdgeGANRob is resilient in different learning tasks under diverse settings.",,OpenReview
SlothBomb: Efficiency Poisoning Attack against Dynamic Neural Networks,"Simin Chen, Hanlin Chen, Mirazul Haque, Cong Liu, Wei Yang",OpenReview,,"<a href=""OpenReview () : SlothBomb: Efficiency Poisoning Attack against Dynamic Neural Networks"" target=""_blank"">[https://openreview.net/pdf/97a39e9e6ae3c0c4d954e834fc2c336aa7f578ca.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/97a39e9e6ae3c0c4d954e834fc2c336aa7f578ca.pdf]</a>","Recent increases in deploying deep neural networks (DNNs) on resource-constrained devices, combined with the observation that not all input samples require the same amount of computations, have sparked interest in input-adaptive dynamic neural networks (DyNNs). These DyNNs bring in more efficient inferences and enable deploying DNNs on resource-constrained devices e.g., mobile devices. In this work, we study a new vulnerability about DyNNs: can adversaries manipulate the DyNNs efficiency to provide a false sense of efficiency? To answer this question, we design SlothBomb, an adversarial attack that injects efficiency backdoors in DyNNs. SlothBomb can poison just a minimal percentage of DyNNs training data to inject a backdoor trigger into DyNNs. In the inference time, SlothBomb can use the backdoor trigger to slow down DyNNs and abuse the computational resources of DyNNs - an availability threat analogous to the denial-of-service attacks. We evaluate SlothBomb on three DNN backbone architectures (based on VGG16, MobileNet, and ResNet56) on two popular datasets (CIFAR-10 and Tiny ImageNet) We show that a SlothBomb reduces the efficacy of DyNNs on triggered input samples while keeping almost the same efficiency on clean samples.",,OpenReview
Sself: Robust Federated Learning against Stragglers and Adversaries,"Jungwuk Park, Dong-Jun Han, Minseok Choi, Jaekyun Moon",,,"<a href=""OpenReview () : Sself: Robust Federated Learning against Stragglers and Adversaries"" target=""_blank"">[https://openreview.net/pdf/e201319c3e9b74cfed5468dd8cd7824b9c983cf2.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/e201319c3e9b74cfed5468dd8cd7824b9c983cf2.pdf]</a>","While federated learning allows efficient model training with local data at edge devices, two major issues that need to be resolved are: slow devices known as stragglers and malicious attacks launched by adversaries. While the presence of both stragglers and adversaries raises serious concerns for the deployment of practical federated learning systems, no known schemes or known combinations of schemes, to our best knowledge, effectively address these two issues at the same time. In this work, we propose Sself, a semi-synchronous entropy and loss based filtering/averaging, to tackle both stragglers and adversaries simultaneously. The stragglers are handled by exploiting different staleness (arrival delay) information when combining locally updated models during periodic global aggregation. Various adversarial attacks are tackled by utilizing a small amount of public data collected at the server in each aggregation step, to first filter out the model-poisoned devices using computed entropies, and then perform weighted averaging based on the estimated losses to combat data poisoning and backdoor attacks. A theoretical convergence bound is established to provide insights on the convergence of Sself. Extensive experimental results show that Sself outperforms various combinations of existing methods aiming to handle stragglers/adversaries.",,OpenReview
Stealthy and Flexible Trojan in Deep Learning Framework,"Yajie Wang, Kongyang Chen, Yu-an Tan, Shuxin Huang, Wencong Ma, Yuanzhang Li",,,"<a href=""OpenReview () : Stealthy and Flexible Trojan in Deep Learning Framework"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9747995]</a>","<a href=""OpenReview"" target=""_blank"">[https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9747995]</a>","Deep neural networks (DNNs) are increasingly used as the critical component of applications, bringing high computational costs. Many practitioners host their models on third-party platforms. This practice exposes DNNs to risks: A third party hosting the model may use a malicious deep learning framework to implement a backdoor attack. Our goal is to develop the realistic potential for backdoor attacks in third-party hosting platforms. We introduce a threatening and realistically implementable backdoor attack that is highly stealthy and flexible. We inject trojans by hijacking the built-in functions of the deep learning framework. Existing backdoor attacks rely on poisoning, its trigger is a special pattern superimposed on the input. Unlike existing backdoor attacks, the proposed sequential trigger is a specific sequence of clean image sets. Moreover, our attack is model agnostic and does not require retraining the model or modifying the parameters. Its stealthy is that injecting trojans will not change the model’s prediction for a clean image, so existing backdoor defenses cannot detect it. Its flexibility lies in that adversary can remodify the trojan behavior at any time. Extensive experiments on multiple benchmarks with different frameworks demonstrate that our attack achieves a perfect success rate (up to 100%) with minimal damage to model performance. And we can inject multiple trojans which do not affect each other at the same time, trojans hidden in the framework make a universal backdoor attack possible. Analysis and experiments further show that state-of-the-art defenses are ineffective against our attacks. Our work suggests that backdoor attacks in the supply chain need to be urgently explored.",,OpenReview
Trojans and Adversarial Examples: A Lethal Combination,"Guanxiong Liu, Issa Khalil, Abdallah Khreishah, Hai Phan",,,"<a href=""OpenReview () : Trojans and Adversarial Examples: A Lethal Combination"" target=""_blank"">[https://openreview.net/pdf/6c70f3c04ce551e071f1ed5fb2fb7910bd437f23.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/6c70f3c04ce551e071f1ed5fb2fb7910bd437f23.pdf]</a>","In this work, we naturally unify adversarial examples and Trojan backdoors into a new stealthy attack, that is activated only when 1) adversarial perturbation is injected into the input examples and 2) a Trojan backdoor is used to poison the training process simultaneously. Different from traditional attacks, we leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, thus making it difficult to be detected. Our attack can fool the user into accidentally trusting the infected model as a robust classifier against adversarial examples. We perform a thorough analysis and conduct an extensive set of experiments on several benchmark datasets to show that our attack can bypass existing defenses with a success rate close to 100%.",,OpenReview
VeRe: Verification Guided Synthesis for Repairing Deep Neural Networks,"Jianan Ma, Pengfei Yang, Jingyi Wang, Youcheng Sun, Cheng-Chao Huang, Zhen Wang",,,"<a href=""OpenReview () : VeRe: Verification Guided Synthesis for Repairing Deep Neural Networks"" target=""_blank"">[https://pure.manchester.ac.uk/ws/portalfiles/portal/277365839/icse2024-vere.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://pure.manchester.ac.uk/ws/portalfiles/portal/277365839/icse2024-vere.pdf]</a>","Neural network repair aims to fix the ‘bugs’ of neural networks by modifying the model’s architecture or parameters. However, due to the data-driven nature of neural networks, it is difficult to explain the relationship between the internal neurons and erro- neous behaviors, making further repair challenging. While several work exists to identify responsible neurons based on gradient or causality analysis, their effectiveness heavily rely on the quality of available ‘bugged’ data and multiple heuristics in layer or neuron selection. In this work, we address the issue utilizing the power of formal verification (in particular for neural networks). Specifi- cally, we propose VeRe, a verification-guided neural network repair framework that performs fault localization based on linear relax- ation to symbolically calculate the repair significance of neurons and furthermore optimize the parameters of problematic neurons to repair erroneous behaviors. We evaluated VeRe on various repair tasks, and our experimental results show that VeRe can efficiently and effectively repair all neural networks without degrading the model’s performance. For the task of removing backdoors, VeRe successfully reduces attack success rate from 98.47% to 0.38% on average, while causing an average performance drop of 0.9%. For the task of repairing safety properties, VeRe successfully repairs all the 36 tasks and achieves 99.87% generalization on average.",,OpenReview
WAFFLE: Watermarking in federated learning,"Buse G. A., Yuxi Xia, Samuel Marchal, N Asokan",,,"<a href=""OpenReview () : WAFFLE: Watermarking in federated learning"" target=""_blank"">[]</a>","<a href=""OpenReview"" target=""_blank"">[]</a>","Federated learning is a distributed learning technique where machine learning models are trained on client devices in which the local training data resides. The training is coordinated via a central server which is, typically, controlled by the intended owner of the resulting model. By avoiding the need to transport the training data to the central server, federated learning improves privacy and efficiency. But it raises the risk of model theft by clients because the resulting model is available on every client device. Even if the application software used for local training may attempt to prevent direct access to the model, a malicious client may bypass any such restrictions by reverse engineering the application software. Watermarking is a well-known deterrence method against model theft by providing the means for model owners to demonstrate ownership of their models. Several recent deep neural network (DNN) watermarking techniques use backdooring: training the models with additional mislabeled data. Backdooring requires full access to the training data and control of the training process. This is feasible when a single party trains the model in a centralized manner, but not in a federated learning setting where the training process and training data are distributed among several client devices. In this paper, we present WAFFLE, the first approach to watermark DNN models trained using federated learning. It introduces a retraining step at the server after each aggregation of local models into the global model. We show that WAFFLE efficiently embeds a resilient watermark into models incurring only negligible degradation in test accuracy (-0.17%), and does not require access to training data. We also introduce a novel technique to generate the backdoor used as a watermark. It outperforms prior techniques, imposing no communication, and low computational (+3.2%) overhead. The research report version of this paper is also available in https://arxiv.org/abs/2008.07298, and the code for reproducing our work can be found at https://github.com/ssg-research/WAFFLE.","<a href=""OpenReview"" target=""_blank"">[https://github.com/ssg-research/WAFFLE]</a>",OpenReview
Watermarking PLMs by Combining Contrastive Learning with Weight Perturbation,Anonymous,,,"<a href=""OpenReview () : Watermarking PLMs by Combining Contrastive Learning with Weight Perturbation"" target=""_blank"">[https://openreview.net/pdf/dfda36bbea5a5008843b1a583d2e078de8044bef.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/dfda36bbea5a5008843b1a583d2e078de8044bef.pdf]</a>","Large pre-trained language models (PLMs) have achieved remarkable success, making them highly valuable intellectual property due to their expensive training costs. Consequently, model watermarking, a method developed to protect the intellectual property of neural models, has emerged as a crucial yet underexplored technique. The problem of watermarking PLMs has remained unsolved since the parameters of PLMs will be updated when fine-tuned on downstream datasets, and then embedded watermarks could be removed easily due to the catastrophic forgetting phenomenon. This study investigates the feasibility of watermarking PLMs by embedding backdoors that can be triggered by specific inputs. We employ contrastive learning during the watermarking phase, allowing the representations of specific inputs to be isolated from others and mapped to a particular label after fine-tuning. Moreover, we demonstrate that by combining weight perturbation with the proposed method, watermarks can be embedded in a flatter region of the loss landscape, thereby increasing their robustness to watermark removal.Extensive experiments on multiple datasets demonstrate that the embedded watermarks can be robustly extracted without any knowledge about downstream tasks, and with a high success rate.",,OpenReview
What Do Deep Nets Learn? Class-wise Patterns Revealed in the Input Space,"Shihao Zhao, Xingjun Ma, Yisen Wang, James Bailey, Bo Li, Yu-Gang Jiang",,,"<a href=""OpenReview () : What Do Deep Nets Learn? Class-wise Patterns Revealed in the Input Space"" target=""_blank"">[https://openreview.net/pdf/46341969d496b2539623833dec6eac3eccf03e78.pdf]</a>","<a href=""OpenReview"" target=""_blank"">[https://openreview.net/pdf/46341969d496b2539623833dec6eac3eccf03e78.pdf]</a>","Deep neural networks (DNNs) have been widely adopted in different applications to achieve state-of-the-art performance. However, they are often applied as a black box with limited understanding of what the model has learned from the data. In this paper, we focus on image classification and propose a method to visualize and understand the class-wise patterns learned by DNNs trained under three different settings including natural, backdoored and adversarial. Different from existing class-wise deep representation visualizations, our method searches for a single predictive pattern in the input (i.e. pixel) space for each class. Based on the proposed method, we show that DNNs trained on natural (clean) data learn abstract shapes along with some texture, and backdoored models learn a small but highly predictive pattern for the backdoor target class. Interestingly, the existence of class-wise predictive patterns in the input space indicates that even DNNs trained on clean data can have backdoors, and the class-wise patterns identified by our method can be readily applied to “backdoor” attack the model. In the adversarial setting, we show that adversarially trained models learn more simplified shape patterns. Our method can serve as a useful tool to better understand DNNs trained on different datasets under different settings.",,OpenReview
