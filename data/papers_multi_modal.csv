title,author,venue_name,publish_date,url,code,created_date
Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning,,arXiv,2024,"<a href=""https://arxiv.org/abs/2401.15935"" target=""_blank"">2401.15935</a>",,2025-05-30
ViFiT: Reconstructing Vision Trajectories from IMU and Wi-Fi Fine Time Measurements,,MobiCom,2023,"<a href=""https://dl.acm.org/doi/10.1145/3615984.3616503"" target=""_blank"">3615984.3616503</a>",,2025-05-30
Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study,,ECIR,2023,"<a href=""https://arxiv.org/abs/2301.05174"" target=""_blank"">2301.05174</a>",,2025-05-30
Multimodal Representation Learning: Perks and Pitfalls,,ICLR,2023,"<a href=""https://mrl-workshop.github.io/iclr-2023/"" target=""_blank""></a>",,2025-05-30
"Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions",,arxiv,2023,"<a href=""https://arxiv.org/abs/2209.03430"" target=""_blank"">2209.03430</a>",,2025-05-30
PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization,,ICCV,2023,"<a href=""https://arxiv.org/abs/2307.15199"" target=""_blank"">2307.15199</a>",,2025-05-30
Identifiability Results for Multimodal Contrastive Learning,,ICLR,2023,"<a href=""https://arxiv.org/abs/2303.09166"" target=""_blank"">2303.09166</a>","<a href=""https://github.com/imantdaunhawer/multimodal-contrastive-learning"" target=""_blank"">imantdaunhawer</a>",2025-05-30
Tutorial on MultiModal Machine Learning,,ICML,2023,"<a href=""https://cmu-multicomp-lab.github.io/mmml-tutorial/icml2023/"" target=""_blank""></a>",,2025-05-30
Multimodal KDD 2023: International Workshop on Multimodal Learning,,KDD,2023,"<a href=""https://multimodal-kdd-2023.github.io"" target=""_blank"">multimodal-kdd-2</a>",,2025-05-30
Multimodal Learning with Transformers: A Survey,,TPAMI,2023,"<a href=""https://arxiv.org/abs/2206.06488"" target=""_blank"">2206.06488</a>",,2025-05-30
MMVAE+: Enhancing the Generative Quality of Multimodal VAEs without Compromises,,ICLR,2023,"<a href=""https://openreview.net/forum?id=sdQGxouELX"" target=""_blank"">forum?id=sdQGxou</a>","<a href=""https://github.com/epalu/mmvaeplus"" target=""_blank"">epalu</a>",2025-05-30
ViTag: Online WiFi Fine Time Measurements Aided Vision-Motion Identity Association in Multi-person Environments,,SECON,2022,"<a href=""https://ieeexplore.ieee.org/document/9918171"" target=""_blank"">9918171</a>","<a href=""https://github.com/bryanbocao/vitag"" target=""_blank"">bryanbocao</a>",2025-05-30
Learning to Answer Questions in Dynamic Audio-Visual Scenarios,,CVPR,2022,"<a href=""https://arxiv.org/abs/2203.14072"" target=""_blank"">2203.14072</a>",,2025-05-30
M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis,,IEEE TVCG,2022,"<a href=""https://arxiv.org/abs/2107.08264"" target=""_blank"">2107.08264</a>",,2025-05-30
Vi-Fi: Associating Moving Subjects across Vision and Wireless Sensors,,IPSN,2022,"<a href=""https://ieeexplore.ieee.org/document/9826015"" target=""_blank"">9826015</a>","<a href=""https://github.com/vifi2021/Vi-Fi"" target=""_blank"">vifi2021</a>",2025-05-30
Robust Contrastive Learning against Noisy Views,,arXiv,2022,"<a href=""https://arxiv.org/abs/2201.04309"" target=""_blank"">2201.04309</a>",,2025-05-30
Cooperative Learning for Multi-view Analysis,,arXiv,2022,"<a href=""https://arxiv.org/abs/2112.12337"" target=""_blank"">2112.12337</a>",,2025-05-30
TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation,,arXiv,2022,"<a href=""https://arxiv.org/abs/2208.01813"" target=""_blank"">2208.01813</a>","<a href=""https://github.com/HenryJunW/TAG"" target=""_blank"">HenryJunW</a>",2025-05-30
Balanced Multimodal Learning via On-the-fly Gradient Modulation,,CVPR,2022,"<a href=""https://arxiv.org/abs/2203.15332"" target=""_blank"">2203.15332</a>",,2025-05-30
Pretrained Transformers As Universal Computation Engines,,AAAI,2022,"<a href=""https://arxiv.org/abs/2103.05247"" target=""_blank"">2103.05247</a>",,2025-05-30
On the Limitations of Multimodal VAEs,,ICLR,2022,"<a href=""https://arxiv.org/abs/2110.04121"" target=""_blank"">2110.04121</a>","<a href=""https://openreview.net/attachment?id=w-CPUXXrAj&name=supplementary_material"" target=""_blank"">mentary_material</a>",2025-05-30
Unpaired Vision-Language Pre-training via Cross-Modal CutMix,,ICML,2022,"<a href=""https://arxiv.org/abs/2206.08919"" target=""_blank"">2206.08919</a>",,2025-05-30
VMLoc: Variational Fusion For Learning-Based Multimodal Camera Localization,,AAAI,2021,"<a href=""https://arxiv.org/abs/2003.07289"" target=""_blank"">2003.07289</a>",,2025-05-30
Core Challenges in Embodied Vision-Language Planning,,arXiv,2021,"<a href=""https://arxiv.org/abs/2106.13948"" target=""_blank"">2106.13948</a>",,2025-05-30
What Makes Multi-modal Learning Better than Single (Provably),,NeurIPS,2021,"<a href=""https://arxiv.org/abs/2106.04538"" target=""_blank"">2106.04538</a>",,2025-05-30
Efficient Multi-Modal Fusion with Diversity Analysis,,ACMMM,2021,"<a href=""https://dl.acm.org/doi/abs/10.1145/3474085.3475188"" target=""_blank"">3474085.3475188</a>",,2025-05-30
Attention Bottlenecks for Multimodal Fusion,,NeurIPS,2021,"<a href=""https://arxiv.org/abs/2107.00135"" target=""_blank"">2107.00135</a>",,2025-05-30
"Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers",,TACL,2021,"<a href=""https://arxiv.org/abs/2102.00529"" target=""_blank"">2102.00529</a>",,2025-05-30
Trusted Multi-View Classification,,ICLR,2021,"<a href=""https://openreview.net/forum?id=OOsR8BzCnl5"" target=""_blank"">forum?id=OOsR8Bz</a>","<a href=""https://github.com/hanmenghan/TMC"" target=""_blank"">hanmenghan</a>",2025-05-30
Multi-Modal Video Reasoning and Analyzing Competition,,ICCV,2021,"<a href=""https://sutdcv.github.io/multi-modal-video-reasoning"" target=""_blank"">multi-modal-vide</a>",,2025-05-30
"PolyViT: Co-training Vision Transformers on Images, Videos and Audio",,arXiv,2021,"<a href=""https://arxiv.org/abs/2111.12993"" target=""_blank"">2111.12993</a>",,2025-05-30
"VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",,NeurIPS,2021,"<a href=""https://arxiv.org/abs/2104.11178"" target=""_blank"">2104.11178</a>","<a href=""https://github.com/google-research/google-research/tree/master/vatt"" target=""_blank"">master</a>",2025-05-30
Zero-Shot Text-to-Image Generation,,ICML,2021,"<a href=""https://arxiv.org/abs/2102.12092"" target=""_blank"">2102.12092</a>","<a href=""https://github.com/openai/DALL-E"" target=""_blank"">openai</a>",2025-05-30
Grounding 'Grounding' in NLP,,ACL,2021,"<a href=""https://arxiv.org/abs/2106.02192"" target=""_blank"">2106.02192</a>",,2025-05-30
"MultiModalQA: complex question answering over text, tables and images",,ICLR,2021,"<a href=""https://openreview.net/forum?id=ee6W5UgQLa"" target=""_blank"">forum?id=ee6W5Ug</a>",,2025-05-30
SUTD-TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events,,CVPR,2021,"<a href=""https://openaccess.thecvf.com/content/CVPR2021/html/Xu_SUTD-TrafficQA_A_Question_Answering_Benchmark_and_an_Efficient_Network_for_CVPR_2021_paper.html"" target=""_blank"">Xu_SUTD-TrafficQ</a>","<a href=""https://github.com/SUTDCV/SUTD-TrafficQA"" target=""_blank"">SUTDCV</a>",2025-05-30
Multimodal Co-Attention Transformer for Survival Prediction in Gigapixel Whole Slide Images,,"ICCV,",2021,"<a href=""https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Multimodal_Co-Attention_Transformer_for_Survival_Prediction_in_Gigapixel_Whole_Slide_ICCV_2021_paper.html"" target=""_blank"">Chen_Multimodal_</a>",,2025-05-30
History Aware Multimodal Transformer for Vision-and-Language Navigation,,NeurIPS,2021,"<a href=""https://arxiv.org/abs/2110.13309"" target=""_blank"">2110.13309</a>","<a href=""https://cshizhe.github.io/projects/vln_hamt.html"" target=""_blank"">projects</a>",2025-05-30
Reconsidering Representation Alignment for Multi-view Clustering,,CVPR,2021,"<a href=""https://openaccess.thecvf.com/content/CVPR2021/html/Trosten_Reconsidering_Representation_Alignment_for_Multi-View_Clustering_CVPR_2021_paper.html"" target=""_blank"">Trosten_Reconsid</a>","<a href=""https://github.com/DanielTrosten/mvc"" target=""_blank"">DanielTrosten</a>",2025-05-30
MaRVL: Multicultural Reasoning over Vision and Language,,EMNLP,2021,"<a href=""https://arxiv.org/pdf/2109.13238"" target=""_blank"">2109.13238</a>","<a href=""https://marvl-challenge.github.io/"" target=""_blank"">marvl-challenge.github.io</a>",2025-05-30
Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation,,arXiv,2021,"<a href=""https://arxiv.org/abs/2111.05759"" target=""_blank"">2111.05759</a>",,2025-05-30
Non-Linear Consumption of Videos Using a Sequence of Personalized Multimodal Fragments,,IUI,2021,"<a href=""https://gaurav22verma.github.io/assets/papers/NonLinearConsumption.pdf"" target=""_blank"">NonLinearConsump</a>",,2025-05-30
Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models,,arXiv,2021,"<a href=""https://arxiv.org/abs/2104.08666"" target=""_blank"">2104.08666</a>",,2025-05-30
"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods",,JAIR,2021,"<a href=""https://doi.org/10.1613/jair.1.11688"" target=""_blank"">jair.1.11688</a>",,2025-05-30
"Detect, Reject, Correct: Crossmodal Compensation of Corrupted Sensors",,ICRA,2021,"<a href=""https://arxiv.org/abs/2012.00201"" target=""_blank"">2012.00201</a>",,2025-05-30
Generalized Multimodal ELBO,,ICLR,2021,"<a href=""https://openreview.net/forum?id=5Y21V0RDBV"" target=""_blank"">forum?id=5Y21V0R</a>","<a href=""https://github.com/thomassutter/MoPoE"" target=""_blank"">thomassutter</a>",2025-05-30
MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research,,NeurIPS,2021,"<a href=""https://arxiv.org/abs/2109.13202"" target=""_blank"">2109.13202</a>","<a href=""https://github.com/facebookresearch/minihack"" target=""_blank"">facebookresearch</a>",2025-05-30
SMIL: Multimodal Learning with Severely Missing Modality,,AAAI,2021,"<a href=""https://arxiv.org/abs/2103.05677"" target=""_blank"">2103.05677</a>",,2025-05-30
Grounded Language Learning Fast and Slow,,ICLR,2021,"<a href=""https://arxiv.org/abs/2009.01719"" target=""_blank"">2009.01719</a>",,2025-05-30
Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast,,IJCAI,2021,"<a href=""https://arxiv.org/abs/2204.14057"" target=""_blank"">2204.14057</a>","<a href=""https://github.com/Cocoxili/CMPC"" target=""_blank"">Cocoxili</a>",2025-05-30
Towards a Unified Foundation Model: Jointly Pre-Training Transformers on Unpaired Images and Text,,arXiv,2021,"<a href=""https://arxiv.org/abs/2112.07074"" target=""_blank"">2112.07074</a>",,2025-05-30
FLAVA: A Foundational Language And Vision Alignment Model,,arXiv,2021,"<a href=""https://arxiv.org/abs/2112.04482"" target=""_blank"">2112.04482</a>",,2025-05-30
Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer,,arXiv,2021,"<a href=""https://arxiv.org/abs/2102.10772"" target=""_blank"">2102.10772</a>",,2025-05-30
MultiBench: Multiscale Benchmarks for Multimodal Representation Learning,,NeurIPS,2021,"<a href=""https://arxiv.org/abs/2107.07502"" target=""_blank"">2107.07502</a>","<a href=""https://github.com/pliang279/MultiBench"" target=""_blank"">pliang279</a>",2025-05-30
Perceiver: General Perception with Iterative Attention,,ICML,2021,"<a href=""https://arxiv.org/abs/2103.03206"" target=""_blank"">2103.03206</a>","<a href=""https://github.com/deepmind/deepmind-research/tree/master/perceiver"" target=""_blank"">master</a>",2025-05-30
Learning Transferable Visual Models From Natural Language Supervision,,arXiv,2021,"<a href=""https://arxiv.org/abs/2103.00020"" target=""_blank"">2103.00020</a>",,2025-05-30
VinVL: Revisiting Visual Representations in Vision-Language Models,,arXiv,2021,"<a href=""https://arxiv.org/abs/2101.00529"" target=""_blank"">2101.00529</a>",,2025-05-30
A Variational Information Bottleneck Approach to Multi-Omics Data Integration,,AISTATS,2021,"<a href=""https://arxiv.org/abs/2102.03014"" target=""_blank"">2102.03014</a>","<a href=""https://github.com/chl8856/DeepIMV"" target=""_blank"">chl8856</a>",2025-05-30
Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling,,CVPR,2021,"<a href=""https://arxiv.org/abs/2102.06183"" target=""_blank"">2102.06183</a>","<a href=""https://github.com/jayleicn/ClipBERT"" target=""_blank"">jayleicn</a>",2025-05-30
Align before Fuse: Vision and Language Representation Learning with Momentum Distillation,,NeurIPS,2021,"<a href=""https://arxiv.org/abs/2107.07651"" target=""_blank"">2107.07651</a>",,2025-05-30
"Multimodal Co-learning: Challenges, Applications with Datasets, Recent Advances and Future Directions",,arXiv,2021,"<a href=""https://arxiv.org/abs/2107.13782"" target=""_blank"">2107.13782</a>",,2025-05-30
Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision,,ICML,2021,"<a href=""https://arxiv.org/abs/2102.05918"" target=""_blank"">2102.05918</a>",,2025-05-30
DABS: A Domain-Agnostic Benchmark for Self-Supervised Learning,,NeurIPS,2021,"<a href=""https://arxiv.org/abs/2111.12062"" target=""_blank"">2111.12062</a>",,2025-05-30
Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation,,ICRA,2021,"<a href=""https://arxiv.org/abs/2104.10674"" target=""_blank"">2104.10674</a>",,2025-05-30
ALFWorld: Aligning Text and Embodied Environments for Interactive Learning,,ICLR,2021,"<a href=""https://arxiv.org/abs/2010.03768"" target=""_blank"">2010.03768</a>","<a href=""http://alfworld.github.io/"" target=""_blank"">alfworld.github.io</a>",2025-05-30
Parameter Efficient Multimodal Transformers for Video Representation Learning,,ICLR,2021,"<a href=""https://arxiv.org/abs/2012.04124"" target=""_blank"">2012.04124</a>","<a href=""https://github.com/sangho-vision/avbert"" target=""_blank"">sangho-vision</a>",2025-05-30
"MURAL: Multimodal, Multitask Retrieval Across Languages",,arXiv,2021,"<a href=""https://arxiv.org/abs/2109.05125"" target=""_blank"">2109.05125</a>",,2025-05-30
Learning with Noisy Correspondence for Cross-modal Matching,,NeurIPS,2021,"<a href=""https://proceedings.neurips.cc/paper/2021/file/f5e62af885293cf4d511ceef31e61c80-Paper.pdf"" target=""_blank"">f5e62af885293cf4</a>","<a href=""https://github.com/XLearning-SCU/2021-NeurIPS-NCR"" target=""_blank"">XLearning-SCU</a>",2025-05-30
Towards Debiasing Sentence Representations,,ACL,2020,"<a href=""https://arxiv.org/abs/2007.08100"" target=""_blank"">2007.08100</a>","<a href=""https://github.com/pliang279/sent_debias"" target=""_blank"">pliang279</a>",2025-05-30
Advances in Language and Vision Research,,ACL,2020,"<a href=""https://alvr-workshop.github.io/"" target=""_blank""></a>",,2025-05-30
Shaping Visual Representations with Language for Few-shot Classification,,ACL,2020,"<a href=""https://arxiv.org/abs/1911.02683"" target=""_blank"">1911.02683</a>",,2025-05-30
Language to Network: Conditional Parameter Adaptation with Natural Language Descriptions,,ACL,2020,"<a href=""https://www.aclweb.org/anthology/2020.acl-main.625/"" target=""_blank""></a>",,2025-05-30
Grand Challenge and Workshop on Human Multimodal Language,,ACL,2020,"<a href=""http://multicomp.cs.cmu.edu/acl2020multimodalworkshop/"" target=""_blank""></a>",,2025-05-30
Music Gesture for Visual Sound Separation,,CVPR,2020,"<a href=""https://arxiv.org/abs/2004.09476"" target=""_blank"">2004.09476</a>",,2025-05-30
Labelling Unlabelled Videos from Scratch with Multi-modal Self-supervision,,NeurIPS,2020,"<a href=""https://arxiv.org/abs/2006.13662"" target=""_blank"">2006.13662</a>","<a href=""https://www.robots.ox.ac.uk/~vgg/research/selavi/"" target=""_blank"">research/selavi/</a>",2025-05-30
Self-Supervised Learning by Cross-Modal Audio-Video Clustering,,NeurIPS,2020,"<a href=""https://arxiv.org/abs/1911.12667"" target=""_blank"">1911.12667</a>","<a href=""https://github.com/HumamAlwassel/XDC"" target=""_blank"">HumamAlwassel</a>",2025-05-30
Self-supervised Disentanglement of Modality-specific and Shared Factors Improves Multimodal Generative Models,,GCPR,2020,"<a href=""https://rdcu.be/c8WUU"" target=""_blank"">c8WUU</a>","<a href=""https://github.com/imantdaunhawer/DMVAE"" target=""_blank"">imantdaunhawer</a>",2025-05-30
Multimodal Generative Learning Utilizing Jensen-Shannon-Divergence,,NeurIPS,2020,"<a href=""https://arxiv.org/abs/2006.08242"" target=""_blank"">2006.08242</a>","<a href=""https://github.com/thomassutter/mmjsd"" target=""_blank"">thomassutter</a>",2025-05-30
FairCVtest Demo: Understanding Bias in Multimodal Learning with a Testbed in Fair Automatic Recruitment,,ICMI,2020,"<a href=""https://arxiv.org/abs/2009.07025"" target=""_blank"">2009.07025</a>","<a href=""https://github.com/BiDAlab/FairCVtest"" target=""_blank"">BiDAlab</a>",2025-05-30
11-777 Multimodal Machine Learning,,Fall,2020,"<a href=""https://cmu-multicomp-lab.github.io/mmml-course/fall2020/"" target=""_blank""></a>",,2025-05-30
Generating Need-Adapted Multimodal Fragments,,IUI,2020,"<a href=""https://gaurav22verma.github.io/assets/MultimodalFragments.pdf"" target=""_blank"">MultimodalFragme</a>",,2025-05-30
Two Causal Principles for Improving Visual Dialog,,CVPR,2020,"<a href=""https://arxiv.org/abs/1911.10496"" target=""_blank"">1911.10496</a>",,2025-05-30
Adventures in Flatland: Perceiving Social Interactions Under Physical Dynamics,,CogSci,2020,"<a href=""https://www.tshu.io/HeiderSimmel/CogSci20/Flatland_CogSci20.pdf"" target=""_blank"">Flatland_CogSci2</a>",,2025-05-30
Multi-agent Communication meets Natural Language: Synergies between Functional and Structural Language Learning,,ACL,2020,"<a href=""https://arxiv.org/abs/2005.07064"" target=""_blank"">2005.07064</a>",,2025-05-30
Multimodal sensor fusion with differentiable filters,,IROS,2020,"<a href=""https://arxiv.org/abs/2010.13021"" target=""_blank"">2010.13021</a>",,2025-05-30
Concept2Robot: Learning Manipulation Concepts from Instructions and Human Demonstrations,,RSS,2020,"<a href=""http://www.roboticsproceedings.org/rss16/p082.pdf"" target=""_blank"">p082.pdf</a>",,2025-05-30
Imitating Interactive Intelligence,,arXiv,2020,"<a href=""https://arxiv.org/abs/2012.05672"" target=""_blank"">2012.05672</a>",,2025-05-30
Visual Agreement Regularized Training for Multi-Modal Machine Translation,,AAAI,2020,"<a href=""https://arxiv.org/abs/1912.12014"" target=""_blank"">1912.12014</a>",,2025-05-30
Neural Machine Translation with Universal Visual Representation,,ICLR,2020,"<a href=""https://openreview.net/forum?id=Byl8hhNYPS"" target=""_blank"">forum?id=Byl8hhN</a>","<a href=""https://github.com/cooelf/UVR-NMT"" target=""_blank"">cooelf</a>",2025-05-30
Multimodal Transformer for Multimodal Machine Translation,,ACL,2020,"<a href=""https://www.aclweb.org/anthology/2020.acl-main.400/"" target=""_blank""></a>",,2025-05-30
Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting,,ACL,2020,"<a href=""https://arxiv.org/abs/2005.03119"" target=""_blank"">2005.03119</a>",,2025-05-30
RTFM: Generalising to Novel Environment Dynamics via Reading,,ICLR,2020,"<a href=""https://arxiv.org/abs/1910.08210"" target=""_blank"">1910.08210</a>","<a href=""https://github.com/facebookresearch/RTFM"" target=""_blank"">facebookresearch</a>",2025-05-30
Embodied Multimodal Multitask Learning,,IJCAI,2020,"<a href=""https://arxiv.org/abs/1902.01385"" target=""_blank"">1902.01385</a>",,2025-05-30
Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training,,CVPR,2020,"<a href=""https://arxiv.org/abs/2002.10638"" target=""_blank"">2002.10638</a>","<a href=""https://github.com/weituo12321/PREVALENT"" target=""_blank"">weituo12321</a>",2025-05-30
Improving Vision-and-Language Navigation with Image-Text Pairs from the Web,,ECCV,2020,"<a href=""https://arxiv.org/abs/2004.14973"" target=""_blank"">2004.14973</a>",,2025-05-30
VIOLIN: A Large-Scale Dataset for Video-and-Language Inference,,CVPR,2020,"<a href=""https://arxiv.org/abs/2003.11618"" target=""_blank"">2003.11618</a>","<a href=""https://github.com/jimmy646/violin"" target=""_blank"">jimmy646</a>",2025-05-30
Human in the Loop Dialogue Systems,,NeurIPS,2020,"<a href=""https://sites.google.com/view/hlds-2020/home"" target=""_blank"">home</a>",,2025-05-30
Visual Grounding in Video for Unsupervised Word Translation,,CVPR,2020,"<a href=""https://arxiv.org/abs/2003.05078"" target=""_blank"">2003.05078</a>","<a href=""https://github.com/gsig/visual-grounding"" target=""_blank"">gsig</a>",2025-05-30
What Does BERT with Vision Look At?,,ACL,2020,"<a href=""https://www.aclweb.org/anthology/2020.acl-main.469/"" target=""_blank""></a>",,2025-05-30
The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes,,NeurIPS,2020,"<a href=""https://arxiv.org/abs/2005.04790"" target=""_blank"">2005.04790</a>","<a href=""https://ai.facebook.com/blog/hateful-memes-challenge-and-data-set/"" target=""_blank"">ge-and-data-set/</a>",2025-05-30
"Deep Multi-modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges",,IEEE TITS,2020,"<a href=""https://arxiv.org/pdf/1902.07830.pdf"" target=""_blank"">1902.07830.pdf</a>",,2025-05-30
nuScenes: A multimodal dataset for autonomous driving,,CVPR,2020,"<a href=""https://openaccess.thecvf.com/content_CVPR_2020/papers/Caesar_nuScenes_A_Multimodal_Dataset_for_Autonomous_Driving_CVPR_2020_paper.pdf"" target=""_blank"">Caesar_nuScenes_</a>",,2025-05-30
Multimodal End-to-End Autonomous Driving,,arXiv,2020,"<a href=""https://arxiv.org/abs/1906.03199"" target=""_blank"">1906.03199</a>",,2025-05-30
A Multimodal Event-driven LSTM Model for Stock Prediction Using Online News,,TKDE,2020,"<a href=""https://ailab-ua.github.io/courses/resources/Qing_TKDE_2020.pdf"" target=""_blank"">Qing_TKDE_2020.p</a>",,2025-05-30
Pathomic Fusion: An Integrated Framework for Fusing Histopathology and Genomic Features for Cancer Diagnosis and Prognosis,,"IEEE TMI,",2020,"<a href=""https://arxiv.org/abs/1912.08937"" target=""_blank"">1912.08937</a>",,2025-05-30
Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!,,EMNLP,2020,"<a href=""https://www.aclweb.org/anthology/2020.emnlp-main.62.pdf"" target=""_blank"">2020.emnlp-main.</a>",,2025-05-30
Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA,,CVPR,2020,"<a href=""https://arxiv.org/abs/1911.06258"" target=""_blank"">1911.06258</a>",,2025-05-30
ManyModalQA: Modality Disambiguation and QA over Diverse Inputs,,AAAI,2020,"<a href=""https://arxiv.org/abs/2001.08034"" target=""_blank"">2001.08034</a>","<a href=""https://github.com/hannandarryl/ManyModalQA"" target=""_blank"">hannandarryl</a>",2025-05-30
Human-centric dialog training via offline reinforcement learning,,EMNLP,2020,"<a href=""https://arxiv.org/abs/2010.05848"" target=""_blank"">2010.05848</a>","<a href=""https://github.com/natashamjaques/neural_chat/tree/master/BatchRL"" target=""_blank"">master</a>",2025-05-30
Human And Machine in-the-Loop Evaluation and Learning Strategies,,NeurIPS,2020,"<a href=""https://hamlets-workshop.github.io/"" target=""_blank""></a>",,2025-05-30
PET-Guided Attention Network for Segmentation of Lung Tumors from PET/CT Images,,GCPR,2020,"<a href=""https://rdcu.be/c8WWl"" target=""_blank"">c8WWl</a>","<a href=""https://github.com/pvk95/PAG"" target=""_blank"">pvk95</a>",2025-05-30
Self-Supervised MultiModal Versatile Networks,,NeurIPS,2020,"<a href=""https://arxiv.org/abs/2006.16228"" target=""_blank"">2006.16228</a>","<a href=""https://tfhub.dev/deepmind/mmv/s3d/1"" target=""_blank"">epmind/mmv/s3d/1</a>",2025-05-30
What Makes Training Multi-Modal Classification Networks Hard?,,CVPR,2020,"<a href=""https://arxiv.org/abs/1905.12681"" target=""_blank"">1905.12681</a>",,2025-05-30
"Vokenization: Improving Language Understanding via Contextualized, Visually-Grounded Supervision",,EMNLP,2020,"<a href=""https://arxiv.org/abs/2010.06775"" target=""_blank"">2010.06775</a>",,2025-05-30
12-in-1: Multi-Task Vision and Language Representation Learning,,CVPR,2020,"<a href=""https://arxiv.org/abs/1912.02315"" target=""_blank"">1912.02315</a>","<a href=""https://github.com/facebookresearch/vilbert-multi-task"" target=""_blank"">facebookresearch</a>",2025-05-30
Watching the World Go By: Representation Learning from Unlabeled Videos,,arXiv,2020,"<a href=""https://arxiv.org/abs/2003.07990"" target=""_blank"">2003.07990</a>",,2025-05-30
CoMIR: Contrastive Multimodal Image Representation for Registration,,NeurIPS,2020,"<a href=""https://arxiv.org/pdf/2006.06325.pdf"" target=""_blank"">2006.06325.pdf</a>","<a href=""https://github.com/MIDA-group/CoMIR"" target=""_blank"">MIDA-group</a>",2025-05-30
Large-Scale Adversarial Training for Vision-and-Language Representation Learning,,NeurIPS,2020,"<a href=""https://arxiv.org/abs/2006.06195"" target=""_blank"">2006.06195</a>","<a href=""https://github.com/zhegan27/VILLA"" target=""_blank"">zhegan27</a>",2025-05-30
Deep-HOSeq: Deep Higher-Order Sequence Fusion for Multimodal Sentiment Analysis,,ICDM,2020,"<a href=""https://arxiv.org/pdf/2010.08218.pdf"" target=""_blank"">2010.08218.pdf</a>",,2025-05-30
Experience Grounds Language,,EMNLP,2020,"<a href=""https://arxiv.org/abs/2004.10151"" target=""_blank"">2004.10151</a>",,2025-05-30
Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies,,NeurIPS,2020,"<a href=""https://arxiv.org/abs/2010.10802"" target=""_blank"">2010.10802</a>","<a href=""https://github.com/itaigat/removing-bias-in-multi-modal-classifiers"" target=""_blank"">itaigat</a>",2025-05-30
Recent Advances in Vision-and-Language Research,,CVPR,2020,"<a href=""https://rohit497.github.io/Recent-Advances-in-Vision-and-Language-Research/"" target=""_blank""></a>",,2025-05-30
Deep Multimodal Fusion by Channel Exchanging,,NeurIPS,2020,"<a href=""https://arxiv.org/abs/2011.05005?context=cs.LG"" target=""_blank"">2011.05005?conte</a>","<a href=""https://github.com/yikaiw/CEN"" target=""_blank"">yikaiw</a>",2025-05-30
Integrating Multimodal Information in Large Pretrained Transformers,,ACL,2020,"<a href=""https://arxiv.org/abs/1908.05787"" target=""_blank"">1908.05787</a>",,2025-05-30
"Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision",,EMNLP,2020,"<a href=""https://arxiv.org/abs/2010.06775"" target=""_blank"">2010.06775</a>","<a href=""https://github.com/airsplay/vokenization"" target=""_blank"">airsplay</a>",2025-05-30
Foundations of Multimodal Co-learning,,Information Fusion,2020,"<a href=""https://www.sciencedirect.com/science/article/pii/S1566253520303006"" target=""_blank"">S156625352030300</a>",,2025-05-30
From Recognition to Cognition: Visual Commonsense Reasoning,,CVPR,2019,"<a href=""https://arxiv.org/abs/1811.10830"" target=""_blank"">1811.10830</a>","<a href=""https://visualcommonsense.com/"" target=""_blank"">commonsense.com/</a>",2025-05-30
CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge,,NAACL,2019,"<a href=""https://arxiv.org/abs/1811.00937"" target=""_blank"">1811.00937</a>",,2025-05-30
Lattice Transformer for Speech Translation,,ACL,2019,"<a href=""https://arxiv.org/abs/1906.05551"" target=""_blank"">1906.05551</a>",,2025-05-30
"See, Feel, Act: Hierarchical Learning for Complex Manipulation Skills with Multi-sensory Fusion",,Science Robotics,2019,"<a href=""https://robotics.sciencemag.org/content/4/26/eaav3123"" target=""_blank"">eaav3123</a>",,2025-05-30
Grounded Video Description,,CVPR,2019,"<a href=""https://arxiv.org/abs/1812.06587"" target=""_blank"">1812.06587</a>",,2025-05-30
"Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions",,CVPR,2019,"<a href=""https://arxiv.org/abs/1811.10652"" target=""_blank"">1811.10652</a>",,2025-05-30
Multilevel Language and Vision Integration for Text-to-Clip Retrieval,,AAAI,2019,"<a href=""https://arxiv.org/abs/1804.05113"" target=""_blank"">1804.05113</a>","<a href=""https://github.com/VisionLearningGroup/Text-to-Clip_Retrieval"" target=""_blank"">VisionLearningGroup</a>",2025-05-30
Binary Image Selection (BISON): Interpretable Evaluation of Visual Grounding,,arXiv,2019,"<a href=""https://arxiv.org/abs/1901.06595"" target=""_blank"">1901.06595</a>","<a href=""https://github.com/facebookresearch/binary-image-selection"" target=""_blank"">facebookresearch</a>",2025-05-30
CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog,,NAACL,2019,"<a href=""https://www.aclweb.org/anthology/N19-1058"" target=""_blank"">N19-1058</a>","<a href=""https://github.com/satwikkottur/clevr-dialog"" target=""_blank"">satwikkottur</a>",2025-05-30
MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations,,ACL,2019,"<a href=""https://arxiv.org/abs/1810.02508"" target=""_blank"">1810.02508</a>","<a href=""http://affective-meld.github.io/"" target=""_blank"">affective-meld.github.io</a>",2025-05-30
Early Fusion for Goal Directed Robotic Vision,,IROS,2019,"<a href=""https://arxiv.org/abs/1811.08824"" target=""_blank"">1811.08824</a>",,2025-05-30
Joint Event Detection and Description in Continuous Video Streams,,WACVW,2019,"<a href=""https://arxiv.org/abs/1802.10250"" target=""_blank"">1802.10250</a>",,2025-05-30
Deep Multimodal Representation Learning: A Survey,,arXiv,2019,"<a href=""https://ieeexplore.ieee.org/abstract/document/8715409"" target=""_blank"">8715409</a>",,2025-05-30
Learning to Compose and Reason with Language Tree Structures for Visual Grounding,,TPAMI,2019,"<a href=""https://arxiv.org/abs/1906.01784"" target=""_blank"">1906.01784</a>",,2025-05-30
"Multimodal Intelligence: Representation Learning, Information Fusion, and Applications",,arXiv,2019,"<a href=""https://arxiv.org/abs/1911.03977"" target=""_blank"">1911.03977</a>",,2025-05-30
Leveraging Medical Visual Question Answering with Supporting Facts,,arXiv,2019,"<a href=""https://arxiv.org/abs/1905.12008"" target=""_blank"">1905.12008</a>",,2025-05-30
Multimodal Deep Learning for Finance: Integrating and Forecasting International Stock Markets,,,2019,"<a href=""https://arxiv.org/abs/1903.06478"" target=""_blank"">1903.06478</a>",,2025-05-30
Multimodal Machine Learning: A Survey and Taxonomy,,TPAMI,2019,"<a href=""https://arxiv.org/abs/1705.09406"" target=""_blank"">1705.09406</a>",,2025-05-30
Dynamic Fusion for Multimodal Data,,arXiv,2019,"<a href=""https://arxiv.org/abs/1911.03821"" target=""_blank"">1911.03821</a>",,2025-05-30
Closing the Loop Between Vision and Language & LSMD Challenge,,ICCV,2019,"<a href=""https://sites.google.com/site/iccv19clvllsmdc/"" target=""_blank""></a>",,2025-05-30
Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation,,ACL,2019,"<a href=""https://arxiv.org/abs/1906.01199"" target=""_blank"">1906.01199</a>",,2025-05-30
Audio Caption: Listen and Tell,,ICASSP,2019,"<a href=""https://arxiv.org/abs/1902.09254"" target=""_blank"">1902.09254</a>",,2025-05-30
Audio-Linguistic Embeddings for Spoken Sentences,,ICASSP,2019,"<a href=""https://arxiv.org/abs/1902.07817"" target=""_blank"">1902.07817</a>",,2025-05-30
From Semi-supervised to Almost-unsupervised Speech Recognition with Very-low Resource by Jointly Learning Phonetic Structures from Audio and Text Embeddings,,arXiv,2019,"<a href=""https://arxiv.org/abs/1904.05078"" target=""_blank"">1904.05078</a>",,2025-05-30
Probabilistic Neural-symbolic Models for Interpretable Visual Question Answering,,ICML,2019,"<a href=""https://arxiv.org/abs/1902.07864"" target=""_blank"">1902.07864</a>","<a href=""https://github.com/kdexd/probnmn-clevr"" target=""_blank"">kdexd</a>",2025-05-30
End-to-end Facial and Physiological Model for Affective Computing and Applications,,arXiv,2019,"<a href=""https://arxiv.org/abs/1912.04711"" target=""_blank"">1912.04711</a>",,2025-05-30
Habitat: A Platform for Embodied AI Research,,ICCV,2019,"<a href=""https://arxiv.org/abs/1904.01201"" target=""_blank"">1904.01201</a>","<a href=""https://aihabitat.org/"" target=""_blank"">//aihabitat.org/</a>",2025-05-30
Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey,,ACM TOMM,2019,"<a href=""https://arxiv.org/abs/1911.05609"" target=""_blank"">1911.05609</a>",,2025-05-30
Attention Based Natural Language Grounding by Navigating Virtual Environment,,IEEE WACV,2019,"<a href=""https://arxiv.org/abs/1804.08454"" target=""_blank"">1804.08454</a>",,2025-05-30
Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation,,CVPR,2019,"<a href=""https://arxiv.org/abs/1903.02547"" target=""_blank"">1903.02547</a>","<a href=""https://github.com/Kelym/FAST"" target=""_blank"">Kelym</a>",2025-05-30
Multi-modal Discriminative Model for Vision-and-Language Navigation,,NAACL SpLU-RoboNLP Workshop,2019,"<a href=""https://www.aclweb.org/anthology/W19-1605"" target=""_blank"">W19-1605</a>",,2025-05-30
Self-Monitoring Navigation Agent via Auxiliary Progress Estimation,,ICLR,2019,"<a href=""https://arxiv.org/abs/1901.03035"" target=""_blank"">1901.03035</a>","<a href=""https://github.com/chihyaoma/selfmonitoring-agent"" target=""_blank"">chihyaoma</a>",2025-05-30
From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following,,ICLR,2019,"<a href=""https://arxiv.org/abs/1902.07742"" target=""_blank"">1902.07742</a>",,2025-05-30
"Read, Watch, and Move: Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos",,AAAI,2019,"<a href=""https://arxiv.org/abs/1901.06829"" target=""_blank"">1901.06829</a>",,2025-05-30
Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout,,NAACL,2019,"<a href=""https://www.aclweb.org/anthology/N19-1268"" target=""_blank"">N19-1268</a>","<a href=""https://github.com/airsplay/R2R-EnvDrop"" target=""_blank"">airsplay</a>",2025-05-30
Language as an Abstraction for Hierarchical Deep Reinforcement Learning,,NeurIPS,2019,"<a href=""https://arxiv.org/abs/1906.07343"" target=""_blank"">1906.07343</a>",,2025-05-30
Towards Multimodal Sarcasm Detection (An Obviously_Perfect Paper),,ACL,2019,"<a href=""https://arxiv.org/abs/1906.01815"" target=""_blank"">1906.01815</a>","<a href=""https://github.com/soujanyaporia/MUStARD"" target=""_blank"">soujanyaporia</a>",2025-05-30
Learning to Speak and Act in a Fantasy Text Adventure Game,,arXiv,2019,"<a href=""https://arxiv.org/abs/1903.03094"" target=""_blank"">1903.03094</a>","<a href=""https://parl.ai/projects/light/"" target=""_blank"">/projects/light/</a>",2025-05-30
Learning Representations by Maximizing Mutual Information Across Views,,arXiv,2019,"<a href=""https://arxiv.org/abs/1906.00910"" target=""_blank"">1906.00910</a>","<a href=""https://github.com/Philip-Bachman/amdim-public"" target=""_blank"">Philip-Bachman</a>",2025-05-30
OmniNet: A Unified Architecture for Multi-modal Multi-task Learning,,arXiv,2019,"<a href=""https://arxiv.org/abs/1907.07804"" target=""_blank"">1907.07804</a>","<a href=""https://github.com/subho406/OmniNet"" target=""_blank"">subho406</a>",2025-05-30
Visual Concept-Metaconcept Learning,,NeurIPS,2019,"<a href=""https://papers.nips.cc/paper/8745-visual-concept-metaconcept-learning.pdf"" target=""_blank"">8745-visual-conc</a>","<a href=""http://vcml.csail.mit.edu/"" target=""_blank"">l.csail.mit.edu/</a>",2025-05-30
Learning Video Representations using Contrastive Bidirectional Transformer,,arXiv,2019,"<a href=""https://arxiv.org/abs/1906.05743"" target=""_blank"">1906.05743</a>",,2025-05-30
"VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research",,ICCV,2019,"<a href=""https://arxiv.org/abs/1904.03493"" target=""_blank"">1904.03493</a>","<a href=""http://vatex.org/main/index.html"" target=""_blank"">/main/index.html</a>",2025-05-30
The Regretful Navigation Agent for Vision-and-Language Navigation,,CVPR,2019,"<a href=""https://arxiv.org/abs/1903.01602"" target=""_blank"">1903.01602</a>","<a href=""https://github.com/chihyaoma/regretful-agent"" target=""_blank"">chihyaoma</a>",2025-05-30
Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation,,CVPR,2019,"<a href=""https://arxiv.org/abs/1811.10092"" target=""_blank"">1811.10092</a>",,2025-05-30
Touchdown: Natural Language Navigation and Spatial Reasoning in Visual Street Environments,,CVPR,2019,"<a href=""https://arxiv.org/abs/1811.12354"" target=""_blank"">1811.12354</a>","<a href=""https://github.com/lil-lab/touchdown"" target=""_blank"">lil-lab</a>",2025-05-30
Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation,,ACL,2019,"<a href=""https://arxiv.org/abs/1906.00347"" target=""_blank"">1906.00347</a>",,2025-05-30
Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation,,ACL,2019,"<a href=""https://arxiv.org/abs/1905.12255"" target=""_blank"">1905.12255</a>",,2025-05-30
Hierarchical Decision Making by Generating and Following Natural Language Instructions,,arXiv,2019,"<a href=""https://arxiv.org/abs/1906.00744"" target=""_blank"">1906.00744</a>","<a href=""https://www.minirts.net/"" target=""_blank"">www.minirts.net/</a>",2025-05-30
Vision-and-Dialog Navigation,,arXiv,2019,"<a href=""https://arxiv.org/abs/1907.04957"" target=""_blank"">1907.04957</a>","<a href=""https://github.com/mmurray/cvdn"" target=""_blank"">mmurray</a>",2025-05-30
VideoNavQA: Bridging the Gap between Visual and Embodied Question Answering,,BMVC,2019,"<a href=""https://arxiv.org/abs/1908.04950"" target=""_blank"">1908.04950</a>","<a href=""https://github.com/catalina17/VideoNavQA"" target=""_blank"">catalina17</a>",2025-05-30
ViCo: Word Embeddings from Visual Co-occurrences,,ICCV,2019,"<a href=""https://arxiv.org/abs/1908.08527"" target=""_blank"">1908.08527</a>","<a href=""https://github.com/BigRedT/vico"" target=""_blank"">BigRedT</a>",2025-05-30
Unified Visual-Semantic Embeddings: Bridging Vision and Language With Structured Meaning Representations,,CVPR,2019,"<a href=""http://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Unified_Visual-Semantic_Embeddings_Bridging_Vision_and_Language_With_Structured_Meaning_CVPR_2019_paper.pdf"" target=""_blank"">Wu_Unified_Visua</a>",,2025-05-30
Multi-Task Learning of Hierarchical Vision-Language Representation,,CVPR,2019,"<a href=""https://arxiv.org/abs/1812.00500"" target=""_blank"">1812.00500</a>",,2025-05-30
Learning Factorized Multimodal Representations,,ICLR,2019,"<a href=""https://arxiv.org/abs/1806.06176"" target=""_blank"">1806.06176</a>","<a href=""https://github.com/pliang279/factorized/"" target=""_blank"">factorized</a>",2025-05-30
Simultaneously Learning Vision and Feature-based Control Policies for Real-world Ball-in-a-Cup,,RSS,2019,"<a href=""https://arxiv.org/abs/1902.04706"" target=""_blank"">1902.04706</a>",,2025-05-30
Probabilistic Multimodal Modeling for Human-Robot Interaction Tasks,,RSS,2019,"<a href=""http://www.roboticsproceedings.org/rss15/p47.pdf"" target=""_blank"">p47.pdf</a>",,2025-05-30
Probing the Need for Visual Context in Multimodal Machine Translation,,NAACL,2019,"<a href=""https://www.aclweb.org/anthology/N19-1422"" target=""_blank"">N19-1422</a>",,2025-05-30
Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks,,ICRA,2019,"<a href=""https://arxiv.org/abs/1810.10191"" target=""_blank"">1810.10191</a>",,2025-05-30
MUREL: Multimodal Relational Reasoning for Visual Question Answering,,CVPR,2019,"<a href=""https://arxiv.org/abs/1902.09487"" target=""_blank"">1902.09487</a>","<a href=""https://github.com/Cadene/murel.bootstrap.pytorch"" target=""_blank"">Cadene</a>",2025-05-30
Social-IQ: A Question Answering Benchmark for Artificial Social Intelligence,,CVPR,2019,"<a href=""http://openaccess.thecvf.com/content_CVPR_2019/html/Zadeh_Social-IQ_A_Question_Answering_Benchmark_for_Artificial_Social_Intelligence_CVPR_2019_paper.html"" target=""_blank"">Zadeh_Social-IQ_</a>","<a href=""https://github.com/A2Zadeh/Social-IQ"" target=""_blank"">A2Zadeh</a>",2025-05-30
Fusion of Detected Objects in Text for Visual Question Answering,,arXiv,2019,"<a href=""https://arxiv.org/abs/1908.05054"" target=""_blank"">1908.05054</a>",,2025-05-30
OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge,,CVPR,2019,"<a href=""https://arxiv.org/abs/1906.00067"" target=""_blank"">1906.00067</a>","<a href=""http://okvqa.allenai.org/"" target=""_blank"">vqa.allenai.org/</a>",2025-05-30
Beyond Vision and Language: Integrating Real-World Knowledge,,EMNLP,2019,"<a href=""https://www.lantern.uni-saarland.de/"" target=""_blank""></a>",,2025-05-30
"Capture, Learning, and Synthesis of 3D Speaking Styles",,CVPR,2019,"<a href=""https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/510/paper_final.pdf"" target=""_blank"">paper_final.pdf</a>","<a href=""https://github.com/TimoBolkart/voca"" target=""_blank"">TimoBolkart</a>",2025-05-30
Disjoint Mapping Network for Cross-modal Matching of Voices and Faces,,ICLR,2019,"<a href=""https://arxiv.org/abs/1807.04836"" target=""_blank"">1807.04836</a>",,2025-05-30
Reconstructing Faces from Voices,,NeurIPS,2019,"<a href=""https://arxiv.org/abs/1905.10604"" target=""_blank"">1905.10604</a>","<a href=""https://github.com/cmu-mlsp/reconstructing_faces_from_voices"" target=""_blank"">cmu-mlsp</a>",2025-05-30
Speech2Face: Learning the Face Behind a Voice,,CVPR,2019,"<a href=""https://arxiv.org/abs/1905.09773"" target=""_blank"">1905.09773</a>","<a href=""https://speech2face.github.io/"" target=""_blank"">speech2face.github.io</a>",2025-05-30
Wav2Pix: Speech-conditioned Face Generation using Generative Adversarial Networks,,ICASSP,2019,"<a href=""https://arxiv.org/abs/1903.10195"" target=""_blank"">1903.10195</a>","<a href=""https://imatge-upc.github.io/wav2pix/"" target=""_blank"">wav2pix</a>",2025-05-30
Neural Language Modeling with Visual Features,,arXiv,2019,"<a href=""https://arxiv.org/abs/1903.02930"" target=""_blank"">1903.02930</a>",,2025-05-30
Found in Translation: Learning Robust Joint Representations by Cyclic Translations Between Modalities,,AAAI,2019,"<a href=""https://arxiv.org/abs/1812.07809"" target=""_blank"">1812.07809</a>","<a href=""https://github.com/hainow/MCTN"" target=""_blank"">hainow</a>",2025-05-30
Visually Grounded Interaction and Language,,NeurIPS,2019,"<a href=""https://vigilworkshop.github.io/"" target=""_blank""></a>",,2025-05-30
Emergence of Compositional Language with Deep Generational Transmission,,ICML,2019,"<a href=""https://arxiv.org/abs/1904.09067"" target=""_blank"">1904.09067</a>",,2025-05-30
On the Pitfalls of Measuring Emergent Communication,,AAMAS,2019,"<a href=""https://arxiv.org/abs/1903.05168"" target=""_blank"">1903.05168</a>","<a href=""https://github.com/facebookresearch/measuring-emergent-comm"" target=""_blank"">facebookresearch</a>",2025-05-30
Self-Supervised Learning from Web Data for Multimodal Retrieval,,arXiv,2019,"<a href=""https://arxiv.org/abs/1901.02004"" target=""_blank"">1901.02004</a>",,2025-05-30
Heterogeneous Graph Learning for Visual Commonsense Reasoning,,NeurIPS,2019,"<a href=""https://arxiv.org/abs/1910.11475"" target=""_blank"">1910.11475</a>",,2025-05-30
Emergent Communication: Towards Natural Language,,NeurIPS,2019,"<a href=""https://sites.google.com/view/emecom2019"" target=""_blank"">emecom2019</a>",,2025-05-30
Workshop on Multimodal Understanding and Learning for Embodied Applications,,ACM Multimedia,2019,"<a href=""https://sites.google.com/view/mulea2019/home"" target=""_blank"">home</a>",,2025-05-30
The How2 Challenge: New Tasks for Vision & Language,,ICML,2019,"<a href=""https://srvk.github.io/how2-challenge/"" target=""_blank""></a>",,2025-05-30
Language2Pose: Natural Language Grounded Pose Forecasting,,3DV,2019,"<a href=""https://arxiv.org/abs/1907.01108"" target=""_blank"">1907.01108</a>","<a href=""http://chahuja.com/language2pose/"" target=""_blank"">m/language2pose/</a>",2025-05-30
Visual Question Answering and Dialog,,CVPR,2019,"<a href=""https://visualqa.org/workshop.html"" target=""_blank"">workshop.html</a>",,2025-05-30
Learning Affective Correspondence between Music and Image,,ICASSP,2019,"<a href=""https://arxiv.org/abs/1904.00150"" target=""_blank"">1904.00150</a>",,2025-05-30
Few-shot Video-to-Video Synthesis,,NeurIPS,2019,"<a href=""https://arxiv.org/abs/1910.12713"" target=""_blank"">1910.12713</a>","<a href=""https://nvlabs.github.io/few-shot-vid2vid/"" target=""_blank"">few-shot-vid2vid</a>",2025-05-30
Variational Mixture-of-Experts Autoencodersfor Multi-Modal Deep Generative Models,,NeurIPS,2019,"<a href=""https://arxiv.org/pdf/1911.03393.pdf"" target=""_blank"">1911.03393.pdf</a>","<a href=""https://github.com/iffsid/mmvae"" target=""_blank"">iffsid</a>",2025-05-30
Factorized Inference in Deep Markov Models for Incomplete Multimodal Time Series,,arXiv,2019,"<a href=""https://arxiv.org/abs/1905.13570"" target=""_blank"">1905.13570</a>",,2025-05-30
Learning Representations from Imperfect Time Series Data via Tensor Rank Regularization,,ACL,2019,"<a href=""https://arxiv.org/abs/1907.01011"" target=""_blank"">1907.01011</a>",,2025-05-30
Multi-modal Video Analysis and Moments in Time Challenge,,ICCV,2019,"<a href=""https://sites.google.com/view/multimodalvideo/"" target=""_blank""></a>",,2025-05-30
Multi-modal Learning from Videos,,CVPR,2019,"<a href=""https://sites.google.com/view/mmlv/home"" target=""_blank"">home</a>",,2025-05-30
A Logical Model for Supporting Social Commonsense Knowledge Acquisition,,arXiv,2019,"<a href=""https://arxiv.org/abs/1912.11599"" target=""_blank"">1912.11599</a>",,2025-05-30
Multimodal Explanations by Predicting Counterfactuality in Videos,,CVPR,2019,"<a href=""https://arxiv.org/abs/1812.01263"" target=""_blank"">1812.01263</a>",,2025-05-30
Multimodal Learning and Applications Workshop,,CVPR,2019,"<a href=""https://mula-workshop.github.io/"" target=""_blank""></a>",,2025-05-30
Habitat: Embodied Agents Challenge and Workshop,,CVPR,2019,"<a href=""https://aihabitat.org/workshop/"" target=""_blank""></a>",,2025-05-30
Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs,,AKBC,2019,"<a href=""https://arxiv.org/abs/1709.02314"" target=""_blank"">1709.02314</a>",,2025-05-30
MMKG: Multi-Modal Knowledge Graphs,,ESWC,2019,"<a href=""https://arxiv.org/abs/1903.05485"" target=""_blank"">1903.05485</a>",,2025-05-30
GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering,,CVPR,2019,"<a href=""https://arxiv.org/abs/1902.09506"" target=""_blank"">1902.09506</a>","<a href=""https://cs.stanford.edu/people/dorarad/gqa/"" target=""_blank"">ple/dorarad/gqa/</a>",2025-05-30
Learning Individual Styles of Conversational Gesture,,CVPR,2019,"<a href=""https://arxiv.org/abs/1906.04160"" target=""_blank"">1906.04160</a>","<a href=""http://people.eecs.berkeley.edu/~shiry/speech2gesture"" target=""_blank"">y/speech2gesture</a>",2025-05-30
Translate-to-Recognize Networks for RGB-D Scene Recognition,,CVPR,2019,"<a href=""https://openaccess.thecvf.com/content_CVPR_2019/papers/Du_Translate-to-Recognize_Networks_for_RGB-D_Scene_Recognition_CVPR_2019_paper.pdf"" target=""_blank"">Du_Translate-to-</a>","<a href=""https://github.com/ownstyledu/Translate-to-Recognize-Networks"" target=""_blank"">ownstyledu</a>",2025-05-30
Multimodal Transformer for Unaligned Multimodal Language Sequences,,ACL,2019,"<a href=""https://arxiv.org/abs/1906.00295"" target=""_blank"">1906.00295</a>","<a href=""https://github.com/yaohungt/Multimodal-Transformer"" target=""_blank"">yaohungt</a>",2025-05-30
RUBi: Reducing Unimodal Biases in Visual Question Answering,,NeurIPS,2019,"<a href=""https://arxiv.org/abs/1906.10169"" target=""_blank"">1906.10169</a>","<a href=""https://github.com/cdancette/rubi.bootstrap.pytorch"" target=""_blank"">cdancette</a>",2025-05-30
Distilling Translations with Visual Awareness,,ACL,2019,"<a href=""https://arxiv.org/pdf/1906.07701"" target=""_blank"">1906.07701</a>",,2025-05-30
Interactive Language Learning by Question Answering,,EMNLP,2019,"<a href=""https://arxiv.org/abs/1908.10909"" target=""_blank"">1908.10909</a>","<a href=""https://github.com/xingdi-eric-yuan/qait_public"" target=""_blank"">xingdi-eric-yuan</a>",2025-05-30
DeepCU: Integrating Both Common and Unique Latent Information for Multimodal Sentiment Analysis,,IJCAI,2019,"<a href=""https://www.ijcai.org/proceedings/2019/503"" target=""_blank"">503</a>","<a href=""https://github.com/sverma88/DeepCU-IJCAI19"" target=""_blank"">sverma88</a>",2025-05-30
Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling,,NeurIPS,2019,"<a href=""https://papers.nips.cc/paper/9381-deep-multimodal-multilinear-fusion-with-high-order-polynomial-pooling"" target=""_blank"">9381-deep-multim</a>",,2025-05-30
XFlow: Cross-modal Deep Neural Networks for Audiovisual Classification,,IEEE TNNLS,2019,"<a href=""https://ieeexplore.ieee.org/abstract/document/8894404"" target=""_blank"">8894404</a>","<a href=""https://github.com/catalina17/XFlow"" target=""_blank"">catalina17</a>",2025-05-30
MFAS: Multimodal Fusion Architecture Search,,CVPR,2019,"<a href=""https://arxiv.org/abs/1903.06496"" target=""_blank"">1903.06496</a>",,2025-05-30
"The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",,ICLR,2019,"<a href=""https://arxiv.org/abs/1904.12584"" target=""_blank"">1904.12584</a>","<a href=""http://nscl.csail.mit.edu/"" target=""_blank"">l.csail.mit.edu/</a>",2025-05-30
Video Relationship Reasoning using Gated Spatio-Temporal Energy Graph,,CVPR,2019,"<a href=""https://arxiv.org/abs/1903.10547"" target=""_blank"">1903.10547</a>","<a href=""https://github.com/yaohungt/GSTEG_CVPR_2019"" target=""_blank"">yaohungt</a>",2025-05-30
SocialIQA: Commonsense Reasoning about Social Interactions,,arXiv,2019,"<a href=""https://arxiv.org/abs/1904.09728"" target=""_blank"">1904.09728</a>",,2025-05-30
A Survey of Reinforcement Learning Informed by Natural Language,,IJCAI,2019,"<a href=""https://arxiv.org/abs/1906.03926"" target=""_blank"">1906.03926</a>",,2025-05-30
Towards Unsupervised Image Captioning with Shared Multimodal Embeddings,,ICCV,2019,"<a href=""https://arxiv.org/abs/1908.09317"" target=""_blank"">1908.09317</a>",,2025-05-30
Episodic Memory in Lifelong Language Learning,,NeurIPS,2019,"<a href=""https://arxiv.org/abs/1906.01076"" target=""_blank"">1906.01076</a>",,2025-05-30
VideoBERT: A Joint Model for Video and Language Representation Learning,,ICCV,2019,"<a href=""https://arxiv.org/abs/1904.01766"" target=""_blank"">1904.01766</a>",,2025-05-30
The Large Scale Movie Description Challenge (LSMDC),,ICCV,2019,"<a href=""https://sites.google.com/site/describingmovies/"" target=""_blank""></a>",,2025-05-30
Temporal Cycle-Consistency Learning,,CVPR,2019,"<a href=""https://arxiv.org/abs/1904.07846"" target=""_blank"">1904.07846</a>","<a href=""https://github.com/google-research/google-research/tree/master/tcc"" target=""_blank"">master</a>",2025-05-30
Sight and Sound,,CVPR,2019,"<a href=""http://sightsound.org/"" target=""_blank""></a>",,2025-05-30
LXMERT: Learning Cross-Modality Encoder Representations from Transformers,,EMNLP,2019,"<a href=""https://arxiv.org/abs/1908.07490"" target=""_blank"">1908.07490</a>","<a href=""https://github.com/airsplay/lxmert"" target=""_blank"">airsplay</a>",2025-05-30
Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training,,arXiv,2019,"<a href=""https://arxiv.org/abs/1908.06066"" target=""_blank"">1908.06066</a>",,2025-05-30
ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks,,NeurIPS,2019,"<a href=""https://arxiv.org/abs/1908.02265"" target=""_blank"">1908.02265</a>","<a href=""https://github.com/jiasenlu/vilbert_beta"" target=""_blank"">jiasenlu</a>",2025-05-30
VisualBERT: A Simple and Performant Baseline for Vision and Language,,arXiv,2019,"<a href=""https://arxiv.org/abs/1908.03557"" target=""_blank"">1908.03557</a>","<a href=""https://github.com/uclanlp/visualbert"" target=""_blank"">uclanlp</a>",2025-05-30
VL-BERT: Pre-training of Generic Visual-Linguistic Representations,,arXiv,2019,"<a href=""https://arxiv.org/abs/1908.08530"" target=""_blank"">1908.08530</a>","<a href=""https://github.com/jackroos/VL-BERT"" target=""_blank"">jackroos</a>",2025-05-30
Model Cards for Model Reporting,,FAccT,2019,"<a href=""https://arxiv.org/abs/1810.03993"" target=""_blank"">1810.03993</a>",,2025-05-30
Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings,,NAACL,2019,"<a href=""https://arxiv.org/abs/1904.04047"" target=""_blank"">1904.04047</a>","<a href=""https://github.com/TManzini/DebiasMulticlassWordEmbedding"" target=""_blank"">TManzini</a>",2025-05-30
Co-Compressing and Unifying Deep CNN Models for Efficient Human Face and Speaker Recognition,,CVPRW,2019,"<a href=""http://openaccess.thecvf.com/content_CVPRW_2019/papers/MULA/Wan_Co-Compressing_and_Unifying_Deep_CNN_Models_for_Efficient_Human_Face_CVPRW_2019_paper.pdf"" target=""_blank"">Wan_Co-Compressi</a>",,2025-05-30
Cross-Modal Learning in Real World,,ICCV,2019,"<a href=""https://cromol.github.io/"" target=""_blank""></a>",,2025-05-30
Spatial Language Understanding and Grounded Communication for Robotics,,NAACL,2019,"<a href=""https://splu-robonlp.github.io/"" target=""_blank""></a>",,2025-05-30
YouTube-8M Large-Scale Video Understanding,,ICCV,2019,"<a href=""https://research.google.com/youtube8m/workshop2018/"" target=""_blank""></a>",,2025-05-30
Language and Vision Workshop,,CVPR,2019,"<a href=""http://languageandvision.com/"" target=""_blank""></a>",,2025-05-30
Latent Variable Model for Multi-modal Translation,,ACL,2019,"<a href=""https://arxiv.org/pdf/1811.00357"" target=""_blank"">1811.00357</a>",,2025-05-30
Multimodal Medical Image Retrieval based on Latent Topic Modeling,,ML4H,2018,"<a href=""https://aiforsocialgood.github.io/2018/pdfs/track1/75_aisg_neurips2018.pdf"" target=""_blank"">75_aisg_neurips2</a>",,2025-05-30
Improving Hospital Mortality Prediction with Medical Named Entities and Multimodal Learning,,ML4H,2018,"<a href=""https://arxiv.org/abs/1811.12276"" target=""_blank"">1811.12276</a>",,2025-05-30
Multi-attention Recurrent Network for Human Communication Comprehension,,AAAI,2018,"<a href=""https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17390/16123"" target=""_blank"">16123</a>","<a href=""https://github.com/A2Zadeh/CMU-MultimodalSDK"" target=""_blank"">A2Zadeh</a>",2025-05-30
Multi-modal Approach for Affective Computing,,EMBC,2018,"<a href=""https://arxiv.org/abs/1804.09452"" target=""_blank"">1804.09452</a>",,2025-05-30
From Audio to Semantics: Approaches To End-to-end Spoken Language Understanding,,arXiv,2018,"<a href=""https://arxiv.org/abs/1809.09190"" target=""_blank"">1809.09190</a>",,2025-05-30
Deep Audio-Visual Speech Recognition,,IEEE TPAMI,2018,"<a href=""https://arxiv.org/abs/1809.02108"" target=""_blank"">1809.02108</a>",,2025-05-30
Learning to Separate Object Sounds by Watching Unlabeled Video,,CVPR,2018,"<a href=""http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w49/Gao_Learning_to_Separate_CVPR_2018_paper.pdf"" target=""_blank"">Gao_Learning_to_</a>",,2025-05-30
Seeing Voices and Hearing Faces: Cross-modal Biometric Matching,,CVPR,2018,"<a href=""https://arxiv.org/abs/1804.00326"" target=""_blank"">1804.00326</a>","<a href=""https://github.com/a-nagrani/SVHF-Net"" target=""_blank"">a-nagrani</a>",2025-05-30
Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input,,ECCV,2018,"<a href=""https://arxiv.org/abs/1804.01452"" target=""_blank"">1804.01452</a>","<a href=""https://github.com/LiqunChen0606/Jointly-Discovering-Visual-Objects-and-Spoken-Words"" target=""_blank"">LiqunChen0606</a>",2025-05-30
Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph,,ACL,2018,"<a href=""http://aclweb.org/anthology/P18-1208"" target=""_blank"">P18-1208</a>","<a href=""https://github.com/A2Zadeh/CMU-MultimodalSDK"" target=""_blank"">A2Zadeh</a>",2025-05-30
"Multimodal Depression Detection: Fusion Analysis of Paralinguistic, Head Pose and Eye Gaze Behaviors",,TAC,2018,"<a href=""https://ieeexplore.ieee.org/document/7763752"" target=""_blank"">7763752</a>",,2025-05-30
Multimodal Language Analysis with Recurrent Multistage Fusion,,EMNLP,2018,"<a href=""https://arxiv.org/abs/1808.03920"" target=""_blank"">1808.03920</a>",,2025-05-30
Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning,,ICLR,2018,"<a href=""https://arxiv.org/abs/1710.07654"" target=""_blank"">1710.07654</a>",,2025-05-30
Dialog-based Interactive Image Retrieval,,NeurIPS,2018,"<a href=""https://arxiv.org/abs/1805.00145"" target=""_blank"">1805.00145</a>","<a href=""https://github.com/XiaoxiaoGuo/fashion-retrieval"" target=""_blank"">XiaoxiaoGuo</a>",2025-05-30
Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented Visual Dialog,,SIGDIAL,2018,"<a href=""https://arxiv.org/abs/1805.03257"" target=""_blank"">1805.03257</a>",,2025-05-30
Talk the Walk: Navigating New York City through Grounded Dialogue,,arXiv,2018,"<a href=""https://arxiv.org/abs/1807.03367"" target=""_blank"">1807.03367</a>",,2025-05-30
Neural Baby Talk,,CVPR,2018,"<a href=""https://arxiv.org/abs/1803.09845"" target=""_blank"">1803.09845</a>","<a href=""https://github.com/jiasenlu/NeuralBabyTalk"" target=""_blank"">jiasenlu</a>",2025-05-30
Grounding Referring Expressions in Images by Variational Context,,CVPR,2018,"<a href=""https://arxiv.org/abs/1712.01892"" target=""_blank"">1712.01892</a>",,2025-05-30
Video Captioning via Hierarchical Reinforcement Learning,,CVPR,2018,"<a href=""https://arxiv.org/abs/1711.11135"" target=""_blank"">1711.11135</a>",,2025-05-30
Charades-Ego: A Large-Scale Dataset of Paired Third and First Person Videos,,CVPR,2018,"<a href=""https://arxiv.org/abs/1804.09626"" target=""_blank"">1804.09626</a>","<a href=""https://allenai.org/plato/charades/"" target=""_blank"">/plato/charades/</a>",2025-05-30
Neural Motifs: Scene Graph Parsing with Global Context,,CVPR,2018,"<a href=""https://arxiv.org/abs/1711.06640"" target=""_blank"">1711.06640</a>","<a href=""http://github.com/rowanz/neural-motifs"" target=""_blank"">rowanz</a>",2025-05-30
No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling,,ACL,2018,"<a href=""https://arxiv.org/abs/1804.09160"" target=""_blank"">1804.09160</a>",,2025-05-30
Unsupervised Multimodal Representation Learning across Medical Images and Reports,,ML4H,2018,"<a href=""https://arxiv.org/abs/1811.08615"" target=""_blank"">1811.08615</a>",,2025-05-30
Knowledge-driven Generative Subspaces for Modeling Multi-view Dependencies in Medical Data,,ML4H,2018,"<a href=""https://arxiv.org/abs/1812.00509"" target=""_blank"">1812.00509</a>",,2025-05-30
ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection,,EMNLP,2018,"<a href=""https://aclanthology.org/D18-1280.pdf"" target=""_blank"">D18-1280.pdf</a>",,2025-05-30
Learning the Joint Representation of Heterogeneous Temporal Events for Clinical Endpoint Prediction,,AAAI,2018,"<a href=""https://arxiv.org/abs/1803.04837"" target=""_blank"">1803.04837</a>",,2025-05-30
"Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models",,CVPR,2018,"<a href=""https://arxiv.org/abs/1711.06420"" target=""_blank"">1711.06420</a>",,2025-05-30
Multimodal Memory Modelling for Video Captioning,,CVPR,2018,"<a href=""https://arxiv.org/abs/1611.05592"" target=""_blank"">1611.05592</a>",,2025-05-30
Datasheets for Datasets,,arXiv,2018,"<a href=""https://arxiv.org/abs/1803.09010"" target=""_blank"">1803.09010</a>",,2025-05-30
Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification,,FAccT,2018,"<a href=""http://proceedings.mlr.press/v81/buolamwini18a.html?mod=article_inline"" target=""_blank"">buolamwini18a.ht</a>",,2025-05-30
Fooling Vision and Language Models Despite Localization and Attention Mechanism,,CVPR,2018,"<a href=""https://arxiv.org/abs/1709.08693"" target=""_blank"">1709.08693</a>",,2025-05-30
Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning,,ACL,2018,"<a href=""https://arxiv.org/abs/1712.02051"" target=""_blank"">1712.02051</a>","<a href=""https://github.com/huanzhang12/ImageCaptioningAttack"" target=""_blank"">huanzhang12</a>",2025-05-30
Attend and Attack: Attention Guided Adversarial Attacks on Visual Question Answering Models,,NeurIPS Workshop on Visually Grounded Interaction and Language,2018,"<a href=""https://nips2018vigil.github.io/static/papers/accepted/33.pdf"" target=""_blank"">33.pdf</a>",,2025-05-30
Learning Multi-Modal Word Representation Grounded in Visual Context,,AAAI,2018,"<a href=""https://arxiv.org/abs/1711.03483"" target=""_blank"">1711.03483</a>",,2025-05-30
Multimodal Generative Models for Scalable Weakly-Supervised Learning,,NeurIPS,2018,"<a href=""https://arxiv.org/abs/1802.05335"" target=""_blank"">1802.05335</a>",,2025-05-30
Do Explanations make VQA Models more Predictable to a Human?,,EMNLP,2018,"<a href=""https://arxiv.org/abs/1810.12366"" target=""_blank"">1810.12366</a>",,2025-05-30
Multimodal Explanations: Justifying Decisions and Pointing to the Evidence,,CVPR,2018,"<a href=""https://arxiv.org/abs/1802.08129"" target=""_blank"">1802.08129</a>","<a href=""https://github.com/Seth-Park/MultimodalExplanations"" target=""_blank"">Seth-Park</a>",2025-05-30
A Multimodal Translation-Based Approach for Knowledge Graph Representation Learning,,SEM,2018,"<a href=""https://www.aclweb.org/anthology/S18-2027"" target=""_blank"">S18-2027</a>","<a href=""https://github.com/UKPLab/starsem18-multimodalKB"" target=""_blank"">UKPLab</a>",2025-05-30
Embedding Multimodal Relational Data for Knowledge Base Completion,,EMNLP,2018,"<a href=""https://arxiv.org/abs/1809.01341"" target=""_blank"">1809.01341</a>",,2025-05-30
Blindfold Baselines for Embodied QA,,NIPS,2018,"<a href=""https://arxiv.org/abs/1811.05013"" target=""_blank"">1811.05013</a>",,2025-05-30
Natural TTS Synthesis by Conditioning Wavenet on Mel Spectrogram Predictions,,ICASSP,2018,"<a href=""https://arxiv.org/abs/1712.05884"" target=""_blank"">1712.05884</a>","<a href=""https://github.com/NVIDIA/tacotron2"" target=""_blank"">NVIDIA</a>",2025-05-30
Learning to Count Objects in Natural Images for Visual Question Answering,,ICLR,2018,"<a href=""https://arxiv.org/abs/1802.05766"" target=""_blank"">1802.05766</a>",,2025-05-30
Wordplay: Reinforcement and Language Learning in Text-based Games,,NeurIPS,2018,"<a href=""https://www.wordplay2018.com/"" target=""_blank""></a>",,2025-05-30
"Interpretability and Robustness in Audio, Speech, and Language",,NeurIPS,2018,"<a href=""https://irasl.gitlab.io/"" target=""_blank""></a>",,2025-05-30
Memory Fusion Network for Multi-view Sequential Learning,,AAAI,2018,"<a href=""https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17341/16122"" target=""_blank"">16122</a>","<a href=""https://github.com/pliang279/MFN"" target=""_blank"">pliang279</a>",2025-05-30
Efficient Low-rank Multimodal Fusion with Modality-Specific Factors,,ACL,2018,"<a href=""https://arxiv.org/abs/1806.00064"" target=""_blank"">1806.00064</a>","<a href=""https://github.com/Justin1904/Low-rank-Multimodal-Fusion"" target=""_blank"">Justin1904</a>",2025-05-30
Unifying and merging well-trained deep neural networks for inference stage,,IJCAI,2018,"<a href=""https://www.ijcai.org/Proceedings/2018/0283.pdf"" target=""_blank"">0283.pdf</a>","<a href=""https://github.com/ivclab/NeuralMerger"" target=""_blank"">ivclab</a>",2025-05-30
Multimodal Robot Perception,,ICRA,2018,"<a href=""https://natanaso.github.io/rcw-icra18/"" target=""_blank""></a>",,2025-05-30
WMT18: Shared Task on Multimodal Machine Translation,,EMNLP,2018,"<a href=""http://www.statmt.org/wmt18/multimodal-task.html"" target=""_blank"">multimodal-task.</a>",,2025-05-30
Shortcomings in Vision and Language,,ECCV,2018,"<a href=""https://sites.google.com/view/sivl/"" target=""_blank""></a>",,2025-05-30
"Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",,EMNLP,2018,"<a href=""https://wt-public.emm4u.eu/wassa2018/"" target=""_blank""></a>",,2025-05-30
Do Neural Network Cross-Modal Mappings Really Bridge Modalities?,,ACL,2018,"<a href=""https://aclweb.org/anthology/P18-2074"" target=""_blank"">P18-2074</a>",,2025-05-30
A Probabilistic Framework for Multi-view Feature Learning with Many-to-many Associations via Neural Networks,,ICML,2018,"<a href=""https://arxiv.org/abs/1802.04630"" target=""_blank"">1802.04630</a>",,2025-05-30
Connecting Language and Vision to Actions,,ACL,2018,"<a href=""https://lvatutorial.github.io/"" target=""_blank""></a>",,2025-05-30
Machine Learning for Clinicians: Advances for Multi-Modal Health Data,,MLHC,2018,"<a href=""https://www.michaelchughes.com/mlhc2018_tutorial.html"" target=""_blank"">mlhc2018_tutoria</a>",,2025-05-30
Multimodal deep learning for short-term stock volatility prediction,,,2018,"<a href=""https://arxiv.org/abs/1812.10479"" target=""_blank"">1812.10479</a>",,2025-05-30
Image Generation from Scene Graphs,,CVPR,2018,"<a href=""https://arxiv.org/abs/1804.01622"" target=""_blank"">1804.01622</a>",,2025-05-30
Overcoming Language Priors in Visual Question Answering with Adversarial Regularization,,NeurIPS,2018,"<a href=""https://arxiv.org/abs/1810.03649"" target=""_blank"">1810.03649</a>",,2025-05-30
Grounding language acquisition by training semantic parsers using captioned videos,,ACL,2018,"<a href=""https://cbmm.mit.edu/sites/default/files/publications/Ross-et-al_ACL2018_Grounding%20language%20acquisition%20by%20training%20semantic%20parsing%20using%20caption%20videos.pdf"" target=""_blank"">Ross-et-al_ACL20</a>",,2025-05-30
Emergence of Grounded Compositional Language in Multi-Agent Populations,,AAAI,2018,"<a href=""https://arxiv.org/abs/1703.04908"" target=""_blank"">1703.04908</a>",,2025-05-30
Emergent Communication through Negotiation,,ICLR,2018,"<a href=""https://openreview.net/pdf?id=Hk6WhagRW"" target=""_blank"">pdf?id=Hk6WhagRW</a>","<a href=""https://github.com/ASAPPinc/emergent_comms_negotiation"" target=""_blank"">ASAPPinc</a>",2025-05-30
Emergence of Linguistic Communication From Referential Games with Symbolic and Pixel Input,,ICLR,2018,"<a href=""https://openreview.net/pdf?id=HJGv1Z-AW"" target=""_blank"">pdf?id=HJGv1Z-AW</a>",,2025-05-30
"Emergent Communication in a Multi-Modal, Multi-Step Referential Game",,ICLR,2018,"<a href=""https://openreview.net/pdf?id=rJGZq6g0-"" target=""_blank"">pdf?id=rJGZq6g0-</a>","<a href=""https://github.com/nyu-dl/MultimodalGame"" target=""_blank"">nyu-dl</a>",2025-05-30
Adversarial Evaluation of Multimodal Machine Translation,,EMNLP,2018,"<a href=""http://aclweb.org/anthology/D18-1329"" target=""_blank"">D18-1329</a>",,2025-05-30
A Visual Attention Grounding Neural Model for Multimodal Machine Translation,,EMNLP,2018,"<a href=""http://aclweb.org/anthology/D18-1400"" target=""_blank"">D18-1400</a>",,2025-05-30
Learning Translations via Images with a Massively Multilingual Image Dataset,,ACL,2018,"<a href=""http://aclweb.org/anthology/P18-1239"" target=""_blank"">P18-1239</a>",,2025-05-30
Zero-Resource Neural Machine Translation with Multi-Agent Communication Game,,AAAI,2018,"<a href=""https://arxiv.org/pdf/1802.03116"" target=""_blank"">1802.03116</a>",,2025-05-30
Emergent Translation in Multi-Agent Communication,,ICLR,2018,"<a href=""https://openreview.net/pdf?id=H1vEXaxA-"" target=""_blank"">pdf?id=H1vEXaxA-</a>",,2025-05-30
Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation,,ECCV,2018,"<a href=""https://arxiv.org/abs/1803.07729"" target=""_blank"">1803.07729</a>",,2025-05-30
Embodied Question Answering,,CVPR,2018,"<a href=""https://arxiv.org/abs/1711.11543"" target=""_blank"">1711.11543</a>","<a href=""https://embodiedqa.org/"" target=""_blank"">/embodiedqa.org/</a>",2025-05-30
Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding,,NeurIPS,2018,"<a href=""https://arxiv.org/abs/1810.02338"" target=""_blank"">1810.02338</a>","<a href=""https://github.com/kexinyi/ns-vqa"" target=""_blank"">kexinyi</a>",2025-05-30
Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction,,EMNLP,2018,"<a href=""https://arxiv.org/abs/1809.00786"" target=""_blank"">1809.00786</a>","<a href=""https://github.com/lil-lab/ciff"" target=""_blank"">lil-lab</a>",2025-05-30
Multi-modal Predicate Identification using Dynamically Learned Robot Controllers,,IJCAI,2018,"<a href=""https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/IJCAI18-saeid.pdf"" target=""_blank"">IJCAI18-saeid.pd</a>",,2025-05-30
Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments,,CVPR,2018,"<a href=""https://arxiv.org/abs/1711.07280"" target=""_blank"">1711.07280</a>","<a href=""https://bringmeaspoon.org/"" target=""_blank"">ingmeaspoon.org/</a>",2025-05-30
Using Syntax to Ground Referring Expressions in Natural Images,,AAAI,2018,"<a href=""https://arxiv.org/abs/1805.10547"" target=""_blank"">1805.10547</a>","<a href=""https://github.com/volkancirik/groundnet"" target=""_blank"">volkancirik</a>",2025-05-30
Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering,,CVPR,2018,"<a href=""https://arxiv.org/abs/1712.00377"" target=""_blank"">1712.00377</a>","<a href=""https://github.com/AishwaryaAgrawal/GVQA"" target=""_blank"">AishwaryaAgrawal</a>",2025-05-30
RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes,,EMNLP,2018,"<a href=""https://arxiv.org/abs/1809.00812"" target=""_blank"">1809.00812</a>","<a href=""https://hucvl.github.io/recipeqa/"" target=""_blank"">recipeqa</a>",2025-05-30
Gated-Attention Architectures for Task-Oriented Language Grounding,,AAAI,2018,"<a href=""https://arxiv.org/abs/1706.07230"" target=""_blank"">1706.07230</a>","<a href=""https://github.com/devendrachaplot/DeepRL-Grounding"" target=""_blank"">devendrachaplot</a>",2025-05-30
"TVQA: Localized, Compositional Video Question Answering",,EMNLP,2018,"<a href=""https://www.aclweb.org/anthology/D18-1167"" target=""_blank"">D18-1167</a>","<a href=""https://github.com/jayleicn/TVQA"" target=""_blank"">jayleicn</a>",2025-05-30
Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering,,CVPR,2018,"<a href=""https://arxiv.org/abs/1707.07998"" target=""_blank"">1707.07998</a>","<a href=""https://github.com/facebookresearch/pythia"" target=""_blank"">facebookresearch</a>",2025-05-30
Learning to Color from Language,,NAACL,2018,"<a href=""https://arxiv.org/abs/1804.06026"" target=""_blank"">1804.06026</a>",,2025-05-30
Stacked Latent Attention for Multimodal Reasoning,,CVPR,2018,"<a href=""http://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_Stacked_Latent_Attention_CVPR_2018_paper.pdf"" target=""_blank"">Fan_Stacked_Late</a>",,2025-05-30
Finding “It”: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos,,CVPR,2018,"<a href=""http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Finding_It_Weakly-Supervised_CVPR_2018_paper.pdf"" target=""_blank"">Huang_Finding_It</a>",,2025-05-30
SCAN: Learning Hierarchical Compositional Visual Concepts,,ICLR,2018,"<a href=""https://arxiv.org/abs/1707.03389"" target=""_blank"">1707.03389</a>",,2025-05-30
Visual Coreference Resolution in Visual Dialog using Neural Module Networks,,ECCV,2018,"<a href=""https://arxiv.org/abs/1809.01816"" target=""_blank"">1809.01816</a>","<a href=""https://github.com/facebookresearch/corefnmn"" target=""_blank"">facebookresearch</a>",2025-05-30
Multimodal Probabilistic Model-Based Planning for Human-Robot Interaction,,arXiv,2017,"<a href=""https://arxiv.org/abs/1710.09483"" target=""_blank"">1710.09483</a>",,2025-05-30
End-to-End Multimodal Emotion Recognition using Deep Neural Networks,,arXiv,2017,"<a href=""https://arxiv.org/abs/1704.08619"" target=""_blank"">1704.08619</a>",,2025-05-30
Visual Understanding Across Modalities,,CVPR,2017,"<a href=""http://vuchallenge.org/"" target=""_blank""></a>",,2025-05-30
International Workshop on Computer Vision for Audio-Visual Media,,ICCV,2017,"<a href=""https://cvavm2017.wordpress.com/"" target=""_blank""></a>",,2025-05-30
Language Grounding for Robotics,,ACL,2017,"<a href=""https://robo-nlp.github.io/2017_index.html"" target=""_blank"">2017_index.html</a>",,2025-05-30
AMHUSE - A Multimodal dataset for HUmor SEnsing,,ICMI,2017,"<a href=""https://dl.acm.org/citation.cfm?id=3136806"" target=""_blank"">citation.cfm?id=</a>","<a href=""http://amhuse.phuselab.di.unimi.it/"" target=""_blank"">lab.di.unimi.it/</a>",2025-05-30
Understanding Coagulopathy using Multi-view Data in the Presence of Sub-Cohorts: A Hierarchical Subspace Approach,,MLHC,2017,"<a href=""http://mucmd.org/CameraReadySubmissions/67%5CCameraReadySubmission%5Cunderstanding-coagulopathy-multi%20(6).pdf"" target=""_blank"">67%5CCameraReady</a>",,2025-05-30
Machine Learning in Multimodal Medical Imaging,,,2017,"<a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5357511/"" target=""_blank""></a>",,2025-05-30
Multimodal Machine Learning,,ACL,2017,"<a href=""https://sites.google.com/site/multiml2016cvpr/"" target=""_blank""></a>",,2025-05-30
Cross-modal Recurrent Models for Weight Objective Prediction from Multimodal Time-series Data,,ML4H,2017,"<a href=""https://arxiv.org/abs/1709.08073"" target=""_blank"">1709.08073</a>",,2025-05-30
Vision and Language: Bridging Vision and Language with Deep Learning,,ICIP,2017,"<a href=""https://www.microsoft.com/en-us/research/publication/vision-language-bridging-vision-language-deep-learning/"" target=""_blank""></a>",,2025-05-30
Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog,,EMNLP,2017,"<a href=""https://arxiv.org/abs/1706.08502"" target=""_blank"">1706.08502</a>",,2025-05-30
The Multi-Entity Variational Autoencoder,,NeurIPS,2017,"<a href=""http://charlienash.github.io/assets/docs/mevae2017.pdf"" target=""_blank"">mevae2017.pdf</a>",,2025-05-30
Deep Voice 2: Multi-Speaker Neural Text-to-Speech,,NeurIPS,2017,"<a href=""https://arxiv.org/abs/1705.08947"" target=""_blank"">1705.08947</a>",,2025-05-30
Visual Dialog,,CVPR,2017,"<a href=""https://arxiv.org/abs/1611.08669"" target=""_blank"">1611.08669</a>","<a href=""https://github.com/batra-mlp-lab/visdial"" target=""_blank"">batra-mlp-lab</a>",2025-05-30
Towards Building Large Scale Multimodal Domain-Aware Conversation Systems,,arXiv,2017,"<a href=""https://arxiv.org/abs/1704.00200"" target=""_blank"">1704.00200</a>","<a href=""https://amritasaha1812.github.io/MMD/"" target=""_blank"">MMD</a>",2025-05-30
Semi-supervised Vision-language Mapping via Variational Learning,,ICRA,2017,"<a href=""https://ieeexplore.ieee.org/document/7989160"" target=""_blank"">7989160</a>",,2025-05-30
Mapping Instructions and Visual Observations to Actions with Reinforcement Learning,,EMNLP,2017,"<a href=""https://www.cs.cornell.edu/~dkm/papers/mla-emnlp.2017.pdf"" target=""_blank"">mla-emnlp.2017.p</a>",,2025-05-30
"See, Hear, and Read: Deep Aligned Representations",,arXiv,2017,"<a href=""https://people.csail.mit.edu/yusuf/see-hear-read/paper.pdf"" target=""_blank"">paper.pdf</a>",,2025-05-30
Multi-agent Cooperation and the Emergence of (natural) Language,,ICLR,2017,"<a href=""https://arxiv.org/abs/1612.07182"" target=""_blank"">1612.07182</a>",,2025-05-30
Incorporating Global Visual Features into Attention-based Neural Machine Translation,,EMNLP,2017,"<a href=""http://aclweb.org/anthology/D17-1105"" target=""_blank"">D17-1105</a>","<a href=""https://github.com/iacercalixto/MultimodalNMT"" target=""_blank"">iacercalixto</a>",2025-05-30
Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning,,ICCV,2017,"<a href=""https://arxiv.org/abs/1703.06585"" target=""_blank"">1703.06585</a>",,2025-05-30
"Zero-Shot Learning - The Good, the Bad and the Ugly",,CVPR,2017,"<a href=""https://arxiv.org/abs/1703.04394"" target=""_blank"">1703.04394</a>",,2025-05-30
Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols,,NeurIPS,2017,"<a href=""https://arxiv.org/abs/1705.11192"" target=""_blank"">1705.11192</a>",,2025-05-30
Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts,,NeurIPS,2017,"<a href=""https://arxiv.org/abs/1803.11209"" target=""_blank"">1803.11209</a>",,2025-05-30
Localizing Moments in Video with Natural Language,,ICCV,2017,"<a href=""https://arxiv.org/abs/1708.01641"" target=""_blank"">1708.01641</a>",,2025-05-30
Doubly-Attentive Decoder for Multi-modal Neural Machine Translation,,ACL,2017,"<a href=""http://aclweb.org/anthology/P17-1175"" target=""_blank"">P17-1175</a>","<a href=""https://github.com/iacercalixto/MultimodalNMT"" target=""_blank"">iacercalixto</a>",2025-05-30
Tensor Fusion Network for Multimodal Sentiment Analysis,,EMNLP,2017,"<a href=""https://arxiv.org/abs/1707.07250"" target=""_blank"">1707.07250</a>","<a href=""https://github.com/A2Zadeh/TensorFusionNetwork"" target=""_blank"">A2Zadeh</a>",2025-05-30
Semi-supervised Multimodal Hashing,,arXiv,2017,"<a href=""https://arxiv.org/abs/1712.03404"" target=""_blank"">1712.03404</a>",,2025-05-30
An empirical study on the effectiveness of images in Multimodal Neural Machine Translation,,EMNLP,2017,"<a href=""http://aclweb.org/anthology/D17-1095"" target=""_blank"">D17-1095</a>",,2025-05-30
Human-In-The-Loop Machine Learning with Intelligent Multimodal Interfaces,,ICML,2017,"<a href=""https://csjzhou.github.io/homepage/papers/ICML2017_Syed.pdf"" target=""_blank"">ICML2017_Syed.pd</a>",,2025-05-30
Guest Editorial: Image and Language Understanding,,IJCV,2017,"<a href=""https://link.springer.com/article/10.1007/s11263-017-0993-y"" target=""_blank"">s11263-017-0993-</a>",,2025-05-30
"Look, Listen and Learn",,ICCV,2017,"<a href=""http://openaccess.thecvf.com/content_ICCV_2017/papers/Arandjelovic_Look_Listen_and_ICCV_2017_paper.pdf"" target=""_blank"">Arandjelovic_Loo</a>",,2025-05-30
Are You Smarter Than A Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension,,CVPR,2017,"<a href=""https://ieeexplore.ieee.org/document/8100054/"" target=""_blank""></a>","<a href=""http://vuchallenge.org/tqa.html"" target=""_blank"">nge.org/tqa.html</a>",2025-05-30
CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning,,CVPR,2017,"<a href=""https://arxiv.org/abs/1612.06890"" target=""_blank"">1612.06890</a>","<a href=""https://github.com/facebookresearch/clevr-iep"" target=""_blank"">facebookresearch</a>",2025-05-30
Generating Descriptions with Grounded and Co-Referenced People,,CVPR,2017,"<a href=""https://arxiv.org/abs/1704.01518"" target=""_blank"">1704.01518</a>",,2025-05-30
Learning to Reason: End-to-End Module Networks for Visual Question Answering,,ICCV,2017,"<a href=""https://arxiv.org/abs/1704.05526"" target=""_blank"">1704.05526</a>","<a href=""https://github.com/ronghanghu/n2nmn"" target=""_blank"">ronghanghu</a>",2025-05-30
Deep Voice: Real-time Neural Text-to-Speech,,ICML,2017,"<a href=""https://arxiv.org/abs/1702.07825"" target=""_blank"">1702.07825</a>",,2025-05-30
Self-Supervised Learning of Visual Features through Embedding Images into Text Topic Spaces,,CVPR,2017,"<a href=""https://ieeexplore.ieee.org/document/8099701"" target=""_blank"">8099701</a>",,2025-05-30
Learning Robust Visual-Semantic Embeddings,,ICCV,2017,"<a href=""https://arxiv.org/abs/1703.05908"" target=""_blank"">1703.05908</a>",,2025-05-30
Deep Multimodal Representation Learning from Temporal Data,,CVPR,2017,"<a href=""https://arxiv.org/abs/1704.03152"" target=""_blank"">1704.03152</a>",,2025-05-30
Order-Embeddings of Images and Language,,ICLR,2016,"<a href=""https://arxiv.org/abs/1511.06361"" target=""_blank"">1511.06361</a>","<a href=""https://github.com/ivendrov/order-embedding"" target=""_blank"">ivendrov</a>",2025-05-30
Semi-Supervised Multimodal Deep Learning for RGB-D Object Recognition,,IJCAI,2016,"<a href=""https://www.ijcai.org/Proceedings/16/Papers/473.pdf"" target=""_blank"">473.pdf</a>",,2025-05-30
Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes,,CVPR,2016,"<a href=""https://arxiv.org/abs/1511.07067"" target=""_blank"">1511.07067</a>",,2025-05-30
Multimodal Dynamics : Self-supervised Learning in Perceptual and Motor Systems,,,2016,"<a href=""https://dl.acm.org/citation.cfm?id=1269207"" target=""_blank"">citation.cfm?id=</a>",,2025-05-30
Dynamic Memory Networks for Visual and Textual Question Answering,,ICML,2016,"<a href=""https://arxiv.org/abs/1603.01417"" target=""_blank"">1603.01417</a>",,2025-05-30
Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding,,EMNLP,2016,"<a href=""https://arxiv.org/abs/1606.01847"" target=""_blank"">1606.01847</a>","<a href=""https://github.com/akirafukui/vqa-mcb"" target=""_blank"">akirafukui</a>",2025-05-30
Towards Transparent AI Systems: Interpreting Visual Question Answering Models,,ICML Workshop on Visualization for Deep Learning,2016,"<a href=""https://arxiv.org/abs/1608.08974"" target=""_blank"">1608.08974</a>",,2025-05-30
MovieQA: Understanding Stories in Movies through Question-Answering,,CVPR,2016,"<a href=""https://arxiv.org/abs/1512.02902"" target=""_blank"">1512.02902</a>","<a href=""http://movieqa.cs.toronto.edu/home/"" target=""_blank"">oronto.edu/home/</a>",2025-05-30
Multimodal Pivots for Image Caption Translation,,ACL,2016,"<a href=""http://aclweb.org/anthology/P16-1227"" target=""_blank"">P16-1227</a>",,2025-05-30
Multi30K: Multilingual English-German Image Descriptions,,ACL Workshop on Language and Vision,2016,"<a href=""https://aclweb.org/anthology/W16-3210.pdf"" target=""_blank"">W16-3210.pdf</a>","<a href=""https://github.com/multi30k/dataset"" target=""_blank"">multi30k</a>",2025-05-30
Language and Vision,,ACL,2016,"<a href=""https://vision.cs.hacettepe.edu.tr/vl2016/"" target=""_blank""></a>",,2025-05-30
Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge,,TPAMI,2016,"<a href=""https://arxiv.org/abs/1609.06647"" target=""_blank"">1609.06647</a>","<a href=""https://github.com/tensorflow/models/tree/master/research/im2txt"" target=""_blank"">research</a>",2025-05-30
Generative Adversarial Text to Image Synthesis,,ICML,2016,"<a href=""https://arxiv.org/abs/1605.05396"" target=""_blank"">1605.05396</a>",,2025-05-30
Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding,,ECCV,2016,"<a href=""https://arxiv.org/abs/1604.01753"" target=""_blank"">1604.01753</a>","<a href=""https://allenai.org/plato/charades/"" target=""_blank"">/plato/charades/</a>",2025-05-30
Review Networks for Caption Generation,,NeurIPS,2016,"<a href=""https://arxiv.org/abs/1605.07912"" target=""_blank"">1605.07912</a>","<a href=""https://github.com/kimiyoung/review_net"" target=""_blank"">kimiyoung</a>",2025-05-30
DenseCap: Fully Convolutional Localization Networks for Dense Captioning,,CVPR,2016,"<a href=""https://cs.stanford.edu/people/karpathy/densecap/"" target=""_blank""></a>",,2025-05-30
SoundNet: Learning Sound Representations from Unlabeled Video,,NeurIPS,2016,"<a href=""https://arxiv.org/abs/1610.09001"" target=""_blank"">1610.09001</a>","<a href=""http://projects.csail.mit.edu/soundnet/"" target=""_blank"">it.edu/soundnet/</a>",2025-05-30
Unsupervised Learning of Spoken Language with Visual Context,,NeurIPS,2016,"<a href=""https://papers.nips.cc/paper/6186-unsupervised-learning-of-spoken-language-with-visual-context.pdf"" target=""_blank"">6186-unsupervise</a>",,2025-05-30
Computer Vision for Audio-visual Media,,ECCV,2016,"<a href=""https://cvavm2016.wordpress.com/"" target=""_blank""></a>",,2025-05-30
Is an Image Worth More than a Thousand Words? On the Fine-Grain Semantic Differences between Visual and Linguistic Representations,,COLING,2016,"<a href=""https://www.aclweb.org/anthology/C16-1264"" target=""_blank"">C16-1264</a>",,2025-05-30
Learning multiagent communication with backpropagation,,NIPS,2016,"<a href=""http://papers.nips.cc/paper/6398-learning-multiagent-communication-with-backpropagation.pdf"" target=""_blank"">6398-learning-mu</a>",,2025-05-30
Learning to Communicate with Deep Multi-agent Reinforcement Learning,,NIPS,2016,"<a href=""https://arxiv.org/abs/1605.06676"" target=""_blank"">1605.06676</a>",,2025-05-30
Does Multimodality Help Human and Machine for Translation and Image Captioning?,,ACL WMT,2016,"<a href=""http://www.statmt.org/wmt16/pdf/W16-2358.pdf"" target=""_blank"">W16-2358.pdf</a>",,2025-05-30
Analyzing the Behavior of Visual Question Answering Models,,EMNLP,2016,"<a href=""https://arxiv.org/abs/1606.07356"" target=""_blank"">1606.07356</a>",,2025-05-30
Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings,,NeurIPS,2016,"<a href=""https://arxiv.org/abs/1607.06520"" target=""_blank"">1607.06520</a>",,2025-05-30
Combining Language and Vision with a Multimodal Skip-gram Model,,NAACL,2015,"<a href=""https://www.aclweb.org/anthology/N15-1016"" target=""_blank"">N15-1016</a>",,2025-05-30
Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework,,AAAI,2015,"<a href=""http://web.eecs.umich.edu/~jjcorso/pubs/xu_corso_AAAI2015_v2t.pdf"" target=""_blank"">xu_corso_AAAI201</a>",,2025-05-30
On Deep Multi-View Representation Learning,,ICML,2015,"<a href=""http://proceedings.mlr.press/v37/wangb15.pdf"" target=""_blank"">wangb15.pdf</a>",,2025-05-30
Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries,,arXiv,2015,"<a href=""https://arxiv.org/abs/1507.05670"" target=""_blank"">1507.05670</a>",,2025-05-30
Multimodal Deep Learning for Robust RGB-D Object Recognition,,IROS,2015,"<a href=""https://arxiv.org/abs/1507.06821"" target=""_blank"">1507.06821</a>",,2025-05-30
Deep Visual-Semantic Alignments for Generating Image Descriptions,,CVPR,2015,"<a href=""https://arxiv.org/abs/1412.2306v2"" target=""_blank"">1412.2306v2</a>","<a href=""https://github.com/karpathy/neuraltalk2"" target=""_blank"">karpathy</a>",2025-05-30
A Dataset for Movie Description,,CVPR,2015,"<a href=""https://arxiv.org/abs/1501.02530"" target=""_blank"">1501.02530</a>","<a href=""https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/mpii-movie-description-dataset/"" target=""_blank"">ription-dataset/</a>",2025-05-30
"What’s Cookin’? Interpreting Cooking Videos using Text, Speech and Vision",,NAACL,2015,"<a href=""https://arxiv.org/abs/1503.01558"" target=""_blank"">1503.01558</a>","<a href=""https://github.com/malmaud/whats_cookin"" target=""_blank"">malmaud</a>",2025-05-30
Show and Tell: A Neural Image Caption Generator,,CVPR,2015,"<a href=""https://arxiv.org/abs/1411.4555"" target=""_blank"">1411.4555</a>","<a href=""https://github.com/karpathy/neuraltalk2"" target=""_blank"">karpathy</a>",2025-05-30
"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",,ICML,2015,"<a href=""https://arxiv.org/abs/1502.03044"" target=""_blank"">1502.03044</a>","<a href=""https://github.com/kelvinxu/arctic-captions"" target=""_blank"">kelvinxu</a>",2025-05-30
VQA: Visual Question Answering,,ICCV,2015,"<a href=""https://arxiv.org/abs/1505.00468"" target=""_blank"">1505.00468</a>","<a href=""https://visualqa.org/"" target=""_blank"">://visualqa.org/</a>",2025-05-30
What are you talking about? Text-to-Image Coreference,,CVPR,2014,"<a href=""https://ieeexplore.ieee.org/abstract/document/6909850/"" target=""_blank""></a>",,2025-05-30
Multimodal Learning with Deep Boltzmann Machines,,JMLR,2014,"<a href=""https://dl.acm.org/citation.cfm?id=2697059"" target=""_blank"">citation.cfm?id=</a>",,2025-05-30
Unsupervised Alignment of Natural Language Instructions with Video Segments,,AAAI,2014,"<a href=""https://dl.acm.org/citation.cfm?id=2892753.2892769"" target=""_blank"">citation.cfm?id=</a>",,2025-05-30
Multimodal Alignment of Videos,,MM,2014,"<a href=""https://dl.acm.org/citation.cfm?id=2654862"" target=""_blank"">citation.cfm?id=</a>",,2025-05-30
Dyadic Behavior Analysis in Depression Severity Assessment Interviews,,ICMI,2014,"<a href=""https://dl.acm.org/citation.cfm?doid=2663204.2663238"" target=""_blank"">citation.cfm?doi</a>",,2025-05-30
Learning Grounded Meaning Representations with Autoencoders,,ACL,2014,"<a href=""https://www.aclweb.org/anthology/P14-1068"" target=""_blank"">P14-1068</a>",,2025-05-30
Perching and Vertical Climbing: Design of a Multimodal Robot,,ICRA,2014,"<a href=""https://ieeexplore.ieee.org/document/6907472"" target=""_blank"">6907472</a>",,2025-05-30
Deep Fragment Embeddings for Bidirectional Image Sentence Mapping,,NIPS,2014,"<a href=""https://arxiv.org/abs/1406.5679"" target=""_blank"">1406.5679</a>",,2025-05-30
Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models,,ICML,2014,"<a href=""http://proceedings.mlr.press/v32/kiros14.html"" target=""_blank"">kiros14.html</a>","<a href=""https://github.com/ryankiros/visual-semantic-embedding"" target=""_blank"">ryankiros</a>",2025-05-30
Microsoft COCO: Common Objects in Context,,ECCV,2014,"<a href=""https://arxiv.org/abs/1405.0312"" target=""_blank"">1405.0312</a>","<a href=""http://cocodataset.org/#home"" target=""_blank"">ataset.org/#home</a>",2025-05-30
SimSensei Kiosk: A Virtual Human Interviewer for Healthcare Decision Support,,AAMAS,2014,"<a href=""https://dl.acm.org/citation.cfm?id=2617388.2617415"" target=""_blank"">citation.cfm?id=</a>",,2025-05-30
Decoding Children’s Social Behavior,,CVPR,2013,"<a href=""http://www.cbi.gatech.edu/mmdb/docs/mmdb_paper.pdf"" target=""_blank"">mmdb_paper.pdf</a>","<a href=""http://www.cbi.gatech.edu/mmdb/"" target=""_blank"">gatech.edu/mmdb/</a>",2025-05-30
DeViSE: A Deep Visual-Semantic Embedding Model,,NeurIPS,2013,"<a href=""https://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model"" target=""_blank"">5204-devise-a-de</a>",,2025-05-30
Representation Learning: A Review and New Perspectives,,TPAMI,2013,"<a href=""https://arxiv.org/abs/1206.5538"" target=""_blank"">1206.5538</a>",,2025-05-30
Zero-Shot Learning Through Cross-Modal Transfer,,NIPS,2013,"<a href=""https://nlp.stanford.edu/~socherr/SocherGanjooManningNg_NIPS2013.pdf"" target=""_blank"">SocherGanjooMann</a>",,2025-05-30
Deep Canonical Correlation Analysis,,ICML,2013,"<a href=""http://proceedings.mlr.press/v28/andrew13.html"" target=""_blank"">andrew13.html</a>","<a href=""https://github.com/VahidooX/DeepCCA"" target=""_blank"">VahidooX</a>",2025-05-30
Audiovisual Behavior Descriptors for Depression Assessment,,ICMI,2013,"<a href=""https://dl.acm.org/citation.cfm?doid=2522848.2522886"" target=""_blank"">citation.cfm?doi</a>",,2025-05-30
Grounded Compositional Semantics for Finding and Describing Images with Sentences,,TACL,2013,"<a href=""https://nlp.stanford.edu/~socherr/SocherKarpathyLeManningNg_TACL2013.pdf"" target=""_blank"">SocherKarpathyLe</a>",,2025-05-30
Grounded Language Learning from Video Described with Sentences,,ACL,2013,"<a href=""https://www.aclweb.org/anthology/P13-1006"" target=""_blank"">P13-1006</a>",,2025-05-30
"Collecting Large, Richly Annotated Facial-Expression Databases from Movies",,IEEE Multimedia,2012,"<a href=""http://users.cecs.anu.edu.au/%7Eadhall/Dhall_Goecke_Lucey_Gedeon_M_2012.pdf"" target=""_blank"">Dhall_Goecke_Luc</a>","<a href=""https://cs.anu.edu.au/few/AFEW.html"" target=""_blank"">au/few/AFEW.html</a>",2025-05-30
Multi-Modal Scene Understanding for Robotic Grasping,,,2011,"<a href=""http://kth.diva-portal.org/smash/get/diva2:459199/FULLTEXT01"" target=""_blank"">FULLTEXT01</a>",,2025-05-30
Multimodal Deep Learning,,ICML,2011,"<a href=""https://dl.acm.org/citation.cfm?id=3104569"" target=""_blank"">citation.cfm?id=</a>",,2025-05-30
Strategies for Multi-Modal Scene Exploration,,IROS,2010,"<a href=""https://am.is.tuebingen.mpg.de/uploads_file/attachment/attachment/307/2010_IROS_bjbk_camred.pdf"" target=""_blank"">2010_IROS_bjbk_c</a>",,2025-05-30
Multimodal Semi-supervised Learning for Image Classification,,CVPR,2010,"<a href=""https://ieeexplore.ieee.org/abstract/document/5540120"" target=""_blank"">5540120</a>",,2025-05-30
Reinforcement Learning for Mapping Instructions to Actions,,ACL,2009,"<a href=""https://people.csail.mit.edu/regina/my_papers/RL.pdf"" target=""_blank"">RL.pdf</a>",,2025-05-30
Text-to-Speech Synthesis,,,2009,"<a href=""https://dl.acm.org/citation.cfm?id=1592988"" target=""_blank"">citation.cfm?id=</a>",,2025-05-30
The Interactive Emotional Dyadic Motion Capture (IEMOCAP) Database,,,2008,"<a href=""https://sail.usc.edu/iemocap/Busso_2008_iemocap.pdf"" target=""_blank"">Busso_2008_iemoc</a>","<a href=""https://sail.usc.edu/iemocap/"" target=""_blank"">usc.edu/iemocap/</a>",2025-05-30
Affective multimodal human-computer interaction,,Multimedia,2005,"<a href=""https://dl.acm.org/doi/10.1145/1101149.1101299"" target=""_blank"">1101149.1101299</a>",,2025-05-30
A co-regularized approach to semi-supervised learning with multiple views,,ICML,2005,"<a href=""https://web.cse.ohio-state.edu/~belkin.8/papers/CASSL_ICML_05.pdf"" target=""_blank"">CASSL_ICML_05.pd</a>",,2025-05-30
Multimodal Human Computer Interaction: A Survey,,HCI,2005,"<a href=""https://link.springer.com/chapter/10.1007/11573425_1"" target=""_blank"">11573425_1</a>",,2025-05-30
The Emergence of Compositional Structures in Perceptually Grounded Language Games,,AI,2005,"<a href=""https://www.cs.utexas.edu/~kuipers/readings/Vogt-aij-05.pdf"" target=""_blank"">Vogt-aij-05.pdf</a>",,2025-05-30
A Survey of Socially Interactive Robots,,,2003,"<a href=""https://www.cs.cmu.edu/~illah/PAPERS/socialroboticssurvey.pdf"" target=""_blank"">socialroboticssu</a>",,2025-05-30
Building a multimodal human-robot interface,,IEEE Intelligent Systems,2001,"<a href=""https://ieeexplore.ieee.org/abstract/document/1183338?casa_token=tdKeY0Q0e-4AAAAA:XfKwp5Di1O5bCEOnebeaS58waSbWm80lxNuY8IhWW7DqDLvRQj-8ettJW1NrFrmoR_ShudTgzw"" target=""_blank"">1183338?casa_tok</a>",,2025-05-30
